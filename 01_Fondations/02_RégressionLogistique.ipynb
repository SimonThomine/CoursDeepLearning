{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le neurone artificiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps de passer aux choses sérieuses et de présenter le neurone articifiel. \n",
    "\n",
    "La figure ci-dessous montre le fonctionnement d'un neurone artificiel : \n",
    "\n",
    "<img src=\"images/NeuroneArtificiel.png\" alt=\"Neurone artificiel\" width=\"500\"/>  \n",
    "\n",
    "Le neurone artificiel prend en entrée un vecteur $\\mathbf{x}=(x_1,x_2,...,x_n)$, chaque élément $x_i$ du vecteur $\\mathbf{x}$ est ensuite multiplié par un poids $w_i$ puis on somme l'ensemble et on ajoute un biais $b$. Cette somme est ensuite passé dans une fonction qu'on appelle **fonction d'activation** $\\phi$.  \n",
    "$Sortie = \\phi(\\sum_{i=0}^{n} w_i x_i  + b)$  \n",
    "On appelle ce procédé neurone artificiel par analogie avec le fonctionnement d'un [neurone biologique](https://en.wikipedia.org/wiki/Artificial_neuron).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les fonctions d'activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fonction de Heaviside :** A l'origine, le premier neurone articiel ([le perceptron](https://fr.wikipedia.org/wiki/Perceptron)) utilisait une fonction de seuillage comme fonction d'activation. Cela permet de prendre une décision (0 ou 1) par rapport à la somme pondérée et un seuil défini.  \n",
    "$heaviside(x) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1 & \\text{si } x > 0 \\\\\n",
    "        0 & \\text{sinon}\n",
    "    \\end{array}\n",
    "\\right. \\text{avec } x=\\sum_{i=0}^{n} w_i x_i  + b$  \n",
    "Cette fonction d'activation est efficace pour obtenir une classification binaire mais elle ne fonctionne pas pour plusieurs classes. De plus, la fonction n'est pas dérivable, il est donc compliqué d'utiliser l'algorithme de la descente du gradient pour optimiser les poids $w_i$ du neurone.  \n",
    "\n",
    "Les fonctions d'activation récente sont beaucoup plus intéressante pour l'entraînement de réseaux de neurones par descente du gradient. Premièrement, on choisit des fonctions dérivables ce qui permet d'appliquer notre algorithme de descente du gradient. Deuxièmement, on choisit des fonctions non-linéaire ce qui permet aux réseaux d'apprendre des représentations complexes. Il y également d'autres avantages spécifiques à chaque fonction d'activation.   \n",
    "\n",
    "Une des fonctions d'activation \"récente\" est la fonction sigmoïde que nous détaillons ici : \n",
    "\n",
    "**Fonction sigmoïde :** Une autre fonction d'activation particulièrement intéressante par son analogie avec une probabilité est la fonction sigmoïde. Cette fois-ci, la fonction va permettre d'obtenir une valeur entre 0 et 1 par la formule :  \n",
    "$sigmoid(x) = \\frac{1}{1 + e^{-x}} \\text{ avec } x=\\sum_{i=0}^{n} w_i x_i  + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tracé de la fonction sigmoïde\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('Fonction sigmoïde')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction est dérivable et sa dérivée vaut :  \n",
    "$sigmoid'(x) = sigmoid(x) \\cdot (1 - f(x))$  \n",
    "On peut donc appliquer la descente du gradient sur notre neurone artificiel lorsqu'on utilise cette fonction d'activation.\n",
    "\n",
    "Il existe de nombreuses autres [fonctions d'activations](https://fr.wikipedia.org/wiki/Fonction_d%27activation) qui ont chacune leurs utilités que nous verrons dans les cours suivants. ($Tanh$,$ReLU$,$Softmax$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour comprendre la régression logistique, le mieux est de passer par un example d'application.  \n",
    "Dans cet exemple, nous allons déterminer si un étudiant va être admis dans l'université de ses rêves à partir de trois informations : son score à l'examen d'entrée, ses notes moyennes de l'année précédente et la qualité de sa lettre de motivation.  \n",
    "Nous n'avons pas accès à la méthode de calcul pour l'admission ou non d'un étudiant mais nous disposons de données et de la décision correspondantes.  \n",
    "Les informations d'entrée sont entre 0 et 1, avec 1 indiquant le meilleur score. Admis = 1 correspond à une admission tandis qu'admis = 0 correspond à un refus.   \n",
    "Nous disposons des informations suivantes :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═══════════╤══════════════╤═════════╕\n",
      "│   Examen │   Moyenne │   Motivation │   Admis │\n",
      "╞══════════╪═══════════╪══════════════╪═════════╡\n",
      "│      0.7 │       0.8 │          0.1 │       1 │\n",
      "├──────────┼───────────┼──────────────┼─────────┤\n",
      "│      0.4 │       0.9 │          0.5 │       0 │\n",
      "├──────────┼───────────┼──────────────┼─────────┤\n",
      "│      0.2 │       0.3 │          0.9 │       0 │\n",
      "├──────────┼───────────┼──────────────┼─────────┤\n",
      "│      0.9 │       0.9 │          0.6 │       1 │\n",
      "╘══════════╧═══════════╧══════════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Définition des données d'entraînement\n",
    "values_train = [[0.7, 0.8, 0.1], [0.4, 0.9, 0.5], [0.2, 0.3, 0.9], [0.9, 0.9, 0.6]]\n",
    "labels_train = [1, 0, 0, 1]\n",
    "\n",
    "# Ajout des noms de colonnes\n",
    "data = [['Examen', 'Moyenne', 'Motivation', 'Admis']]\n",
    "data.extend([[values_train[i][0], values_train[i][1], values_train[i][2], labels_train[i]] for i in range(len(values_train))])\n",
    "\n",
    "# Affichage du tableau\n",
    "print(tabulate(data, headers=\"firstrow\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre but dans ce problème est de déterminer si les étudiants ayant eu les scores $[Examen=0.8, Moyenne=0.7, Motivation=0.2]$ et $[Examen=0.4, Moyenne=0.5, Motivation=0.9]$ ont été admis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous l'aurez compris, les données $Examen$, $Moyenne$ et $Motivation$ correspondent à nos $x_i$. Notre but avec la régression logistique est de trouver une valeur optimale des $w_i$ en concordance avec nos données d'entrainement.   \n",
    "Par simplicité, posons $x_0=Examen$, $x_1=Moyenne$ et $x_2=Motivation$ et $y_{true}=Admis$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de coût"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre exemple de descente du gradient, notre but était de trouver le minimum d'une fonction. C'est dans ces scénarios que la descente du gradient excelle. Pour notre nouveau problème, il nous faut trouver une fonction qui, lorsqu'on la minimise, améliore les prédictions.  \n",
    "Dans notre classification binaire, $y_{true}$ vaut 1 si l'élève est admis et 0 sinon.  \n",
    "Notre but est de prédire si l'élève est admis ou non en prédisant la sortie $pred$.  \n",
    "Lors de l'entraînement, on veut entraîner notre modèle de régression logistique à prédire $ pred \\approx y_{true}$. \n",
    "Pour cela, on utilise la fonction de vraisemblance négative qui s'exprime de la manière suivante :   \n",
    "$\\text{loss} = - \\left( y_{\\text{true}} \\cdot \\log(\\text{pred}) + (1 - y_{\\text{true}}) \\cdot \\log(1 - \\text{pred}) \\right)$\n",
    "\n",
    "Pour plus de détails sur la régression logistique et la perte de vraisemblance négative, vous pouvez consulter ce [lien](https://blog.demir.io/understanding-logistic-regression-26802c0da856)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'important est de comprendre comment varie cette fonction en fonction de notre prédiction $pred$ et du label $y_{true}$.  \n",
    "Pour cela, prenons le cas où le label est $y_{true}=1$. Analysons deux cas :  \n",
    "Si $pred=0.9$, c'est-à-dire que notre modèle prédit que l'élève sera admis à 90% de chance ce qui est une bonne prédiction alors :  \n",
    "$\\text{loss} = - \\left( 1.0 \\cdot \\log(0.9) + (1 - 1.0) \\cdot \\log(1 - 0.9) \\right)$  \n",
    "$\\text{loss} = - \\left( 1.0 \\cdot \\log(0.9) + 0 \\cdot \\log(1 - 0.9) \\right)$  \n",
    "$\\text{loss} = - \\left( 1.0 \\cdot \\log(0.9)\\right)$   \n",
    "$\\text{loss} = - \\left( 1.0 \\cdot \\log(0.9)\\right)$  \n",
    "$\\text{loss} = 0.046$   \n",
    "Le loss est faible, c'est une bonne chose car la prédiction est bonne.\n",
    "\n",
    "Si $pred=0.2$, c'est-à-dire que notre modèle prédit que l'élève sera admis à 20% de chance ce qui est une mauvaise prédiction alors :  \n",
    "$\\text{loss} = - \\left( 1.0 \\cdot \\log(0.2) + (1 - 1.0) \\cdot \\log(1 - 0.2) \\right)$  \n",
    "$\\text{loss} = - \\left( 1.0 \\cdot \\log(0.2) + 0 \\cdot \\log(1 - 0.2) \\right)$  \n",
    "$\\text{loss} = - \\left( 1.0 \\cdot \\log(0.2)\\right)$   \n",
    "$\\text{loss} = - \\left( 1.0 \\cdot \\log(0.2)\\right)$  \n",
    "$\\text{loss} = 0.70$   \n",
    "Le loss est important, c'est une bonne chose car la prédiction est mauvaise.\n",
    "\n",
    "Pour un cas où $y_{true}=0$, on retrouve un loss faible quand la $pred$ est proche 0 et un loss important quand $pred$ est proche de 1 (faire le calcul pour s'exercer si besoin). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des dérivées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que l'on dispose d'une fonction à minimiser, il est nécessaire de calculer la dérivée de cette fonction en fonction de chacun des poids $w_0$, $w_1$, $w_2$ et $b$. \n",
    "On doit donc calculer $\\frac{\\partial loss}{\\partial w_0}$, $\\frac{\\partial loss}{\\partial w_1}$, $\\frac{\\partial loss}{\\partial w_2}$ et $\\frac{\\partial loss}{\\partial b}$. \n",
    "Pour les poids $w_0$, $w_1$ et $w_2$, la dérivée s'effectue de la même manière.  \n",
    "Avec la règle de la chaîne, pour $w_0$, on a : \n",
    "$\\frac{\\partial loss}{\\partial w_0} = \\frac{\\partial loss}{\\partial pred} \\cdot \\frac{\\partial pred}{\\partial w_0}$   \n",
    "\n",
    "Pour rappel, notre prédiction $pred$ correspond à la sortie de notre régression logistique avec la fonction d'activation $sigmoid$. \n",
    "\n",
    "Pour le premier terme, la dérivée du loss en fonction de pred nous donne :  \n",
    "$\\frac{\\partial loss}{\\partial pred} = -(\\frac{y_{true}}{pred} - \\frac{1-y_{true}}{1-pred}) $  \n",
    "Le calcul ne sera pas détaillé ici mais vous pouvez le faire vous-même pour vous en assurer.  \n",
    "\n",
    "Pour le second terme, la dérivée de pred en fonction de $w_0$ nous donne :    \n",
    "$\\frac{\\partial pred}{\\partial w_0} = pred \\cdot (1-pred) \\cdot x_0$\n",
    "\n",
    "En combinant les deux termes, on obtient :  \n",
    "$\\frac{\\partial loss}{\\partial w_0} =-(\\frac{y_{true}}{pred} - \\frac{1-y_{true}}{1-pred}) \\cdot pred \\cdot (1-pred) \\cdot x_0$  \n",
    "\n",
    "Et après simplification (magique),  \n",
    "$\\frac{\\partial loss}{\\partial w_0} = (pred-y_{true}) \\cdot x_0$  \n",
    "\n",
    "Sans détailler le calcul, on obtient également :  \n",
    "$\\frac{\\partial loss}{\\partial b} = pred-y_{true}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que l'on a tous les éléments, \n",
    "Définissons notre fonction de regression logistique en python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notre classe de regression logistique\n",
    "class logistic_regression():\n",
    "  def __init__(self) -> None:\n",
    "    self.w0=np.random.randn()\n",
    "    self.w1=np.random.randn()\n",
    "    self.w2=np.random.randn()\n",
    "    self.b=0\n",
    "  def __call__(self,x0,x1,x2):\n",
    "    # Somme pondérée et ajout du biais\n",
    "    pond=self.w0*x0+self.w1*x1+self.w2*x2+self.b\n",
    "    # Application de la sigmoïde\n",
    "    pred=sigmoid(pond)\n",
    "    return pred\n",
    "    \n",
    "def loss(y_true, y_pred):\n",
    "  # Calcul du loss (log vraisemblance négative)\n",
    "  loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "  return loss\n",
    "    \n",
    "\n",
    "def update_weights(model,pred, x0, x1, x2, y_true, learning_rate):\n",
    "       \n",
    "  # On calcule les dérivées en fonction des poids et du biais \n",
    "  dL_dw0 = (pred - y_true) * x0\n",
    "  dL_dw1 = (pred - y_true) * x1\n",
    "  dL_dw2 = (pred - y_true) * x2\n",
    "  dL_db = pred - y_true\n",
    "      \n",
    "  # On modifie les paramètres pour réduire le loss \n",
    "  # La modification des poids dépend du learning rate, du signe de la dérivée et de la valeur de la dérivée\n",
    "  model.w0 -= learning_rate * dL_dw0\n",
    "  model.w1 -= learning_rate * dL_dw1\n",
    "  model.w2 -= learning_rate * dL_dw2\n",
    "  model.b -= learning_rate * dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialisation du modèle et des hyperparamètres\n",
    "learning_rate = 0.01\n",
    "epochs = 1000 # le nombre d'itérations d'entrainement\n",
    "model = logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'entraîner le modèle, testons nos prédictions sur les deux élèves dont on désire connaître le résultat de l'admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'élève avec Examen = 0.8, Moyenne = 0.7 et Motivation = 0.7 a 60% de chance d'être admis\n",
      "L'élève avec Examen = 0.4, Moyenne = 0.5 et Motivation = 0.9 a 59% de chance d'être admis\n"
     ]
    }
   ],
   "source": [
    "values_test=[[0.8,0.7,0.7],[0.4,0.5,0.9]]\n",
    "for value in values_test:\n",
    "  x0,x1,x2=value\n",
    "  pred = model(x0, x1, x2)\n",
    "  print(\"L'élève avec Examen = \"+str(x0)+ \", Moyenne = \"+str(x1)+\" et Motivation = \"+str(x2)+ \" a \"+str(round(pred*100)) + \"% de chance d'être admis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que le modèle est très incertain et qu'il donne des probabilités au hasard ce qui est logique car ses poids sont initialisés aléatoirement.  \n",
    "Maintenant, entrainons le modèle sur nos données d'entrainement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40000, Loss: 0.01468091027998586\n",
      "Epoch 5000/40000, Loss: 0.013032955086147595\n",
      "Epoch 10000/40000, Loss: 0.011715352279809266\n",
      "Epoch 15000/40000, Loss: 0.010638348324912276\n",
      "Epoch 20000/40000, Loss: 0.009741762611763436\n",
      "Epoch 25000/40000, Loss: 0.008983896958517028\n",
      "Epoch 30000/40000, Loss: 0.008334957514714105\n",
      "Epoch 35000/40000, Loss: 0.007773096000082178\n",
      "Epoch 40000/40000, Loss: 0.007281930357182074\n"
     ]
    }
   ],
   "source": [
    "# Entraînement\n",
    "for epoch in range(epochs):\n",
    "  # Mise à jour des poids pour chaque exemple\n",
    "  total_loss = 0\n",
    "  for (x0, x1, x2), y_true in zip(values_train, labels_train):\n",
    "    pred = model(x0, x1, x2)\n",
    "    update_weights(model,pred, x0, x1, x2, y_true, learning_rate)\n",
    "    total_loss += loss(y_true, pred)\n",
    "\n",
    "  avg_loss = total_loss / len(labels_train)\n",
    "  \n",
    "  # Affichage de la perte pour suivre la progression de l'entraînement\n",
    "  if ((epoch + 1) % 5000 == 0) or (epoch==0):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.464301071981186 -3.27230109363944 -8.244865180820856 -4.903197398150705\n",
      "L'élève avec Examen = 0.8, Moyenne = 0.7 et Motivation = 0.7 a 93% de chance d'être admis\n",
      "L'élève avec Examen = 0.4, Moyenne = 0.5 et Motivation = 0.9 a 0% de chance d'être admis\n"
     ]
    }
   ],
   "source": [
    "print(model.w0, model.w1, model.w2, model.b)\n",
    "\n",
    "for value in values_test:\n",
    "  x0,x1,x2=value\n",
    "  pred = model(x0, x1, x2)\n",
    "  print(\"L'élève avec Examen = \"+str(x0)+ \", Moyenne = \"+str(x1)+\" et Motivation = \"+str(x2)+ \" a \"+str(round(pred*100)) + \"% de chance d'être admis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, notre modèle est maintenant beaucoup plus confiant des ses prédictions et il nous donne des prédictions cohérentes par rapport aux données d'entrainement. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
