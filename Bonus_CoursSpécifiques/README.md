# üåü Cours sp√©cifiques üåü
Ce cours pr√©sente des concepts tr√®s int√©ressant √† comprendre mais non essentiels dans une pratique courante du deep learning. Si vous √™tes int√©ress√© par comprendre le fonctionnement d'un r√©seau de neurones de mani√®re plus approfondie et de d√©couvrir la raison de l'utilisation de techniques comme la BatchNorm, les connexions r√©siduelles, les optimizers, le dropout, la data augmentation etc ..., ce cours est fait pour vous !

## Notebook 1Ô∏è‚É£ : [Activation et initialisation](01_ActivationEtInitialisation.ipynb)
Ce notebook pr√©sente les consid√©rentions importantes √† prendre en compte lors de l'initialisation d'un r√©seau de neurones.

## Notebook 2Ô∏è‚É£ : [BatchNorm](02_BatchNorm.ipynb)
Ce notebook introduit un d√©tail la *batch normalization* en pr√©sentant ses int√™rets et son impl√©mentation.

## Notebook 3Ô∏è‚É£ : [Data augmentation](03_DataAugmentation.ipynb)
Ce notebook pr√©sente la *data augmentation* et son utilit√© pour l'entra√Ænement des r√©seaux de neurones.

## Notebook 4Ô∏è‚É£ : [Broadcasting](04_Broadcasting.ipynb)
Ce notebook pr√©sente les *broadcasting rules* de pytorch qui sont des r√®gles sur la manipulation des tenseurs torch. Ces r√®gles sont tr√®s importantes √† ma√Ætriser.

## Notebook 5Ô∏è‚É£ : [Optimizer](05_Optimizer.ipynb)
Ce notebook d√©crit les diff√©rents *optimizer* utilisables pour entra√Æner un r√©seau de neurones.

## Notebook 6Ô∏è‚É£ : [R√©gularisation](06_Regularisation.ipynb)
Ce notebook pr√©sente en d√©tails deux m√©thodes de r√©gularisation : le r√©gularisation L2 et le *dropout.*

## Notebook 7Ô∏è‚É£ : [Connexions R√©siduelles](07_ConnexionsResiduelles.ipynb)
Ce notebook introduit les connexions r√©siduelles et leurs utilit√©s pour l'entra√Ænement de r√©seaux de neurones profonds.