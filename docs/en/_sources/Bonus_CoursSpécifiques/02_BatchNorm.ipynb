{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Batch Normalization* (or batch normalization) was introduced in 2015 in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167). It had a major impact in the field of Deep Learning. Today, normalization is almost systematically used, whether it is *BatchNorm*, *LayerNorm*, or *GroupNorm* (and others).\n",
    "\n",
    "The idea behind *BatchNorm* is simple and related to the previous notebook. We aim to obtain preactivations following a Gaussian distribution at each layer of the network. We saw that good initialization allows this behavior, but it is not always easy, especially with many different layers.\n",
    "\n",
    "*BatchNorm* normalizes the preactivations with respect to the batch dimension before passing them into the activation functions. This ensures a Gaussian distribution at each step.\n",
    "\n",
    "This normalization does not affect optimization as it is a differentiable function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will review the code from the previous notebook to implement batch normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../05_NLP/prenoms.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180834, 3]) torch.Size([180834])\n",
      "torch.Size([22852, 3]) torch.Size([22852])\n",
      "torch.Size([22639, 3]) torch.Size([22639])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # Contexte\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] \n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=10 # Dimension de l'embedding de C\n",
    "hidden_dim=200 # Dimension de la couche cachée\n",
    "\n",
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our forward propagation code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing BatchNorm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article, here is the information:\n",
    "\n",
    "![Norm](./images/norm.png)\n",
    "\n",
    "First, we need to normalize.\n",
    "\n",
    "To do this, we calculate the mean and standard deviation of *hpreact* and then normalize using these values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-6\n",
    "hpreact_mean = hpreact.mean(dim=0, keepdim=True)\n",
    "hpreact_std= hpreact.std(dim=0, keepdim=True)\n",
    "hpreact_norm = (hpreact - hpreact_mean) / (hpreact_std+epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now integrate this normalization into the forward propagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in forward propagation, we will have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmean = hpreact.mean(0, keepdim=True)\n",
    "bnstd = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem with Batch Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon reflection, we can identify potential issues related to *BatchNorm*:\n",
    "\n",
    "**An example is affected by other elements in the batch**: Normalizing along the batch dimension means that the values of each example within the batch are influenced by other examples. This might seem problematic, but in practice, it's actually a good thing. Using random batches at each epoch provides regularization, reducing the risk of overfitting the data.\n",
    "However, if you want to avoid this issue, you can use other normalization methods that do not normalize along the batch dimension. In practice, *BatchNorm* is still widely used because it works very well empirically.\n",
    "\n",
    "**Testing on a single element**: During training, each element is influenced by other elements in its batch. However, during inference, when using the model on a single element, we can no longer apply *BatchNorm*. This is a problem because we want to avoid different behavior during training and inference.\n",
    "\n",
    "To solve this problem, we have two solutions:\n",
    "- We can calculate the mean and variance over all elements at the end of training and use these values. In practice, we don't want to perform an additional iteration over the entire dataset just for this, so no one does it this way.\n",
    "- Another solution is to update the mean and variance throughout training using an EMA (exponential moving average). At the end of training, we will have a good approximation of the mean and variance of all training elements.\n",
    "\n",
    "In practice, we can implement it like this in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "bnmean_running = torch.zeros((1, hidden_dim))\n",
    "bnstd_running = torch.ones((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)\n",
    "bnstdi = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In our implementation, we chose 0.001 for our EMA. In the [*BatchNorm* layer of PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html), this parameter is defined by *momentum* and its default value is 0.1. In practice, the choice of this value depends on the size of the *batch* relative to the size of the training dataset. For a large *batch* with a small dataset, you can take 0.1, for example. For a small *batch* with a large dataset, we prefer a smaller value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the training of our model to verify that the layer works. For this small model, we won't see a difference in performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.8241\n",
      "  10000/ 200000: 1.9756\n",
      "  20000/ 200000: 2.7151\n",
      "  30000/ 200000: 2.3287\n",
      "  40000/ 200000: 2.1411\n",
      "  50000/ 200000: 2.3207\n",
      "  60000/ 200000: 2.3250\n",
      "  70000/ 200000: 2.0320\n",
      "  80000/ 200000: 2.0615\n",
      "  90000/ 200000: 2.2468\n",
      " 100000/ 200000: 2.2081\n",
      " 110000/ 200000: 2.1418\n",
      " 120000/ 200000: 1.9665\n",
      " 130000/ 200000: 1.8572\n",
      " 140000/ 200000: 2.0577\n",
      " 150000/ 200000: 2.1804\n",
      " 160000/ 200000: 1.8604\n",
      " 170000/ 200000: 1.9810\n",
      " 180000/ 200000: 1.8228\n",
      " 190000/ 200000: 1.9977\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "max_steps = 200000\n",
    "\n",
    "for i in range(max_steps):\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "  emb = C[Xb] \n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1 \n",
    "  \n",
    "  # Batch normalization\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "      bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "      bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    \n",
    "  h = torch.tanh(hpreact) \n",
    "  logits = h @ W2 + b2 \n",
    "  loss = F.cross_entropy(logits, Yb)\n",
    "  \n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  lr = 0.1 if i < 100000 else 0.01 \n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "  if i % 10000 == 0:\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Considerations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias**: *Batch norm* normalizes the preactivations of the weights. This normalization cancels out the bias (since it shifts the distribution, while we recentre it). When using *BatchNorm*, we can do without the bias. In practice, if you leave a bias, it's not a problem, but it's a network parameter that will be unnecessary.\n",
    "\n",
    "**Placement of BatchNorm**: Based on what we've seen, it makes sense to place *BatchNorm* before the activation function. In practice, some prefer to place it after the activation layer, so don't be surprised if you encounter this in the literature or in code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Normalizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will quickly review other normalizations used for training neural networks.\n",
    "\n",
    "![Types of Normalization](./images/types.png)\n",
    "\n",
    "Figure extracted from the [article](https://arxiv.org/pdf/1803.08494)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer Normalization**: This normalization layer is also very frequently used, especially in language models (GPT, Llama). It involves normalizing across all activations of the layer rather than along the batch axis. In our implementation, this would simply mean changing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)  \n",
    "bnstdi = hpreact.std(0, keepdim=True)   \n",
    "# Layer normalization\n",
    "bnmeani = hpreact.mean(1, keepdim=True)  \n",
    "bnstdi = hpreact.std(1, keepdim=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instance Normalization**: This layer normalizes activations on each channel of each element independently.\n",
    "\n",
    "**Group Normalization**: This layer is a kind of fusion between *LayerNorm* and *InstanceNorm*, as normalization is calculated on groups of channels (if the size of a group is 1, it's *InstanceNorm* and if the size of a group is C, it's *LayerNorm*)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
