{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we will explore Recurrent Neural Networks (RNNs) to predict the next character. We will use the architecture described in the paper [Recurrent neural network based language model](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf), which proposes a simple version of RNN for this task.\n",
    "\n",
    "The advantage of RNNs is that they do not require a fixed context size, unlike the fully connected network-based models we saw previously.\n",
    "\n",
    "RNNs keep the context in memory, regardless of the sequence length. This is a cool idea in theory, but we will see at the end of the course that they have their limitations.\n",
    "\n",
    "![RNN](./images/rnn.png)\n",
    "\n",
    "Figure extracted from the original paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does an RNN work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs operate sequentially: characters are processed one by one. The next character depends on both the current element and the stored state, which contains information from previous characters.\n",
    "\n",
    "Mathematically, an RNN has three components:\n",
    "- The input $x$\n",
    "- The hidden state $s$\n",
    "- The output $y$\n",
    "\n",
    "We also add time $t$ to handle the sequence.\n",
    "\n",
    "The input at time $t$ is given by:\n",
    "$x(t) = w(t) + s(t-1)$\n",
    "where $w()$ is the one-hot encoding and $s(t-1)$ is the previous state.\n",
    "\n",
    "Then, we calculate the state and the output:\n",
    "$s(t) = sigmoid(x(t))$\n",
    "$y(t) = softmax(s(t))$\n",
    "\n",
    "The only parameter to adjust is the size of the hidden layer $s$.\n",
    "\n",
    "For initialization, $s(0)$ can be a small vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating names with an RNN is not very useful, as names are short and the context is limited. For more interesting tasks, we need a dataset with a broader context.\n",
    "\n",
    "We therefore use a text file containing Molière's dialogues. This dataset was created from the complete works available on [Gutenberg.org](https://www.gutenberg.org/). I cleaned the data to keep only the dialogues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is large, we only take a portion (for example, the first 50,000 characters) for faster processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  50000\n"
     ]
    }
   ],
   "source": [
    "text=text[:50000]\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 250 characters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALÈRE.\n",
      "\n",
      "Eh bien, Sabine, quel conseil me donnes-tu?\n",
      "\n",
      "SABINE.\n",
      "\n",
      "Vraiment, il y a bien des nouvelles. Mon oncle veut résolûment que ma\n",
      "cousine épouse Villebrequin, et les affaires sont tellement avancées,\n",
      "que je crois qu'ils eussent été mariés dès aujo\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the number of unique characters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJLMNOPQRSTUVYabcdefghijlmnopqrstuvxyzÇÈÉàâæçèéêîïôùû\n",
      "Nombre de caractères différents :  73\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a mapping between characters and integers (and vice versa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the dataset by converting character strings to integers, then to PyTorch tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 12, 22, 59, 28, 16,  8,  0,  0, 16, 41,  1, 35, 42, 38, 46,  6,  1,\n",
      "        29, 34, 35, 42, 46, 38,  6,  1, 49, 53, 38, 44,  1, 36, 47, 46, 51, 38,\n",
      "        42, 44,  1, 45, 38,  1, 37, 47, 46, 46, 38, 51,  7, 52, 53, 11,  0,  0,\n",
      "        29, 12, 13, 20, 24, 16,  8,  0,  0, 32, 50, 34, 42, 45, 38, 46, 52,  6,\n",
      "         1, 42, 44,  1, 56,  1, 34,  1, 35, 42, 38, 46,  1, 37, 38, 51,  1, 46,\n",
      "        47, 53, 54, 38, 44, 44, 38, 51,  8,  1, 23, 47, 46,  1, 47, 46, 36, 44,\n",
      "        38,  1, 54, 38, 53, 52,  1, 50, 66, 51, 47, 44, 72, 45, 38, 46, 52,  1,\n",
      "        49, 53, 38,  1, 45, 34,  0, 36, 47, 53, 51, 42, 46, 38,  1, 66, 48, 47,\n",
      "        53, 51, 38,  1, 32, 42, 44, 44, 38, 35, 50, 38, 49, 53, 42, 46,  6,  1,\n",
      "        38, 52,  1, 44, 38, 51,  1, 34, 39, 39, 34, 42, 50, 38, 51,  1, 51, 47,\n",
      "        46, 52,  1, 52, 38, 44, 44, 38, 45, 38, 46, 52,  1, 34, 54, 34, 46, 36,\n",
      "        66, 38, 51,  6,  0, 49, 53, 38,  1, 43, 38,  1, 36, 50, 47, 42, 51,  1,\n",
      "        49, 53,  3, 42, 44, 51,  1, 38, 53, 51, 51, 38, 46, 52,  1, 66, 52, 66,\n",
      "         1, 45, 34, 50, 42, 66, 51,  1, 37, 65, 51,  1, 34, 53, 43, 47])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:250]) # Les 250 premiers caractères encodé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour le test\n",
    "train_data = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: At each iteration, we go through the entire dataset sequentially.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build the model!\n",
    "\n",
    "As indicated in the paper, the input (the character) is encoded in one-hot, then added to the previous state. We therefore need two fully connected layers:\n",
    "- The first transforms the input $x(t)$ into state $s(t)$\n",
    "- The second transforms $s(t)$ into prediction $y(t)$\n",
    "\n",
    "![RNN](./images/rnn_math.png)\n",
    "\n",
    "Equation taken from the paper. $f$ is the sigmoid function and $g$ is the softmax.\n",
    "\n",
    "**Note**: The [paper](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) is clear and concise, I recommend you read it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(nn.Module): \n",
    "  def __init__(self,hidden_dim,vocab_size) -> None:\n",
    "    super(rnn, self).__init__()\n",
    "    self.hidden_to_hidden=nn.Linear(hidden_dim+vocab_size, hidden_dim)\n",
    "    self.hidden_to_output=nn.Linear(hidden_dim, vocab_size)\n",
    "    self.vocab_size=vocab_size\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.sigmoid=nn.Sigmoid() \n",
    "    \n",
    "  # Le réseau prend en entrée le caractère actuel et le state précédent\n",
    "  def forward(self, x,state):\n",
    "    # On one-hot encode le caractère\n",
    "    x = torch.nn.functional.one_hot(x, self.vocab_size).float()\n",
    "    if state is None:\n",
    "      # Si on a pas de state (début de la séquence), on initialise le state avec des petites valeurs aléatoires\n",
    "      state = torch.randn(self.hidden_dim) * 0.1\n",
    "    x = torch.cat((x, state), dim=-1)  # Concaténation de x et du state\n",
    "    state = self.sigmoid(self.hidden_to_hidden(x)) # Calcul du nouveau state\n",
    "    output = self.hidden_to_output(state) # Calcul de l'output\n",
    "    # On renvoie l'output et le state pour le prochain pas de temps\n",
    "    return output, state.detach() # detach() pour éviter de propager le gradient dans le state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the training parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr=0.1\n",
    "hidden_dim=128\n",
    "model=rnn(hidden_dim,vocab_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 2.63949568\n",
      "Epoch: 1 \t Loss: 2.16456994\n",
      "Epoch: 2 \t Loss: 2.00850788\n",
      "Epoch: 3 \t Loss: 1.91673251\n",
      "Epoch: 4 \t Loss: 1.84440742\n",
      "Epoch: 5 \t Loss: 1.78986003\n",
      "Epoch: 6 \t Loss: 1.74923073\n",
      "Epoch: 7 \t Loss: 1.71709289\n",
      "Epoch: 8 \t Loss: 1.68791167\n",
      "Epoch: 9 \t Loss: 1.66215199\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    state=None\n",
    "    running_loss = 0\n",
    "    n=0\n",
    "    for i in range(len(train_data)-1):\n",
    "        x = train_data[i]\n",
    "        y = train_data[i+1]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the model on the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.77312289\n"
     ]
    }
   ],
   "source": [
    "state=None\n",
    "running_loss = 0\n",
    "n=0\n",
    "for i in range(len(train_data)-1):\n",
    "    with torch.no_grad():\n",
    "        x = train_data[i]\n",
    "        y = train_data[i+1]\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "print(\"Loss: {0:.8f}\".format(running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss on the test data is slightly higher than during training. The model has slightly overfitted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can generate text in the style of Molière!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "VARDILE.\n",
      "\n",
      "Vout on est nt, jes l'un ouint; sabhil.\n",
      "\n",
      "LE DOCTE.\n",
      "\n",
      "Si vous dicefalassîntes\n",
      "GIRGIB.\n",
      "\n",
      "MARGRIILÉ.\n",
      "\n",
      "LE DOCTE. Jort; et\n",
      "; bieu,\n",
      "et je mu tu d'ais d'ai coupce!\n",
      "\n",
      "SGÉLLÉ.\n",
      "\n",
      "Il Sgnous elli massit que\n",
      "Suis pluagil dés.\n",
      "Cais téscompas: y totte demes\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "moliere='.'\n",
    "sequence_length=250\n",
    "state=None\n",
    "for i in range(sequence_length):\n",
    "    x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()\n",
    "    y_pred,state = model.forward(x,state)\n",
    "    probs=F.softmax(torch.squeeze(y_pred), dim=0)\n",
    "    sample=torch.multinomial(probs, 1)\n",
    "    moliere+=itos[sample.item()]\n",
    "print(moliere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not perfect, but we can recognize a few words and a sentence structure close to the \"moliere.txt\" file. Not bad for a single-layer RNN!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to improve the results?** Here are some suggestions:\n",
    "- Increase the number of layers or the size of the hidden layer\n",
    "- Use an *embedding* instead of *one-hot encoding*\n",
    "- Test other RNN variants like [LSTM](https://arxiv.org/pdf/1308.0850) or [GRU](https://arxiv.org/abs/1409.1259)\n",
    "- ~~Use a *transformer* architecture~~ (spoiler!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Limitations of RNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long at the heart of research in NLP and deep learning, RNNs have several limitations that make them impractical for large models:\n",
    "- Their architecture allows for infinite context in theory, but the sequential structure complicates information propagation over long sequences.\n",
    "- The *vanishing gradient* on long sequences is a real problem.\n",
    "- The sequential structure makes parallelization difficult, while GPUs are optimized for parallel computations. Training is therefore slower.\n",
    "- The fixed structure is not always suitable for capturing complex relationships.\n",
    "\n",
    "Since the advent of [transformers](https://arxiv.org/pdf/1706.03762), RNNs are used less and less.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
