{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we introduced the classic RNN layer. Since its invention, many other recurrent layers have been created.\n",
    "\n",
    "Here, we will explore the [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf) (long short-term memory) layer, an alternative to the classic RNN layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an LSTM Layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM layer consists of a *memory unit* with 4 fully connected layers. Three of these layers are used to select relevant information from previous steps: the *forget gate*, the *input gate*, and the *output gate*.\n",
    "\n",
    "- **Forget gate**: Removes information from memory\n",
    "- **Input gate**: Inserts information into memory\n",
    "- **Output gate**: Uses stored information\n",
    "\n",
    "The last fully connected layer generates a \"candidate information\" for the LSTM layer's memory.\n",
    "\n",
    "![LSTM](./images/lstm.png)\n",
    "\n",
    "Figure from the [blog post](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the figure, the LSTM layer receives 3 input vectors: $H_{t-1}$, $C_{t-1}$, and $X_{t}$. The first two come directly from the LSTM, and the third corresponds to the input at time $t$ (the character in our case).\n",
    "\n",
    "Simplified: $H_{t-1}$ contains short-term memory, and $C_{t-1}$ contains long-term memory. This allows retaining important information over a broad context without neglecting local context.\n",
    "\n",
    "The idea is to solve the problem of information propagation over long sequences that occurs in classic RNNs.\n",
    "\n",
    "For further exploration, you can read [the article](https://www.bioinf.jku.at/publications/older/2604.pdf) or check out [the blog post](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the dataset, we still use the moliere.txt file and reuse the code from the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the number of elements for faster training (uncomment if you want to train on all data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  100000\n"
     ]
    }
   ],
   "source": [
    "text=text[:100000]\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJLMNOPQRSTUVYabcdefghijlmnopqrstuvxyz«»ÇÈÉÊàâæçèéêîïôùû\n",
      "Nombre de caractères différents :  76\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour le test\n",
    "train_data = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the model, we directly use PyTorch's implementation of the LSTM layer. Unlike linear or convolutional layers, [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) allows stacking multiple layers with the *num_layers* parameter. If you want to define them one by one, you need to use [nn.LSTMCell](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size,num_layers=1):\n",
    "        super(lstm, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # On utilise un embedding pour transformer les entiers(caractères) en vecteurs\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        # La couche LSTM peut prendre l'argument num_layers pour empiler plusieurs couches LSTM\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
    "        # Une dernière couche linéaire pour prédire le prochain caractère\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr=0.001\n",
    "hidden_dim=128\n",
    "seq_len=100\n",
    "num_layers=1\n",
    "model=lstm(vocab_size,hidden_dim,num_layers)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM layer takes a sequence as input and returns a sequence of the same size. This speeds up training as we can process multiple examples at once.\n",
    "\n",
    "**Note**: Training can also be accelerated by batch processing with multiple sequences in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 2.17804336\n",
      "Epoch: 1 \t Loss: 1.76270216\n",
      "Epoch: 2 \t Loss: 1.62740668\n",
      "Epoch: 3 \t Loss: 1.54147145\n",
      "Epoch: 4 \t Loss: 1.47995140\n",
      "Epoch: 5 \t Loss: 1.43100239\n",
      "Epoch: 6 \t Loss: 1.39074463\n",
      "Epoch: 7 \t Loss: 1.35526441\n",
      "Epoch: 8 \t Loss: 1.32519794\n",
      "Epoch: 9 \t Loss: 1.29712536\n",
      "Epoch: 10 \t Loss: 1.27268774\n",
      "Epoch: 11 \t Loss: 1.24876227\n",
      "Epoch: 12 \t Loss: 1.22720749\n",
      "Epoch: 13 \t Loss: 1.20663312\n",
      "Epoch: 14 \t Loss: 1.18768359\n",
      "Epoch: 15 \t Loss: 1.16936996\n",
      "Epoch: 16 \t Loss: 1.15179397\n",
      "Epoch: 17 \t Loss: 1.13514291\n",
      "Epoch: 18 \t Loss: 1.11997525\n",
      "Epoch: 19 \t Loss: 1.10359089\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    state=None\n",
    "    running_loss = 0\n",
    "    n=0\n",
    "    data_ptr = torch.randint(100,(1,1)).item()\n",
    "    # On train sur des séquences de seq_len caractères et on break si on dépasse la taille du dataset\n",
    "    while True:\n",
    "        x = train_data[data_ptr : data_ptr+seq_len]\n",
    "        y = train_data[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        data_ptr+=seq_len\n",
    "        # Pour éviter de sortir de l'index du dataset\n",
    "        if data_ptr + seq_len + 1 > len(train_data):\n",
    "            break\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the loss on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss de test: 1.51168611\n"
     ]
    }
   ],
   "source": [
    "state=None\n",
    "running_loss = 0\n",
    "n=0\n",
    "data_ptr = torch.randint(100,(1,1)).item()\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        x = test[data_ptr : data_ptr+seq_len]\n",
    "        y = test[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "    running_loss += loss.item()\n",
    "    n+=1\n",
    "    data_ptr+=seq_len\n",
    "    if data_ptr + seq_len + 1 > len(test):\n",
    "        break\n",
    "print(\"Loss de test: {0:.8f}\".format(running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model overfits quite a bit... Try fixing this on your own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test text generation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Çà coeuse, et bon enfin l'avoir faire.\n",
      "\n",
      "MASCARILLE.\n",
      "\n",
      "En me donner d vous, Le pas.\n",
      "\n",
      "MASCARILLE, à dans un pour sûte matinix! cette ma foi.\n",
      "\n",
      "PANDOLFE.\n",
      "\n",
      "Ma foi, tu te le sy sois touves d'arrête sa bien sans les bonheur.\n",
      "\n",
      "MASCARILLE.\n",
      "\n",
      "Moi, je me suis to\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "moliere='.'\n",
    "sequence_length=250\n",
    "state=None\n",
    "for i in range(sequence_length):\n",
    "    x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()\n",
    "    y_pred,state = model.forward(x.unsqueeze(0),state)\n",
    "    probs=F.softmax(torch.squeeze(y_pred), dim=0)\n",
    "    sample=torch.multinomial(probs, 1)\n",
    "    moliere+=itos[sample.item()]\n",
    "print(moliere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation is a bit better than the basic RNN model, but not yet convincing. You can try improving performance by modifying parameters (number of stacked layers, hidden dimension, etc.).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
