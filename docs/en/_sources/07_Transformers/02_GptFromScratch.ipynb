{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GPT from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how to create a language model to predict the next character, based on the *transformer* architecture (specifically the decoder).\n",
    "For this, we use a text file `moliere.txt` containing all the dialogues from Molière's plays.\n",
    "This dataset was created from Molière's complete works available on [Gutenberg.org](https://www.gutenberg.org/). I cleaned the data to keep only the dialogues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Pour utiliser le GPU automatiquement si vous en avez un \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by opening and viewing the content of our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the first 250 characters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALÈRE.\n",
      "\n",
      "Eh bien, Sabine, quel conseil me donnes-tu?\n",
      "\n",
      "SABINE.\n",
      "\n",
      "Vraiment, il y a bien des nouvelles. Mon oncle veut résolûment que ma\n",
      "cousine épouse Villebrequin, et les affaires sont tellement avancées,\n",
      "que je crois qu'ils eussent été mariés dès aujo\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `set()` to retrieve the unique characters present in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijlmnopqrstuvxyz«»ÇÈÉÊÏàâæçèéêëìîïòôùûŒœ\n",
      "Nombre de caractères différents :  85\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Training Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Course 5, we will create a *mapping* to convert characters into integers. This *mapping* is a very simple form of *tokenization*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Note on Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Tokenization?**\n",
    "*Tokenization* is the process of converting text into a sequence of integers. Each integer can represent a character, a group of characters, or a word, depending on the method used.\n",
    "\n",
    "**Balance Between Vocabulary and Sequence Length**\n",
    "A good *tokenizer* balances vocabulary size (26 for the alphabet and about 100,000 for French words) with sequence length. A too-small vocabulary increases sequence length (e.g., \"Bonjour\" becomes 7 *tokens* if using characters, or 1 *token* if using words). In practice, extremes are problematic, and we aim for a middle ground.\n",
    "\n",
    "**Popular Tokenizers**\n",
    "*Tokenizers* are essential for the proper functioning of a language model. Their design depends on the method and training data. Among the most widely used are [SentencePiece](https://github.com/google/sentencepiece) by Google and [tiktoken](https://github.com/openai/tiktoken) by OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 50, 49, 46, 50, 56, 53, 1, 68, 1, 55, 50, 56, 54]\n",
      "Bonjour à Tous\n"
     ]
    }
   ],
   "source": [
    "# Creation d'un mapping de caractère à entiers et inversement\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "\n",
    "print(encode(\"Bonjour à tous\"))\n",
    "print(decode(encode(\"Bonjour à Tous\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will transform our dataset into sequences of integers and store them as PyTorch tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33, 12, 23, 64, 29, 16,  8,  0,  0, 16, 44,  1, 38, 45, 41, 49,  6,  1,\n",
      "        30, 37, 38, 45, 49, 41,  6,  1, 52, 56, 41, 47,  1, 39, 50, 49, 54, 41,\n",
      "        45, 47,  1, 48, 41,  1, 40, 50, 49, 49, 41, 54,  7, 55, 56, 11,  0,  0,\n",
      "        30, 12, 13, 20, 25, 16,  8,  0,  0, 33, 53, 37, 45, 48, 41, 49, 55,  6,\n",
      "         1, 45, 47,  1, 59,  1, 37,  1, 38, 45, 41, 49,  1, 40, 41, 54,  1, 49,\n",
      "        50, 56, 57, 41, 47, 47, 41, 54,  8,  1, 24, 50, 49,  1, 50, 49, 39, 47,\n",
      "        41,  1, 57, 41, 56, 55,  1, 53, 73, 54, 50, 47, 82, 48, 41, 49, 55,  1,\n",
      "        52, 56, 41,  1, 48, 37,  0, 39, 50, 56, 54, 45, 49, 41,  1, 73, 51, 50,\n",
      "        56, 54, 41,  1, 33, 45, 47, 47, 41, 38, 53, 41, 52, 56, 45, 49,  6,  1,\n",
      "        41, 55,  1, 47, 41, 54,  1, 37, 42, 42, 37, 45, 53, 41, 54,  1, 54, 50,\n",
      "        49, 55,  1, 55, 41, 47, 47, 41, 48, 41, 49, 55,  1, 37, 57, 37, 49, 39,\n",
      "        73, 41, 54,  6,  0, 52, 56, 41,  1, 46, 41,  1, 39, 53, 50, 45, 54,  1,\n",
      "        52, 56,  3, 45, 47, 54,  1, 41, 56, 54, 54, 41, 49, 55,  1, 73, 55, 73,\n",
      "         1, 48, 37, 53, 45, 73, 54,  1, 40, 72, 54,  1, 37, 56, 46, 50])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:250]) # Les 250 premiers caractères encodé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split our text into training and validation parts, using a 0.9-0.1 ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour la validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our language model, we will also define a *block_size* context size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33, 12, 23, 64, 29, 16,  8,  0,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the first 8 characters represent the context, and the 9th is the label. This simple example actually includes several cases, as our model must predict the next character regardless of the context. In this list, we have 8 examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quand l'entrée est [33] le label est : 12\n",
      "Quand l'entrée est [33 12] le label est : 23\n",
      "Quand l'entrée est [33 12 23] le label est : 64\n",
      "Quand l'entrée est [33 12 23 64] le label est : 29\n",
      "Quand l'entrée est [33 12 23 64 29] le label est : 16\n",
      "Quand l'entrée est [33 12 23 64 29 16] le label est : 8\n",
      "Quand l'entrée est [33 12 23 64 29 16  8] le label est : 0\n",
      "Quand l'entrée est [33 12 23 64 29 16  8  0] le label est : 0\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Quand l'entrée est {context.numpy()} le label est : {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know how to create a set of inputs/labels from a single example.\n",
    "Let's adapt this method for *batch* processing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrée : \n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 69, 39, 41,  2,  0,  0, 27],\n",
      "        [53,  1, 56, 49,  1, 39, 84, 56],\n",
      "        [54, 11,  0,  0, 24, 12, 30, 14],\n",
      "        [ 1, 51, 72, 53, 41,  8,  0,  0]], device='cuda:0')\n",
      "Labels :\n",
      "torch.Size([4, 8])\n",
      "tensor([[69, 39, 41,  2,  0,  0, 27, 19],\n",
      "        [ 1, 56, 49,  1, 39, 84, 56, 53],\n",
      "        [11,  0,  0, 24, 12, 30, 14, 12],\n",
      "        [51, 72, 53, 41,  8,  0,  0, 33]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # La taille de batch (les séquences calculés en parallèles)\n",
    "block_size = 8 # La taille de contexte maximale pour une prédiction du modèle\n",
    "\n",
    "def get_batch(split):\n",
    "    # On genere un batch de données (sur train ou val)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    #On génére batch_size indice de début de séquence pris au hasard dans le dataset\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # On stocke dans notre tenseur torch\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # On met les sur le GPU si on en a un \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('Entrée : ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('Labels :')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these 4 examples includes 8 distinct examples (as explained earlier), totaling 32 examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Course 5 on NLP, we saw the bigram, the simplest language model. It predicts the next character from a single context character. Let $B$ be the *batch* size, $T$ the *block* size, and $C$ the vocabulary size.\n",
    "\n",
    "To test its performance on the `moliere.txt` dataset, let's quickly implement it in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 85])\n",
      "tensor(4.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # Chaque token va directement lire la valeur du prochain à partir d'une look-up table entrainé\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # Taille (B,T)\n",
    "    logits = self.token_embedding_table(idx) \n",
    "    # Taille (B,T,C)\n",
    "    \n",
    "    # Pour gérer le cas de la génération (pas de target)\n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else: # Cas de l'entraînement\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx est de la taille (B,T) avec T le contexte actuel\n",
    "    for _ in range(max_new_tokens):\n",
    "      # Forward du modèle pour récuperer les prédictions\n",
    "      logits, _ = self(idx)\n",
    "      # On prend uniquement le dernier caractère\n",
    "      logits = logits[:, -1, :] # devient (B, C)\n",
    "      # On applique la softmax pour récuperer les probabilités\n",
    "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "      # On sample avec torch.multinomial\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # devient (B, 1)\n",
    "      # On ajouter l'élément sample à la séquence actuelle\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is implemented but not trained. If we test it like this, we get disastrous results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CZjb!DzPGŒR?'hô.ù\n",
      "cddhhf,séÇqmp.ÉMjôCùÊF:TAFYèL  àP;zbVmëtuPipL.ôHtSEé,t:æéÉYÈìïë?VGYxoùyçnï'lpôHà!ô\n"
     ]
    }
   ],
   "source": [
    "base=torch.zeros((1, 1), dtype=torch.long).to(device) # Le premier élément est un 0 (token de retour à la ligne)\n",
    "# On génère 100 éléments\n",
    "print(decode(m.generate(idx = base , max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is purely random, which makes sense since the model is randomly initialized.\n",
    "\n",
    "We will now train the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2493152618408203\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "steps=10000\n",
    "for step in range(steps): # Nombre d'étape d'entraînement (élements traités = steps*batch_size)\n",
    "\n",
    "    # On récupère un batch de données aléatoires\n",
    "    xb, yb = get_batch('train')\n",
    "    # On calcule le loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Retropropagation\n",
    "    loss.backward()\n",
    "    # Mise à jour des poids du modèle\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate from our trained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "ELASGOXûÏï!\n",
      "ANDann donde se ns ntrar pous fa àTEn!.\n",
      "\n",
      "TELITEL'enomouvûûKbeue\n",
      "SGAvore oue mesontre\n",
      "t de pou n qur quvabou qude dente je père e em'eni\n",
      "\n",
      "La d'euhèmpon, j'es en paiqus de rau plenoilà jonont DARLysontausqus es ei voisangur s ve.\n",
      "\n",
      "\n",
      "\n",
      "DO lar dire tré quseuqu'arme à ai? t pe ne ndome l pa, \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice an improvement in data structure, and some words seem almost correct. However, the result remains disastrous, which is logical since the bigram is too simple a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now present step-by-step the concept of *self-attention*, a key element of the *transformer* architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Do We Want to Do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple idea. We have a tensor of size $(B,T,C)$. We want each element $T$ to be the average of the current element and the previous elements, without considering the following elements. This is the simplest way to give importance to previous elements to predict the current value (this is the idea behind the attention mechanism).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we can implement this idea as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création de notre tenseur random\n",
    "B,T,C = 4,4,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5023, -0.5911],\n",
      "        [ 1.0199, -0.2976],\n",
      "        [-1.7581,  0.0969],\n",
      "        [ 0.7444, -0.3360]])\n",
      "tensor([[ 1.5023, -0.5911],\n",
      "        [ 1.2611, -0.4443],\n",
      "        [ 0.2547, -0.2639],\n",
      "        [ 0.3771, -0.2819]])\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne des éléments précédents (incluant l'élément actuel) pour chaque valeur.\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get what we wanted: each element corresponds to the average of the current element with the previous elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we know that `for` loops are inefficient for calculations. We would prefer a matrix operation to do the same thing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Matrix Multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Multiplication**: $(3 \\times 3)$ Matrix by $(3 \\times 2)$ Matrix\n",
    "Starting Matrices\n",
    "\n",
    "Let $A$ be a $(3 \\times 3)$ matrix:\n",
    "\n",
    "$A =\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "and $B$ be a $(3 \\times 2)$ matrix:\n",
    "\n",
    "$B =\n",
    "\\begin{pmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "b_{31} & b_{32}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "The matrix multiplication $C = A \\times B$ yields a $(3 \\times 2)$ matrix:\n",
    "\n",
    "$C =\n",
    "\\begin{pmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22} \\\\\n",
    "c_{31} & c_{32}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "where each element $c_{ij}$ is calculated as follows:\n",
    "\n",
    "$c_{ij} = \\sum_{k=1}^{3} a_{ik} \\cdot b_{kj}$\n",
    "\n",
    "That is:\n",
    "\n",
    "- $c_{11} = a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}$\n",
    "- $c_{12} = a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32}$\n",
    "- $c_{21} = a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31}$\n",
    "- $c_{22} = a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}$\n",
    "- $c_{31} = a_{31}b_{11} + a_{32}b_{21} + a_{33}b_{31}$\n",
    "- $c_{32} = a_{31}b_{12} + a_{32}b_{22} + a_{33}b_{32}$\n",
    "\n",
    "Here is a Python example that illustrates this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[7., 6.],\n",
      "        [5., 0.],\n",
      "        [1., 8.]])\n",
      "--\n",
      "c=\n",
      "tensor([[13., 14.],\n",
      "        [13., 14.],\n",
      "        [13., 14.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mathematical Trick for Self-Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic happens. Instead of a matrix of 1s, we take a lower triangular matrix and perform the calculation again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[1., 2.],\n",
      "        [1., 4.],\n",
      "        [6., 6.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 2.,  6.],\n",
      "        [ 8., 12.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value in the matrix is the sum of the current value and the previous values. That's almost what we want! We just need to normalize by rows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[1., 2.],\n",
      "        [8., 6.],\n",
      "        [9., 8.]])\n",
      "--\n",
      "c=\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [4.5000, 4.0000],\n",
      "        [6.0000, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it! We've replaced our double `for` loop with a simple matrix multiplication and value normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use it to calculate *xbow* and compare its value with the one calculated using our double loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C) fonctionne grâce au broadcasting de pytorch\n",
    "torch.allclose(xbow, xbow2) # Vérifie que tous les éléments sont identiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of normalization, we can use the *softmax* function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "# On met toutes les valeurs égales à 0 à la valeur -inf\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the *softmax* to the matrix and TADAAA:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the version with *softmax* is used for the *self-attention* layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention: The Heart of the Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the matrix $wei$ contains uniform values across each row, providing no real information about the importance of previous information.\n",
    "\n",
    "This is where the concept of *self-attention* comes into play. What we want is a trainable matrix $wei$.\n",
    "\n",
    "We will create 3 values from our value $x$:\n",
    "\n",
    "**query**: *What am I looking for?* This value represents what each position in the sequence is trying to find in other positions.\n",
    "\n",
    "**key**: *What do I contain?* This value represents what each position in the sequence contains as information, which could be relevant to other positions.\n",
    "\n",
    "**value**: *What is my value?* This value represents the actual information to extract from each position in the sequence, if deemed relevant.\n",
    "\n",
    "To extract the *query*, *key*, and *value* values, we use a linear layer that projects the input into a *head_size* dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the importance of a previous element in the sequence relative to the current element, we perform the dot product between the *query* $Q$ and the *key* $K$ (transposed):\n",
    "\n",
    "$wei = QK^T$\n",
    "\n",
    "To obtain attention weights (sum equal to 1), we apply the *softmax* and multiply by the *value* $V$:\n",
    "\n",
    "$Output = \\text{softmax}\\left(wei\\right) \\cdot V$\n",
    "\n",
    "![Attention](./images/attention2.png)\n",
    "\n",
    "In Python, we implement it as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer l'importance d'un élément précédent de la séquence par rapport à l'élément actuel, on effectue le produit scalaire entre les *query* $Q$ et les *key* $K$ (transposée) :\n",
    "\n",
    "$wei = QK^T$\n",
    "\n",
    "Pour obtenir des poids d'attention (somme égale à 1), on applique la *softmax* et on multiplie par les *value* $V$ :\n",
    "\n",
    "$Output = \\text{softmax}\\left(wei\\right) \\cdot V$\n",
    "\n",
    "![Attention](./images/attention2.png)\n",
    "\n",
    "En Python, on l'implémente de cette manière :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "\n",
    "head_size = 16 # Valeur de head_size (projection de x)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # Pour appliquer le softmax, il faut des valeurs -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our matrix $wei$ is now fully trainable, and it is possible to use this layer to train a neural network.\n",
    "\n",
    "**Notes on the *self-attention* layer**:\n",
    "\n",
    "- Attention is a communication mechanism that can be seen as a graph with connections between nodes (in our case, the end nodes are connected to all previous nodes).\n",
    "- In the attention layer, there is no notion of the position of elements relative to each other. To address this, we will need to add a *positional embedding* (see next part of the course).\n",
    "- To clarify, there is no interaction along the *batch* dimension: each element in the *batch* is processed independently of the others. It's as if we had *batch_size* independent graphs.\n",
    "- This *attention block* is called a *decoder block*. Its distinctive feature is that each element only communicates with the past (thanks to the lower triangular matrix). However, there are other attention layers (*encoder*) that allow communication among all elements (for translation, sentiment analysis, or image processing).\n",
    "- We refer to *self-attention* because the *query*, *key*, and *value* come from the same source. It is possible to have *query*, *key*, and *value* from different sources: in this case, we refer to *cross-attention*.\n",
    "- If you read the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762), you will notice that there is a normalization by the square root of the *head_size*:\n",
    "\n",
    "![Attention](./images/attention.png)\n",
    "\n",
    "This allows for stability of the *softmax* function, especially during weight initialization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement a *head* class that will perform the operations of *self-attention*. This is simply what we saw earlier in class form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentons maintenant une classe *head* qui va effectuer les opérations de la *self-attention*. C'est simplement ce qu'on a vu précédemment sous forme de classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "\n",
    "    def __init__(self, head_size,n_embd,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Ajout de dropout pour la regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762), a variant of *self-attention* is proposed. This variant is called *multi-head attention* and simply involves having multiple *self-attention* layers in parallel. The goal of this layer is to parallelize processing to make it faster on GPUs.\n",
    "\n",
    "![Multi-Head Attention](./images/multihead.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is quite simple as it involves multiple *head* layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head de taille head_size\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Couche pour Linear (voir schema) après concatenation\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # Dropout si besoin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another element of the *transformer* that we can see in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762) is the *Feed Forward* layer, which is simply a small fully connected network.\n",
    "\n",
    "We implement it in Python as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 4*n_embd comme dans le papier\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the elements to implement our *transformer* layer, which will use *multi-head attention* and *feed forward*. In the main figure of the paper, we also notice that there are residual connections between the *input* and *output* of the *attention* and *feed forward* layers. These connections facilitate the training of a deep model (more details in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)). We will therefore also implement these residual connections. For the *layer norm*, we will not go into details here, but we can compare its usefulness to a *batch norm* layer (more details in this [blogpost](https://medium.com/@hunter-j-phillips/layer-normalization-e9ae93eb3c9c)). We will therefore simply use the PyTorch implementation of [layer norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
    "\n",
    "Here is the Python implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Block transformer\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # x+ car c'est une connexion résiduelle\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We apply the *layer norm* before the layers (unlike in the paper). This is the only part of the *transformer* that has been modified since the paper's publication, and it improves performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity, we will create our model and optimize it in the next notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
