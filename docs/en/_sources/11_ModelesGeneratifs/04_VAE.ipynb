{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we introduce variational autoencoders, also known as *variational autoencoders* (VAE). We start with a quick recap of [Course 4 on Autoencoders](../04_Autoencodeurs/README.md), then we introduce the use of VAEs as generative models. This course is inspired by a [blog post](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) and does not delve into the mathematical details of how VAEs work.\n",
    "The figures used in this notebook also come from the [blog post](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a neural network shaped like an hourglass. It consists of an encoder that encodes information into a reduced-dimensional latent space and a decoder that reconstructs the original data from the latent representation.\n",
    "![AE](./images/AEbase.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders can be used for many things, but their primary role is data compression. It is a compression method that uses gradient descent optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that the latent space of our decoder is regular (represented by a known probability distribution). In this case, we could *sample* a random element from this distribution to generate new data. In practice, in a classic autoencoder, the latent representation is not regular, making it impossible to use for generating data.\n",
    "Thinking about it, this makes sense. The *loss* function of the autoencoder is based solely on the quality of the reconstruction and imposes no constraints on the shape of the latent space.\n",
    "Therefore, we would like to impose the shape of the latent space of our autoencoder to generate new data from this latent space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *variational autoencoder* (VAE) is an autoencoder constrained to have a latent space that allows data generation. It has regulated training for this purpose.\n",
    "The idea is to encode our *input* into a data distribution instead of a single value (as in an AE). In practice, our encoder will predict two values representing a normal distribution: the mean $\\mu$ and the variance $\\sigma^2$.\n",
    "The VAE works as follows during training:\n",
    "- The encoder encodes the *input* into a probability distribution by predicting $\\mu$ and $\\sigma^2$.\n",
    "- A value is *sampled* from the Gaussian distribution described by $\\mu$ and $\\sigma^2$.\n",
    "- The decoder reconstructs the data from the *sampled* value.\n",
    "- Backpropagation is applied to update the weights.\n",
    "![VAE](./images/VAE.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the training does what we want, we need to add a term to the *loss* function: the [Kullback-Leibler divergence](https://fr.wikipedia.org/wiki/Divergence_de_Kullback-Leibler). This term helps push the distribution to be a standard normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For consistent data generation, there are two things to consider:\n",
    "- Continuity: Points close in the latent space will produce close data in the output space.\n",
    "- Completeness: Decoded points must make sense in the output space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler divergence ensures these two properties. If we only used the reconstruction *loss*, the VAE could behave like an AE by predicting almost zero variances (which would be almost equivalent to a point, as predicted by an AE's encoder).\n",
    "The Kullback-Leibler divergence encourages the distributions in the latent space to be close to each other. This allows for consistent data generation when *sampling*.\n",
    "![Regularization](./images/regu.png)\n",
    "**Note**: There is an important theoretical aspect behind *Variational Autoencoders*, but we will not delve into the details in this course. To learn more, you can refer to Stanford's CS236 course and in particular this [link](https://deepgenerativemodels.github.io/notes/vae/).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
