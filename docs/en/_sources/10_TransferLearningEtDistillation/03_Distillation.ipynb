{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of knowledge distillation was introduced in the 2015 paper *Distilling the Knowledge in a Neural Network*. The idea is to use a model called the *teacher* (a pre-trained deep model) to transfer its knowledge to a smaller model called the *student*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the *student* model is trained on two objectives:\n",
    "- Minimizing the distance between its prediction and the *teacher*'s prediction for the same item.\n",
    "- Minimizing the distance between its prediction and the input label.\n",
    "\n",
    "These two losses are combined with a weighting factor $\\alpha$ that can be chosen. Thus, the *student* model uses both the image label and the *teacher*'s prediction (a probability distribution).\n",
    "\n",
    "**Note**: In practice, for the first part of the loss, the logits are compared before applying the *softmax* function rather than the probabilities. For clarity, we will use the term \"predictions\" instead of \"logits\".\n",
    "\n",
    "![distill](./images/distill.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why It Works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might wonder why this method works better than directly training the *student* with a classic prediction/label loss. Several reasons explain this:\n",
    "- **Transfer of Implicit Knowledge**: Using the *teacher*'s predictions allows the *student* to learn implicit knowledge about the data. The *teacher*'s prediction is a probability distribution that indicates the similarity between several classes, for example.\n",
    "- **Preservation of Complex Relationships**: The *teacher* is very complex and can capture complex structures in the data, which may not be the case for a smaller model trained from scratch. Distillation allows the *student* to learn these complex relationships more easily, while improving speed and reducing memory usage (as it is a smaller model).\n",
    "- **Training Stabilization**: In practice, training is more stable for the *student* with this distillation method.\n",
    "- **Mitigation of Annotation Issues**: The *teacher* has learned to generalize and can predict correctly, even if it was trained on images with incorrect labels. In the context of distillation, the significant difference between the *teacher*'s output and the label provides additional information to the *student* about the data quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is possible to transfer the knowledge from a high-performing model to a smaller one without significant loss of prediction quality. This is very useful for reducing model size, for example, for embedded applications or CPU processing. It is also possible to distill multiple *teachers* into a single *student*. In some cases, the *student* can even outperform each *teacher* individually.\n",
    "\n",
    "This is a useful technique to know for many situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its invention, knowledge distillation has been adapted to solve various problems. We present two examples here: improving classification with *NoisyStudent* and unsupervised anomaly detection with *STPM*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Student: Improving Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a long time, the race for performance on the *ImageNet* dataset was at the heart of deep learning research. The goal was to constantly improve performance on this dataset. In 2020, the paper *Self-training with Noisy Student improves ImageNet classification* proposes using distillation to train a *student* model that is more performant than the *teacher* at each iteration.\n",
    "\n",
    "A *student* model is trained from *pseudo-labels* generated by a *teacher* model (labels created by the *teacher* on unannotated images). During training, noise is added to increase its robustness. Once the *student* is trained, it is used to obtain new *pseudo-labels* and train another *student*. The process is repeated several times, resulting in a model that is far more performant than the original *teacher*.\n",
    "\n",
    "![noisystudent](./images/noisystudent.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STPM: Unsupervised Anomaly Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting application of knowledge distillation is unsupervised anomaly detection. The paper *Student-Teacher Feature Pyramid Matching for Anomaly Detection* adapts this technique for this use case.\n",
    "\n",
    "In this case, the *teacher* and *student* models have the same architecture. Instead of focusing on predictions, we are interested in the *feature maps* of the intermediate layers of the network. During training, we have data without anomalies. The *teacher* model is pre-trained on *ImageNet* (for example) and is frozen during training. The *student* model is randomly initialized and is the one we train. Specifically, we train it to reproduce the *feature maps* of the *teacher* on defect-free data. At the end of training, the *student* and *teacher* will have identical *feature maps* on a defect-free item.\n",
    "\n",
    "During the testing phase, the model is evaluated on defect-free data and data with defects. On defect-free data, the *student* perfectly mimics the *teacher*, while on defective data, the *feature maps* of the *student* and *teacher* differ. This allows calculating a similarity score, which serves as an anomaly score.\n",
    "\n",
    "![kdad](./images/kdad.png)\n",
    "\n",
    "In practice, this method is one of the most performant for unsupervised anomaly detection. This is the method we will implement in the following notebook.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
