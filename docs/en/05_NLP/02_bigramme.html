
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bigram &#8212; Deep Learning Course</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '05_NLP/02_bigramme';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Fully Connected Network" href="03_R%C3%A9seauFullyConnected.html" />
    <link rel="prev" title="Introduction to NLP" href="01_Introduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content"><span style="font-size:2em; font-weight:bold;">üöÄ Learn Deep Learning from scratch üöÄ</span></div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning Course</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Deep Learning Course üöÄ
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üßÆ Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html">Derivative and Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/02_R%C3%A9gressionLogistique.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîó Fully Connected Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html">My First Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html">Introduction to PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html">Advanced Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üñºÔ∏è Convolutional Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html">Convolutional Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/02_R%C3%A9seauConvolutif.html">Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/03_ConvImplementation.html">Implementing the Convolution Layer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html">Convolutional Networks with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html">Application on a color image dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html">Applying Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîÑ Autoencoders</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/01_IntuitionEtPremierAE.html">Introduction to Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/02_DenoisingAE.html">Autoencoder for Denoising</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üìù NLP</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_Introduction.html">Introduction to NLP</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Bigram</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_R%C3%A9seauFullyConnected.html">Fully Connected Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_WaveNet.html">PyTorch and WaveNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Rnn.html">Recurrent Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Lstm.html">Long Short-Term Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ó HuggingFace</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/01_introduction.html">Introduction to Hugging Face</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/02_ComputerVisionWithTransformers.html">Computer Vision with Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/03_NlpWithTransformers.html">Natural Language Processing with Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/04_AudioWithTransformers.html">Audio Processing with Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/05_ImageGenerationWithDiffusers.html">Image Generation with Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/06_DemoAvecGradio.html">Demo with Gradio</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚ö° Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/01_Introduction.html">Introduction to Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/02_GptFromScratch.html">Building a GPT from Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/03_TrainingOurGpt.html">Training our GPT model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html">Transformer architecture and features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/05_UtilisationsPossibles.html">Possible Applications of the Transformer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/06_VisionTransformerImplementation.html">Implementing the Vision Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/07_SwinTransformer.html">Swin Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéØ  Object Detection (YOLO)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/01_Introduction.html">Introduction to Object Detection in Images</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/02_YoloEnDetail.html">YOLO in Detail</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/03_Ultralytics.html">Ultralytics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîç Contrastive Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/01_FaceVerification.html">Facial Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/02_NonSupervis%C3%A9.html">Unsupervised Contrastive Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéì Transfer Learning and Distillation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/01_TransferLearning.html">Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/02_TransferLearningPytorch.html">Transfer Learning with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/03_Distillation.html">Knowledge Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/04_DistillationAnomalie.html">Knowledge Distillation for Unsupervised Anomaly Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/05_FineTuningLLM.html">Fine-Tuning of LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/06_FineTuningBertHF.html">Fine-tuning BERT with Hugging Face</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üé® Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/01_Introduction.html">Introduction to Generative Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/02_GAN.html">Generative Adversarial Networks (GANs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/03_GanImplementation.html">Implementing a GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/04_VAE.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/05_VaeImplementation.html">Implementing a VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/06_NormalizingFlows.html">Normalizing Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/07_DiffusionModels.html">Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/08_DiffusionImplementation.html">Implementing a Diffusion Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/qcm.html">Interactive Quiz</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéÅ Bonus ‚Äì Specific Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html">Activations and Initializations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_01_ActivationEtInitialisation.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html">Batch Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_02_BatchNorm.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html">Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_03_DataAugmentation.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html">Broadcasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_04_Broadcasting.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html">Understanding Different Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_05_Optimizer.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html">Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_06_Regularisation.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/07_ConnexionsResiduelles.html">Residual Connections</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_07_ConnexionsResiduelles.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html">Introduction to Cross-Validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_08_CrossValidation.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html">Model Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_09_MetriquesEvaluation.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html">Introduction to Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_10_Tokenization.html">Interactive Quiz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/11_Quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/qcm_11_Quantization.html">Interactive Quiz</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/edit/main/en/05_NLP/02_bigramme.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/issues/new?title=Issue%20on%20page%20%2F05_NLP/02_bigramme.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/05_NLP/02_bigramme.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bigram</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-analysis">Dataset Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-bigram">What is a Bigram?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-method">Counting Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#occurrence-matrix">Occurrence Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilities">Probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generation">Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">Model Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-or-likelihood">Maximum Likelihood or Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">Log-Likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-approach">Neural Network Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-with-the-counting-approach">Problem with the ‚ÄúCounting‚Äù Approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-for-our-neural-network">Dataset for Our Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#our-neural-network">Our Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-notes">Additional Notes</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bigram">
<h1>Bigram<a class="headerlink" href="#bigram" title="Link to this heading">#</a></h1>
<section id="dataset-analysis">
<h2>Dataset Analysis<a class="headerlink" href="#dataset-analysis" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;prenoms.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les 5 pr√©noms les plus populaires : &#39;</span><span class="p">,</span><span class="n">words</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les 5 pr√©noms les moins populaires : &#39;</span><span class="p">,</span><span class="n">words</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le pr√©nom le plus long : &#39;</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le pr√©nom le plus court : &#39;</span><span class="p">,</span><span class="nb">min</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les 5 pr√©noms les plus populaires :  [&#39;MARIE&#39;, &#39;JEAN&#39;, &#39;PIERRE&#39;, &#39;MICHEL&#39;, &#39;ANDR√â&#39;]
Les 5 pr√©noms les moins populaires :  [&#39;√âLOUEN&#39;, &#39;CHEYNA&#39;, &#39;BLONDIE&#39;, &#39;IMANN&#39;, &#39;GHILAIN&#39;]
Le pr√©nom le plus long :  GUILLAUME-ALEXANDRE
Le pr√©nom le plus court :  GUY
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unique_characters</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="c1"># Ajouter chaque caract√®re de la ligne √† l&#39;ensemble des caract√®res uniques</span>
  <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">word</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
    <span class="n">unique_characters</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Nombre de caract√®res uniques : &#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">unique_characters</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Caract√®res uniques : &#39;</span><span class="p">,</span><span class="n">unique_characters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nombre de caract√®res uniques :  45
Caract√®res uniques :  {&#39;√è&#39;, &#39;√ú&#39;, &#39;≈∏&#39;, &#39;U&#39;, &#39;√î&#39;, &#39;S&#39;, &#39;√Ü&#39;, &#39;√Ä&#39;, &#39;√à&#39;, &#39;-&#39;, &#39;W&#39;, &#39;H&#39;, &#39;√ä&#39;, &#39;√â&#39;, &#39;R&#39;, &#39;M&#39;, &#39;E&#39;, &#39;√ã&#39;, &#39;N&#39;, &#39;√é&#39;, &#39;X&#39;, &#39;√Ñ&#39;, &#39;F&#39;, &#39;√Ç&#39;, &#39;K&#39;, &#39;D&#39;, &#39;√ñ&#39;, &#39;I&#39;, &#39;J&#39;, &#39;Y&#39;, &#39;A&#39;, &#39;C&#39;, &#39;O&#39;, &#39;√õ&#39;, &#39;√ô&#39;, &#39;B&#39;, &#39;Z&#39;, &#39;P&#39;, &#39;T&#39;, &quot;&#39;&quot;, &#39;Q&#39;, &#39;√á&#39;, &#39;G&#39;, &#39;L&#39;, &#39;V&#39;}
</pre></div>
</div>
</div>
</div>
</section>
<section id="what-is-a-bigram">
<h2>What is a Bigram?<a class="headerlink" href="#what-is-a-bigram" title="Link to this heading">#</a></h2>
<p>I remind you that the goal of the project is to predict the next character based on the previous ones. In the <strong>bigram</strong> model, we only rely on the previous character to predict the current one. This is the simplest version of this type of model.</p>
<p>Of course, to predict a name, we must start with nothing. To predict the first letter, we need to know the probability that a letter is the first one (and the same for the last letter). Therefore, we add a special character ‚Äò.‚Äô at the beginning and end of each word before constructing our bigrams.</p>
<p>In each name, we have several examples of bigrams (each is independent).
Let‚Äôs take the first name and look at the number of bigrams it contains:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
  <span class="n">bigram</span> <span class="o">=</span> <span class="p">(</span><span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">bigram</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;.&#39;, &#39;M&#39;)
(&#39;M&#39;, &#39;A&#39;)
(&#39;A&#39;, &#39;R&#39;)
(&#39;R&#39;, &#39;I&#39;)
(&#39;I&#39;, &#39;E&#39;)
(&#39;E&#39;, &#39;.&#39;)
</pre></div>
</div>
</div>
</div>
<p>The name ‚ÄúMarie‚Äù contains 6 bigrams.</p>
</section>
<section id="counting-method">
<h2>Counting Method<a class="headerlink" href="#counting-method" title="Link to this heading">#</a></h2>
<p>Let‚Äôs now create a Python dictionary that groups all the bigrams in the dataset and counts their occurrences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">bigram</span> <span class="o">=</span> <span class="p">(</span><span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span><span class="p">)</span>
    <span class="n">b</span><span class="p">[</span><span class="n">bigram</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">kv</span><span class="p">:</span> <span class="o">-</span><span class="n">kv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les 5 bigrammes les plus fr√©quents : &#39;</span><span class="p">,</span><span class="nb">sorted</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">kv</span><span class="p">:</span> <span class="o">-</span><span class="n">kv</span><span class="p">[</span><span class="mi">1</span><span class="p">])[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les 5 bigrammes les plus fr√©quents :  [((&#39;A&#39;, &#39;.&#39;), 7537), ((&#39;E&#39;, &#39;.&#39;), 6840), ((&#39;A&#39;, &#39;N&#39;), 6292), ((&#39;N&#39;, &#39;.&#39;), 3741), ((&#39;N&#39;, &#39;E&#39;), 3741)]
</pre></div>
</div>
</div>
</div>
<p>We now have our dictionary of bigram frequencies in the entire dataset. As we can see, names often end with A, E, or N, and the letters A and N often follow each other, as do the letters N and E.</p>
<section id="occurrence-matrix">
<h3>Occurrence Matrix<a class="headerlink" href="#occurrence-matrix" title="Link to this heading">#</a></h3>
<p>It is easier to visualize and process the data in matrix form. We will build a 46x46 matrix (45 characters + the special character ‚Äò.‚Äô) where the row corresponds to the first letter and the column to the second.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">46</span><span class="p">,</span> <span class="mi">46</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will sort our characters and create lookup tables using Python‚Äôs dictionary object. We want to be able to convert a character to an integer (for indexing in the matrix) and vice versa (to reconstruct names from integers).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">))))</span>
<span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">stoi</span><span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">itos</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="n">stoi</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>
</div>
</div>
</div>
<p>We will now fill our matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">N</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>And we can now display the matrix (lookup table).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Code pour dessiner une jolie matrice</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">46</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">46</span><span class="p">):</span>
    <span class="n">chstr</span> <span class="o">=</span> <span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">itos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">chstr</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;bottom&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/96ce12bf8300ffce2957c0bdc85dadbf28244c9c48c5c995ed2a013ee17708cf.png" src="../_images/96ce12bf8300ffce2957c0bdc85dadbf28244c9c48c5c995ed2a013ee17708cf.png" />
</div>
</div>
</section>
<section id="probabilities">
<h3>Probabilities<a class="headerlink" href="#probabilities" title="Link to this heading">#</a></h3>
<p>To know the probability that a name starts with a certain letter, we need to look at the row of the character ‚Äò.‚Äô, i.e., row 0, and normalize each value by the sum of the values in that row (to obtain values between 0 and 1 whose sum is equal to 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Compte de la premi√®re ligne : &quot;</span><span class="p">,</span><span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probabilit√©s : &quot;</span><span class="p">,</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compte de la premi√®re ligne :  tensor([   0,    0,    0, 3399,  825, 1483, 1208, 1400,  864,  907, 1039,  788,
        1352, 1503, 2108, 3606, 1501,  546,  620,   32, 1142, 2539, 1185,   72,
         329,  294,   29,  661,  393,    0,    2,    0,    0,    1,    2,  161,
           0,    0,    2,    2,    0,    5,    0,    0,    0,    0],
       dtype=torch.int32)
Probabilit√©s :  tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1330e-01, 2.7500e-02, 4.9433e-02,
        4.0267e-02, 4.6667e-02, 2.8800e-02, 3.0233e-02, 3.4633e-02, 2.6267e-02,
        4.5067e-02, 5.0100e-02, 7.0267e-02, 1.2020e-01, 5.0033e-02, 1.8200e-02,
        2.0667e-02, 1.0667e-03, 3.8067e-02, 8.4633e-02, 3.9500e-02, 2.4000e-03,
        1.0967e-02, 9.8000e-03, 9.6667e-04, 2.2033e-02, 1.3100e-02, 0.0000e+00,
        6.6667e-05, 0.0000e+00, 0.0000e+00, 3.3333e-05, 6.6667e-05, 5.3667e-03,
        0.0000e+00, 0.0000e+00, 6.6667e-05, 6.6667e-05, 0.0000e+00, 1.6667e-04,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])
</pre></div>
</div>
</div>
</div>
<p>To generate names randomly, we do not always want to choose the most probable letter (as we would always generate the same name). We want to choose a letter based on its probability. If the letter ‚Äòn‚Äô has a probability of 0.1, we want to choose it 10% of the time.
For this, we use the <code class="docutils literal notranslate"><span class="pre">torch.multinomial</span></code> function from PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">itos</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Z&#39;
</pre></div>
</div>
</div>
</div>
<p>With each call, we get a different letter based on its probability of appearing in our test dataset.</p>
<p>With all these elements, we are now ready to generate names from our matrix N. Ideally, we would create a matrix with the probabilities directly to avoid renormalizing each time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># On copie N et on la convertit en float</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">N</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="c1"># On normalise chaque ligne</span>
<span class="c1"># On somme sur la premi√®re dimension (les colonnes)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Somme des lignes : &quot;</span><span class="p">,</span><span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">P</span> <span class="o">/=</span> <span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># /= est un raccourci pour P = P / P.sum(1, keepdims=True)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matrice normalis√©e P est de taille : &quot;</span><span class="p">,</span><span class="n">P</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># On v√©rifie que la somme d&#39;une ligne est √©gale √† 1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Somme de la premi√®re ligne de P : &quot;</span><span class="p">,</span><span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Somme des lignes :  torch.Size([46, 1])
Matrice normalis√©e P est de taille :  torch.Size([46, 46])
Somme de la premi√®re ligne de P :  1.0
</pre></div>
</div>
</div>
</div>
<p><strong>Note on dividing matrices of different sizes</strong>: As you have noticed, we divide a matrix of size (46,46) by a matrix of size (46,1), which seems impossible. With PyTorch, there are <a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html">broadcasting rules</a>. I highly recommend familiarizing yourself with this concept, as it is a common source of errors. To understand the broadcasting rules in detail, you can consult the <a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html"><span class="std std-doc">bonus course</span></a>.
In practice, dividing a matrix of size (46,46) by a matrix of size (46,1) will ‚Äúbroadcast‚Äù the matrix (46,1) to (46,46) by copying the base matrix 46 times. This allows the operation to be performed as desired.</p>
</section>
<section id="generation">
<h3>Generation<a class="headerlink" href="#generation" title="Link to this heading">#</a></h3>
<p>It is finally time to generate names with our bigram method!
We will define a function to generate names:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">genName</span><span class="p">():</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># On commence par &#39;.&#39;</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span> <span class="c1"># Tant qu&#39;on n&#39;a pas g√©n√©r√© le caract√®re &#39;.&#39;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="c1"># On r√©cup√®re la distribution de probabilit√© de la ligne correspondant au caract√®re actuel</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># On tire un √©chantillon</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">itos</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span> <span class="c1"># On ajoute le caract√®re √† notre pr√©nom</span>
    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>
  <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">genName</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;MARAUSUR.&#39;
</pre></div>
</div>
</div>
</div>
<p>For example, we can generate 10 random names:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">genName</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DA.
TYEYSE-SSCL.
DE.
ANINEDANDVI.
SOKE.
RENNA.
FUXA.
EROA.
FA.
KALEN.
</pre></div>
</div>
</div>
</div>
<p>As you can see, the generation is quite poor‚Ä¶
Why? Because the bigram is a very limited method. Relying only on the last character does not provide enough knowledge to generate correct names.</p>
</section>
</section>
<section id="model-evaluation">
<h2>Model Evaluation<a class="headerlink" href="#model-evaluation" title="Link to this heading">#</a></h2>
<section id="maximum-likelihood-or-likelihood">
<h3>Maximum Likelihood or Likelihood<a class="headerlink" href="#maximum-likelihood-or-likelihood" title="Link to this heading">#</a></h3>
<p>We now want to evaluate our model on the training set. For this, we use the maximum likelihood, as in <a class="reference internal" href="../01_Fondations/02_R%C3%A9gressionLogistique.html"><span class="std std-doc">the second notebook of course 1</span></a>.
The maximum likelihood or <em>likelihood</em> is a measure corresponding to the product of the probabilities of the events. To have a good model, we seek to maximize the <em>likelihood</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">productOfProbs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">2</span><span class="p">]:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="n">productOfProbs</span> <span class="o">*=</span> <span class="n">prob</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;La probabilit√© de </span><span class="si">{</span><span class="n">ch1</span><span class="si">}</span><span class="s2">-&gt;</span><span class="si">{</span><span class="n">ch2</span><span class="si">}</span><span class="s2"> est </span><span class="si">{</span><span class="n">prob</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Le produit des probabilit√©s est : &quot;</span><span class="p">,</span><span class="n">productOfProbs</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La probabilit√© de .-&gt;M est 0.120
La probabilit√© de M-&gt;A est 0.431
La probabilit√© de A-&gt;R est 0.084
La probabilit√© de R-&gt;I est 0.256
La probabilit√© de I-&gt;E est 0.119
La probabilit√© de E-&gt;. est 0.321
La probabilit√© de .-&gt;J est 0.045
La probabilit√© de J-&gt;E est 0.232
La probabilit√© de E-&gt;A est 0.024
La probabilit√© de A-&gt;N est 0.201
La probabilit√© de N-&gt;. est 0.212
Le produit des probabilit√©s est :  4.520583629652464e-10
</pre></div>
</div>
</div>
</div>
<p>We quickly see that multiplying probabilities is problematic. Here, we multiply them over 2 of the 30,000 elements of the dataset and get a very small value. If we multiply them over the entire dataset, we get a value that cannot be represented by a computer.</p>
</section>
<section id="log-likelihood">
<h3>Log-Likelihood<a class="headerlink" href="#log-likelihood" title="Link to this heading">#</a></h3>
<p>To solve this precision problem, we use the logarithm for several reasons:</p>
<ul class="simple">
<li><p>The log function is monotonic, meaning that if <span class="math notranslate nohighlight">\(a &gt; b\)</span>, then <span class="math notranslate nohighlight">\(log(a) &gt; log(b)\)</span>. Maximizing the <em>log-likelihood</em> is equivalent to maximizing the <em>likelihood</em> in an optimization context.</p></li>
<li><p>An interesting property of logarithms (which explains why this function is often used in optimization and probability) is the following rule: <span class="math notranslate nohighlight">\(log(a \times b) = log(a) + log(b)\)</span>. This allows us to avoid multiplying small values that could exceed the precision of a computer.</p></li>
</ul>
<p>We can therefore maximize the <em>log-likelihood</em> rather than the <em>likelihood</em>. Let‚Äôs take the previous loop and see what this gives:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sumOfLogs</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">2</span><span class="p">]:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="n">sumOfLogs</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La somme des log est : &quot;</span><span class="p">,</span><span class="n">sumOfLogs</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La somme des log est :  -21.517210006713867
</pre></div>
</div>
</div>
</div>
<p>We get a much more reasonable value. For optimization problems, we often prefer to have a function to minimize. In the case of a perfect model, each probability is 1, so each log is 0, and the sum of the logs is 0. Otherwise, we get negative values, because a probability is always less than 1 and <span class="math notranslate nohighlight">\(log(a) &lt; 0 \text{ if } a &lt; 1\)</span>.
To have a minimization problem, we use the <em>negative log-likelihood</em>, which simply corresponds to the opposite of the <em>log-likelihood</em>.</p>
<p>Often, we take the mean rather than the sum, as it is more readable and equivalent in terms of optimization. And we will calculate it over all the names in the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sumOfLogs</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">n</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="n">sumOfLogs</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="n">n</span><span class="o">+=</span><span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La somme des negative log est : &quot;</span><span class="p">,</span><span class="n">sumOfLogs</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La moyenne des negative log est : &quot;</span><span class="p">,</span><span class="n">sumOfLogs</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La somme des negative log est :  564925.125
La moyenne des negative log est :  2.4960792002651053
</pre></div>
</div>
</div>
</div>
<p>The <em>negative log-likelihood</em> of the dataset is therefore 2.49.
You can also see if your name is common or uncommon compared to the dataset average. To do this, simply replace my name ‚ÄúSIMON‚Äù with yours (in uppercase).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sumOfLogs</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">n</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="s2">&quot;SIMON&quot;</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="n">sumOfLogs</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="n">n</span><span class="o">+=</span><span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La moyenne des negative log est : &quot;</span><span class="p">,</span><span class="n">sumOfLogs</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La moyenne des negative log est :  2.598056602478027
</pre></div>
</div>
</div>
</div>
<p>If the value of the <em>negative log-likelihood</em> corresponding to your name is lower than that of the dataset, your name is quite common. Otherwise, it is rather uncommon.</p>
</section>
</section>
<section id="neural-network-approach">
<h2>Neural Network Approach<a class="headerlink" href="#neural-network-approach" title="Link to this heading">#</a></h2>
<section id="problem-with-the-counting-approach">
<h3>Problem with the ‚ÄúCounting‚Äù Approach<a class="headerlink" href="#problem-with-the-counting-approach" title="Link to this heading">#</a></h3>
<p>We will now try to solve the same problem in a different way. We solved this problem by simply counting the occurrences of bigrams and calculating the probability based on that. This method works for bigrams, but will not work for more complex things like N-grams.</p>
<p>Indeed, our lookup table is 46x46 in size for two characters. If we consider N characters (i.e., N-1 characters to predict the Nth), we immediately have many more possibilities. We can simply calculate that the table will be of size <span class="math notranslate nohighlight">\(46^N\)</span>. For N=4, this would give a table of size 4,477,456. Suffice it to say that for important context values (today‚Äôs models have a context of tens of thousands of tokens and there are more than 46 possibilities each time), this approach will not work at all.</p>
<p>This is why the neural network approach is very interesting. In the rest of the course, we will show how to solve the same problem using a neural network, which will give you an intuition about the network‚Äôs capabilities as the context increases.</p>
</section>
<section id="dataset-for-our-neural-network">
<h3>Dataset for Our Neural Network<a class="headerlink" href="#dataset-for-our-neural-network" title="Link to this heading">#</a></h3>
<p>Our neural network will receive a character as input and will have to predict the next character. As a loss function, we can use the <em>negative log-likelihood</em> function to try to get close to the value of the bigram by ‚Äúcounting‚Äù.</p>
<p>Let‚Äôs start by creating our training dataset. We take the loop for traversing bigrams from the previous part and this time, we index two lists: <code class="docutils literal notranslate"><span class="pre">xs</span></code> for the inputs and <code class="docutils literal notranslate"><span class="pre">ys</span></code> for the labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the training set of bigrams (x,y)</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span><span class="p">)</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>
    
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>. M
M A
A R
R I
I E
E .
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;valeurs d&#39;entr√©e : &quot;</span><span class="p">,</span><span class="n">xs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;valeurs de sortie : &quot;</span><span class="p">,</span><span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>valeurs d&#39;entr√©e :  tensor([ 0, 15,  3, 20, 11,  7])
valeurs de sortie :  tensor([15,  3, 20, 11,  7,  0])
</pre></div>
</div>
</div>
</div>
<p>For the input value 0, which corresponds to ‚Äò.‚Äô, we want to predict a label 15, which corresponds to ‚ÄòM‚Äô.</p>
<p>The problem with these lists is that they contain integers, and it is not possible to give an integer as input to a neural network. In the field of NLP, we often use <em>one-hot encoding</em>, which consists of converting an index into a vector of 0s with a 1 at the index position. The size of the vector corresponds to the number of possible classes, which is 46 here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="c1"># one-hot encoding</span>
<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">46</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># conversion en float pour le NN</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encodage one-hot des deux premiers caract√®res: &quot;</span><span class="p">,</span><span class="n">xenc</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encodage one-hot des deux premiers caract√®res:  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</pre></div>
</div>
</div>
</div>
<p>As you can see, we have a 1 at position 0 of the first vector and a 1 at position 15 of the second. These are the vectors that will serve as input to our neural network. We can visualize what these vectors look like to better understand what <em>one-hot encoding</em> does.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Les 5 premiers vecteurs one-hot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">xenc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x784579d81f10&gt;
</pre></div>
</div>
<img alt="../_images/ff5371c82572c1f90d9fa93adee2bc0e1c73e24631447002e6deaa6c08c003db.png" src="../_images/ff5371c82572c1f90d9fa93adee2bc0e1c73e24631447002e6deaa6c08c003db.png" />
</div>
</div>
</section>
<section id="our-neural-network">
<h3>Our Neural Network<a class="headerlink" href="#our-neural-network" title="Link to this heading">#</a></h3>
<p>We will now create our neural network. It will be an extremely simple neural network containing a single layer. For the layer size, we take a vector of size <span class="math notranslate nohighlight">\(n \times 46\)</span> as input, so the first dimension will be of size 46. In output, we want a probability distribution over all characters. Our network layer will therefore be of size <span class="math notranslate nohighlight">\(46 \times 46\)</span>.</p>
<p>Let‚Äôs start by initializing our layer with random values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># On met le param√®tre requires_grad √† True pour pouvoir optimiser la matrice par descente de gradient</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">46</span><span class="p">,</span> <span class="mi">46</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<p>The <em>forward</em> of our neural network will simply consist of a matrix multiplication between the input and the layer. We will then apply the <em>softmax</em> function (see CNN course) to obtain a probability distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># One hot encoding sur les entr√©es</span>
<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">46</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> 
<span class="c1"># Multiplication matricielle (forward pass)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>  <span class="c1"># @ est la multiplication matricielle</span>
<span class="c1">#Softmax pour obtenir des probabilit√©s</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> 
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([6, 46])
</pre></div>
</div>
</div>
</div>
<p>We obtain a probability distribution for each of our 6 characters. We will visualize the outputs of our untrained neural network and calculate the <em>negative log-likelihood</em> to see where we stand compared to our model obtained by ‚Äúcounting‚Äù.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nlls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># index de l&#39;entr√©e</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># index du label</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--------&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bigramme actuel </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">itos</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="si">}{</span><span class="n">itos</span><span class="p">[</span><span class="n">y</span><span class="p">]</span><span class="si">}</span><span class="s1"> (indexes </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;entr√©e du r√©seau de neurones :&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sortie du r√©seau (probabilit√©) :&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vrai label :&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;probabilit√© donn√© par le r√©seau sur le caract√®re r√©el :&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">logp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
  <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">logp</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;negative log likelihood:&#39;</span><span class="p">,</span> <span class="n">nll</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">nlls</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">nll</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=========&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;negative log likelihood moyen, i.e. loss =&#39;</span><span class="p">,</span> <span class="n">nlls</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--------
bigramme actuel 1: .M (indexes 0,15)
entr√©e du r√©seau de neurones : 0
sortie du r√©seau (probabilit√©) : tensor([0.0146, 0.0210, 0.0823, 0.0077, 0.0160, 0.0483, 0.0943, 0.0204, 0.0079,
        0.0112, 0.0085, 0.0179, 0.0188, 0.0292, 0.0022, 0.0092, 0.0200, 0.0094,
        0.0097, 0.0191, 0.1091, 0.0122, 0.0092, 0.0287, 0.0120, 0.0088, 0.0053,
        0.0217, 0.0177, 0.0050, 0.0038, 0.0483, 0.0320, 0.0441, 0.0105, 0.0126,
        0.0266, 0.0092, 0.0262, 0.0081, 0.0430, 0.0012, 0.0102, 0.0025, 0.0126,
        0.0116], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 15
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.009214116260409355
negative log likelihood: 4.687018394470215
--------
bigramme actuel 2: MA (indexes 15,3)
entr√©e du r√©seau de neurones : 15
sortie du r√©seau (probabilit√©) : tensor([0.0574, 0.1353, 0.0227, 0.0032, 0.1142, 0.0148, 0.1007, 0.0162, 0.0242,
        0.0089, 0.0040, 0.0459, 0.0023, 0.0081, 0.0064, 0.0124, 0.0083, 0.0112,
        0.0172, 0.0062, 0.0033, 0.0045, 0.0131, 0.0144, 0.0218, 0.0080, 0.0225,
        0.0097, 0.0164, 0.0074, 0.0165, 0.0091, 0.0412, 0.0087, 0.0100, 0.0039,
        0.0080, 0.0036, 0.0377, 0.0150, 0.0345, 0.0048, 0.0253, 0.0036, 0.0164,
        0.0210], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 3
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.0031920599285513163
negative log likelihood: 5.74708890914917
--------
bigramme actuel 3: AR (indexes 3,20)
entr√©e du r√©seau de neurones : 3
sortie du r√©seau (probabilit√©) : tensor([0.0199, 0.0169, 0.0239, 0.0122, 0.0174, 0.0203, 0.0043, 0.0822, 0.0517,
        0.0228, 0.0118, 0.0121, 0.0210, 0.0088, 0.0063, 0.0128, 0.1041, 0.0100,
        0.0338, 0.0772, 0.0056, 0.0565, 0.0134, 0.0032, 0.0253, 0.0120, 0.0337,
        0.0080, 0.0083, 0.0060, 0.0068, 0.0020, 0.0405, 0.0120, 0.0366, 0.0080,
        0.0111, 0.0135, 0.0164, 0.0038, 0.0133, 0.0029, 0.0094, 0.0047, 0.0504,
        0.0271], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 20
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.005596297327429056
negative log likelihood: 5.185649871826172
--------
bigramme actuel 4: RI (indexes 20,11)
entr√©e du r√©seau de neurones : 20
sortie du r√©seau (probabilit√©) : tensor([0.0030, 0.0300, 0.0056, 0.0311, 0.0361, 0.0294, 0.0462, 0.0163, 0.0369,
        0.0178, 0.0251, 0.0125, 0.0162, 0.0019, 0.0828, 0.0173, 0.0068, 0.0113,
        0.0204, 0.0124, 0.0653, 0.0059, 0.0038, 0.0075, 0.0165, 0.0332, 0.0065,
        0.0354, 0.0169, 0.0062, 0.0683, 0.0203, 0.0189, 0.0179, 0.0113, 0.0119,
        0.0549, 0.0035, 0.0051, 0.0061, 0.0569, 0.0268, 0.0164, 0.0021, 0.0146,
        0.0088], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 11
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.012452212162315845
negative log likelihood: 4.385857105255127
--------
bigramme actuel 5: IE (indexes 11,7)
entr√©e du r√©seau de neurones : 11
sortie du r√©seau (probabilit√©) : tensor([0.0265, 0.0211, 0.0312, 0.0235, 0.0020, 0.0151, 0.0145, 0.0083, 0.0141,
        0.0062, 0.0168, 0.0183, 0.0600, 0.0047, 0.0969, 0.0438, 0.0083, 0.0584,
        0.0572, 0.0061, 0.0159, 0.0475, 0.0079, 0.0116, 0.0331, 0.0043, 0.0049,
        0.0134, 0.0057, 0.0077, 0.0350, 0.0276, 0.0174, 0.0050, 0.0176, 0.0022,
        0.0169, 0.0029, 0.0281, 0.0115, 0.0291, 0.0250, 0.0071, 0.0126, 0.0277,
        0.0491], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 7
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.008320074528455734
negative log likelihood: 4.789083957672119
--------
bigramme actuel 6: E. (indexes 7,0)
entr√©e du r√©seau de neurones : 7
sortie du r√©seau (probabilit√©) : tensor([0.0397, 0.0266, 0.0185, 0.0024, 0.0054, 0.0061, 0.0143, 0.0269, 0.0398,
        0.0084, 0.0134, 0.0247, 0.1220, 0.0039, 0.0062, 0.0829, 0.0452, 0.0086,
        0.0062, 0.0130, 0.0106, 0.0137, 0.0073, 0.1132, 0.0146, 0.0252, 0.0112,
        0.0955, 0.0133, 0.0196, 0.0091, 0.0122, 0.0160, 0.0092, 0.0128, 0.0337,
        0.0058, 0.0112, 0.0070, 0.0029, 0.0033, 0.0073, 0.0052, 0.0049, 0.0125,
        0.0087], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 0
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.0397193469107151
negative log likelihood: 3.225916862487793
=========
negative log likelihood moyen, i.e. loss = 4.670102596282959
</pre></div>
</div>
</div>
</div>
<p>For the loss calculation, we will calculate the <em>negative log-likelihood</em> of the output of our network with respect to the label as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calcul de la loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="c1"># On remet les gradients √† z√©ro (None est plus efficace)</span>
<span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span> 
<span class="c1"># Calcul des gradients automatique de pytorch</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4.670102596282959
tensor([[0.0024, 0.0035, 0.0137,  ..., 0.0004, 0.0021, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
</pre></div>
</div>
</div>
</div>
<p>As you can see, we have calculated the gradients of our matrix W with respect to the <em>loss</em>. In the same way as in previous courses, we can update the model weights in the direction of the gradient with a step (the <em>learning_rate</em>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># avec un learning_rate de 0.1</span>
<span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="optimization">
<h3>Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h3>
<p>Based on everything we have just seen, we can now gather the elements and optimize our model.</p>
<p><strong>Creating the Complete Dataset</strong>
We will start by creating our complete dataset by taking the previous loop, but traversing all the names.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
<span class="n">num</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;number of examples: &#39;</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>number of examples:  226325
</pre></div>
</div>
</div>
</div>
<p><strong>Model Initialization</strong>
We can now initialize our model as before, choose the <em>learning_rate</em> and the number of iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">46</span><span class="p">,</span> <span class="mi">46</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lr</span><span class="o">=</span><span class="mi">50</span> <span class="c1"># en pratique, dans ce petit probl√®me, un learning rate de 50 fonctionne bien ce qui peut sembler √©tonnant</span>
<span class="n">iterations</span><span class="o">=</span><span class="mi">100</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Gradient Descent</strong>
Let‚Äôs now apply the gradient descent algorithm to our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Descente du gradient</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
  
  <span class="c1"># forward pass</span>
  <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">46</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># transformation one hot sur les entr√©es</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
  <span class="n">probs</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># On applique le softmax</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># Calcul du negative log likelihood (loss)</span>
  <span class="k">if</span> <span class="n">k</span><span class="o">%</span><span class="k">10</span>==0:
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loss iteration &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39; : &#39;</span><span class="p">,</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  
  <span class="c1"># retropropagation</span>
  <span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># Remettre la gradient √† z√©ro √† chaque it√©ration (√† ne pas oublier !!!!)</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  
  <span class="c1"># Mise √† jour des poids</span>
  <span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">50</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss iteration 0 :  4.346113204956055
loss iteration 10 :  2.94492769241333
loss iteration 20 :  2.7590363025665283
loss iteration 30 :  2.6798315048217773
loss iteration 40 :  2.637108087539673
loss iteration 50 :  2.610524892807007
loss iteration 60 :  2.5923469066619873
loss iteration 70 :  2.5791807174682617
loss iteration 80 :  2.569261074066162
loss iteration 90 :  2.561541795730591
</pre></div>
</div>
</div>
</div>
<p>After 100 iterations, we get a <em>negative log-likelihood</em> close to that of the ‚Äúcounting‚Äù model. This is in fact the maximum capacity of the bigram model on the training data.</p>
<p><strong>Generating Names with Our Model</strong>
We can now generate names with our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">ix</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">46</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span> 
    <span class="c1"># Pr√©diction des probabilit√©s de la lettre suivante</span>
    <span class="n">p</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># On fait un tirage al√©atoire de la prochaine lettre en suivante la distribution p </span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="c1"># Conversion en lettre</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">itos</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>JE.
S.
ADJULA.
M.
LVERTY√úCI.
</pre></div>
</div>
</div>
</div>
</section>
<section id="additional-notes">
<h3>Additional Notes<a class="headerlink" href="#additional-notes" title="Link to this heading">#</a></h3>
<p>The weight matrix <span class="math notranslate nohighlight">\(W\)</span> is the same size as the matrix <span class="math notranslate nohighlight">\(N\)</span> used in the counting method. What we have just achieved with the neural network approach is in fact the learning of the matrix <span class="math notranslate nohighlight">\(N\)</span>.
We can confirm this intuition by looking at what happens when we perform the operation <code class="docutils literal notranslate"><span class="pre">xenc</span> <span class="pre">&#64;</span> <span class="pre">W</span></code>. This is a matrix multiplication of a row matrix of size <span class="math notranslate nohighlight">\(1 \times 46\)</span> by a square matrix of size <span class="math notranslate nohighlight">\(46 \times 46\)</span>. Moreover, the row matrix contains only zeros, except for a 1 at the index <span class="math notranslate nohighlight">\(i\)</span> of the letter. This matrix multiplication results in the row <span class="math notranslate nohighlight">\(i\)</span> of the matrix <span class="math notranslate nohighlight">\(W\)</span>.
This corresponds exactly to what we did in the counting method, where we retrieved the probabilities from row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(P\)</span>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./05_NLP"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_Introduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction to NLP</p>
      </div>
    </a>
    <a class="right-next"
       href="03_R%C3%A9seauFullyConnected.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Fully Connected Network</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-analysis">Dataset Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-bigram">What is a Bigram?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-method">Counting Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#occurrence-matrix">Occurrence Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilities">Probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generation">Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">Model Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-or-likelihood">Maximum Likelihood or Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">Log-Likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-approach">Neural Network Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-with-the-counting-approach">Problem with the ‚ÄúCounting‚Äù Approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-for-our-neural-network">Dataset for Our Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#our-neural-network">Our Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-notes">Additional Notes</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Simon Thomine
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div id="language-switcher" style="text-align: center; margin-top: 20px; padding: 10px; border-top: 1px solid #eee;">
  <span style="margin-right: 10px;">üåê Language / Langue:</span>
  <a href="#" onclick="switchToEnglish()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #4CAF50; color: white; border-radius: 5px; font-weight: bold; transition: all 0.3s;">üá∫üá∏ English</a>
  <a href="#" onclick="switchToFrench()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #f0f0f0; border-radius: 5px; transition: all 0.3s;">üá´üá∑ Fran√ßais</a>
  <a href="#" onclick="switchToSpanish()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #ffd700; border-radius: 5px; transition: all 0.3s;">üá™üá∏ Espa√±ol</a>
  <a href="#" onclick="switchToChinese()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #ff4b4b; color: white; border-radius: 5px; transition: all 0.3s;">üá®üá≥ ‰∏≠Êñá</a>
</div>
<script>
function getBaseUrl() {
  let baseUrl = window.location.origin;
  let pathname = window.location.pathname;
  if (pathname.includes('fr/')) {
    baseUrl += pathname.split('fr/')[0];
  } else if (pathname.includes('en/')) {
    baseUrl += pathname.split('en/')[0];
  } else if (pathname.includes('es/')) {
    baseUrl += pathname.split('es/')[0];
  } else if (pathname.includes('zh/')) {
    baseUrl += pathname.split('zh/')[0];
  } else {
    baseUrl += pathname.split('/').slice(0, -1).join('/') + '/';
  }
  return baseUrl;
}

function getCurrentPage() {
  let pathname = window.location.pathname;
  if (pathname.includes('fr/')) {
    return pathname.split('fr/')[1] || 'index.html';
  } else if (pathname.includes('en/')) {
    return pathname.split('en/')[1] || 'index.html';
  } else if (pathname.includes('es/')) {
    return pathname.split('es/')[1] || 'index.html';
  } else if (pathname.includes('zh/')) {
    return pathname.split('zh/')[1] || 'index.html';
  }
  return 'index.html';
}

function switchToEnglish() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'en/' + currentPage;
  window.location.href = newUrl;
}

function switchToFrench() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'fr/' + currentPage;
  window.location.href = newUrl;
}

function switchToSpanish() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'es/' + currentPage;
  window.location.href = newUrl;
}

function switchToChinese() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'zh/' + currentPage;
  window.location.href = newUrl;
}
</script>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>