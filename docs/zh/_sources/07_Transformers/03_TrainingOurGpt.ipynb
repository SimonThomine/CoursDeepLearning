{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练我们的 GPT 模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本笔记本基于前一笔记本的内容。强烈建议在开始本笔记本之前，先完成前一笔记本的学习。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "block_size = 128 # Longueur du contexte \n",
    "max_iters = 5000 # Nombre d'itérations d'entraînement\n",
    "eval_interval = 500 # Intervalle pour l'évaluation sur les données de validation\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128 # Dimension de la couche d'attention\n",
    "n_head = 4 # Nombre de head d'attention (pour la multi-head attention)\n",
    "n_layer = 3 # Nombre de couches d'attention\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour la validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "def get_batch(split):\n",
    "    # On genere un batch de données (sur train ou val)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    #On génére batch_size indice de début de séquence pris au hasard dans le dataset\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # On stocke dans notre tenseur torch\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # On met les sur le GPU si on en a un \n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "# Fonction pour estimer le loss plus précisement\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head 层、多头注意力层、前馈层与 Transformer 块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Ajout de dropout pour la regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head de taille head_size\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Couche pour Linear (voir schema) après concatenation\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # Dropout si besoin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 4*n_embd comme dans le papier\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GeLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Block transformer\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # x+ car c'est une connexion résiduelle\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在是时候实现我们自己的 GPT 模型了。总体来说，该模型由多个 *Transformer* 块组成，并使用 *嵌入矩阵* 将 *token* 转换为 *嵌入向量*。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Chaque token recupere son embedding à partir d'une look-up table (entrainable)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # Agencement de n_layer TransformerBlock de taille n_embed avec n_head heads.\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # Une manière optimile d'initialiser les poids (à regarder plus en détails si ça vous interesse)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx et targets sont des tenseurs d'entier de taille (B,T)\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        # Position embedding pour ajouter une information spatiale sur les éléments\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # Fonction pour générer du texte : très proche de celle utilisée dans le modèle bigramme du notebook précédent\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx contient des entiers dans un tenseur de taille (B, T), c'est le contexte actuel\n",
    "        for _ in range(max_new_tokens):\n",
    "            # On limite le contexte à block_size (maximum que le réseau peut prendre)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # On calcule la prédictions du prochain token\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # On sample depuis les probabilités obtenues avec le softmax\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # On ajouter l'élément sample à notre texte\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以开始训练模型了。训练时间会根据您的电脑性能以及设定的超参数而有所不同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres du modèle :  632149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4823, val loss 4.4809\n",
      "step 500: train loss 1.9838, val loss 2.0682\n",
      "step 1000: train loss 1.6177, val loss 1.7505\n",
      "step 1500: train loss 1.4865, val loss 1.6591\n",
      "step 2000: train loss 1.4180, val loss 1.6133\n",
      "step 2500: train loss 1.3637, val loss 1.5702\n",
      "step 3000: train loss 1.3256, val loss 1.5459\n",
      "step 3500: train loss 1.3022, val loss 1.5306\n",
      "step 4000: train loss 1.2784, val loss 1.5076\n",
      "step 4500: train loss 1.2622, val loss 1.4943\n",
      "step 4999: train loss 1.2467, val loss 1.4835\n"
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(\"Nombre de paramètres du modèle : \",sum(p.numel() for p in m.parameters()))\n",
    "\n",
    "# Optimizer AdamW\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # On evalue le loss sur train et validation de temps en temps (tous les eval_interval itérations)\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # On recupère un batch aléatoire du dataset\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Calcul du loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以自动生成受莫里哀风格启发的文本了！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FMEROT.\n",
      "\n",
      "Il a rien qu'il mieux a plus que me le sortir engage.\n",
      "\n",
      "DONE ELVIRE.\n",
      "\n",
      "Ah! ot, et vous bons aime aussi seule abuer?\n",
      "\n",
      "SGANARELLE.\n",
      "\n",
      "Vous plaissez. Vous savez non pas!\n",
      "\n",
      "LE MONCTAGNE.\n",
      "\n",
      "Vous êtes, cet autres que changus!\n",
      "\n",
      "DON JUAN.\n",
      "\n",
      "Ne sont point tout ce qui ne vous désie.\n",
      "\n",
      "VALÈRE.\n",
      "\n",
      "Une son méclai que expère\n",
      "Que je suis sous-moi qu'en le consie,\n",
      "Que je tiens peurté aun la régle, que je suis en me ducens mois.\n",
      "Au comoqu'on disant fait ce le vite avisenve\n",
      "Qui voir de cettte réjoustoisse dis, et \n"
     ]
    }
   ],
   "source": [
    "# Generation d'un texte par le modèle\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如您所见，生成的文本在某种程度上类似于莫里哀的风格。通过增加模型的参数数量，可以获得更好的效果，但处理时间也会更长，尤其是在没有 GPU 的情况下。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
