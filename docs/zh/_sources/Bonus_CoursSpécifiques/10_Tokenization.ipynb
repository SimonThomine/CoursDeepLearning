{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词元化介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语言模型（LLM）的关键组成部分是**词元化**。这是Transformer网络的第一步，将文本转换为整数序列。本课程大量参考了Andrej Karpathy的视频《Let's build the GPT Tokenizer》。\n",
    "\n",
    "在实现我们的GPT时，我们使用了一个非常简单的词元化器，它将每个字符编码为不同的整数。实际上，我们更倾向于编码字符块，即字符的组合。\n",
    "\n",
    "理解词元化器的工作原理对于理解语言模型的运作至关重要。\n",
    "\n",
    "课程结束时，我们将能够回答以下问题：\n",
    "- 为什么LLM难以拼写单词？\n",
    "- 为什么LLM难以执行简单的字符串操作（如反转字符串）？\n",
    "- 为什么LLM在英语上表现更好？\n",
    "- 为什么LLM在算术上表现不佳？\n",
    "- 为什么GPT-2在Python上表现不佳？\n",
    "- 为什么我的LLM在接收字符串\"<endoftext>\"时会立即停止？\n",
    "- 为什么LLM在提到SolidGoldMagiKarp时会崩溃？\n",
    "- 为什么使用YAML比使用JSON更有利于LLM？\n",
    "\n",
    "**注意**：词元化器是LLM的一个完全独立部分，它有自己的训练数据集，并且是以不同方式训练的。\n",
    "\n",
    "![Tokenizer](./images/tokenizer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2的词元化器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过[Tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2)网站来分析GPT-2的词元化过程，以了解可能存在的问题。GPT-2的词元化器拥有大约50,000个单词的词汇量，这意味着有50,000个不同的词元。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算术\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，如果我们来看算术部分，会发现数字可以以相当任意的方式分割成词元。\n",
    "例如：\n",
    "\n",
    "![Arithmetic](./images/arith.png)\n",
    "\n",
    "998是一个完整的词元，但9988被分成两个词元：99和88。\n",
    "可以轻松想象，对于LLM来说，计数变得复杂。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相同的单词，不同的词元\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于相同的单词，根据其书写方式，我们会得到不同的词元。\n",
    "例如：\n",
    "![Same1](./images/same1.png)\n",
    "![Same2](./images/same2.png)\n",
    "\n",
    "这四个相同的单词由不同的词元表示（词元198对应换行）。因此，模型必须学习这些词元几乎是相同的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他语言\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于相同的句子在不同的语言中，使用的词元数量不同：\n",
    "\n",
    "![Language](./images/langage.png)\n",
    "\n",
    "这是因为GPT-2的词元化器主要在英语数据上进行训练。\n",
    "实际上，这会降低模型在其他语言中的能力，因为信息上的上下文不再相同。可以插入比日语更长的英语文本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以观察词元化器如何处理Python代码：\n",
    "\n",
    "![Python](./images/python.png)\n",
    "\n",
    "每个缩进的空格都被计为一个词元。如果代码包含大量条件或循环，上下文会迅速增加，这使得模型性能不佳。\n",
    "\n",
    "**注意**：此缺陷在GPT的后续版本（3和4）中已修复，例如四个制表符的缩进是一个唯一的词元。\n",
    "\n",
    "![Python2](./images/python2.png)\n",
    "\n",
    "**注意2**：我们的代码编辑器的配置（Python缩进使用2或4个空格）也会影响词元化。\n",
    "\n",
    "**注意3**：专门用于代码的LLM也会有专门的词元化器，这提高了性能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建我们自己的词元化器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要创建我们自己的词元化器，首先看看如何将字符串转换为整数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个可能的方法是使用[Unicode](https://fr.wikipedia.org/wiki/Unicode)。这允许将每个字符转换为整数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 101, 32, 99, 111, 117, 114, 115, 32, 100, 101, 32, 100, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 101, 115, 116, 32, 103, 233, 110, 105, 97, 108]\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Ce cours de deep learning est génial !\"\n",
    "# ord() permet de récupérer le code unicode d'un caractère\n",
    "unicode=[ord(char) for char in sentence]\n",
    "print(unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，我们不能使用这个方法，原因有以下几点：\n",
    "- 目前有大约150,000个字符，这作为词汇量的大小太大了。\n",
    "- 每年都有定期更新，这将使基于Unicode的词元化器在一年后过时。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTF-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种可能性是使用UTF-8编码（16或32位也是可能的，但不太实际），它允许将Unicode编码为4到8位。这样做，我们的基本词汇量将是256。\n",
    "\n",
    "我们保留UTF-8的想法，但希望增加词汇量，因为256太小，会迫使LLM拥有巨大的上下文大小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66, 111, 110, 106, 111, 117, 114]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"Bonjour\"\n",
    "list(sentence.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字节对编码算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了增加我们的词汇量，我们使用字节对编码算法。\n",
    "该算法的工作原理很简单：以迭代方式找到最频繁的字节对，并将其替换为一个新的词元（这增加了词汇量1）。\n",
    "例如，考虑以下序列：\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "在第一次迭代中，我们发现“aa”是最频繁的对，因此将其替换为Z：\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "在第二次迭代中，我们将“ab”替换为Y：\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "最后，在第三次迭代中，我们可以将ZY替换为X：\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "这样，我们增加了词汇量，同时减少了序列的大小（因此减少了处理所需的上下文）。\n",
    "\n",
    "**注意**：训练数据的选择对词元化器至关重要。必须根据我们的目标来选择它们。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该算法的优点在于，我们可以根据需要应用多次，直到获得满意的上下文大小。\n",
    "\n",
    "**注意**：训练数据的选择对词元化器至关重要。必须根据我们的目标来选择它们。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字节对编码的应用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了说明字节对编码的使用，我们以一大段文本为例并计算对。为此，我们使用巴尔扎克《人间喜剧》第一卷的第一章。该文本来自[古腾堡](https://www.gutenberg.org/ebooks/41211)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Au milieu de la rue Saint-Denis, presque au coin de la rue du\n",
      "Petit-Lion, existait naguère une de ces maisons précieuses qui donnent\n",
      "aux historiens la facilité de reconstruire par analogie l'ancien Paris.\n",
      "Les murs menaçants de cette bicoque semblaient avoir été bariolés\n",
      "d'hiéroglyphes. Quel autre nom le flâneur pouvait-il donner aux X et aux\n",
      "V que traçaient sur la façade les pièces de bois transversales ou\n",
      "diagonales dessinées dans le badigeon par de petites lézardes\n",
      "parallèles? Évidemment, au passage de toutes les voitures, chacune de\n",
      "ces solives s'agitait dans sa mortaise. Ce vénérable édifice était\n",
      "surmonté d'un toit triangulaire dont aucun modèle ne se verra bientôt\n",
      "plus à Paris. Cette couverture, tordue par les intempéries du climat\n",
      "parisien, s'avançait de trois pieds sur la rue, autant pour garantir des\n",
      "eaux pluviales le seuil de la porte, que pour abriter le mur d'un\n",
      "grenier et sa lucarne sans appui. Ce dernier étage était construit en\n",
      "planches clouées l'une sur l'autre comme de\n",
      "[65, 117, 32, 109, 105, 108, 105, 101, 117, 32, 100, 101, 32, 108, 97, 32, 114, 117, 101, 32, 83, 97, 105, 110, 116, 45, 68, 101, 110, 105, 115, 44, 32, 112, 114, 101, 115, 113, 117, 101, 32, 97, 117, 32, 99, 111, 105, 110, 32, 100, 101, 32, 108, 97, 32, 114, 117, 101, 32, 100, 117, 10, 80, 101, 116, 105, 116, 45, 76, 105, 111, 110, 44, 32, 101, 120, 105, 115, 116, 97, 105, 116, 32, 110, 97, 103, 117, 195, 168, 114, 101, 32, 117, 110, 101, 32, 100, 101, 32, 99, 101, 115, 32, 109, 97, 105, 115, 111, 110, 115, 32, 112, 114, 195, 169, 99, 105, 101, 117, 115, 101, 115, 32, 113, 117, 105, 32, 100, 111, 110, 110, 101, 110, 116, 10, 97, 117, 120, 32, 104, 105, 115, 116, 111, 114, 105, 101, 110, 115, 32, 108, 97, 32, 102, 97, 99, 105, 108, 105, 116, 195, 169, 32, 100, 101, 32, 114, 101, 99, 111, 110, 115, 116, 114, 117, 105, 114, 101, 32, 112, 97, 114, 32, 97, 110, 97, 108, 111, 103, 105, 101, 32, 108, 39, 97, 110, 99, 105, 101, 110, 32, 80, 97, 114, 105, 115, 46, 10, 76, 101, 115, 32, 109, 117, 114, 115, 32, 109, 101, 110, 97, 195, 167, 97, 110, 116, 115, 32, 100, 101, 32, 99, 101, 116, 116, 101, 32, 98, 105, 99, 111, 113, 117, 101, 32, 115, 101, 109, 98, 108, 97, 105, 101, 110, 116, 32, 97, 118, 111, 105, 114, 32, 195, 169, 116, 195, 169, 32, 98, 97, 114, 105, 111, 108, 195, 169, 115, 10, 100, 39, 104, 105, 195, 169, 114, 111, 103, 108, 121, 112, 104, 101, 115, 46, 32, 81, 117, 101, 108, 32, 97, 117, 116, 114, 101, 32, 110, 111, 109, 32, 108, 101, 32, 102, 108, 195, 162, 110, 101, 117, 114, 32, 112, 111, 117, 118, 97, 105, 116, 45, 105, 108, 32, 100, 111, 110, 110, 101, 114, 32, 97, 117, 120, 32, 88, 32, 101, 116, 32, 97, 117, 120, 10, 86, 32, 113, 117, 101, 32, 116, 114, 97, 195, 167, 97, 105, 101, 110, 116, 32, 115, 117, 114, 32, 108, 97, 32, 102, 97, 195, 167, 97, 100, 101, 32, 108, 101, 115, 32, 112, 105, 195, 168, 99, 101, 115, 32, 100, 101, 32, 98, 111, 105, 115, 32, 116, 114, 97, 110, 115, 118, 101, 114, 115, 97, 108, 101, 115, 32, 111, 117, 10, 100, 105, 97, 103, 111, 110, 97, 108, 101, 115, 32, 100, 101, 115, 115, 105, 110, 195, 169, 101, 115, 32, 100, 97, 110, 115, 32, 108, 101, 32, 98, 97, 100, 105, 103, 101, 111, 110, 32, 112, 97, 114, 32, 100, 101, 32, 112, 101, 116, 105, 116, 101, 115, 32, 108, 195, 169, 122, 97, 114, 100, 101, 115, 10, 112, 97, 114, 97, 108, 108, 195, 168, 108, 101, 115, 63, 32, 195, 137, 118, 105, 100, 101, 109, 109, 101, 110, 116, 44, 32, 97, 117, 32, 112, 97, 115, 115, 97, 103, 101, 32, 100, 101, 32, 116, 111, 117, 116, 101, 115, 32, 108, 101, 115, 32, 118, 111, 105, 116, 117, 114, 101, 115, 44, 32, 99, 104, 97, 99, 117, 110, 101, 32, 100, 101, 10, 99, 101, 115, 32, 115, 111, 108, 105, 118, 101, 115, 32, 115, 39, 97, 103, 105, 116, 97, 105, 116, 32, 100, 97, 110, 115, 32, 115, 97, 32, 109, 111, 114, 116, 97, 105, 115, 101, 46, 32, 67, 101, 32, 118, 195, 169, 110, 195, 169, 114, 97, 98, 108, 101, 32, 195, 169, 100, 105, 102, 105, 99, 101, 32, 195, 169, 116, 97, 105, 116, 10, 115, 117, 114, 109, 111, 110, 116, 195, 169, 32, 100, 39, 117, 110, 32, 116, 111, 105, 116, 32, 116, 114, 105, 97, 110, 103, 117, 108, 97, 105, 114, 101, 32, 100, 111, 110, 116, 32, 97, 117, 99, 117, 110, 32, 109, 111, 100, 195, 168, 108, 101, 32, 110, 101, 32, 115, 101, 32, 118, 101, 114, 114, 97, 32, 98, 105, 101, 110, 116, 195, 180, 116, 10, 112, 108, 117, 115, 32, 195, 160, 32, 80, 97, 114, 105, 115, 46, 32, 67, 101, 116, 116, 101, 32, 99, 111, 117, 118, 101, 114, 116, 117, 114, 101, 44, 32, 116, 111, 114, 100, 117, 101, 32, 112, 97, 114, 32, 108, 101, 115, 32, 105, 110, 116, 101, 109, 112, 195, 169, 114, 105, 101, 115, 32, 100, 117, 32, 99, 108, 105, 109, 97, 116, 10, 112, 97, 114, 105, 115, 105, 101, 110, 44, 32, 115, 39, 97, 118, 97, 110, 195, 167, 97, 105, 116, 32, 100, 101, 32, 116, 114, 111, 105, 115, 32, 112, 105, 101, 100, 115, 32, 115, 117, 114, 32, 108, 97, 32, 114, 117, 101, 44, 32, 97, 117, 116, 97, 110, 116, 32, 112, 111, 117, 114, 32, 103, 97, 114, 97, 110, 116, 105, 114, 32, 100, 101, 115, 10, 101, 97, 117, 120, 32, 112, 108, 117, 118, 105, 97, 108, 101, 115, 32, 108, 101, 32, 115, 101, 117, 105, 108, 32, 100, 101, 32, 108, 97, 32, 112, 111, 114, 116, 101, 44, 32, 113, 117, 101, 32, 112, 111, 117, 114, 32, 97, 98, 114, 105, 116, 101, 114, 32, 108, 101, 32, 109, 117, 114, 32, 100, 39, 117, 110, 10, 103, 114, 101, 110, 105, 101, 114, 32, 101, 116, 32, 115, 97, 32, 108, 117, 99, 97, 114, 110, 101, 32, 115, 97, 110, 115, 32, 97, 112, 112, 117, 105, 46, 32, 67, 101, 32, 100, 101, 114, 110, 105, 101, 114, 32, 195, 169, 116, 97, 103, 101, 32, 195, 169, 116, 97, 105, 116, 32, 99, 111, 110, 115, 116, 114, 117, 105, 116, 32, 101, 110, 10, 112, 108, 97, 110, 99, 104, 101, 115, 32, 99, 108, 111, 117, 195, 169]\n"
     ]
    }
   ],
   "source": [
    "with open('balzac.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()\n",
    "print(text[:1000])\n",
    "\n",
    "tokens = list(map(int, text.encode('utf-8')))\n",
    "print(list(tokens[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在计算对：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 5 paires les plus fréquentes :  [(5025, (101, 32)), (2954, (115, 32)), (2429, (32, 100)), (2332, (116, 32)), (2192, (101, 115))]\n",
      "La paire la plus fréquente est :  (101, 32)\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): \n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(\"Les 5 paires les plus fréquentes : \",sorted(((v,k) for k,v in stats.items()), reverse=True)[:5])\n",
    "\n",
    "top_pair = max(stats, key=stats.get)\n",
    "print(\"La paire la plus fréquente est : \", top_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在定义一个函数来合并最频繁的对：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n",
      "taille du texte avant : 128987\n",
      "[65, 117, 32, 109, 105, 108, 105, 101, 117, 32, 100, 256, 108, 97, 32, 114, 117, 256, 83, 97, 105, 110, 116, 45, 68, 101, 110, 105, 115, 44, 32, 112, 114, 101, 115, 113, 117, 256, 97, 117, 32, 99, 111, 105, 110, 32, 100, 256, 108, 97, 32, 114, 117, 256, 100, 117, 10, 80, 101, 116, 105, 116, 45, 76, 105, 111, 110, 44, 32, 101, 120, 105, 115, 116, 97, 105, 116, 32, 110, 97, 103, 117, 195, 168, 114, 256, 117, 110, 256, 100, 256, 99, 101, 115, 32, 109, 97, 105, 115, 111]\n",
      "taille du texte après : 123962\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour fusionner les paires les plus fréquentes, on donne en entrée la liste des tokens, la paire à fusionner et le nouvel index\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # Si on est pas à la dernière position et que la paire correspond, on la remplace\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# Test de la fonction merge\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "\n",
    "print(\"taille du texte avant :\", len(tokens))\n",
    "# On fusionne la paire la plus fréquente et on lui donne un nouvel index (256 car on a déjà les caractères de 0 à 255)\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2[:100])\n",
    "print(\"taille du texte après :\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仅通过一次合并，我们已经大大减小了文本的编码大小。\n",
    "现在，我们将定义所需的词汇量大小，并根据需要合并多次！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (115, 32) into a new token 257\n",
      "merging (116, 32) into a new token 258\n",
      "merging (195, 169) into a new token 259\n",
      "merging (101, 110) into a new token 260\n",
      "merging (97, 105) into a new token 261\n",
      "merging (44, 32) into a new token 262\n",
      "merging (111, 110) into a new token 263\n",
      "merging (101, 257) into a new token 264\n",
      "merging (111, 117) into a new token 265\n",
      "merging (114, 32) into a new token 266\n",
      "merging (97, 110) into a new token 267\n",
      "merging (113, 117) into a new token 268\n",
      "merging (100, 256) into a new token 269\n",
      "merging (97, 32) into a new token 270\n",
      "merging (101, 117) into a new token 271\n",
      "merging (101, 115) into a new token 272\n",
      "merging (108, 256) into a new token 273\n",
      "merging (105, 110) into a new token 274\n",
      "merging (46, 32) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276 # La taille du vocabulaire que l'on souhaite\n",
    "num_merges = vocab_size - 256\n",
    "tokens_merged=tokens\n",
    "\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(tokens_merged)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  tokens_merged = merge(tokens_merged, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以看到两个词元序列之间的区别：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de base: 128987\n",
      "Taille après merge: 98587\n",
      "compression ratio: 1.31X\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille de base:\", len(tokens))\n",
    "print(\"Taille après merge:\", len(tokens_merged))\n",
    "print(f\"compression ratio: {len(tokens) / len(tokens_merged):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经压缩了序列的大小，同时仅增加了20个词汇量。\n",
    "GPT-2将词汇量增加到50,000，因此可以想象这大大减小了序列的大小。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码/编码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经构建了词元化器，我们希望能够在整数（词元）和文本之间相互转换。\n",
    "\n",
    "为此，首先构建解码函数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "# Fonction pour décoder les ids en texte, prend en entrée une liste d'entiers et retourne une chaine de caractères\n",
    "def decode(ids):\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\") # errors=\"replace\" permet de remplacer les caractères non reconnus par le caractére spécial �\n",
    "  return text\n",
    "\n",
    "print(decode([87]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以及编码函数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 263, 106, 265, 114]\n",
      "Bonjour\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour encoder le texte en ids, prend en entrée une chaine de caractères et retourne une liste d'entiers \n",
    "def encode(text):\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break \n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"Bonjour\"))\n",
    "\n",
    "# On eut véifier que l'encodage et le décodage fonctionne correctement\n",
    "print(decode(encode(\"Bonjour\")))\n",
    "\n",
    "# Et sur le text en entier\n",
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则表达式模式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT系列使用正则表达式模式来分隔文本以创建词汇表。这允许我们对生成的词元类型有更多的控制（例如，避免“狗”、“狗!”和“狗?”有不同的词元）。在Tiktoken（GPT的词元化器）的源代码中，我们可以找到以下模式：**'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+**。\n",
    "\n",
    "语法相当复杂，但我们将其分解以理解其作用：\n",
    "- **'s|'t|'re|'ve|'m|'ll|'d**：对应于英语的缩略形式，如“is”、“it”、“are”、“have”、“am”、“will”和“had”。这些词元在自然语言处理中通常需要被隔离。\n",
    "- **?\\p{L}+**：对应于由字母组成的单词。开头的“?”表示单词可以前面有一个空格，这允许捕获带或不带初始空格的单词。\n",
    "- **?\\p{N}+**：对应于数字序列（数字）。同样，一个可选的空格可以在数字序列之前。\n",
    "- **?[^\\s\\p{L}\\p{N}]+**：对应于一个或多个字符，这些字符既不是空格，也不是字母，也不是数字。这捕获了符号和标点符号，开头有一个可选的空格。\n",
    "- **\\s+(?!\\S)**：对应于一个或多个空格，后面只有空格（因此是字符串末尾或换行符之前的空格序列）。\n",
    "- **\\s+**：对应于一个或多个空格。这是一个通用的匹配，用于单词之间的多个空格。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本已根据正则表达式模式中描述的条件进行分隔。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特殊词元\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和微调时还会添加特殊词元：\n",
    "- **<|endoftext|>**：此词元用于在训练数据中分隔不同文档。\n",
    "- **<|im_start|>**和**<|im_end|>**：这些词元标记用户消息的开始和结束，例如聊天机器人。\n",
    "\n",
    "**注意**：在微调期间，可以添加特定于要执行的任务的词元（例如**<|im_start|>**和**<|im_end|>**）。当然，这将需要修改嵌入矩阵并重新训练它。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他类型的词元化器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们实现的词元化器基于OpenAI的[tiktoken](https://github.com/openai/tiktoken)，用于GPT模型。另一个常见的词元化器是[sentencepiece](https://github.com/google/sentencepiece)，用于Google和Meta的模型等。\n",
    "\n",
    "**注意**：Sentencepiece比tiktoken复杂得多，并且有许多参数需要设置。实际上，它可能是因为代码是开源的（而tiktoken的训练代码不是开源的，我们只能访问编码和解码的代码）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在其他模态上的词元化？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们想进行多模态处理（这目前很流行）时，我们需要从不同于文本的模态（如声音或图像）中生成词元。\n",
    "理想情况下，我们将将声音或图像转换为词元，并将其提供给Transformer，就好像它们是文本一样。\n",
    "\n",
    "对于图像，我们可以使用[VQVAE](https://arxiv.org/pdf/1711.00937)或[VQGAN](https://arxiv.org/pdf/2012.09841)。其思想是使用VAE或GAN在潜在空间中生成离散值。这些离散值然后被用作词元。\n",
    "\n",
    "![VQGAN](./images/VQGAN.png)\n",
    "\n",
    "图片来自[论文](https://arxiv.org/pdf/2012.09841)。\n",
    "\n",
    "OpenAI的SORA模型在视频上做了类似的事情：\n",
    "\n",
    "![SORA](./images/SORA.png)\n",
    "\n",
    "图片来自[论文](https://arxiv.org/pdf/2402.17177)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对开头问题的回答\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们将利用所学知识来回答课程开头提出的问题：\n",
    "- **为什么LLM难以拼写单词？**\n",
    "词元的分割使得每个单词不被分解成所有字符，而是分解成字符块。这样，模型难以将其分解。\n",
    "\n",
    "- **为什么LLM难以执行简单的字符串操作（如反转字符串）？**\n",
    "这与前一个问题的原因大致相同。要反转一个单词，仅反转表示该单词的词元是不够的。\n",
    "\n",
    "- **为什么LLM在英语上表现更好？**\n",
    "这有几个原因：Transformer的训练数据和词元化器的训练数据。对于Transformer，更多的英语数据使其能够更好地学习语言及其细微差别。对于词元化器，如果它在英语数据上进行训练，生成的词元将主要适用于英语单词，因此需要比其他语言更少的上下文。\n",
    "\n",
    "- **为什么LLM在算术上表现不佳？**\n",
    "数字根据训练数据以相当任意的方式表示。在这些词元上执行操作对LLM来说并不容易。\n",
    "\n",
    "- **为什么GPT-2在Python上表现不佳？**\n",
    "如前所述，GPT-2的词元化器将简单的空格转换为一个词元。在Python中，由于缩进和多个条件/循环，很快会有很多空格，这会显著影响上下文。\n",
    "\n",
    "- **为什么我的LLM在接收字符串\"<endoftext>\"时会立即停止？**\n",
    "这是一个在训练数据中添加的特殊词元，用于分隔文本。当LLM遇到它时，必须停止生成。\n",
    "\n",
    "- **为什么LLM在提到SolidGoldMagiKarp时会崩溃？**\n",
    "这个问题稍微不太明显，我建议阅读这篇[博客文章](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)。简单来说，如果单词出现在词元化器的训练数据中但不在LLM的训练数据中，那么该词元的嵌入将根本没有训练，LLM在遇到该词元时会表现出随机行为。SolidGoldMagiKarp是一个Reddit用户，应该经常出现在词元化器的训练数据中，但不在Transformer的训练数据中。\n",
    "\n",
    "- **为什么使用YAML比使用JSON更有利于LLM？**\n",
    "这与Python的情况类似。GPT-2的词元化器（以及大多数模型）将JSON文档转换为比其YAML等效项更多的词元。因此，从JSON转换为YAML会减少处理文档所需的上下文。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
