{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 长短期记忆网络（LSTM）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在之前的笔记本中，我们介绍了经典的循环神经网络（RNN）层。自其发明以来，已开发出多种其他循环层结构。\n",
    "\n",
    "这里，我们将介绍[长短期记忆网络（LSTM）](https://www.bioinf.jku.at/publications/older/2604.pdf)层，它是经典 RNN 层的一种替代方案。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是 LSTM 层？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 层由一个*记忆单元*和 4 个全连接层组成。其中三个层用于选择前一步骤中的相关信息：即*遗忘门*（forget gate）、*输入门*（input gate）和*输出门*（output gate）。\n",
    "\n",
    "- **遗忘门**：从记忆中删除信息\n",
    "- **输入门**：向记忆中插入信息\n",
    "- **输出门**：使用存储的信息\n",
    "\n",
    "第四个全连接层生成记忆单元的“候选信息”。\n",
    "\n",
    "![LSTM 架构图](./images/lstm.png)\n",
    "\n",
    "图片来源：[博客文章](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图所示，LSTM 层接收 3 个输入向量：$H_{t-1}$、$C_{t-1}$ 和 $X_{t}$。前两个直接来自 LSTM 本身，$X_{t}$ 是当前时刻 $t$ 的输入（在本例中是一个字符）。\n",
    "\n",
    "简单来说：\n",
    "- $H_{t-1}$ 包含短期记忆\n",
    "- $C_{t-1}$ 包含长期记忆\n",
    "\n",
    "这种结构能够在保留广泛上下文的重要信息的同时，不忽视局部上下文。\n",
    "\n",
    "LSTM 的核心思想是解决经典 RNN 在长序列中信息传播的问题。\n",
    "\n",
    "如需深入了解，可阅读[原始论文](https://www.bioinf.jku.at/publications/older/2604.pdf)或参考[博客文章](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为创建数据集，我们仍使用 `moliere.txt` 文件，并沿用前一个笔记本中的代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们减少了元素数量以加快训练（若需完整训练，可取消注释相关代码）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  100000\n"
     ]
    }
   ],
   "source": [
    "text=text[:100000]\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJLMNOPQRSTUVYabcdefghijlmnopqrstuvxyz«»ÇÈÉÊàâæçèéêîïôùû\n",
      "Nombre de caractères différents :  76\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集分为训练集和测试集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour le test\n",
    "train_data = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型时，我们直接使用 PyTorch 提供的 LSTM 层实现。与线性层或卷积层不同，[`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) 允许通过 `num_layers` 参数堆叠多个层。若需逐层定义，可使用 [`nn.LSTMCell`](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size,num_layers=1):\n",
    "        super(lstm, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # On utilise un embedding pour transformer les entiers(caractères) en vecteurs\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        # La couche LSTM peut prendre l'argument num_layers pour empiler plusieurs couches LSTM\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
    "        # Une dernière couche linéaire pour prédire le prochain caractère\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr=0.001\n",
    "hidden_dim=128\n",
    "seq_len=100\n",
    "num_layers=1\n",
    "model=lstm(vocab_size,hidden_dim,num_layers)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 层接收一个序列并输出相同长度的序列。这种方式加速了训练，因为可以同时处理多个示例。\n",
    "\n",
    "**注意**：还可以通过并行处理多个序列（即*批处理*）来进一步加速训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 2.17804336\n",
      "Epoch: 1 \t Loss: 1.76270216\n",
      "Epoch: 2 \t Loss: 1.62740668\n",
      "Epoch: 3 \t Loss: 1.54147145\n",
      "Epoch: 4 \t Loss: 1.47995140\n",
      "Epoch: 5 \t Loss: 1.43100239\n",
      "Epoch: 6 \t Loss: 1.39074463\n",
      "Epoch: 7 \t Loss: 1.35526441\n",
      "Epoch: 8 \t Loss: 1.32519794\n",
      "Epoch: 9 \t Loss: 1.29712536\n",
      "Epoch: 10 \t Loss: 1.27268774\n",
      "Epoch: 11 \t Loss: 1.24876227\n",
      "Epoch: 12 \t Loss: 1.22720749\n",
      "Epoch: 13 \t Loss: 1.20663312\n",
      "Epoch: 14 \t Loss: 1.18768359\n",
      "Epoch: 15 \t Loss: 1.16936996\n",
      "Epoch: 16 \t Loss: 1.15179397\n",
      "Epoch: 17 \t Loss: 1.13514291\n",
      "Epoch: 18 \t Loss: 1.11997525\n",
      "Epoch: 19 \t Loss: 1.10359089\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    state=None\n",
    "    running_loss = 0\n",
    "    n=0\n",
    "    data_ptr = torch.randint(100,(1,1)).item()\n",
    "    # On train sur des séquences de seq_len caractères et on break si on dépasse la taille du dataset\n",
    "    while True:\n",
    "        x = train_data[data_ptr : data_ptr+seq_len]\n",
    "        y = train_data[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        data_ptr+=seq_len\n",
    "        # Pour éviter de sortir de l'index du dataset\n",
    "        if data_ptr + seq_len + 1 > len(train_data):\n",
    "            break\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以在测试数据上评估损失值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss de test: 1.51168611\n"
     ]
    }
   ],
   "source": [
    "state=None\n",
    "running_loss = 0\n",
    "n=0\n",
    "data_ptr = torch.randint(100,(1,1)).item()\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        x = test[data_ptr : data_ptr+seq_len]\n",
    "        y = test[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "    running_loss += loss.item()\n",
    "    n+=1\n",
    "    data_ptr+=seq_len\n",
    "    if data_ptr + seq_len + 1 > len(test):\n",
    "        break\n",
    "print(\"Loss de test: {0:.8f}\".format(running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该模型存在较明显的过拟合现象... 请尝试自行解决。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本生成\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以测试文本生成功能！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Çà coeuse, et bon enfin l'avoir faire.\n",
      "\n",
      "MASCARILLE.\n",
      "\n",
      "En me donner d vous, Le pas.\n",
      "\n",
      "MASCARILLE, à dans un pour sûte matinix! cette ma foi.\n",
      "\n",
      "PANDOLFE.\n",
      "\n",
      "Ma foi, tu te le sy sois touves d'arrête sa bien sans les bonheur.\n",
      "\n",
      "MASCARILLE.\n",
      "\n",
      "Moi, je me suis to\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "moliere='.'\n",
    "sequence_length=250\n",
    "state=None\n",
    "for i in range(sequence_length):\n",
    "    x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()\n",
    "    y_pred,state = model.forward(x.unsqueeze(0),state)\n",
    "    probs=F.softmax(torch.squeeze(y_pred), dim=0)\n",
    "    sample=torch.multinomial(probs, 1)\n",
    "    moliere+=itos[sample.item()]\n",
    "print(moliere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成的文本比基础 RNN 模型略有改善，但尚不理想。您可以尝试通过调整参数（如层数、隐藏层维度等）来提升性能。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
