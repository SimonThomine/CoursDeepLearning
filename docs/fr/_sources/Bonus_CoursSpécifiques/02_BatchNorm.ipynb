{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La *Batch Normalization* (ou normalisation par lot) a été introduite en 2015 dans l'article [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167). Elle a eu un impact majeur dans le domaine du Deep Learning. Aujourd'hui, la normalisation est utilisée presque systématiquement, qu'il s'agisse de *BatchNorm*, *LayerNorm* ou *GroupNorm* (et d'autres).\n",
    "\n",
    "L'idée de la *BatchNorm* est simple et liée au notebook précédent. On cherche à obtenir des preactivations suivant une distribution gaussienne à chaque couche du réseau. On a vu qu'une bonne initialisation permet d'avoir ce comportement, mais elle n'est pas toujours évidente, surtout avec de nombreuses couches différentes.\n",
    "\n",
    "La *BatchNorm* normalise les preactivations par rapport à la dimension du *batch* avant de les passer dans les fonctions d'activation. Cela garantit une distribution gaussienne à chaque étape.\n",
    "\n",
    "Cette normalisation n'affecte pas l'optimisation car il s'agit d'une fonction dérivable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprise du code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va reprendre le code du notebook précédent pour implémenter la *batch normalization*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../05_NLP/prenoms.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180834, 3]) torch.Size([180834])\n",
      "torch.Size([22852, 3]) torch.Size([22852])\n",
      "torch.Size([22639, 3]) torch.Size([22639])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # Contexte\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] \n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=10 # Dimension de l'embedding de C\n",
    "hidden_dim=200 # Dimension de la couche cachée\n",
    "\n",
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici notre code de propagation avant : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après l'article, voici les informations :\n",
    "\n",
    "![Norm](./images/norm.png)\n",
    "\n",
    "Dans un premier temps, il s'agit de normaliser.\n",
    "\n",
    "Pour cela, on calcule la moyenne et l'écart type de *hpreact* puis on normalise avec ces valeurs :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-6\n",
    "hpreact_mean = hpreact.mean(dim=0, keepdim=True)\n",
    "hpreact_std= hpreact.std(dim=0, keepdim=True)\n",
    "hpreact_norm = (hpreact - hpreact_mean) / (hpreact_std+epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant intégrer cette normalisation à la propagation avant.\n",
    "\n",
    "Avant cela, notons qu'on n'a pas encore implémenté la partie *scale and shift* :\n",
    "\n",
    "![Scale and Shift](./images/scaleshift.png)\n",
    "\n",
    "**À quoi ça sert ?** : La normalisation confine les poids à prendre des valeurs d'une gaussienne centrée réduite. Cela réduit les capacités d'expression du modèle. Les paramètres apprenables $\\gamma$ et $\\beta$ permettent de contourner ce problème en ajoutant un *shift* avec $\\beta$ et un *scale* avec $\\gamma$.\n",
    "\n",
    "Comme il s'agit de paramètres apprenables, on doit aussi les ajouter aux paramètres du modèle :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et en propagation avant, on aura donc : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmean = hpreact.mean(0, keepdim=True)\n",
    "bnstd = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le problème de la Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En y réfléchissant un peu, on peut identifier des problèmes potentiels liés à la *BatchNorm* :\n",
    "\n",
    "**Un exemple est impacté par les autres éléments du *batch*** : Normaliser selon la dimension du *batch* signifie que les valeurs de chaque exemple au sein du *batch* sont influencées par les autres exemples. Cela pourrait sembler problématique, mais en pratique, c'est plutôt une bonne chose. L'utilisation de *batchs* aléatoires à chaque époque permet une régularisation, ce qui réduit le risque de *overfit* sur les données.\n",
    "Néanmoins, si on veut éviter ce problème, on peut utiliser d'autres méthodes de normalisation qui ne normalisent pas selon la dimension du *batch*. En pratique, la *BatchNorm* est encore largement utilisée car elle fonctionne très bien empiriquement.\n",
    "\n",
    "**Phase de test sur un seul élément** : Pendant l'entraînement, chaque élément est influencé par les autres éléments de son *batch*. Cependant, en phase d'inférence, lorsqu'on utilise le modèle sur un seul élément, on ne peut plus appliquer la *BatchNorm*. C'est un problème car on veut éviter un comportement différent pendant l'entraînement et l'inférence.\n",
    "\n",
    "Pour résoudre ce problème, on a deux solutions :\n",
    "- On peut calculer la moyenne et la variance sur l'ensemble des éléments à la fin de l'entraînement et utiliser ces valeurs. En pratique, on ne veut pas faire une itération supplémentaire sur l'ensemble du dataset juste pour ça, donc personne ne fait comme ça.\n",
    "- Une autre solution consiste à mettre à jour la moyenne et la variance tout au long de l'entraînement grâce à un EMA (*exponential moving average*). À la fin de l'entraînement, on aura une bonne approximation de la moyenne et de la variance de l'ensemble des éléments d'entraînement.\n",
    "\n",
    "En pratique, on peut l'implémenter comme ça en Python :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "bnmean_running = torch.zeros((1, hidden_dim))\n",
    "bnstd_running = torch.ones((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)\n",
    "bnstdi = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Dans notre implémentation, on a choisi 0.001 pour notre EMA. Dans la couche [*BatchNorm* de PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html), ce paramètre est défini par *momentum* et sa valeur par défaut est 0.1. En pratique, le choix de cette valeur dépend de la taille du *batch* par rapport à la taille du jeu de données d'entraînement. Pour un gros *batch* avec un petit jeu de données, on peut prendre 0.1, par exemple. Pour un petit *batch* avec un gros jeu de données, on prend plutôt une plus petite valeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons maintenant l'entraînement de notre modèle pour vérifier que la couche fonctionne. Pour ce petit modèle, on n'aura pas de différence de performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.8241\n",
      "  10000/ 200000: 1.9756\n",
      "  20000/ 200000: 2.7151\n",
      "  30000/ 200000: 2.3287\n",
      "  40000/ 200000: 2.1411\n",
      "  50000/ 200000: 2.3207\n",
      "  60000/ 200000: 2.3250\n",
      "  70000/ 200000: 2.0320\n",
      "  80000/ 200000: 2.0615\n",
      "  90000/ 200000: 2.2468\n",
      " 100000/ 200000: 2.2081\n",
      " 110000/ 200000: 2.1418\n",
      " 120000/ 200000: 1.9665\n",
      " 130000/ 200000: 1.8572\n",
      " 140000/ 200000: 2.0577\n",
      " 150000/ 200000: 2.1804\n",
      " 160000/ 200000: 1.8604\n",
      " 170000/ 200000: 1.9810\n",
      " 180000/ 200000: 1.8228\n",
      " 190000/ 200000: 1.9977\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "max_steps = 200000\n",
    "\n",
    "for i in range(max_steps):\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "  emb = C[Xb] \n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1 \n",
    "  \n",
    "  # Batch normalization\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "      bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "      bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    \n",
    "  h = torch.tanh(hpreact) \n",
    "  logits = h @ W2 + b2 \n",
    "  loss = F.cross_entropy(logits, Yb)\n",
    "  \n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  lr = 0.1 if i < 100000 else 0.01 \n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "  if i % 10000 == 0:\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considérations supplémentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Biais** : La *batch norm* normalise les preactivations des poids. Cette normalisation annule le biais (car celui-ci décale la distribution, alors que nous la recentrons). Lorsqu'on utilise la *BatchNorm*, on peut se passer du biais. En pratique, si on laisse un biais, ça ne pose pas de problème, mais c'est un paramètre du réseau qui sera inutile.\n",
    "\n",
    "**Placement de la BatchNorm** : D'après ce qu'on a vu, il est logique de placer la *BatchNorm* avant la fonction d'activation. En pratique, certains préfèrent la placer après la couche d'activation, donc ne soyez pas étonné si vous tombez sur ça dans la littérature ou dans un code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autres normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons faire un tour rapide des autres normalisations utilisées pour l'entraînement des réseaux de neurones.\n",
    "\n",
    "![Types of Normalization](./images/types.png)\n",
    "\n",
    "Figure extraite de l'[article](https://arxiv.org/pdf/1803.08494)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer Normalization** : Cette couche de normalisation est également très fréquemment utilisée, notamment dans les modèles de langage (GPT, Llama). Il s'agit de normaliser sur l'ensemble des activations de la couche plutôt que sur l'axe du *batch*. Dans notre implémentation, cela reviendrait simplement à changer :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)  \n",
    "bnstdi = hpreact.std(0, keepdim=True)   \n",
    "# Layer normalization\n",
    "bnmeani = hpreact.mean(1, keepdim=True)  \n",
    "bnstdi = hpreact.std(1, keepdim=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instance Normalization** : Cette couche normalise les activations sur chaque canal de chaque élément indépendamment.\n",
    "\n",
    "**Group Normalization** : Cette couche est une sorte de fusion entre la *LayerNorm* et l'*InstanceNorm*, puisqu'on calcule la normalisation sur des groupes de canaux (si la taille d'un groupe vaut 1, c'est l'*InstanceNorm* et si la taille d'un groupe vaut C, c'est la *LayerNorm*)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
