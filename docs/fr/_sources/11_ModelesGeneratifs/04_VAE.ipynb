{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, nous présentons les autoencodeurs variationnels ou *variational autoencoders* (VAE). Le cours commence par un rappel rapide du [cours 4 sur les autoencodeurs](../04_Autoencodeurs/README.md) puis introduit l'utilisation des VAE en tant que modèle génératif. Ce cours s'inspire du [blogpost](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) et n'entre pas dans les détails mathématiques du fonctionnement du VAE.\n",
    "Les figures utilisées dans ce notebook sont également extraite du [blogpost](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel autoencodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autoencodeur est un réseau de neurones en forme de sablier constitué d'en encodeur qui encode l'information dans un espace latent de dimension réduite et d'un décodeur qui reconstruit la donnée initiale à partir de la répresentation latente. \n",
    "\n",
    "![AE](./images/AEbase.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les autoencodeurs peuvent être utilisé pour de nombreuses choses mais leur rôle de base est la compression de données. C'est une méthode de compression de données utilisant l'optimisation par descente du gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut imaginer que si l'espace latent de notre décodeur est régulier (qu'il est representé par une distribution de probabilité connue), on pourrait *sample* un élément aléatoire de cette distribution pour générer une nouvelle donnée. En pratique, dans un autoencodeur classique, la représentation latente n'est pas du tout régulière et il est impossible de l'utiliser pour générer des données.  \n",
    "\n",
    "En y reflechissant, c'est tout à fait logique, la fonction de *loss* de l'autoencodeur se base uniquement sur la qualité de la reconstruction et n'impose aucune contrainte sur la forme de l'espace latent.\n",
    "\n",
    "On voudrait pouvoir imposer la forme de l'espace latent de notre autoencodeur pour générer des données nouvelles à partir de cet espace latent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *variational autoencoder* (VAE) peut être décrit comme un autoencodeur contraint à avoir un espace latent permettant la génération de données et qui a un entraînement régularisé dans cet objectif. \n",
    "\n",
    "L'idée va être d'encoder notre *input* en une distrubution de données au lieu d'une simple valeur (comme dans l'AE). En pratique, notre encodeur va prédire deux valeurs répresentant une distribution normale : la moyenne $\\mu$ et la variance $\\sigma^2$\n",
    "\n",
    "Le VAE fonctionne de la manière suivante lors de l'entraînement :\n",
    "- L'encodeur encode l'*input* en une distribution de probabilité en prédisant $\\mu$ et $\\sigma^2$.\n",
    "- Une valeur est *sample* de la distribution gaussienne décrite par $\\mu$ et $\\sigma^2$.\n",
    "- Le décodeur reconstruit la données à partir de la valeur *sampled*.\n",
    "- On applique la backpropagation pour mettre à jour les poids.\n",
    "\n",
    "![VAE](./images/VAE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour garantir que l'entraînement fait bien ce que l'on veut, il est nécessaire d'ajouter un terme à la fonction de *loss* : la [divergence de Kullback-Leibler](https://fr.wikipedia.org/wiki/Divergence_de_Kullback-Leibler). Ce terme va permettre de pousser la distribution à être une distribution centrée réduite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir une génération de données cohérentes, il y a deux choses à prendre en compte : \n",
    "- La continuité : Des points proches dans l'espace latent vont produire des données proches dans l'espace de sortie.\n",
    "- La complétude : Les points décodés doivent avoir du sens dans l'espace de sortie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La divergence de Kullback-Leibler va permettre de garantir ces deux propriétés. Si on se contentait du *loss* de reconstruction, le VAE pourrait se compoter comme un AE en prédisant des variances presque nulles (ce qui serait presque équivalent à un point comme ce que l'on prédit avec un encoder d'un AE).\n",
    "\n",
    "La divergence de Kullback-Leibler va encourager les distributions de l'espace latent à être proches les unes des autres ce qui permet de générer des données toujours cohérentes lorsque l'on *sample*.  \n",
    "\n",
    "![Regularization](./images/regu.png)\n",
    "\n",
    "\n",
    "**Note** : Il y a un aspect théorique important derrière les *Variational autoencoders* mais nous n'allons pas entrer dans les détails dans ce cours. Pour en apprendre plus, vous pouvez vous réferrer au cours CS236 de stanford et en particulier à ce [lien](https://deepgenerativemodels.github.io/notes/vae/)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
