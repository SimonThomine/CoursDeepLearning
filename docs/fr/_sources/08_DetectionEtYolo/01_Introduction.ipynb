{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à la détection d'objets dans les images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le traitement d'images se divise en trois grandes catégories :\n",
    "- **La classification** : Détermine si un objet est présent sur l'image (exemple : une photo de chien ?).\n",
    "- **La détection** : Localise la position d'un objet sur l'image (exemple : où se trouve le chien ?).\n",
    "- **La segmentation** : Identifie les pixels appartenant à un objet (exemple : quels sont les pixels du chien ?).\n",
    "\n",
    "![ClassDetSeg](./images/ClassDetSeg.jpeg)\n",
    "\n",
    "Image extraite de ce [site](https://docs.ultralytics.com/fr/guides/steps-of-a-cv-project/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cours sur les CNN, nous avons abordé des problèmes de classification avec une architecture CNN classique terminée par une couche Fully Connected, ainsi que des problèmes de segmentation avec le modèle U-Net.\n",
    "\n",
    "La détection d'objets est plus complexe à expliquer, donc ce cours se concentre sur les méthodes existantes et une description détaillée du modèle [YOLO](https://arxiv.org/pdf/1506.02640).\n",
    "\n",
    "Nous allons d'abord expliquer les différences entre les deux principales catégories de détecteurs :\n",
    "- **Méthodes en deux étapes (*Two-Stage Detectors*)** : Elles regroupent la famille des [RCNN (Region-based Convolutional Neural Networks)](https://arxiv.org/pdf/1311.2524).\n",
    "- **Méthodes en une étape (*Single-Stage Detectors*)** : Elles regroupent la famille des [YOLO (You Only Look Once)](https://arxiv.org/pdf/1506.02640).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Detectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme son nom l'indique, le *two-stage detector* suit deux étapes pour détecter des objets :\n",
    "- **Première étape** : Proposition de régions (*region proposal*) où des objets d'intérêt pourraient se trouver.\n",
    "- **Deuxième étape** : Affinage de la détection, c'est-à-dire l'association de la classe de l'objet et la précision de la *bounding box* (si un objet est présent).\n",
    "\n",
    "![rcnn](./images/rcnn.png)\n",
    "\n",
    "Image extraite de l'[article](https://arxiv.org/pdf/1311.2524).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En général, les *two-stage detectors* sont très précis et permettent des détections complexes, mais ils sont assez lents et ne permettent pas un traitement en temps réel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les réseaux *two-stage* les plus connus sont la famille des RCNN. Pour en savoir plus, consultez ce [blogpost](https://blog.roboflow.com/what-is-r-cnn/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Stage Detectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *one-stage detector* ne nécessite qu'une seule étape pour générer les *bounding box* avec les labels correspondants. Le réseau divise l'image en une grille et prédit plusieurs *bounding box* et leurs probabilités pour chaque cellule de la grille.\n",
    "\n",
    "![yolo](./images/yolo.png)\n",
    "\n",
    "Figure extraite de l'[article](https://arxiv.org/pdf/1506.02640).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les *one-stage detectors* sont généralement moins précis que les *two-stage detectors*, mais ils sont beaucoup plus rapides et permettent un traitement en temps réel. C'est la famille de détecteurs la plus utilisée aujourd'hui.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Maximum Suppression et Ancres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS (Non-Maximum Suppression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de la détection d'objets avec notre modèle, l'architecture ne permet pas d'éviter que plusieurs *bounding box* se chevauchent sur le même objet. Avant de transmettre les détections à l'utilisateur, on souhaite avoir une seule détection par objet, la plus pertinente possible.\n",
    "\n",
    "C'est là qu'intervient la *non-maximum suppression*. L'algorithme ne sera pas détaillé dans ce cours, mais vous pouvez consulter les ressources suivantes pour plus de détails : [blogpost](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c) et [site](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/).\n",
    "\n",
    "![nms](./images/nms.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ancres (Anchor boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les ancres sont des *bounding boxes* prédéfinies placées sur une grille régulière couvrant l'image. Elles peuvent avoir différents ratios (longueur/hauteur) et des tailles variables pour couvrir un maximum de tailles d'objets possibles. Les ancres réduisent le nombre de positions à étudier pour le modèle. Avec les ancres, le modèle prédit le décalage par rapport à l'ancre pré-générée et la probabilité d'appartenance à un objet.\n",
    "\n",
    "Cette méthode améliore la qualité des détections. Pour en savoir plus, consultez le [blogpost](https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9).\n",
    "\n",
    "En pratique, il y a souvent beaucoup d'ancres. La figure suivante montre 1% des ancres du modèle [retinaNet](https://arxiv.org/pdf/1708.02002) :\n",
    "\n",
    "![anchor](./images/anchors.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Détection d'objets avec l'architecture transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récemment, l'architecture du *transformer* a été adaptée pour la détection d'objets. Le modèle [DETR](https://arxiv.org/pdf/2005.12872) utilise un modèle CNN pour extraire des caractéristiques visuelles. Ces *features* sont ensuite passées à travers un *transformer encoder* (avec un *positional embedding*) pour déterminer les relations spatiales entre les caractéristiques grâce au mécanisme d'*attention*. Un *transformer decoder* (différent de celui utilisé en NLP) prend en entrée la sortie de l'*encoder* (*keys* et *values*) et des *embeddings* de labels d'objets (*queries*), convertissant ces *embeddings* en prédictions. Enfin, une couche linéaire finale traite la sortie du décodeur pour prédire les labels et les *bounding boxes*.\n",
    "\n",
    "Pour en savoir plus, consultez l'[article](https://arxiv.org/pdf/2005.12872) ou ce [blogpost](https://medium.com/visionwizard/detr-b677c7016a47).\n",
    "\n",
    "![detr](./images/detr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode offre plusieurs avantages :\n",
    "- Pas besoin de NMS, d'ancres ou de *region proposal*, ce qui simplifie l'architecture et le pipeline d'entraînement.\n",
    "- Le modèle a une meilleure compréhension globale de la scène grâce au mécanisme d'*attention*.\n",
    "\n",
    "Cependant, elle présente aussi quelques inconvénients :\n",
    "- Les *transformers* sont gourmands en calcul, donc ce modèle est moins rapide qu'un *one-stage detector* comme YOLO.\n",
    "- L'apprentissage est souvent plus long que pour un détecteur basé uniquement sur un CNN.\n",
    "\n",
    "**Note** : Les *transformers* utilisés en vision ont souvent des temps d'entraînement plus longs que ceux des CNN. Une explication possible est que les CNN ont un biais qui les rend particulièrement adaptés aux images, nécessitant un temps d'entraînement plus court. Les *transformers*, étant des modèles généralistes sans biais, doivent apprendre depuis zéro.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
