{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook analyse l'article [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030). Il propose une amélioration de l'architecture *transformer* avec un design hiérarchique adapté aux images, rappelant les réseaux de neurones convolutifs.\n",
    "La première partie du notebook explique les propositions de l'article une par une. La seconde partie présente une implémentation simplifiée de l'architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée principale de l'article est d'appliquer l'*attention* de manière hiérarchique sur des parties de plus en plus grandes de l'image. Cette approche repose sur plusieurs fondements :\n",
    "- L'analyse des images commence par les détails locaux avant de considérer les relations entre tous les pixels. C'est pourquoi les CNNs sont si performants.\n",
    "- Le fait que les *tokens* (*patch*) ne communiquent pas avec tous les autres permet d'améliorer le temps de calcul.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture hiérarchique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture hiérarchique du *swin transformer* est résumée dans cette figure :\n",
    "\n",
    "![hierarchique](./images/hierarchique.png)\n",
    "\n",
    "Dans notre implémentation, le modèle ViT convertit les *patchs* en *tokens* et applique un *transformer encoder* sur tous les éléments. C'est une architecture simple et sans biais sur les données, applicable à divers types de données.\n",
    "\n",
    "L'architecture *swin* ajoute un biais pour la rendre plus performante sur les images et plus rapide en traitement. Comme le montre la figure, l'image est d'abord divisée en petits *patchs* (taille $4 \\times 4$ dans l'article) regroupés en fenêtres. La couche d'attention est ensuite appliquée uniquement sur chaque fenêtre de manière indépendante. Plus on descend dans le réseau, plus la dimension C (taille des *patchs* relative à l'image) et des fenêtres augmente jusqu'à couvrir toute l'image, avec le même nombre de *patchs* que l'architecture ViT. À la manière d'un CNN, le réseau traite d'abord les informations locales, puis, au fur et à mesure (avec l'augmentation du *receptive field*), des informations de plus en plus globales. Cela se fait en augmentant le nombre de filtres et en diminuant la résolution de l'image.\n",
    "\n",
    "Les nouveaux *blocks* de *transformer* correspondants sont appelés *Window Multi-Head Self-Attention* (W-MSA dans l'article, attention le M signifie *Multi-Head* et pas *Masked*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fenêtre glissante\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans leur analogie avec le CNN, les auteurs ont remarqué qu'il peut être problématique de séparer l'image en fenêtres à des positions arbitraires. Cela brise la connexion entre des pixels voisins situés aux extrémités des fenêtres.\n",
    "\n",
    "Pour corriger ce problème, les auteurs proposent d'utiliser un système de fenêtre glissante (*shifting window*) dans chaque *block swin*. Les *blocks swin* sont agencés par paires comme décrit dans la figure du début du notebook.\n",
    "\n",
    "Voici à quoi ressemble la fenêtre glissante :\n",
    "\n",
    "![shifting](./images/shifting.png)\n",
    "\n",
    "Comme vous pouvez le constater, avec cette technique, on passe de $2 \\times 2$ *patchs* à $3 \\times 3$ *patchs* (de manière générale de $n \\times n$ *patchs* à $(n+1) \\times (n+1)$). Cela pose problème pour le traitement par le réseau, en particulier en *batch*.\n",
    "\n",
    "Les auteurs proposent d'incorporer un *cyclic shift* qui consiste à faire cette opération sur l'image pour permettre un traitement plus efficace :\n",
    "\n",
    "![cyclic](./images/cyclic.png)\n",
    "\n",
    "Notez que pour utiliser cette méthode, il est nécessaire de masquer les informations des *patchs* ne provenant pas d'une même partie de l'image. Les parties blanches, jaunes, vertes et bleues de la figure ne communiquent pas ensemble grâce à une couche d'*attention* masquée.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative position bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture ViT utilisait un *position embedding* absolu pour ajouter une information de position sur les différents *patchs*. Le problème de *position embedding* est qu'il ne capture pas les relations entre les *patchs* et est donc moins performant avec des images de résolutions différentes.\n",
    "\n",
    "Le *swin transformer* utilise un biais de positions relatives pour compenser cela. Ce biais dépend de la distance relative entre les différents *patchs*. Il est ajouté lorsque l'attention est calculée entre deux *patchs*. Son principal intérêt est d'améliorer la capture des relations spatiales et de s'adapter à des images de résolutions différentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détails supplémentaires sur l'architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comme on le voit sur la première figure du notebook, il y a plus de couches dans le stage 3 du *swin transformer*. Lorsqu'on augmente le nombre de couches du réseau, seules les couches du stage 3 sont augmentées, les autres restent fixes. Cela permet de bénéficier de l'architecture *swin* (*shifting*, etc.) tout en étant suffisamment profond et performant en termes de temps de traitement.\n",
    "\n",
    "- Supposons que chaque fenêtre contienne des *patchs* de $M \\times M$. La complexité computationnelle d'une couche *multi-head self-attention* (MSA) et celle d'une couche *window multi-head self-attention* (W-MSA) pour une image de $h \\times w$ *patchs* sont :\n",
    "$\\Omega(\\text{MSA}) = 4hwC^2 + 2(h w)^2 C$\n",
    "$\\Omega(\\text{W-MSA}) = 4hwC^2 + 2M^2hwC$\n",
    "Le premier est de complexité quadratique tandis que le second est linéaire si $M$ est fixe. L'architecture *swin* permet de gagner en vitesse de traitement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation simplifiée\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant à l'implémentation en PyTorch du *swin transformer*. Certaines parties sont assez complexes en termes d'implémentation et nous ne les couvrirons pas ici : la partie fenêtre glissante et la partie *relative position bias*. Nous allons donc nous contenter d'implémenter l'architecture hiérarchique.\n",
    "\n",
    "Si vous souhaitez consulter l'implémentation complète du *swin transformer* par les auteurs, vous pouvez aller voir leur [github](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py). Notre implémentation s'inspire du code des auteurs et reprend notre implémentation du ViT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detection automatique du GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion de l'image en patch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la conversion de l'image en *patch*, nous reprenons notre fonction du notebook précédent :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    # On rajoute une dimension pour le batch\n",
    "    B,C,_,_ = image.shape\n",
    "    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(B,-1, C, patch_size, patch_size)\n",
    "    patches_flat = patches.flatten(2, 4)\n",
    "    return patches_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head self-attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'implémentation du *swin*, la *couche multi-head self-attention* ne change pas par rapport à l'implémentation du ViT. C'est essentiellement la même couche, mais ce qui change est la manière de l'utiliser dans le *swin block*.\n",
    "\n",
    "Reprenons donc notre code du notebook précédent :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head_enc(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "    def __init__(self, head_size,n_embd,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape   \n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # On a supprimer le masquage du futur\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head_enc de taille head_size\n",
    "        self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Si on voulait implémenter le *relative position bias*, il faudrait modifier la fonction car ce *bias* s'ajoute directement lors du calcul de l'*attention* (voir [code source](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py) pour aller plus loin).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est pareil pour la *feed forward layer* qui reste la même :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation du swin block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par implémenter la fonction pour partitionner notre image en fenêtres. Pour cela, nous allons reconvertir notre $x$ en dimension $B \\times H \\times W \\times C$ plutôt que $B \\times T \\times C$. Ensuite, nous allons transformer notre tenseur en plusieurs fenêtres qui passeront dans la dimension *batch* (pour traiter chaque fenêtre indépendamment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size,input_resolution):\n",
    "    B,_,C = x.shape\n",
    "    H,W = input_resolution\n",
    "    x = x.view(B, H, W, C)\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'exemple, supposons que, comme dans l'implémentation de l'article, nous divisons notre image de taille 224 en *patchs* de taille $4 \\times 4$. Cela nous donnera $224/4 \\times 224/4$ *patchs*, soit 3136, qui seront ensuite projetés dans une dimension d'embedding $C$ de taille 96 (pour swin-T et swin-S). Nous allons séparer en $M=7$ fenêtres, ce qui nous donnera ce tenseur :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 7, 7, 96])\n"
     ]
    }
   ],
   "source": [
    "# Pour un batch de taille 2\n",
    "window_size = 7\n",
    "n_embed = 96\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "windows=window_partition(dummy,window_size,(56,56))\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de le passer à la couche d'*attention*, nous devons le remettre dans une dimension $B \\times T \\times C$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 49, 96])\n"
     ]
    }
   ],
   "source": [
    "windows=windows.view(-1, window_size * window_size, n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pourrons ensuite appliquer notre couche d'*attention* pour effectuer le *self-attention* sur toutes les fenêtres indépendamment. Une fois que c'est fait, il faut appliquer la transformée inverse pour revenir dans un format sans fenêtres :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 56, 56, 96])\n",
      "torch.Size([2, 3136, 96])\n"
     ]
    }
   ],
   "source": [
    "def window_reverse(windows, window_size,input_resolution):\n",
    "    H,W=input_resolution\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "windows=window_reverse(windows,window_size,(56,56))\n",
    "print(windows.shape)\n",
    "# et revenir en format BxTxC\n",
    "windows=windows.view(2,3136,n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous venons d'implémenter les éléments fondamentaux pour le traitement en fenêtres (hiérarchique du *swin transformer*). Nous pouvons maintenant construire notre *block swin* regroupant toutes ces transformations :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swinblock(nn.Module):\n",
    "  def __init__(self, n_embd,n_head,input_resolution,window_size,dropout=0.) -> None:\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)\n",
    "    self.ffwd = FeedFoward(n_embd,dropout)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "    self.input_resolution = input_resolution\n",
    "    self.window_size = window_size\n",
    "    self.n_embd = n_embd\n",
    "    \n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    x=window_partition(x, self.window_size,self.input_resolution)\n",
    "    x=self.ln1(x)\n",
    "    x=x.view(-1, self.window_size * self.window_size, self.n_embd)\n",
    "    x=self.sa(x)\n",
    "    x=window_reverse(x,self.window_size,self.input_resolution)\n",
    "    x=x.view(B,T,self.n_embd)\n",
    "    x=x+self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch merging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'architecture hiérarchique du *swin transformer*, à chaque fois que nous augmentons notre *receptive field* (en diminuant le nombre de fenêtres), nous allons concaténer les 4 *patchs* adjacents de taille $C$ en dimension $4C$, puis appliquer une couche linéaire pour revenir à une dimension plus petite de $2C$. Cela réduit le nombre de *tokens* par 4 à chaque fois que nous diminuons le nombre de fenêtres.\n",
    "Nous pouvons récupérer les *patchs* adjacents de cette manière :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 96])\n"
     ]
    }
   ],
   "source": [
    "# Reprenons un exemple de nos 56x56 patchs\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "B,T,C = dummy.shape\n",
    "H,W=T**0.5,T**0.5\n",
    "dummy=dummy.view(2,56,56,n_embed)\n",
    "# En python, 0::2 prend un élément sur 2 à partir de 0, 1::2 prend un élément sur 2 à partir de 1\n",
    "# De cette manière, on peut récupérer les à intervalles réguliers \n",
    "dummy0 = dummy[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy1 = dummy[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy2 = dummy[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "dummy3 = dummy[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "print(dummy0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite concaténer nos *patchs* adjacents :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 384])\n",
      "torch.Size([2, 784, 384])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.cat([dummy0, dummy1, dummy2, dummy3], -1)  # B H/2 W/2 4*C\n",
    "print(dummy.shape)\n",
    "# On repasse en BxTxC\n",
    "dummy = dummy.view(B, -1, 4 * C)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons bien divisé par quatre le nombre de *patchs* tout en augmentant les canaux par 4. Nous appliquons maintenant la couche linéaire pour diminuer le nombre de canaux.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(4 * C, 2 * C, bias=False)\n",
    "dummy = layer(dummy)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà, nous avons tous les éléments pour construire notre couche de *merging* :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, input_resolution, in_channels, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)\n",
    "        self.norm = norm_layer(4 * in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du modèle swin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le *swin transformer*, il est compliqué d'ajouter un *cls_token* dans l'implémentation. C'est pourquoi nous allons utiliser l'autre méthode mentionnée dans le notebook précédent, à savoir l'*adaptive average pooling*. Cela nous permet d'avoir une sortie de taille fixe, peu importe la taille de l'image d'entrée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 blocs de 2 couches au lieu de 4 car CIFAR-10 a de plus petites images\n",
    "class SwinTransformer(nn.Module):\n",
    "  def __init__(self,n_embed,patch_size,C,window_size,num_heads,img_dim=[16,8,4],depths=[2,2,2]) -> None:\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.proj_layer = nn.Linear(C*patch_size*patch_size, n_embed)\n",
    "    input_resolution = [(img_dim[0],img_dim[0]),(img_dim[1],img_dim[1]),(img_dim[2],img_dim[2])]\n",
    "    self.blocks1 = nn.Sequential(*[swinblock(n_embed,num_heads,input_resolution[0],window_size) for _ in range(depths[0])])\n",
    "    self.down1 = PatchMerging(input_resolution[0],in_channels=n_embed)\n",
    "    self.blocks2 = nn.Sequential(*[swinblock(n_embed*2,num_heads,input_resolution[1],window_size) for _ in range(depths[1])])\n",
    "    self.down2 = PatchMerging(input_resolution[1],in_channels=n_embed*2)\n",
    "    self.blocks3 = nn.Sequential(*[swinblock(n_embed*4,num_heads,input_resolution[2],window_size) for _ in range(depths[2])])\n",
    "    self.classi_head = nn.Linear(n_embed*4, 10)\n",
    "    self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = image_to_patches(x,self.patch_size)\n",
    "    x = self.proj_layer(x)\n",
    "    x = self.blocks1(x)\n",
    "    x = self.down1(x)\n",
    "    x = self.blocks2(x)\n",
    "    x = self.down2(x)\n",
    "    x = self.blocks3(x)\n",
    "    x = self.avgpool(x.transpose(1, 2)).flatten(1)\n",
    "    x = self.classi_head(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement sur Imagenette\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour tester notre modèle, nous allons à nouveau utiliser CIFAR-10, même si la petite taille des images ne se prête pas forcément bien à l'architecture hiérarchique.\n",
    "\n",
    "**Note** : Vous pouvez sélectionner une sous-partie du dataset pour accélérer l'entraînement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille d'une image :  torch.Size([3, 32, 32])\n",
      "taille du train dataset :  40000\n",
      "taille du val dataset :  10000\n",
      "taille du test dataset :  10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# Transformation des données, normalisation et transformation en tensor pytorch\n",
    "transform = T.Compose([T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = datasets.CIFAR10(root='./../data', train=True,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(dataset))[:5000]\n",
    "# dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "testdataset = datasets.CIFAR10(root='./../data', train=False,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(testdataset))[:1000]\n",
    "# testdataset = torch.utils.data.Subset(testdataset, indices)\n",
    "print(\"taille d'une image : \",dataset[0][0].shape)\n",
    "\n",
    "\n",
    "#Création des dataloaders pour le train, validation et test\n",
    "train_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\n",
    "print(\"taille du train dataset : \",len(train_dataset))\n",
    "print(\"taille du val dataset : \",len(val_dataset))\n",
    "print(\"taille du test dataset : \",len(testdataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "val_loader= DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 2\n",
    "n_embed = 24\n",
    "n_head = 4\n",
    "C=3 \n",
    "window_size = 4\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.0001 #1e-3\n",
    "\n",
    "model = SwinTransformer(n_embed,patch_size,C,window_size,n_head,img_dim=[16,8,4],depths=[2,2,2]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss train 1.9195597559928894, loss val 1.803518475151062,précision 33.94\n",
      "Epoch 1, loss train 1.7417401003360748, loss val 1.6992134885787964,précision 37.84\n",
      "Epoch 2, loss train 1.651085284280777, loss val 1.6203388486862182,précision 40.53\n",
      "Epoch 3, loss train 1.5808091670751572, loss val 1.5558069843292237,précision 43.03\n",
      "Epoch 4, loss train 1.522760990524292, loss val 1.5169190183639527,précision 44.3\n",
      "Epoch 5, loss train 1.4789127678394318, loss val 1.4665142657279968,précision 47.02\n",
      "Epoch 6, loss train 1.4392719486951828, loss val 1.4568698994636535,précision 47.65\n",
      "Epoch 7, loss train 1.4014943064451217, loss val 1.4456377569198609,précision 48.14\n",
      "Epoch 8, loss train 1.3745941290140151, loss val 1.4345624563694,précision 48.38\n",
      "Epoch 9, loss train 1.3492228104948998, loss val 1.398228020954132,précision 50.04\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss_val += F.cross_entropy(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, loss train {loss_train/len(train_loader)}, loss val {loss_val/len(val_loader)},précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entraînement est terminé, nous obtenons une précision de 50% sur les données de validation.\n",
    "\n",
    "Regardons maintenant sur nos données de test :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision 49.6\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La précision est à peu près similaire à celle des données de validation !\n",
    "\n",
    "**Note** : Les résultats ne sont pas très bons pour plusieurs raisons. Tout d'abord, nous traitons des petites images et l'architecture hiérarchique du *swin transformer* est plutôt conçue pour traiter des images de plus grandes dimensions. Ensuite, notre implémentation est vraiment minimaliste puisqu'il manque deux éléments clés de l'architecture *swin* : la partie fenêtre glissante et la partie *relative position bias*. Le but de ce notebook était de vous donner une intuition sur le fonctionnement de l'architecture *swin* et non de vous proposer une implémentation parfaite ;)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
