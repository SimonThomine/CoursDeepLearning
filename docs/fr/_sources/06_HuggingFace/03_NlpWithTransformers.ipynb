{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with Transformers\n",
    "# Traitement du langage naturel avec des Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons utiliser la library transformers de Hugging Face pour le traitement du langage naturel (NLP). Les modèles de langages les plus performants (GPT, Llama etc...) sont très couteux en terme d'espace mémoire et il est souvent impossible de les faire fonctionner sur un ordinateur portable. Nous allons donc utiliser des modèles de taille réduite (et donc moins performants) dans ce notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation la plus commune des modèles de langages (LLM) est aujourd'hui le ChatBot ce qui consiste en un genre d'assistant virtuel qui va répondre à nos questions.   \n",
    "Avec Hugging Face, vous pouvez avoir votre propre ChatBot en local de la manière suivante. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons une version réduite de [BlenderBot](https://ai.meta.com/blog/blenderbot-3-a-175b-parameter-publicly-available-chatbot-that-improves-its-skills-and-safety-over-time/) de Meta (facebook/blenderbot-400M-distill)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(task=\"conversational\",model=\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce ChatBot ne gère que l'anglais, il faudra donc lui poser des questions en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 44c34bd3-ea1b-44b6-bd54-9127133cc941\n",
      "user: What is the best  french deep learning course?\n",
      "assistant:  I'm not sure, but I do know that French is one of the most widely spoken languages in the world.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Conversation\n",
    "user_message = \"\"\"What is the best french deep learning course?\"\"\"\n",
    "conversation = Conversation(user_message)\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous le voyez, le modèle est assez mal entraîné, il ne sait pas que le meilleur cours de Deep Learning est celui-ci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous voulez poser une autre question, la commande suivante permet d'avoir la réponse à une question en une seule ligne de code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: d258da22-78e4-4621-a0e1-90776454a595\n",
      "user: What is the most tasty fruit?\n",
      "assistant:  I would have to say watermelon. It is so juicy and juicy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation=Conversation(\"What is the most tasty fruit?\")\n",
    "print(chatbot(conversation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par contre, si vous souhaitez continuer la conversation déjà entamée, il faut utiliser cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: c3e1a64c-5b40-4808-8632-38d9df14ed9d\n",
      "user: What is the most tasty fruit?\n",
      "assistant:  I would have to say watermelon. It is so juicy and juicy.\n",
      "user: What else do you recommend?\n",
      "assistant:  I would say mangos are pretty good too. They are sweet and tangy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Il faut spécifier le rôle (user) et ajouter votre message dans la conversation déjà existante\n",
    "conversation.add_message({\"role\": \"user\",\"content\": \"\"\"What else do you recommend?\"\"\"})\n",
    "print(chatbot(conversation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous savez maintenant comment utiliser un ChatBot avec la library transformers de Hugging Face. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant regarder comment implémenter un traducteur. Pour cela, nous utilisons le modèle [No Language Left Behind](https://ai.meta.com/research/no-language-left-behind/) de facebook (facebook/nllb-200-distilled-600M) qui permet de faire depuis n'importe quel langage. Par soucis de mémoire, nous utilisons une version réduite du modèle.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "traducteur = pipeline(task=\"translation\",model=\"facebook/nllb-200-distilled-600M\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le texte en anglais :  The best course of deep learning is this one.\n",
      "Le texte en japonais :  深い学習の最高のコースはこれです\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Le meilleur cours de d'apprentissage profond est celui-ci.\"\"\"\n",
    "text_translated = traducteur(text,src_lang=\"fra_Latn\",tgt_lang=\"eng_Latn\")\n",
    "print(\"Le texte en anglais : \", text_translated[0][\"translation_text\"])\n",
    "text_translated = traducteur(text,src_lang=\"fra_Latn\",tgt_lang=\"jpn_Jpan\")\n",
    "print(\"Le texte en japonais : \",text_translated[0][\"translation_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La traduction est très bonne (pour l'anglais en tout cas, pour le japonais je n'ai malheuresement pas l'expertise) !   \n",
    "Vous pouvez également tester d'autres combinaisons de langage, il faut juste spécifier le bon code que l'on peut trouver sur cette [page](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre tâche utile dans le traitement du langage naturel est le résumé de texte. Le modèle doit être capable de rassembler les informations les plus importantes d'un texte. Pour cela, nous utilisons le modèle [BART](https://research.facebook.com/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/) de facebook (facebook/bart-large-cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeur=pipeline(task=\"summarization\",model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le résumé du texte :  Troyes is a commune and the capital of the department of Aube in the Grand Est region of north-central France. It is located on the Seine river about 140 km (87 mi) south-east of Paris. Troyes had a population of 61,996 inhabitants in 2018.\n"
     ]
    }
   ],
   "source": [
    "text= \"Troyes is a beautiful city. Troyes is a commune and the capital of the department of Aube in the Grand Est region of north-central France. It is located on the Seine river about 140 km (87 mi) south-east of Paris. Troyes is situated within the Champagne wine region and is near to the Orient Forest Regional Natural Park.Troyes had a population of 61,996 inhabitants in 2018. It is the center of the Communauté d'agglomération Troyes Champagne Métropole, which was home to 170,145 inhabitants.\"\n",
    "summary = resumeur(text,min_length=10,max_length=100)\n",
    "print(\"Le résumé du texte : \",summary[0][\"summary_text\"]) #[\"summary_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résumé n'est pas très bon car il s'agit d'un petit modèle mais il a quand même su ressortir les informations clés et enlever les informations \"moins importantes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding de phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un aspect important du langage naturel que nous avons abordé dans le cours sur les NLP est l'*embedding*. Pour rappel, cela consiste à projetter nos *tokens* (mots ou caractères par exemple) dans un espace latent. Cela va permettre de faire une première étape de rapprochement de ces mots. Des mots proches comme chiens et chats vont être proches dans l'espace latent tandis que \"chien\" et \"est\" vont être éloignés.   \n",
    "Nous pouvons utiliser ces *embeddings* pour calculer la similarité entre deux phrases. Pour cela, nous allons utiliser la library sentence_transformers qui permet d'extraire l'*embedding* à partir d'un modèle pré-entrainé.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons le modèle \n",
    "[all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons regarder la similarité entre des phrases différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat is chasing the mouse \t\t The dog sleep in the kitchen \t\t Score: 0.0601\n",
      "A man is watching the television \t\t A boy watches TV \t\t Score: 0.7207\n",
      "The latest movie is awesome \t\t The new movie is so great \t\t Score: 0.7786\n"
     ]
    }
   ],
   "source": [
    "sentences1 = ['The cat is chasing the mouse','A man is watching the television','The latest movie is awesome']\n",
    "sentences2 = ['The dog sleeps in the kitchen','A boy watches TV','The new movie is so great']\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2,convert_to_tensor=True)\n",
    "cosine_scores = util.cos_sim(embeddings1,embeddings2)\n",
    "for i in range(len(sentences1)):\n",
    "  print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i],\n",
    "                                                sentences2[i],\n",
    "                                                cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous pouvez le voir, les phrases proches en terme de sens ont des *embeddings* assez similaires. Ce modèle est donc intéressant pour extraire des *embeddings*.   \n",
    "Avoir un bon modèle extracteur d'*embedding* est souvent une première étape dans un projet de traitement du langage naturel.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
