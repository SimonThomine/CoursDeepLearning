{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le notebook précédent, nous avons présenté la couche classique d'un RNN. Depuis l'invention de cette couche classique, de nombreuses autres couches récurrentes ont été inventées. \n",
    "\n",
    "Dans ce notebook, nous présentons la couche [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf) (long short-term memory) qui offre une alternative à la couche classique RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qu'est ce qu'une couche LSTM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La couche LSTM est consistituée d'une *memory unit* qui comporte 4 couches fully connected.    \n",
    "3 de ces 4 couches sont utilisées pour la sélection des informations précédentes pertinentes. Il s'agit de la *forget gate*, l'*input gate* et l'*output gate* : \n",
    "- **forget gate** : Permet de supprimer de l'information de la mémoire\n",
    "- **input gate** : Permet d'insérer de l'information dans la mémoire\n",
    "- **output gate** : Permet d'utiliser l'information présente dans la mémoire\n",
    "\n",
    "La dernière couche fully connected va créer une \"information candidate\" pour l'insertion dans la mémoire de la couche LSTM.  \n",
    "\n",
    "<img src=\"images/lstm.png\" alt=\"lstm\" width=\"600\"/>    \n",
    "\n",
    "Figure extraite du [blogpost](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on le voit sur la figure, la couche LSTM reçoit 3 vecteurs en entrée $H_{t-1}$, $C_{t-1}$ et $X_{t}$. Les deux premiers viennent du LSTM directement et le troisième correspond à l'entrée au temps $t$ (le caractère pour nous).\n",
    "\n",
    "Pour expliquer le concept sans entrer dans les détails : $H_{t-1}$ contient la mémoire à court terme (short term) et $C_{t-1}$ la mémoire à long terme. Cela permet de conserver les informations importantes sur un contexte important tout en ne négligeant pas le contexte plus local.   \n",
    "\n",
    "Le but de cette architecture est de pallier le problème de la propagation de l'information sur de longues séquences rencontré dans les RNN classiques.\n",
    "\n",
    "Pour comprendre la couche LSTM en détail, vous pouvez lire l'[article](https://www.bioinf.jku.at/publications/older/2604.pdf) ou consulter le [blogpost](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la création du dataset, nous utilisons à nouveau le fichier moliere.txt et nous reprenons le code du notebook précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réduit le nombre d'éléments pour avoir un entraînement rapide (à commenter si vous voulez entraîner sur l'ensemble des données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  100000\n"
     ]
    }
   ],
   "source": [
    "text=text[:100000]\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJLMNOPQRSTUVYabcdefghijlmnopqrstuvxyz«»ÇÈÉÊàâæçèéêîïôùû\n",
      "Nombre de caractères différents :  76\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparation en train et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour le test\n",
    "train_data = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer notre modèle, nous allons utiliser directement l'implémentation pytorch de la couche du LSTM. A l'inverse des couches linéaires ou convolutives, la couche [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) permet de stacker plusieurs couches grâce au paramètre *num_layers*. Si on veut définir les couches une par une, il faut utiliser [nn.LSTMCell](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size,num_layers=1):\n",
    "        super(lstm, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # On utilise un embedding pour transformer les entiers(caractères) en vecteurs\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        # La couche LSTM peut prendre l'argument num_layers pour empiler plusieurs couches LSTM\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
    "        # Une dernière couche linéaire pour prédire le prochain caractère\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr=0.001\n",
    "hidden_dim=128\n",
    "seq_len=100\n",
    "num_layers=1\n",
    "model=lstm(vocab_size,hidden_dim,num_layers)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La couche LSTM prend en entrée directement une séquence et renvoie une séquence de la même taille. Cela permet d'accélerer l'entraînement car on peut traîter plusieurs exemples en même temps. \n",
    "\n",
    "**Note** : Il est aussi possible d'accélerer l'entraînement en faisant un traîtement en *batch* avec plusieurs séquences en parallèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 2.17804336\n",
      "Epoch: 1 \t Loss: 1.76270216\n",
      "Epoch: 2 \t Loss: 1.62740668\n",
      "Epoch: 3 \t Loss: 1.54147145\n",
      "Epoch: 4 \t Loss: 1.47995140\n",
      "Epoch: 5 \t Loss: 1.43100239\n",
      "Epoch: 6 \t Loss: 1.39074463\n",
      "Epoch: 7 \t Loss: 1.35526441\n",
      "Epoch: 8 \t Loss: 1.32519794\n",
      "Epoch: 9 \t Loss: 1.29712536\n",
      "Epoch: 10 \t Loss: 1.27268774\n",
      "Epoch: 11 \t Loss: 1.24876227\n",
      "Epoch: 12 \t Loss: 1.22720749\n",
      "Epoch: 13 \t Loss: 1.20663312\n",
      "Epoch: 14 \t Loss: 1.18768359\n",
      "Epoch: 15 \t Loss: 1.16936996\n",
      "Epoch: 16 \t Loss: 1.15179397\n",
      "Epoch: 17 \t Loss: 1.13514291\n",
      "Epoch: 18 \t Loss: 1.11997525\n",
      "Epoch: 19 \t Loss: 1.10359089\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    state=None\n",
    "    running_loss = 0\n",
    "    n=0\n",
    "    data_ptr = torch.randint(100,(1,1)).item()\n",
    "    # On train sur des séquences de seq_len caractères et on break si on dépasse la taille du dataset\n",
    "    while True:\n",
    "        x = train_data[data_ptr : data_ptr+seq_len]\n",
    "        y = train_data[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        data_ptr+=seq_len\n",
    "        # Pour éviter de sortir de l'index du dataset\n",
    "        if data_ptr + seq_len + 1 > len(train_data):\n",
    "            break\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant évaluer le loss sur nos données de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss de test: 1.51168611\n"
     ]
    }
   ],
   "source": [
    "state=None\n",
    "running_loss = 0\n",
    "n=0\n",
    "data_ptr = torch.randint(100,(1,1)).item()\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        x = test[data_ptr : data_ptr+seq_len]\n",
    "        y = test[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "    running_loss += loss.item()\n",
    "    n+=1\n",
    "    data_ptr+=seq_len\n",
    "    if data_ptr + seq_len + 1 > len(test):\n",
    "        break\n",
    "print(\"Loss de test: {0:.8f}\".format(running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle overfit pas mal ... Essayez de corriger ça par vous-même."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant pouvoir test la génération de texte !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Çà coeuse, et bon enfin l'avoir faire.\n",
      "\n",
      "MASCARILLE.\n",
      "\n",
      "En me donner d vous, Le pas.\n",
      "\n",
      "MASCARILLE, à dans un pour sûte matinix! cette ma foi.\n",
      "\n",
      "PANDOLFE.\n",
      "\n",
      "Ma foi, tu te le sy sois touves d'arrête sa bien sans les bonheur.\n",
      "\n",
      "MASCARILLE.\n",
      "\n",
      "Moi, je me suis to\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "moliere='.'\n",
    "sequence_length=250\n",
    "state=None\n",
    "for i in range(sequence_length):\n",
    "    x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()\n",
    "    y_pred,state = model.forward(x.unsqueeze(0),state)\n",
    "    probs=F.softmax(torch.squeeze(y_pred), dim=0)\n",
    "    sample=torch.multinomial(probs, 1)\n",
    "    moliere+=itos[sample.item()]\n",
    "print(moliere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La génération est peut-être un peu mieux que pour le modèle RNN de base mais ce n'est pas encore convaincant. Vous pouvez essayer d'améliorer les performances du modèle en faisant varier les paramètres (nombre de couches en série, hidden dim etc ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
