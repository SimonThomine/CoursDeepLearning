# ğŸ—ï¸ Fondations ğŸ—ï¸ 
Ce cours introduit introduit les bases de l'optimisation par descente du gradient avec une comprÃ©hension intuitive. La rÃ¨gle de la chaÃ®ne est introduite puis un premier exemple de regression logistique est prÃ©sentÃ©. 

## Notebook 1ï¸âƒ£ : [DÃ©rivÃ©es de descente du gradient](01_DÃ©rivÃ©esEtDescenteDuGradient.ipynb)
Ce notebook propose des rappels sur la dÃ©rivÃ©e d'une fonction pour ensuite introduire l'algorithme de descente du gradient et la rÃ¨gle de la chaÃ®ne.

## Notebook 2ï¸âƒ£ : [RÃ©gression logistique](02_RÃ©gressionLogistique.ipynb)
Ce notebook est dÃ©diÃ© Ã  une explication et une implÃ©mentation de la reÃ©gression logistique. Les fonctions d'activation et les fonctions de coÃ»ts (*loss*) sont Ã©galement introduites.