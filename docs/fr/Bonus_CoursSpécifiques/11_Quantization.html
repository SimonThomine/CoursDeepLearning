
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Quantization &#8212; Cours Deep Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Bonus_CoursSp√©cifiques/11_Quantization';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Introduction √† la tokenization" href="10_Tokenization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">üöÄ Apprendre le Deep Learning √† partir de z√©ro üöÄ</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Cours Deep Learning</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Cours Deep Learning üöÄ
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üßÆ Fondations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html">D√©riv√©e et descente du gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/02_R%C3%A9gressionLogistique.html">R√©gression logistique</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîó R√©seau Fully Connected</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html">Mon premier r√©seau</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html">Introduction √† pytorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html">Techniques avanc√©es</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üñºÔ∏è R√©seau Convolutifs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html">Couches de convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/02_R%C3%A9seauConvolutif.html">R√©seau convolutif</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/03_ConvImplementation.html">Implementation de la couche de convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html">R√©seau Convolutif sur Pytorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html">Application sur un dataset d‚Äôimages couleur</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html">Application segmentation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîÑ Autoencodeurs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/01_IntuitionEtPremierAE.html">Introduction aux autoencodeurs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/02_DenoisingAE.html">Denoising Autoencodeur</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üìù NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/01_Introduction.html">Introduction NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/02_bigramme.html">Bigramme</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/03_R%C3%A9seauFullyConnected.html">R√©seau fully connected</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/04_WaveNet.html">Pytorch et WaveNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/05_Rnn.html">R√©seau de neurones r√©currents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/06_Lstm.html">Long Short-Term Memory</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ó HuggingFace</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/01_introduction.html">Hugging Face Library Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/02_ComputerVisionWithTransformers.html">Computer Vision with Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/03_NlpWithTransformers.html">NLP with Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/04_AudioWithTransformers.html">Audio with Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/05_ImageGenerationWithDiffusers.html">Generation d‚Äôimage avec la library Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/06_DemoAvecGradio.html">Demo avec Gradio</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚ö° Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/01_Introduction.html">Introduction aux transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/02_GptFromScratch.html">Construisons GPT √† partir de rien</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/03_TrainingOurGpt.html">Entra√Ænons notre mod√®le GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html">Architecture et particularit√©s du transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/05_UtilisationsPossibles.html">Utilisations possibles de l‚Äôarchitecture Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/06_VisionTransformerImplementation.html">Vision transformer implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/07_SwinTransformer.html">Swin transformer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéØ Detection Et Yolo</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/01_Introduction.html">Introduction √† la d√©tection d‚Äôobjets dans les images</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/02_YoloEnDetail.html">Yolo en d√©tail</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/03_Ultralytics.html">Ultralytics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîç Entrainement Contrastif</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/01_FaceVerification.html">Face Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/02_NonSupervis%C3%A9.html">Entrainement contrastif non supervis√©</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéì Transfer Learning Et Distillation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/01_TransferLearning.html">Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/02_TransferLearningPytorch.html">Transfer Learning avec Pytorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/03_Distillation.html">Distillation des connaissances</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/04_DistillationAnomalie.html">Distillation des connaissances pour la d√©tection d‚Äôanomalies non supervis√©e</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/05_FineTuningLLM.html">Fine Tuning LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/06_FineTuningBertHF.html">Fine tuning BERT avec Hugging Face</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üé® Modeles Generatifs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/01_Introduction.html">Introductions aux mod√®les g√©n√©ratifs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/02_GAN.html">Generative Adversarial Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/03_GanImplementation.html">Impl√©mentation d‚Äôun GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/04_VAE.html">Variational autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/05_VaeImplementation.html">Impl√©mentation d‚Äôun VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/06_NormalizingFlows.html">Normalizing Flows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/07_DiffusionModels.html">Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/08_DiffusionImplementation.html">Implementation Diffusion Model</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéÅ Bonus Cours Sp√©cifiques</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_ActivationEtInitialisation.html">Activations et Initialisations</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_BatchNorm.html">Batch Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_DataAugmentation.html">Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Broadcasting.html">Broadcasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Optimizer.html">Comprendre les diff√©rents optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Regularisation.html">R√©gularisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_ConnexionsResiduelles.html">Connexions r√©siduelles</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_CrossValidation.html">Introduction √† la cross validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_MetriquesEvaluation.html">M√©triques d‚Äô√©valuation de mod√®les</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_Tokenization.html">Introduction √† la tokenization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Quantization</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/edit/main/fr/Bonus_CoursSp√©cifiques/11_Quantization.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/issues/new?title=Issue%20on%20page%20%2FBonus_CoursSp√©cifiques/11_Quantization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Bonus_CoursSp√©cifiques/11_Quantization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Quantization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comment-representer-les-nombres-sur-un-ordinateur">Comment repr√©senter les nombres sur un ordinateur ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-a-la-quantization">Introduction √† la quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#point-rapide-sur-les-precisions-communes">Point rapide sur les pr√©cisions communes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-symetrique">Quantization sym√©trique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-asymetrique">Quantization asym√©trique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clipping-et-modification-de-range">Clipping et modification de range</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration">Calibration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#post-training-quantization-ptq">Post-Training Quantization (PTQ)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-quantization">Dynamic quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#static-quantization">Static quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-entre-dynamic-et-static-quantization">Diff√©rence entre dynamic et static quantization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ptq-la-quantization-en-4-bit">PTQ : la quantization en 4-bit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gguf">GGUF</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-aware-training-qat">Quantization Aware Training (QAT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bitnet-quantization-1-bit">BitNet : quantization 1-bit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bitnet-1-58-on-a-besoin-du-zero">BitNet 1.58 : On a besoin du z√©ro !</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-des-modele-de-langages">Fine-Tuning des mod√®le de langages</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">LoRA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qlora">QLoRA</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Link to this heading">#</a></h1>
<p>Les mod√®les de Deep Learning sont de plus en plus puissants et √©galement de plus en plus gros. Si l‚Äôon regarde le cas des LLM, les meilleurs LLM ont maintenant plusieurs centaines de milliards de param√®tres <a class="reference external" href="https://llama.meta.com/?utm_source=ainews&amp;amp;utm_medium=email&amp;amp;utm_campaign=ainews-llama-31-the-synthetic-data-model">Llama 3.1</a> (pour les LLMs Open-Source).
Avec un simple GPU, c‚Äôest impossible de charger un mod√®le aussi gros. M√™me avec le plus gros GPU du march√© (H100 qui poss√®de 80 giga de VRAM), il faut plusieurs GPU pour l‚Äôinf√©rence et encore plus pour l‚Äôentra√Ænement.</p>
<p>En pratique, on sait qu‚Äôun nombre sup√©rieur de param√®tres est correl√© avec une meilleure performance. On ne veut donc pas diminuer la taille des mod√®les. Ce que l‚Äôon voudrait, c‚Äôest r√©duire l‚Äôespace m√©moire que le mod√®le occupe.</p>
<p>Ce cours s‚Äôinspire fortement du <a class="reference external" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization?utm_source=ainews&amp;amp;utm_medium=email&amp;amp;utm_campaign=ainews-to-be-named-5098">blogpost</a> et du <a class="reference external" href="https://medium.com/&#64;dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766">blogpost</a>. Les images utilis√©es sont √©galement extraites de ces deux blogposts.</p>
<section id="comment-representer-les-nombres-sur-un-ordinateur">
<h2>Comment repr√©senter les nombres sur un ordinateur ?<a class="headerlink" href="#comment-representer-les-nombres-sur-un-ordinateur" title="Link to this heading">#</a></h2>
<p>Pour r√©presenter les nombres flottants sur un ordinateur, on utilise un certain nombre de bits. La norme <a class="reference external" href="https://en.wikipedia.org/wiki/IEEE_754">IEEE_754</a> d√©crit comment les bits peuvent repr√©senter un nombre. Cela se fait via 3 parties : le signe, l‚Äôexposant et la mantisse.</p>
<p>Voici un exemple de la r√©presentation FP16 (16 bits) :</p>
<img src="images/Fp16.png" alt="fp32" width="500"/>
<p>Le signe permet de d√©terminer * <em>roulement de tambour</em> * le signe du nombre, l‚Äôexposant va donner les chiffres avant la virgule et la mantisse les chiffres apr√®s la virgule. Voici un exemple en image de la mani√®re de convertir la representation FP16 en chiffre.</p>
<img src="images/convert.webp" alt="convert" width="500"/>
<p>De mani√®re g√©n√©rale, plus on utilise de bits pour representer une valeur, plus cette valeur peut √™tre pr√©cise ou sur une grande plage de valeur. Par exemple, on peut comparer la pr√©cision FP16 et FP32 :</p>
<img src="images/compareFP.webp" alt="compareFP" width="500"/>
<p>Une derni√®re chose importante √† savoir. Il y a deux fa√ßon de juger une r√©pr√©sentation. D‚Äôune part, la <em>dynamic range</em> qui donne la plage des valeurs que l‚Äôon peut repr√©senter et la <em>precision</em> qui d√©crit l‚Äô√©cart entre deux valeurs voisines.</p>
<p>Plus l‚Äôexposant est important, plus la <em>dynamic range</em> est grande et plus la mantisse est importante, plus la <em>precision</em> est grande (donc 2 valeurs voisines sont proches).</p>
<p>En deep learning, on pr√©fere souvent utiliser la r√©presentation BF16 au lieu de FP16. La repr√©sentation BF16 a un exposant plus important mais une pr√©cision plus faible.</p>
<p>La figure suivante illustre les diff√©rences :</p>
<img src="images/BF16.webp" alt="BF16" width="500"/>
<p>Maitenant que l‚Äôon comprend les notions de pr√©cision des nombres flottants, on peut calculer la place que prend un mod√®le en m√©moire en fonction de la pr√©cision. En FP32, un nombre est repr√©sent√© par 32 bits ce qui correspond √† 4 octets (un octet vaut 8 bits pour rappel). Pour obtenir l‚Äôutilisation m√©moire d‚Äôun mod√®le, on peut faire le calcul suivant :<br />
<span class="math notranslate nohighlight">\(memory= \frac{n_{bits}}{8}*n_{params}\)</span></p>
<p>Prenons l‚Äôexemple d‚Äôun mod√®le de 70 milliards de param√®tres √† plusieurs niveau de pr√©cision : double(FP64), full-precision(FP32) et half-precision(FP16).<br />
Pour FP64 : <span class="math notranslate nohighlight">\(\frac{64}{8} \times 70B = 560GB\)</span><br />
Pour FP32 : <span class="math notranslate nohighlight">\(\frac{32}{8} \times 70B = 280GB\)</span><br />
Pour FP16 : <span class="math notranslate nohighlight">\(\frac{16}{8} \times 70B = 140GB\)</span></p>
<p>On se rend bien compte que c‚Äôest n√©cessaire de trouver une mani√®re de r√©duire la taille des mod√®les. Ici, m√™me le mod√®le en half-precision occupe 140GB ce qui correspond √† 2 GPU H100.</p>
<p><strong>Note</strong> : Ici, on parle de la pr√©cision pour faire l‚Äôinf√©rence. Pour l‚Äôentra√Ænement, comme il faut garder les activations en m√©moire pour la descente du gradient, on se retrouve avec beaucoup plus de param√®tres (voir partie sur QLoRA plus loin dans le cours).</p>
</section>
<section id="introduction-a-la-quantization">
<h2>Introduction √† la quantization<a class="headerlink" href="#introduction-a-la-quantization" title="Link to this heading">#</a></h2>
<p>Le but de la quantization est de r√©duire la pr√©cision d‚Äôun mod√®le en passant d‚Äôune pr√©cision riche comme FP32 √† une pr√©cision plus faible comme INT8.</p>
<p><strong>Note</strong> : INT8 est la fa√ßon de repr√©senter les entiers de -127 √† 127 sur 8 bits.</p>
<img src="images/quantization.webp" alt="quantization" width="500"/>
<p>Bien s√ªr, en diminuant le nombre de bits pour repr√©senter les valeurs, on a une perte de pr√©cision.<br />
Pour illustrer cela, on peut regarder sur une image :</p>
<img src="images/cookies.webp" alt="cookies" width="500"/>
<p>On remarque un ‚Äúgrain‚Äù dans l‚Äôimage ce qui est d√ª √† un manque de couleurs disponibles pour repr√©senter l‚Äôimage.<br />
Ce que l‚Äôon voudrait, c‚Äôest r√©duire le nombre de bits pour repr√©senter l‚Äôimage en gardant au maximum la pr√©cision de l‚Äôimage de base.</p>
<p>Il existe plusieurs mani√®re de faire de la quantization : la quantization sym√©trique et la quantization asym√©trique.</p>
<section id="point-rapide-sur-les-precisions-communes">
<h3>Point rapide sur les pr√©cisions communes<a class="headerlink" href="#point-rapide-sur-les-precisions-communes" title="Link to this heading">#</a></h3>
<p><strong>FP16</strong> : La <em>precision</em> et la <em>dynamic range</em> diminuent par rapport √† FP32.</p>
<img src="images/fp16.webp" alt="fp16" width="500"/>
<p><strong>BF16</strong> : La <em>precision</em> diminue fortement mais la <em>dynamic range</em> reste la m√™me par rapport √† FP32.</p>
<img src="images/bf16.webp" alt="bf16" width="500"/>
<p><strong>INT8</strong> : On passe √† une repr√©sentation en entier.</p>
<img src="images/int8.webp" alt="INT8" width="500"/></section>
<section id="quantization-symetrique">
<h3>Quantization sym√©trique<a class="headerlink" href="#quantization-symetrique" title="Link to this heading">#</a></h3>
<p>Dans le cas de la quantization sym√©trique, la plage de valeurs de nos flottants d‚Äôorigine est mapp√© de mani√®re sym√©trique sur la plage de valeur de quantization. C‚Äôest √† dire que le 0 dans les flottants est mapp√© sur le 0 dans la pr√©cision de quantization.</p>
<img src="images/symmetricq.webp" alt="symmetricq" width="500"/><p>Une des mani√®res la plus commune et √©galement la plus simple de r√©aliser cette op√©ration est d‚Äôutiliser la m√©thode <em>absmax (absolute maximum quantization)</em>. On prend la valeur maximale (en valeur absolue) et on r√©alise le mapping par rapport √† cette valeur :</p>
<img src="images/absmax.webp" alt="absmax" width="500"/><p>La formule est assez basique : Consid√©rons <span class="math notranslate nohighlight">\(b\)</span> le nombre d‚Äôoctets que l‚Äôon veut quantize, <span class="math notranslate nohighlight">\(\alpha\)</span> la plus grande valeur absolue.<br />
Alors on peut calculer le <em>scale factor</em> de la mani√®re suivante :<br />
<span class="math notranslate nohighlight">\(s=\frac{2^{b-1}-1}{\alpha}\)</span><br />
On peut alors effectuer la quantization de <span class="math notranslate nohighlight">\(x\)</span> comme ceci :<br />
<span class="math notranslate nohighlight">\(x_{quantized}=round(s \times x)\)</span><br />
Pour d√©quantizer et retrouver une valeur FP32, on peut faire comme cela :<br />
<span class="math notranslate nohighlight">\(x_{dequantized}=\frac{x_{quantized}}{s}\)</span></p>
<p>Bien entendu, la valeur dequantiz√© ne sera pas √©quivalente √† la valeur avant quantization :</p>
<img src="images/absmaxExample.png" alt="absmaxExample" width="500"/>     
<p>et on peut quantifier les erreurs de quantization :</p>
<img src="images/absmaxError.png" alt="absmaxError" width="500"/></section>
<section id="quantization-asymetrique">
<h3>Quantization asym√©trique<a class="headerlink" href="#quantization-asymetrique" title="Link to this heading">#</a></h3>
<p>A l‚Äôinverse de la quantization sym√©trique, la quantization asym√©trique n‚Äôest pas sym√©trique autour de 0. Au lieu de √ßa, on map le minimum <span class="math notranslate nohighlight">\(\beta\)</span> et le maximum <span class="math notranslate nohighlight">\(\alpha\)</span> de la <em>range</em> des flottants d‚Äôorigine sur le minimum et le maximum de la <em>range</em> quantiz√©.<br />
La m√©thode la plus commune pour cela est appel√© <em>zero-point quantization</em>.</p>
<img src="images/asymetric.png" alt="asymetric" width="500"/><p>Avec cette m√©thode, le 0 a chang√© de position et c‚Äôest pourquoi cette m√©thode est appel√© asym√©trique.</p>
<p>Comme le 0 a √©t√© deplac√©, on a besoin de calculer la position du 0 (<em>zero-point</em>) pour effectuer le mapping lin√©aire.</p>
<p>On peut quantizer de la mani√®re suivante :<br />
<span class="math notranslate nohighlight">\(s=\frac{128 - - 127}{\alpha- \beta}\)</span><br />
On calcule le <em>zero-point</em> :
<span class="math notranslate nohighlight">\(z=round(-s \times \beta)-2^{b-1}\)</span><br />
et :<br />
<span class="math notranslate nohighlight">\(x_{quantized}=round(s \times x + z)\)</span><br />
Pour d√©quantizer, on peut alors appliquer la formule suivante :<br />
<span class="math notranslate nohighlight">\(x_{dequantized}=\frac{x_{quantized}-z}{s}\)</span></p>
<p>Les deux m√©thodes ont leurs avantages et inconv√©nients, on peut les comparer en regardant le comportement sur un <span class="math notranslate nohighlight">\(x\)</span> quelconque :</p>
<img src="images/compare.png" alt="compare" width="500"/></section>
<section id="clipping-et-modification-de-range">
<h3>Clipping et modification de range<a class="headerlink" href="#clipping-et-modification-de-range" title="Link to this heading">#</a></h3>
<p>Les m√©thodes que nous avons pr√©sent√© pr√©sentent un d√©faut majeur. Ces m√©thodes ne sont pas du tout robustes aux <em>outliers</em>. Imaginons que notre vecteur <span class="math notranslate nohighlight">\(x\)</span> contient les valeurs suivante : [-0.59, -0.21, -0.07, 0.13, 0.28, 0.57, 256]. Si l‚Äôon fait notre <em>mapping</em> habituel, on va obtenir des valeurs identiques pour tous les √©l√©ments sauf l‚Äô<em>outlier</em> (256) :</p>
<img src="images/outlier.png" alt="outlier" width="500"/><p>C‚Äôest tr√®s probl√©matique car la perte d‚Äôinformation est colossale.</p>
<p>En pratique, on peut d√©cider de <em>clip</em> certaines valeurs pour diminuer la <em>range</em> dans l‚Äôespace des flottants (avant d‚Äôappliquer la quantization). Par exemple, on pourrait d√©cider de limiter les valeurs dans la plage [-5,5] et toutes les valeurs en dehors de cette plage seront mapp√© aux valeurs maximales ou minimales de quantization (127 ou -127 pour INT8) :</p>
<img src="images/clipping.png" alt="clipping" width="500"/>
<p>En faisant cela, on diminue grandement l‚Äôerreur sur les non-<em>outliers</em> mais on l‚Äôaugmente pour les <em>outliers</em> (ce qui peut √©galement √™tre probl√©matique).</p>
</section>
<section id="calibration">
<h3>Calibration<a class="headerlink" href="#calibration" title="Link to this heading">#</a></h3>
<p>Dans la partie pr√©c√©dente, on a utilis√© arbitrairement une plage de valeur de [-5,5]. La s√©lection de cette plage de valeur n‚Äôest pas al√©atoire et est determin√©e par une m√©thode que l‚Äôon appelle <em>calibration</em>. L‚Äôid√©e est de trouver une plage de valeur qui minimise l‚Äôerreur l‚Äôerreur de quantization pour l‚Äôensemble des valeurs. Les m√©thodes de <em>calibration</em> utilis√©es sont diff√©rentes selon le type de param√®tres que l‚Äôon cherche √† quantizer.</p>
<p><strong>Calibration pour les poids et les biais</strong>  :<br />
Les poids et les biais sont des valeurs statiques (fixes apr√®s l‚Äôentra√Ænement du mod√®le). Ce sont des valeurs que l‚Äôon connait avant de faire l‚Äôinf√©rence.<br />
Souvent, comme il y a beaucoup plus de poids que de biais, on va conserver la pr√©cision de base sur les biais et effectuer la quantization uniquement sur les poids.</p>
<p>Pour les poids, il y a plusieurs m√©thodes de calibration possibles :</p>
<ul class="simple">
<li><p>On peut choisir manuellement un pour√ßentage de la plage d‚Äôentr√©e</p></li>
<li><p>On peut optimiser la distance MSE entre les poids de base et les poids quantiz√©s</p></li>
<li><p>On peut minimiser l‚Äôentropie (avec le KL-divergence) entre les poids de base et les poids quantiz√©s</p></li>
</ul>
<p>La m√©thode avec pour√ßentage est similaire √† la m√©thode que nous avons utilis√© pr√©cedemment. Les deux autres m√©thodes sont plus rigoureuses et efficaces.</p>
<p><strong>Calibration pour les activations</strong> :<br />
A l‚Äôinverse des poids et des biais, les activations sont d√©pendantes de la valeur d‚Äôentr√©e du mod√®le. Il est donc tr√®s compliqu√© de les quantizer efficacement. Ces valeurs sont mises √† jour apr√®s chaque couche et on peut conna√Ætre leurs valeurs uniquement pendant l‚Äôinf√©rence lorsque la couche du mod√®le traite les valeurs.<br />
Cela nous am√®ne √† la partie suivante qui traite de deux m√©thodes diff√©rentes pour la quantization des activations (et √©galement des poids).<br />
Ces m√©thodes sont :</p>
<ul class="simple">
<li><p>La <em>post-training quantization</em> (PTQ) : la quantization intervient apr√®s l‚Äôentra√Ænement du mod√®le</p></li>
<li><p>La <em>quantization aware training</em> (QAT) : la quantization se fait pendant l‚Äôentra√Ænement ou le <em>fine-tuning</em> du mod√®le.</p></li>
</ul>
</section>
</section>
<section id="post-training-quantization-ptq">
<h2>Post-Training Quantization (PTQ)<a class="headerlink" href="#post-training-quantization-ptq" title="Link to this heading">#</a></h2>
<p>Une des mani√®res les plus fr√©quentes de faire de la quantization est de le faire apr√®s l‚Äôentra√Ænement du mod√®le. D‚Äôun point de vue pratique, c‚Äôest assez logique car cela ne n√©cessite pas d‚Äôentra√Æner ou de fine-tune le mod√®le.</p>
<p>La quantization des poids est effectu√©e en utilisant soit la quantization sym√©trique ou asym√©trique.</p>
<p>Pour les activations, ce n‚Äôest pas pareil puisqu‚Äôon ne connait pas la plage de valeurs prises par la distribution des activations.<br />
On a deux formes de quantization pour les activations :</p>
<ul class="simple">
<li><p>La dynamic quantization</p></li>
<li><p>La static quantization</p></li>
</ul>
<section id="dynamic-quantization">
<h3>Dynamic quantization<a class="headerlink" href="#dynamic-quantization" title="Link to this heading">#</a></h3>
<p>Dans la quantization dynamique, on collecte les activations apr√®s que la donn√©e soit pass√©e dans une couche. La distribution de la couche est ensuite quantiz√© en calculant le <em>zeropoint</em> et le <em>scale factor</em>.</p>
<img src="images/dynamicQ.webp" alt="dynamicQ" width="500"/>
<p>Dans ce processus, chaque couche  ses propres valeurs de <em>zeropoint</em> et de <em>scale factor</em> et donc la quantization n‚Äôest pas la m√™me.</p>
<img src="images/dynamicQ2.webp" alt="dynamicQ" width="500"/><p><strong>Note</strong> : Ce processus de quantization a lieu <strong>pendant</strong> l‚Äôinf√©rence.</p>
</section>
<section id="static-quantization">
<h3>Static quantization<a class="headerlink" href="#static-quantization" title="Link to this heading">#</a></h3>
<p>A l‚Äôinverse de la <em>dynamic quantization</em>, la <em>static quantization</em> ne calcule pas le <em>zeropoint</em> et le <em>scale factor</em> pendant l‚Äôinf√©rence. En effet, dans la m√©thode de static quantization, les valeurs de <em>zeropoint</em> et <em>scale factor</em> sont calcul√©s avant l‚Äôinf√©rence √† l‚Äôaide d‚Äôun <em>dataset</em> de <em>calibration</em>. Ce <em>dataset</em> est suppos√© √™tre representatif des donn√©es et permet de calculer les distributions potentiels prises par les activations.</p>
<img src="images/staticQ.png" alt="staticQ" width="500"/>
<p>Apr√®s avoir collect√© les valeurs des activations sur l‚Äôensemble du <em>dataset</em> de <em>calibration</em>, on peut les utiliser pour calculer le <em>scale factor</em> et le <em>zeropoint</em> qui seront ensuite utilis√© pour toutes les activations.</p>
</section>
<section id="difference-entre-dynamic-et-static-quantization">
<h3>Diff√©rence entre dynamic et static quantization<a class="headerlink" href="#difference-entre-dynamic-et-static-quantization" title="Link to this heading">#</a></h3>
<p>En g√©n√©ral, la <em>dynamic quantization</em> est un peu plus pr√©cise car elle calcule les valeurs de <em>scale factor</em> et de <em>zeropoint</em> pour chaque couche mais ce processus a √©galement tendance √† ralentir le temps d‚Äôinf√©rence.</p>
<p>A l‚Äôinverse, la <em>static quantization</em> est moins pr√©cise mais plus rapide.</p>
</section>
</section>
<section id="ptq-la-quantization-en-4-bit">
<h2>PTQ : la quantization en 4-bit<a class="headerlink" href="#ptq-la-quantization-en-4-bit" title="Link to this heading">#</a></h2>
<p>Dans l‚Äôid√©al, on aimerait pousser la quantization au maximum, c‚Äôest-√†-dire 4 bits au lieu de 8 bits. En pratique, ce n‚Äôest pas facile car cela augmente drastiquement l‚Äôerreur si l‚Äôon emploie simplement les m√©thodes que l‚Äôon a vu jusqu‚Äô√† pr√©sent.</p>
<p>Il y a cependant quelques m√©thodes permettant de r√©duire le nombre de bits jusqu‚Äô√† 2 bits (il est recommand√© de rester √† 4 bits).</p>
<p>Parmi ces m√©thodes, on en retrouve deux principales :</p>
<ul class="simple">
<li><p>GPTQ (utilise seulement le GPU)</p></li>
<li><p>GGUF (peut √©galement utilis√© le CPU en partie)</p></li>
</ul>
<section id="gptq">
<h3>GPTQ<a class="headerlink" href="#gptq" title="Link to this heading">#</a></h3>
<p>GPTQ est probablement la m√©thode la plus utilis√©e pour la quantization 4-bits. L‚Äôid√©e est d‚Äôutiliser la quantization asym√©trique sur chaque couche ind√©pendamment :</p>
<img src="images/GPTQ.png" alt="GPTQ" width="500"/>
<p>Pendant le processus de quantization, les poids sont convertis en l‚Äôinverse de la matrice Hessian (d√©riv√©e seconde de la fonction de <em>loss</em>) ce qui nous permet de savoir si la sortie du mod√®le est sensible aux changements de chaque poids. De mani√®re simplifi√©, cela permet de calculer l‚Äôimportance de chaque poids dans une couche. Les poids associ√©s √† de petites valeurs dans la Hessian sont les plus importants car un changement de ces poids va affecter le mod√®le significativement.</p>
<img src="images/hessian.png" alt="hessian" width="500"/>
<p>On va ensuite quantizer puis dequantizer les poids pour obtenir notre <em>quantization error</em>. Cette erreur nous permet pond√©rer l‚Äôerreur de quantization par rapport √† la vraie erreur et √† la matrice Hessian.</p>
<img src="images/GPTQError.png" alt="GPTQError" width="500"/>
<p>L‚Äôerreur pond√©r√©e est calcul√©e comme ceci :<br />
<span class="math notranslate nohighlight">\(q=\frac{x_1-y_1}{h_1}\)</span> o√π <span class="math notranslate nohighlight">\(x_1\)</span> est la valeur avant quantization, <span class="math notranslate nohighlight">\(y_1\)</span> est la valeur apr√®s quantization/dequantization et <span class="math notranslate nohighlight">\(h_1\)</span> est la valeur correspondante dans la matrice Hessian.</p>
<p>Ensuite, nous redistribuons cette erreur de quantification pond√©r√©e sur les autres poids de la ligne. Cela permet de maintenir la fonction globale et la sortie du r√©seau. Par exemple, pour <span class="math notranslate nohighlight">\(x_2\)</span>:<br />
<span class="math notranslate nohighlight">\(x_2=x_2 + q \times h_2\)</span></p>
<img src="images/GPTQprocess.png" alt="GPTQprocess" width="500"/>
<p>On fait ce process jusqu‚Äô√† ce que toutes les valeurs soient quantiz√©s.<br />
En pratique, cette m√©thode marche bien car tous les poids sont corr√©l√©s les uns avec les autres donc si un poids a une grosse erreur de quantization, les autres poids sont chang√©s pour compenser l‚Äôerreur (en se basant sur la Hessian).</p>
</section>
<section id="gguf">
<h3>GGUF<a class="headerlink" href="#gguf" title="Link to this heading">#</a></h3>
<p>GPTQ est une tr√®s bonne m√©thode pour faire tourner un LLM sur un GPU. Cependant, m√™me avec cette quantization, on a parfois pas assez de m√©moire GPU pour faire tourner un mod√®le LLM profond. La m√©thode GGUF permet de d√©placer n‚Äôimporte quelle couche du LLM sur le CPU.</p>
<p>De cette mani√®re, on peut utiliser la m√©moire vive et la m√©moire vid√©o (vram) en m√™me temps.</p>
<p>Cette m√©thode de quantization est chang√©e fr√©quemment et d√©pend du niveau de bit quantization que l‚Äôon souhaite.</p>
<p>De mani√®re g√©n√©rale, la m√©thode fonctionne de la mani√®re suivante :</p>
<p>D‚Äôabord, les poids d‚Äôune couche sont divis√©s en <em>super block</em> o√π chaque <em>super block</em> est √† nouveau divis√© en <em>sub blocks</em>. On va ensuite extraire les valeurs <span class="math notranslate nohighlight">\(s\)</span> et <span class="math notranslate nohighlight">\(\alpha\)</span> (<em>absmax</em>) pour chaque <em>block</em> (le <em>super</em> et les <em>sub</em>).</p>
<img src="images/GGUF.png" alt="GGUF" width="500"/>
<p>Les <em>scales factor</em> <span class="math notranslate nohighlight">\(s\)</span> des <em>sub block</em> sont ensuite quantiz√© √† nouveau en utilisant l‚Äôinformation du <em>super block</em> (qui a son propre <em>scale factor</em>). Cette m√©thode est appel√©e <em>block-wise quantization</em>.</p>
<p><strong>Note</strong> : De mani√®re g√©n√©rale, le niveau de quantization est diff√©rent entre les <em>sub block</em> et le <em>super block</em> : le <em>super block</em> a une pr√©cision sup√©rieure aux <em>sub block</em> le plus souvent.</p>
</section>
</section>
<section id="quantization-aware-training-qat">
<h2>Quantization Aware Training (QAT)<a class="headerlink" href="#quantization-aware-training-qat" title="Link to this heading">#</a></h2>
<p>Au lieu d‚Äôeffectuer la quantization apr√®s l‚Äôentra√Ænement, on peut le faire pendant l‚Äôentra√Ænement. En effet, faire la quantization apr√®s l‚Äôentra√Ænement ne tient pas compte du proc√©d√© d‚Äôentra√Ænement ce qui peut poser des probl√®mes.</p>
<p>La <em>quantization aware training</em> est une m√©thode permettant d‚Äôeffectuer la quantization pendant l‚Äôentra√Ænement et d‚Äôapprendre les diff√©rents param√®tres de quantization pendant la r√©tropropagation :</p>
<img src="images/QAT.png" alt="QAT" width="500"/>
<p>En pratique, cette m√©thode est souvent plus pr√©cise que la PTQ parce que la quantization est d√©j√† pr√©vue lors de l‚Äôentrainement et on peut donc adapter le mod√®le sp√©cifiquement dans un objectif futur de quantization.</p>
<p>Cette approche fonctionne de la mani√®re suivante :<br />
Pendant l‚Äôentra√Ænement, un processus de quantization/dequantization (<em>fake quantization</em>) est introduit (quantize de 32 bits √† 4 bits puis dequantize de 4 bits √† 32 bits par exemple).</p>
<img src="images/fakequantize.png" alt="fakequantize" width="500"/>
<p>Cette approche permet au mod√®le de consid√©rer la quantization pendant l‚Äôentra√Ænement et donc d‚Äôadapter la mise √† jours de poids pour favoriser des bons r√©sultats du mod√®le quantiz√©.</p>
<p>Une fa√ßon de voir les choses est d‚Äôimaginer que le mod√®le va converger vers des minimums larges qui minimize l‚Äôerreur de quantization plut√¥t que des minimuns √©troits qui pourraient provoquer des erreurs lors de la quantization. Pour un mod√®le entra√Æn√© sans <em>fake quantization</em>, il n‚Äôy aurait pas de pr√©f√©rences sur le minimum choisi pour la convergence :</p>
<img src="images/minimums.png" alt="minimums" width="500"/>
<p>En pratique, les mod√®les entrain√© de mani√®re classique ont un <em>loss</em> plus faible que les mod√®le entra√Æn√© en QAT lorsque la pr√©cision est grande (FP32) mais d√®s lors que l‚Äôon quantize le mod√®le, le mod√®le QAT sera bien plus performant qu‚Äôun mod√®le quantiz√© via une m√©thode PTQ.</p>
<section id="bitnet-quantization-1-bit">
<h3>BitNet : quantization 1-bit<a class="headerlink" href="#bitnet-quantization-1-bit" title="Link to this heading">#</a></h3>
<p>L‚Äôid√©al pour r√©duire la taille d‚Äôun mod√®le serait de quantit√© en 1 seul bit. Cela parait fou, comment peut-on imaginer repr√©senter un r√©seau de neurones avec uniquement est 0 et des 1 pour chaque poids.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/2310.11453">BitNet</a> propose de representer les poids d‚Äôun mod√®le avec un seul bit en utilisant la valeur -1 ou 1 pour un poids. Il faut imaginer que l‚Äôon remplace les couches lin√©aires de l‚Äôarchitecture transformers par des couches BitLinear :</p>
<img src="images/bitTransformer.png" alt="bitTransformer" width="500"/>
<p>La couche BitLinear marche exactement comme une couche lin√©aire de base sauf que les poids sont repr√©sent√© avec un unique bit et les activations en INT8.</p>
<p>Comme expliqu√© pr√©cedemment, il y a une forme de <em>fake quantization</em> permettant d‚Äôapprendre au mod√®le l‚Äôeffet de la quantization pour le forcer √† s‚Äôadapter √† cette nouvelle contrainte :</p>
<img src="images/bitnet.png" alt="bitnet" width="500"/>
<p>Analysons cette couche √©tape par etape :</p>
<p><strong>Premi√®re Etape : Quantization des poids</strong><br />
Pendant l‚Äôentra√Ænement, les poids sont stock√©s en INT8 et quantiz√© en 1-bit en utilisant la fonction <em>signum</em>.<br />
Cette fonction permet simplement de centrer la distribution des poids en 0 et convertit tout ce qui est inf√©rieur √† 0 en -1 et tout ce qui est sup√©rieur √† 0 en 1.</p>
<img src="images/weigthquanti.png" alt="weightquanti" width="500"/>
<p>Une valeur <span class="math notranslate nohighlight">\(\beta\)</span> (valeur moyenne absolue) est √©galement extraite pour le processus de d√©quantization.</p>
<p><strong>Deuxi√®me Etape : Quantization des activation</strong><br />
Pour les activations, la couche BitLinear utilise la quantization <em>absmax</em> pour convertir de FP16 √† INT8 et une valeur <span class="math notranslate nohighlight">\(\alpha\)</span> (valeur maximum absolue) est stock√©e pour la d√©quantization.</p>
<p><strong>Troisi√®me Etape : Dequantization</strong><br />
A partir des <span class="math notranslate nohighlight">\(\alpha\)</span> et <span class="math notranslate nohighlight">\(\beta\)</span> que l‚Äôon a gard√©, on peut utiliser ces valeurs pour d√©quantizer et repasser en pr√©cision FP16.</p>
<p>Et c‚Äôest tout, la proc√©dure est assez simple et permet au mod√®le d‚Äô√™tre repr√©sent√© avec uniquement des -1 et des 1.</p>
<p>Les auteurs du papier ont remarqu√© que, en utilisant cette technique, on obtient des bons r√©sultats sur des mod√®les assez profonds (plus de 30B) mais les r√©sultats sont assez moyens pour des mod√®les plus petits.</p>
</section>
<section id="bitnet-1-58-on-a-besoin-du-zero">
<h3>BitNet 1.58 : On a besoin du z√©ro !<a class="headerlink" href="#bitnet-1-58-on-a-besoin-du-zero" title="Link to this heading">#</a></h3>
<p>La m√©thode <a class="reference external" href="https://arxiv.org/pdf/2402.17764">BitNet1.58</a> a √©t√© introduite pour am√©liorer le mod√®le pr√©c√©dent notamment pour le cas des mod√®les plus petits.<br />
Dans cette m√©thode, les auteurs proposent d‚Äôajouter la valeur 0 en plus de -1 et 1. Cela ne parait pas √™tre un gros changement mais cette m√©thode permet d‚Äôam√©liorer grandement le mod√®le BitNet original.</p>
<p><strong>Note</strong> : Le mod√®le est surnomm√© 1.58 bits car <span class="math notranslate nohighlight">\(log_2(3)=1.58\)</span> donc th√©oriquement, une repr√©sentation de 3 valeurs utilise 1.58 bits.</p>
<p>Mais alors pourquoi 0 est si utile ?<br />
En fait, il faut simplement revenir aux bases et regarder la multiplication matricielle.<br />
Une multiplication matricielle peut √™tre d√©compos√©e en deux op√©rations : la multiplication des poids deux par deux et la somme de l‚Äôensemble des ces poids.<br />
Avec -1 et 1, lors de la somme, on pouvait d√©cider uniquement d‚Äôajouter la valeur ou de la soustraire. Avec l‚Äôajout du 0, on peut maintenant ignorer la valeur :</p>
<ul class="simple">
<li><p>1 : Je veux ajouter cette valeur</p></li>
<li><p>0 : Je veux ignorer cette valeur</p></li>
<li><p>-1 : Je veux soustraire cette valeur</p></li>
</ul>
<p>De cette mani√®re, on peut filtrer efficacement les valeurs ce qui permet une bien meilleure repr√©sentation.</p>
<p>Pour r√©aliser le quantization en 1.58b, on utilise la quantization <em>absmean</em> qui est une variante de <em>absmax</em>. Au lieu de se baser sur le maximum, on se base sur la moyenne en valeur absolue <span class="math notranslate nohighlight">\(\alpha\)</span> et on arrondit ensuite les valeurs √† -1, 0 ou 1 :</p>
<img src="images/bitnet158.png" alt="bitnet158" width="500"/>
<p>Et voil√†, c‚Äôest simplement ces deux techniques (representation ternaire et <em>absmean</em> quantization) qui permettent d‚Äôam√©liorer drastiquement la m√©thode BitNet classique et de proposer des mod√®les extr√©mement quantiz√©s et encore performants.</p>
</section>
</section>
<section id="fine-tuning-des-modele-de-langages">
<h2>Fine-Tuning des mod√®le de langages<a class="headerlink" href="#fine-tuning-des-modele-de-langages" title="Link to this heading">#</a></h2>
<p>Lorsque nous avons calcul√© la VRAM n√©cessaire pour un mod√®le, on a regard√© uniquement pour l‚Äôinf√©rence. Si l‚Äôon souhaite entra√Æner le mod√®le, la VRAM n√©cessaire est beaucoup plus importante et va d√©pendre de l‚Äôoptimizer que l‚Äôon utilise (voir <a class="reference internal" href="05_Optimizer.html"><span class="std std-doc">cours sur les optimizers</span></a>). On peut alors imaginer que les LLMs ont besoin d‚Äôune quantit√© √©norme de m√©moire pour √™tre entra√Æn√© ou <em>fine-tune</em>.</p>
<p>Pour r√©duire cette n√©cessit√© en m√©moire, des m√©thodes de <em>parameter efficient fine-tuning</em>(PEFT) ont √©t√© propos√©es et permettent de ne r√©entrainer qu‚Äôune partie du mod√®le. En plus de permettre de fine-tuner les mod√®les, cela a √©galement pour effet d‚Äô√©viter le <em>catastrophic forgetting</em> car on entra√Æne uniquement une petite partie des param√®tres totaux du mod√®le.</p>
<p>Il existe de nombreuses m√©thodes pour le PEFT : LoRA, <em>Adapter</em>, <em>Prefix Tuning</em>, <em>Prompt Tuning</em>, QLoRA etc ‚Ä¶</p>
<p>L‚Äôid√©e avec les m√©thodes type <em>Adapter</em>, LoRA et QLora est d‚Äôajouter une couche entra√Ænable permettant d‚Äôadapter la valeur des poids (sans avoir besoin de r√©-entra√Æner les couches de base du mod√®le).</p>
<section id="lora">
<h3>LoRA<a class="headerlink" href="#lora" title="Link to this heading">#</a></h3>
<p>La m√©thode <a class="reference external" href="https://arxiv.org/pdf/2106.09685">LoRA (low-rank adaptation of large language models)</a> est une technique de <em>fine-tuning</em> permettant d‚Äôadapter un LLM √† une t√¢che ou un domaine sp√©cifique. Cette m√©thode introduit des matrices entra√Ænables de d√©composition en rang √† chaque couche du transformer ce qui r√©duit les param√®tres entra√Ænables du mod√®le car les couches de bases sont <em>frozen</em>. La m√©thode peut potentiellement diminuer le nombre de param√®tres entra√Ænables d‚Äôun facteur 10 000 tout en r√©duisant la VRAM n√©cessaire pour l‚Äôentra√Ænement d‚Äôun facteur allant jusqu‚Äô√† 3. Les performances des mod√®les <em>fine-tune</em> avec cette m√©thode sont √©quivalent ou mieux que les mod√®les <em>fine-tune</em> de mani√®re classique sur de nombreuses t√¢ches.</p>
<img src="images/LoRA.webp" alt="LoRA" width="350"/>
<p>Au lieu de modifier la matrice <span class="math notranslate nohighlight">\(W\)</span> d‚Äôune couche, la m√©thode LoRA ajoute deux nouvelles matrices <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> dont le produit representent les modifications √† apporter √† la matrice <span class="math notranslate nohighlight">\(W\)</span>.<br />
<span class="math notranslate nohighlight">\(Y=W+AB\)</span><br />
Si <span class="math notranslate nohighlight">\(W\)</span> est de taille <span class="math notranslate nohighlight">\(m \times n\)</span> alors <span class="math notranslate nohighlight">\(A\)</span> est de taille <span class="math notranslate nohighlight">\(m \times r\)</span> et <span class="math notranslate nohighlight">\(B\)</span> de taille <span class="math notranslate nohighlight">\(r \times n\)</span> o√π <span class="math notranslate nohighlight">\(r\)</span> est le rang qui est bien plus petit que <span class="math notranslate nohighlight">\(m\)</span> ou <span class="math notranslate nohighlight">\(n\)</span> (ce qui explique la diminution du nombre de param√®tres). Pendant l‚Äôentra√Ænement, seulement <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> sont modifi√© ce qui permet au mod√®le d‚Äôapprendre la t√¢che sp√©cifique.</p>
</section>
<section id="qlora">
<h3>QLoRA<a class="headerlink" href="#qlora" title="Link to this heading">#</a></h3>
<p>QLoRA est une version am√©lior√©e de LoRA qui permet d‚Äôajouter la quantization 4-bit pour les param√®tres du mod√®le pr√©-entrain√©. Comme nous l‚Äôavons vu pr√©c√©demment, la quantization permet de r√©duire drastiquement la m√©moire n√©cessaire pour faire tourner le mod√®le. En combinant LoRA et la quantization, on peut maintenant imaginer faire entra√Æner un LLM sur un simple GPU grand public ce qui paraissait impossible il y encore quelques ann√©es.</p>
<p><strong>Note</strong> : QLoRA quantize les poids en <em>Normal Float</em> 4 (NF4) qui est une m√©thode de quantization sp√©cifique aux mod√®les de deep learning. Pour en savoir plus, vous pouvez consulter cette <a class="reference external" href="https://www.youtube.com/watch?v=TPcXVJ1VSRI&amp;amp;t=563s">vid√©o</a> au temps indiqu√©. Le NF4 est con√ßu sp√©cifiquement pour repr√©senter des distributions gaussiennes (et les r√©seaux de neurones sont suppos√©s avoir des poids suivants une distribution gaussienne).</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Bonus_CoursSp√©cifiques"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10_Tokenization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction √† la tokenization</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comment-representer-les-nombres-sur-un-ordinateur">Comment repr√©senter les nombres sur un ordinateur ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-a-la-quantization">Introduction √† la quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#point-rapide-sur-les-precisions-communes">Point rapide sur les pr√©cisions communes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-symetrique">Quantization sym√©trique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-asymetrique">Quantization asym√©trique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clipping-et-modification-de-range">Clipping et modification de range</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration">Calibration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#post-training-quantization-ptq">Post-Training Quantization (PTQ)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-quantization">Dynamic quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#static-quantization">Static quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-entre-dynamic-et-static-quantization">Diff√©rence entre dynamic et static quantization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ptq-la-quantization-en-4-bit">PTQ : la quantization en 4-bit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gguf">GGUF</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-aware-training-qat">Quantization Aware Training (QAT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bitnet-quantization-1-bit">BitNet : quantization 1-bit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bitnet-1-58-on-a-besoin-du-zero">BitNet 1.58 : On a besoin du z√©ro !</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-des-modele-de-langages">Fine-Tuning des mod√®le de langages</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">LoRA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qlora">QLoRA</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Simon Thomine
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div id="language-switcher" style="text-align: center; margin-top: 20px; padding: 10px; border-top: 1px solid #eee;">
  <span style="margin-right: 10px;">üåê Langue / Language:</span>
  <a href="#" onclick="switchToEnglish()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #f0f0f0; border-radius: 5px; transition: all 0.3s;">üá∫üá∏ English</a>
  <a href="#" onclick="switchToFrench()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #4CAF50; color: white; border-radius: 5px; font-weight: bold; transition: all 0.3s;">üá´üá∑ Fran√ßais</a>
</div>
<script>
function getBaseUrl() {
  // Get the base URL without the build path
  let baseUrl = window.location.origin;
  let pathname = window.location.pathname;
  
  // Remove the build-specific parts
  if (pathname.includes('fr/')) {
    baseUrl += pathname.split('fr/')[0];
  } else if (pathname.includes('en/')) {
    baseUrl += pathname.split('en/')[0];
  } else {
    baseUrl += pathname.split('/').slice(0, -1).join('/') + '/';
  }
  
  return baseUrl;
}

function getCurrentPage() {
  let pathname = window.location.pathname;
  if (pathname.includes('fr/')) {
    return pathname.split('fr/')[1] || 'index.html';
  } else if (pathname.includes('en/')) {
    return pathname.split('en/')[1] || 'index.html';
  }
  return 'index.html';
}

function switchToEnglish() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'en/' + currentPage;
  console.log('Switching to English:', newUrl);
  window.location.href = newUrl;
}

function switchToFrench() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'fr/' + currentPage;
  console.log('Switching to French:', newUrl);
  window.location.href = newUrl;
}
</script>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>