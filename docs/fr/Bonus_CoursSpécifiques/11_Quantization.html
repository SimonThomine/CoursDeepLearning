
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Quantization &#8212; Cours Deep Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Bonus_CoursSp√©cifiques/11_Quantization';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="QCM Interactif" href="qcm_11_Quantization.html" />
    <link rel="prev" title="QCM Interactif" href="qcm_10_Tokenization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content"><span style="font-size:2em; font-weight:bold;">üöÄ Apprendre le Deep Learning √† partir de z√©ro üöÄ</span></div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
  
    <p class="title logo__title">Cours Deep Learning</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Cours Deep Learning üöÄ
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üßÆ Fondations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html">D√©riv√©e et descente du gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/02_R%C3%A9gressionLogistique.html">R√©gression logistique</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîó R√©seau Fully Connected</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html">Mon premier r√©seau de neurones</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html">Introduction √† pytorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html">Techniques avanc√©es</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üñºÔ∏è R√©seau Convolutifs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html">Les couches de convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/02_R%C3%A9seauConvolutif.html">R√©seaux convolutifs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/03_ConvImplementation.html">Impl√©mentation de la couche de convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html">R√©seaux convolutifs avec PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html">Application sur un dataset d‚Äôimages en couleur</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html">Application de la segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîÑ Autoencodeurs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/01_IntuitionEtPremierAE.html">Introduction aux autoencodeurs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/02_DenoisingAE.html">Autoencodeur pour le d√©bruitage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üìù NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/01_Introduction.html">Introduction au NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/02_bigramme.html">Bigramme</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/03_R%C3%A9seauFullyConnected.html">R√©seau enti√®rement connect√©</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/04_WaveNet.html">PyTorch et WaveNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/05_Rnn.html">R√©seaux de neurones r√©currents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/06_Lstm.html">Long Short-Term Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_NLP/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ó HuggingFace</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/01_introduction.html">Introduction √† Hugging Face</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/02_ComputerVisionWithTransformers.html">Vision par ordinateur avec des Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/03_NlpWithTransformers.html">Traitement du langage naturel avec Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/04_AudioWithTransformers.html">Traitement de l‚Äôaudio avec Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/05_ImageGenerationWithDiffusers.html">G√©n√©ration d‚Äôimages avec Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/06_DemoAvecGradio.html">D√©mo avec Gradio</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚ö° Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/01_Introduction.html">Introduction aux transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/02_GptFromScratch.html">Construisons un GPT √† partir de z√©ro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/03_TrainingOurGpt.html">Entra√Ænement de notre mod√®le GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html">Architecture et particularit√©s du transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/05_UtilisationsPossibles.html">Utilisations possibles de l‚Äôarchitecture Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/06_VisionTransformerImplementation.html">Impl√©mentation du Vision Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/07_SwinTransformer.html">Swin Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéØ Detection Et Yolo</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/01_Introduction.html">Introduction √† la d√©tection d‚Äôobjets dans les images</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/02_YoloEnDetail.html">YOLO en d√©tail</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/03_Ultralytics.html">Ultralytics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîç Entrainement Contrastif</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/01_FaceVerification.html">V√©rification faciale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/02_NonSupervis%C3%A9.html">Apprentissage contrastif non supervis√©</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéì Transfer Learning Et Distillation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/01_TransferLearning.html">Apprentissage par transfert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/02_TransferLearningPytorch.html">Transfer Learning avec PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/03_Distillation.html">La distillation des connaissances</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/04_DistillationAnomalie.html">Distillation de connaissances pour la d√©tection d‚Äôanomalies non supervis√©e</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/05_FineTuningLLM.html">Fine-Tuning des LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/06_FineTuningBertHF.html">Fine-tuning de BERT avec Hugging Face</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üé® Modeles Generatifs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/01_Introduction.html">Introduction aux mod√®les g√©n√©ratifs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/02_GAN.html">R√©seaux antagonistes g√©n√©ratifs (GAN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/03_GanImplementation.html">Impl√©mentation d‚Äôun GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/04_VAE.html">Autoencodeurs variationnels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/05_VaeImplementation.html">Impl√©mentation d‚Äôun VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/06_NormalizingFlows.html">Normalizing flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/07_DiffusionModels.html">Mod√®les de diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/08_DiffusionImplementation.html">Impl√©mentation d‚Äôun mod√®le de diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/qcm.html">QCM Interactif</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéÅ Bonus Cours Sp√©cifiques</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="01_ActivationEtInitialisation.html">Activations et initialisations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_01_ActivationEtInitialisation.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="02_BatchNorm.html">Batch Normalization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_02_BatchNorm.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="03_DataAugmentation.html">Data Augmentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_03_DataAugmentation.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="04_Broadcasting.html">Broadcasting</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_04_Broadcasting.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="05_Optimizer.html">Comprendre les diff√©rents optimizers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_05_Optimizer.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="06_Regularisation.html">R√©gularisation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_06_Regularisation.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="07_ConnexionsResiduelles.html">Connexions r√©siduelles</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_07_ConnexionsResiduelles.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="08_CrossValidation.html">Introduction √† la validation crois√©e</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_08_CrossValidation.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="09_MetriquesEvaluation.html">M√©triques d‚Äô√©valuation des mod√®les</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_09_MetriquesEvaluation.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="10_Tokenization.html">Introduction √† la tokenization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_10_Tokenization.html">QCM Interactif</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Quantization</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qcm_11_Quantization.html">QCM Interactif</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/edit/main/fr/Bonus_CoursSp√©cifiques/11_Quantization.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/issues/new?title=Issue%20on%20page%20%2FBonus_CoursSp√©cifiques/11_Quantization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Bonus_CoursSp√©cifiques/11_Quantization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Quantization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comment-representer-les-nombres-sur-un-ordinateur">Comment repr√©senter les nombres sur un ordinateur ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-a-la-quantification">Introduction √† la quantification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#point-rapide-sur-les-precisions-communes">Point rapide sur les pr√©cisions communes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-symetrique">Quantification sym√©trique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-asymetrique">Quantification asym√©trique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clipping-et-modification-de-range">Clipping et modification de range</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration">Calibration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#post-training-quantification-ptq">Post-Training Quantification (PTQ)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-dynamique">Quantification dynamique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-statique">Quantification statique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-entre-quantification-dynamique-et-statique">Diff√©rence entre quantification dynamique et statique</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ptq-la-quantification-en-4-bit">PTQ : la quantification en 4-bit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gguf">GGUF</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-aware-training-qat">Quantification Aware Training (QAT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bitnet-quantification-1-bit">BitNet : quantification 1-bit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bitnet-1-58-on-a-besoin-du-zero">BitNet 1.58 : On a besoin du z√©ro !</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-des-modeles-de-langages">Fine-Tuning des mod√®les de langages</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">LoRA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qlora">QLoRA</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Link to this heading">#</a></h1>
<p>Les mod√®les de Deep Learning deviennent de plus en plus performants et volumineux. Prenons l‚Äôexemple des LLM (Large Language Models) : les meilleurs mod√®les open-source, comme Llama 3.1, comptent d√©sormais des centaines de milliards de param√®tres.</p>
<p>Charger un tel mod√®le sur un seul GPU est impossible. M√™me avec le GPU le plus puissant du march√© (le H100, dot√© de 80 Go de VRAM), il faut plusieurs GPU pour l‚Äôinf√©rence et encore plus pour l‚Äôentra√Ænement.</p>
<p>En pratique, on observe que plus un mod√®le a de param√®tres, meilleures sont ses performances. On ne souhaite donc pas r√©duire la taille des mod√®les. En revanche, on cherche √† diminuer l‚Äôespace m√©moire qu‚Äôils occupent.</p>
<p>Ce cours s‚Äôinspire fortement de deux articles : <a class="reference external" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization?utm_source=ainews&amp;amp;utm_medium=email&amp;amp;utm_campaign=ainews-to-be-named-5098">un guide visuel sur la quantification</a> et <a class="reference external" href="https://medium.com/&#64;dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766">une explication d√©taill√©e de QLoRA</a>. Les images utilis√©es proviennent √©galement de ces deux articles.</p>
<section id="comment-representer-les-nombres-sur-un-ordinateur">
<h2>Comment repr√©senter les nombres sur un ordinateur ?<a class="headerlink" href="#comment-representer-les-nombres-sur-un-ordinateur" title="Link to this heading">#</a></h2>
<p>Pour repr√©senter les nombres flottants sur un ordinateur, on utilise un certain nombre de bits. La norme <a class="reference external" href="https://en.wikipedia.org/wiki/IEEE_754">IEEE 754</a> d√©crit comment les bits peuvent repr√©senter un nombre. Cela se fait via trois parties : le signe, l‚Äôexposant et la mantisse.</p>
<p>Voici un exemple de repr√©sentation FP16 (16 bits) :</p>
<p><img alt="FP16" src="../_images/Fp16.png" /></p>
<p>Le signe d√©termine le signe du nombre, l‚Äôexposant donne les chiffres avant la virgule et la mantisse les chiffres apr√®s la virgule. Voici un exemple en image de la mani√®re de convertir la repr√©sentation FP16 en chiffre.</p>
<p><img alt="Convert" src="../_images/convert.webp" /></p>
<p>En g√©n√©ral, plus on utilise de bits pour repr√©senter une valeur, plus cette valeur peut √™tre pr√©cise ou couvrir une grande plage de valeurs. Par exemple, on peut comparer la pr√©cision FP16 et FP32 :</p>
<p><img alt="Compare FP" src="../_images/compareFP.webp" /></p>
<p>Une derni√®re chose importante √† savoir : il existe deux fa√ßons d‚Äô√©valuer une repr√©sentation. D‚Äôune part, la <em>dynamic range</em> qui indique la plage des valeurs que l‚Äôon peut repr√©senter, et la <em>precision</em> qui d√©crit l‚Äô√©cart entre deux valeurs voisines.</p>
<p>Plus l‚Äôexposant est grand, plus la <em>dynamic range</em> est grande, et plus la mantisse est grande, plus la <em>precision</em> est √©lev√©e (donc deux valeurs voisines sont proches).</p>
<p>En deep learning, on pr√©f√®re souvent utiliser la repr√©sentation BF16 au lieu de FP16. La repr√©sentation BF16 a un exposant plus grand mais une pr√©cision plus faible.</p>
<p>La figure suivante illustre les diff√©rences :</p>
<p><img alt="BF16" src="../_images/BF16.webp" /></p>
<p>Maintenant que l‚Äôon comprend les notions de pr√©cision des nombres flottants, on peut calculer la place qu‚Äôun mod√®le occupe en m√©moire en fonction de la pr√©cision. En FP32, un nombre est repr√©sent√© par 32 bits, ce qui correspond √† 4 octets (un octet vaut 8 bits). Pour obtenir l‚Äôutilisation m√©moire d‚Äôun mod√®le, on peut faire le calcul suivant :
<span class="math notranslate nohighlight">\(memory= \frac{n_{bits}}{8}*n_{params}\)</span></p>
<p>Prenons l‚Äôexemple d‚Äôun mod√®le de 70 milliards de param√®tres √† diff√©rents niveaux de pr√©cision : double (FP64), full-precision (FP32) et half-precision (FP16).
Pour FP64 : <span class="math notranslate nohighlight">\(\frac{64}{8} \times 70B = 560GB\)</span>
Pour FP32 : <span class="math notranslate nohighlight">\(\frac{32}{8} \times 70B = 280GB\)</span>
Pour FP16 : <span class="math notranslate nohighlight">\(\frac{16}{8} \times 70B = 140GB\)</span></p>
<p>On se rend bien compte qu‚Äôil est n√©cessaire de trouver une mani√®re de r√©duire la taille des mod√®les. Ici, m√™me le mod√®le en half-precision occupe 140 Go, ce qui correspond √† 2 GPU H100.</p>
<p><strong>Note</strong> : Ici, on parle de la pr√©cision pour l‚Äôinf√©rence. Pour l‚Äôentra√Ænement, comme il faut garder les activations en m√©moire pour la descente du gradient, on se retrouve avec beaucoup plus de param√®tres (voir partie sur QLoRA plus loin dans le cours).</p>
</section>
<section id="introduction-a-la-quantification">
<h2>Introduction √† la quantification<a class="headerlink" href="#introduction-a-la-quantification" title="Link to this heading">#</a></h2>
<p>Le but de la quantification est de r√©duire la pr√©cision d‚Äôun mod√®le en passant d‚Äôune pr√©cision riche comme FP32 √† une pr√©cision plus faible comme INT8.</p>
<p><strong>Note</strong> : INT8 est la fa√ßon de repr√©senter les entiers de -127 √† 127 sur 8 bits.</p>
<p><img alt="Quantification" src="../_images/quantization.webp" /></p>
<p>Bien s√ªr, en r√©duisant le nombre de bits pour repr√©senter les valeurs, on perd en pr√©cision.
Pour illustrer cela, regardons une image :</p>
<p><img alt="Cookies" src="../_images/cookies.webp" /></p>
<p>On remarque un ‚Äúgrain‚Äù dans l‚Äôimage, d√ª √† un manque de couleurs disponibles pour la repr√©senter.
Ce que l‚Äôon veut, c‚Äôest r√©duire le nombre de bits pour repr√©senter l‚Äôimage tout en conservant au maximum la pr√©cision de l‚Äôimage originale.</p>
<p>Il existe plusieurs mani√®res de faire de la quantification : la quantification sym√©trique et la quantification asym√©trique.</p>
<section id="point-rapide-sur-les-precisions-communes">
<h3>Point rapide sur les pr√©cisions communes<a class="headerlink" href="#point-rapide-sur-les-precisions-communes" title="Link to this heading">#</a></h3>
<p><strong>FP16</strong> : La <em>precision</em> et la <em>dynamic range</em> diminuent par rapport √† FP32.</p>
<p><img alt="FP16" src="../_images/fp16.webp" /></p>
<p><strong>BF16</strong> : La <em>precision</em> diminue fortement, mais la <em>dynamic range</em> reste la m√™me par rapport √† FP32.</p>
<p><img alt="BF16" src="../_images/bf16.webp" /></p>
<p><strong>INT8</strong> : On passe √† une repr√©sentation en entier.</p>
<p><img alt="INT8" src="../_images/int8.webp" /></p>
</section>
<section id="quantification-symetrique">
<h3>Quantification sym√©trique<a class="headerlink" href="#quantification-symetrique" title="Link to this heading">#</a></h3>
<p>Dans le cas de la quantification sym√©trique, la plage de valeurs de nos flottants d‚Äôorigine est mapp√©e de mani√®re sym√©trique sur la plage de valeurs de quantification. Cela signifie que le 0 dans les flottants est mapp√© sur le 0 dans la pr√©cision de quantification.</p>
<p><img alt="Quantification sym√©trique" src="../_images/symmetricq.webp" /></p>
<p>Une des mani√®res les plus courantes et les plus simples de r√©aliser cette op√©ration est d‚Äôutiliser la m√©thode <em>absmax (absolute maximum quantization)</em>. On prend la valeur maximale (en valeur absolue) et on r√©alise le mapping par rapport √† cette valeur :</p>
<p><img alt="Absmax" src="../_images/absmax.webp" /></p>
<p>La formule est assez basique : consid√©rons <span class="math notranslate nohighlight">\(b\)</span> le nombre d‚Äôoctets que l‚Äôon veut quantifier, <span class="math notranslate nohighlight">\(\alpha\)</span> la plus grande valeur absolue.
Alors on peut calculer le <em>scale factor</em> de la mani√®re suivante :
<span class="math notranslate nohighlight">\(s=\frac{2^{b-1}-1}{\alpha}\)</span>
On peut alors effectuer la quantification de <span class="math notranslate nohighlight">\(x\)</span> comme ceci :
<span class="math notranslate nohighlight">\(x_{quantized}=round(s \times x)\)</span>
Pour d√©quantifier et retrouver une valeur FP32, on peut faire comme cela :
<span class="math notranslate nohighlight">\(x_{dequantized}=\frac{x_{quantized}}{s}\)</span></p>
<p>Bien entendu, la valeur d√©quantifi√©e ne sera pas √©quivalente √† la valeur avant quantification :</p>
<p><img alt="Absmax Example" src="../_images/absmaxExample.png" /></p>
<p>et on peut quantifier les erreurs de quantification :</p>
<p><img alt="Absmax Error" src="../_images/absmaxError.png" /></p>
</section>
<section id="quantification-asymetrique">
<h3>Quantification asym√©trique<a class="headerlink" href="#quantification-asymetrique" title="Link to this heading">#</a></h3>
<p>√Ä l‚Äôinverse de la quantification sym√©trique, la quantification asym√©trique n‚Äôest pas sym√©trique autour de 0. Au lieu de cela, on map le minimum <span class="math notranslate nohighlight">\(\beta\)</span> et le maximum <span class="math notranslate nohighlight">\(\alpha\)</span> de la <em>range</em> des flottants d‚Äôorigine sur le minimum et le maximum de la <em>range</em> quantifi√©e.
La m√©thode la plus courante pour cela est appel√©e <em>zero-point quantization</em>.</p>
<p><img alt="Quantification asym√©trique" src="../_images/asymetric.png" /></p>
<p>Avec cette m√©thode, le 0 a chang√© de position, c‚Äôest pourquoi cette m√©thode est appel√©e asym√©trique.</p>
<p>Comme le 0 a √©t√© d√©plac√©, on a besoin de calculer la position du 0 (<em>zero-point</em>) pour effectuer le mapping lin√©aire.</p>
<p>On peut quantifier de la mani√®re suivante :
<span class="math notranslate nohighlight">\(s=\frac{128 - - 127}{\alpha- \beta}\)</span>
On calcule le <em>zero-point</em> :
<span class="math notranslate nohighlight">\(z=round(-s \times \beta)-2^{b-1}\)</span>
et :
<span class="math notranslate nohighlight">\(x_{quantized}=round(s \times x + z)\)</span>
Pour d√©quantifier, on peut alors appliquer la formule suivante :
<span class="math notranslate nohighlight">\(x_{dequantized}=\frac{x_{quantized}-z}{s}\)</span></p>
<p>Les deux m√©thodes ont leurs avantages et inconv√©nients, on peut les comparer en regardant le comportement sur un <span class="math notranslate nohighlight">\(x\)</span> quelconque :</p>
<p><img alt="Comparaison" src="../_images/compare.png" /></p>
</section>
<section id="clipping-et-modification-de-range">
<h3>Clipping et modification de range<a class="headerlink" href="#clipping-et-modification-de-range" title="Link to this heading">#</a></h3>
<p>Les m√©thodes que nous avons pr√©sent√©es pr√©sentent un d√©faut majeur. Ces m√©thodes ne sont pas du tout robustes aux <em>outliers</em>. Imaginons que notre vecteur <span class="math notranslate nohighlight">\(x\)</span> contient les valeurs suivantes : [-0.59, -0.21, -0.07, 0.13, 0.28, 0.57, 256]. Si l‚Äôon fait notre <em>mapping</em> habituel, on va obtenir des valeurs identiques pour tous les √©l√©ments sauf l‚Äô<em>outlier</em> (256) :</p>
<p><img alt="Outlier" src="../_images/outlier.png" /></p>
<p>C‚Äôest tr√®s probl√©matique car la perte d‚Äôinformation est colossale.</p>
<p>En pratique, on peut d√©cider de <em>clipper</em> certaines valeurs pour diminuer la <em>range</em> dans l‚Äôespace des flottants (avant d‚Äôappliquer la quantification). Par exemple, on pourrait d√©cider de limiter les valeurs dans la plage [-5,5] et toutes les valeurs en dehors de cette plage seront mapp√©es aux valeurs maximales ou minimales de quantification (127 ou -127 pour INT8) :</p>
<p><img alt="Clipping" src="../_images/clipping.png" /></p>
<p>En faisant cela, on diminue grandement l‚Äôerreur sur les non-<em>outliers</em> mais on l‚Äôaugmente pour les <em>outliers</em> (ce qui peut √©galement √™tre probl√©matique).</p>
</section>
<section id="calibration">
<h3>Calibration<a class="headerlink" href="#calibration" title="Link to this heading">#</a></h3>
<p>Dans la partie pr√©c√©dente, on a utilis√© arbitrairement une plage de valeur de [-5,5]. La s√©lection de cette plage de valeur n‚Äôest pas al√©atoire et est d√©termin√©e par une m√©thode que l‚Äôon appelle <em>calibration</em>. L‚Äôid√©e est de trouver une plage de valeur qui minimise l‚Äôerreur de quantification pour l‚Äôensemble des valeurs. Les m√©thodes de <em>calibration</em> utilis√©es sont diff√©rentes selon le type de param√®tres que l‚Äôon cherche √† quantifier.</p>
<p><strong>Calibration pour les poids et les biais</strong> :
Les poids et les biais sont des valeurs statiques (fixes apr√®s l‚Äôentra√Ænement du mod√®le). Ce sont des valeurs que l‚Äôon conna√Æt avant de faire l‚Äôinf√©rence.
Souvent, comme il y a beaucoup plus de poids que de biais, on va conserver la pr√©cision de base sur les biais et effectuer la quantification uniquement sur les poids.</p>
<p>Pour les poids, il y a plusieurs m√©thodes de calibration possibles :</p>
<ul class="simple">
<li><p>On peut choisir manuellement un pourcentage de la plage d‚Äôentr√©e</p></li>
<li><p>On peut optimiser la distance MSE entre les poids de base et les poids quantifi√©s</p></li>
<li><p>On peut minimiser l‚Äôentropie (avec le KL-divergence) entre les poids de base et les poids quantifi√©s</p></li>
</ul>
<p>La m√©thode avec pourcentage est similaire √† la m√©thode que nous avons utilis√©e pr√©c√©demment. Les deux autres m√©thodes sont plus rigoureuses et efficaces.</p>
<p><strong>Calibration pour les activations</strong> :
√Ä l‚Äôinverse des poids et des biais, les activations d√©pendent de la valeur d‚Äôentr√©e du mod√®le. Il est donc tr√®s compliqu√© de les quantifier efficacement. Ces valeurs sont mises √† jour apr√®s chaque couche et on peut conna√Ætre leurs valeurs uniquement pendant l‚Äôinf√©rence lorsque la couche du mod√®le traite les valeurs.
Cela nous am√®ne √† la partie suivante qui traite de deux m√©thodes diff√©rentes pour la quantification des activations (et √©galement des poids).
Ces m√©thodes sont :</p>
<ul class="simple">
<li><p>La <em>post-training quantification</em> (PTQ) : la quantification intervient apr√®s l‚Äôentra√Ænement du mod√®le</p></li>
<li><p>La <em>quantification aware training</em> (QAT) : la quantification se fait pendant l‚Äôentra√Ænement ou le <em>fine-tuning</em> du mod√®le.</p></li>
</ul>
</section>
</section>
<section id="post-training-quantification-ptq">
<h2>Post-Training Quantification (PTQ)<a class="headerlink" href="#post-training-quantification-ptq" title="Link to this heading">#</a></h2>
<p>Une des mani√®res les plus fr√©quentes de faire de la quantification est de le faire apr√®s l‚Äôentra√Ænement du mod√®le. D‚Äôun point de vue pratique, c‚Äôest assez logique car cela ne n√©cessite pas d‚Äôentra√Æner ou de <em>fine-tuner</em> le mod√®le.</p>
<p>La quantification des poids est effectu√©e en utilisant soit la quantification sym√©trique, soit la quantification asym√©trique.</p>
<p>Pour les activations, ce n‚Äôest pas pareil puisqu‚Äôon ne conna√Æt pas la plage de valeurs prises par la distribution des activations.
On a deux formes de quantification pour les activations :</p>
<ul class="simple">
<li><p>La quantification dynamique</p></li>
<li><p>La quantification statique</p></li>
</ul>
<section id="quantification-dynamique">
<h3>Quantification dynamique<a class="headerlink" href="#quantification-dynamique" title="Link to this heading">#</a></h3>
<p>Dans la quantification dynamique, on collecte les activations apr√®s que la donn√©e soit pass√©e dans une couche. La distribution de la couche est ensuite quantifi√©e en calculant le <em>zeropoint</em> et le <em>scale factor</em>.</p>
<p><img alt="Quantification dynamique" src="../_images/dynamicQ.webp" /></p>
<p>Dans ce processus, chaque couche a ses propres valeurs de <em>zeropoint</em> et de <em>scale factor</em> et donc la quantification n‚Äôest pas la m√™me.</p>
<p><img alt="Quantification dynamique 2" src="../_images/dynamicQ2.webp" /></p>
<p><strong>Note</strong> : Ce processus de quantification a lieu <strong>pendant</strong> l‚Äôinf√©rence.</p>
</section>
<section id="quantification-statique">
<h3>Quantification statique<a class="headerlink" href="#quantification-statique" title="Link to this heading">#</a></h3>
<p>√Ä l‚Äôinverse de la <em>quantification dynamique</em>, la <em>quantification statique</em> ne calcule pas le <em>zeropoint</em> et le <em>scale factor</em> pendant l‚Äôinf√©rence. En effet, dans la m√©thode de quantification statique, les valeurs de <em>zeropoint</em> et <em>scale factor</em> sont calcul√©es avant l‚Äôinf√©rence √† l‚Äôaide d‚Äôun <em>dataset</em> de <em>calibration</em>. Ce <em>dataset</em> est suppos√© √™tre repr√©sentatif des donn√©es et permet de calculer les distributions potentielles prises par les activations.</p>
<p><img alt="Quantification statique" src="../_images/staticQ.png" /></p>
<p>Apr√®s avoir collect√© les valeurs des activations sur l‚Äôensemble du <em>dataset</em> de <em>calibration</em>, on peut les utiliser pour calculer le <em>scale factor</em> et le <em>zeropoint</em> qui seront ensuite utilis√©s pour toutes les activations.</p>
</section>
<section id="difference-entre-quantification-dynamique-et-statique">
<h3>Diff√©rence entre quantification dynamique et statique<a class="headerlink" href="#difference-entre-quantification-dynamique-et-statique" title="Link to this heading">#</a></h3>
<p>En g√©n√©ral, la <em>quantification dynamique</em> est un peu plus pr√©cise car elle calcule les valeurs de <em>scale factor</em> et de <em>zeropoint</em> pour chaque couche, mais ce processus a √©galement tendance √† ralentir le temps d‚Äôinf√©rence.</p>
<p>√Ä l‚Äôinverse, la <em>quantification statique</em> est moins pr√©cise mais plus rapide.</p>
</section>
</section>
<section id="ptq-la-quantification-en-4-bit">
<h2>PTQ : la quantification en 4-bit<a class="headerlink" href="#ptq-la-quantification-en-4-bit" title="Link to this heading">#</a></h2>
<p>Dans l‚Äôid√©al, on aimerait pousser la quantification au maximum, c‚Äôest-√†-dire 4 bits au lieu de 8 bits. En pratique, ce n‚Äôest pas facile car cela augmente drastiquement l‚Äôerreur si l‚Äôon emploie simplement les m√©thodes que l‚Äôon a vues jusqu‚Äô√† pr√©sent.</p>
<p>Il y a cependant quelques m√©thodes permettant de r√©duire le nombre de bits jusqu‚Äô√† 2 bits (il est recommand√© de rester √† 4 bits).</p>
<p>Parmi ces m√©thodes, on en retrouve deux principales :</p>
<ul class="simple">
<li><p>GPTQ (utilise seulement le GPU)</p></li>
<li><p>GGUF (peut √©galement utiliser le CPU en partie)</p></li>
</ul>
<section id="gptq">
<h3>GPTQ<a class="headerlink" href="#gptq" title="Link to this heading">#</a></h3>
<p>GPTQ est probablement la m√©thode la plus utilis√©e pour la quantification 4-bits. L‚Äôid√©e est d‚Äôutiliser la quantification asym√©trique sur chaque couche ind√©pendamment :</p>
<p><img alt="GPTQ" src="../_images/GPTQ.png" /></p>
<p>Pendant le processus de quantification, les poids sont convertis en l‚Äôinverse de la matrice Hessian (d√©riv√©e seconde de la fonction de <em>loss</em>) ce qui nous permet de savoir si la sortie du mod√®le est sensible aux changements de chaque poids. De mani√®re simplifi√©e, cela permet de calculer l‚Äôimportance de chaque poids dans une couche. Les poids associ√©s √† de petites valeurs dans la Hessian sont les plus importants car un changement de ces poids va affecter le mod√®le significativement.</p>
<p><img alt="Hessian" src="../_images/hessian.png" /></p>
<p>On va ensuite quantifier puis d√©quantifier les poids pour obtenir notre <em>erreur de quantification</em>. Cette erreur nous permet de pond√©rer l‚Äôerreur de quantification par rapport √† la vraie erreur et √† la matrice Hessian.</p>
<p><img alt="GPTQError" src="../_images/GPTQError.png" /></p>
<p>L‚Äôerreur pond√©r√©e est calcul√©e comme ceci :
<span class="math notranslate nohighlight">\(q=\frac{x_1-y_1}{h_1}\)</span> o√π <span class="math notranslate nohighlight">\(x_1\)</span> est la valeur avant quantification, <span class="math notranslate nohighlight">\(y_1\)</span> est la valeur apr√®s quantification/d√©quantification et <span class="math notranslate nohighlight">\(h_1\)</span> est la valeur correspondante dans la matrice Hessian.</p>
<p>Ensuite, nous redistribuons cette erreur de quantification pond√©r√©e sur les autres poids de la ligne. Cela permet de maintenir la fonction globale et la sortie du r√©seau. Par exemple, pour <span class="math notranslate nohighlight">\(x_2\)</span>:
<span class="math notranslate nohighlight">\(x_2=x_2 + q \times h_2\)</span></p>
<p><img alt="GPTQprocess" src="../_images/GPTQprocess.png" /></p>
<p>On fait ce processus jusqu‚Äô√† ce que toutes les valeurs soient quantifi√©es.
En pratique, cette m√©thode marche bien car tous les poids sont corr√©l√©s les uns avec les autres donc si un poids a une grosse erreur de quantification, les autres poids sont chang√©s pour compenser l‚Äôerreur (en se basant sur la Hessian).</p>
</section>
<section id="gguf">
<h3>GGUF<a class="headerlink" href="#gguf" title="Link to this heading">#</a></h3>
<p>GPTQ est une tr√®s bonne m√©thode pour faire tourner un LLM sur un GPU. Cependant, m√™me avec cette quantification, on a parfois pas assez de m√©moire GPU pour faire tourner un mod√®le LLM profond. La m√©thode GGUF permet de d√©placer n‚Äôimporte quelle couche du LLM sur le CPU.</p>
<p>De cette mani√®re, on peut utiliser la m√©moire vive et la m√©moire vid√©o (VRAM) en m√™me temps.</p>
<p>Cette m√©thode de quantification est chang√©e fr√©quemment et d√©pend du niveau de bits de quantification que l‚Äôon souhaite.</p>
<p>De mani√®re g√©n√©rale, la m√©thode fonctionne de la mani√®re suivante :</p>
<p>D‚Äôabord, les poids d‚Äôune couche sont divis√©s en <em>super block</em> o√π chaque <em>super block</em> est √† nouveau divis√© en <em>sub blocks</em>. On va ensuite extraire les valeurs <span class="math notranslate nohighlight">\(s\)</span> et <span class="math notranslate nohighlight">\(\alpha\)</span> (<em>absmax</em>) pour chaque <em>block</em> (le <em>super</em> et les <em>sub</em>).</p>
<p><img alt="GGUF" src="../_images/GGUF.png" /></p>
<p>Les <em>scale factors</em> <span class="math notranslate nohighlight">\(s\)</span> des <em>sub blocks</em> sont ensuite quantifi√©s √† nouveau en utilisant l‚Äôinformation du <em>super block</em> (qui a son propre <em>scale factor</em>). Cette m√©thode est appel√©e <em>block-wise quantification</em>.</p>
<p><strong>Note</strong> : De mani√®re g√©n√©rale, le niveau de quantification est diff√©rent entre les <em>sub blocks</em> et le <em>super block</em> : le <em>super block</em> a une pr√©cision sup√©rieure aux <em>sub blocks</em> le plus souvent.</p>
</section>
</section>
<section id="quantification-aware-training-qat">
<h2>Quantification Aware Training (QAT)<a class="headerlink" href="#quantification-aware-training-qat" title="Link to this heading">#</a></h2>
<p>Au lieu d‚Äôeffectuer la quantification apr√®s l‚Äôentra√Ænement, on peut le faire pendant l‚Äôentra√Ænement. En effet, faire la quantification apr√®s l‚Äôentra√Ænement ne tient pas compte du proc√©d√© d‚Äôentra√Ænement, ce qui peut poser des probl√®mes.</p>
<p>La <em>quantification aware training</em> est une m√©thode permettant d‚Äôeffectuer la quantification pendant l‚Äôentra√Ænement et d‚Äôapprendre les diff√©rents param√®tres de quantification pendant la r√©tropropagation :</p>
<p><img alt="QAT" src="../_images/QAT.png" /></p>
<p>En pratique, cette m√©thode est souvent plus pr√©cise que la PTQ car la quantification est d√©j√† pr√©vue lors de l‚Äôentra√Ænement et on peut donc adapter le mod√®le sp√©cifiquement dans un objectif futur de quantification.</p>
<p>Cette approche fonctionne de la mani√®re suivante :
Pendant l‚Äôentra√Ænement, un processus de quantification/d√©quantification (<em>fake quantification</em>) est introduit (quantification de 32 bits √† 4 bits puis d√©quantification de 4 bits √† 32 bits par exemple).</p>
<p><img alt="Fake Quantification" src="../_images/fakequantize.png" /></p>
<p>Cette approche permet au mod√®le de consid√©rer la quantification pendant l‚Äôentra√Ænement et donc d‚Äôadapter la mise √† jour des poids pour favoriser de bons r√©sultats du mod√®le quantifi√©.</p>
<p>Une fa√ßon de voir les choses est d‚Äôimaginer que le mod√®le va converger vers des minimums larges qui minimisent l‚Äôerreur de quantification plut√¥t que des minimums √©troits qui pourraient provoquer des erreurs lors de la quantification. Pour un mod√®le entra√Æn√© sans <em>fake quantification</em>, il n‚Äôy aurait pas de pr√©f√©rences sur le minimum choisi pour la convergence :</p>
<p><img alt="Minimums" src="../_images/minimums.png" /></p>
<p>En pratique, les mod√®les entra√Æn√©s de mani√®re classique ont un <em>loss</em> plus faible que les mod√®les entra√Æn√©s en QAT lorsque la pr√©cision est grande (FP32), mais d√®s lors que l‚Äôon quantifie le mod√®le, le mod√®le QAT sera bien plus performant qu‚Äôun mod√®le quantifi√© via une m√©thode PTQ.</p>
<section id="bitnet-quantification-1-bit">
<h3>BitNet : quantification 1-bit<a class="headerlink" href="#bitnet-quantification-1-bit" title="Link to this heading">#</a></h3>
<p>L‚Äôid√©al pour r√©duire la taille d‚Äôun mod√®le serait de quantifier en 1 seul bit. Cela para√Æt fou, comment peut-on imaginer repr√©senter un r√©seau de neurones avec uniquement des 0 et des 1 pour chaque poids ?</p>
<p><a class="reference external" href="https://arxiv.org/pdf/2310.11453">BitNet</a> propose de repr√©senter les poids d‚Äôun mod√®le avec un seul bit en utilisant la valeur -1 ou 1 pour un poids. Il faut imaginer que l‚Äôon remplace les couches lin√©aires de l‚Äôarchitecture transformers par des couches BitLinear :</p>
<p><img alt="BitTransformer" src="../_images/bitTransformer.png" /></p>
<p>La couche BitLinear fonctionne exactement comme une couche lin√©aire de base, sauf que les poids sont repr√©sent√©s avec un unique bit et les activations en INT8.</p>
<p>Comme expliqu√© pr√©c√©demment, il y a une forme de <em>fake quantification</em> permettant d‚Äôapprendre au mod√®le l‚Äôeffet de la quantification pour le forcer √† s‚Äôadapter √† cette nouvelle contrainte :</p>
<p><img alt="BitNet" src="../_images/bitnet.png" /></p>
<p>Analysons cette couche √©tape par √©tape :</p>
<p><strong>Premi√®re √©tape : Quantification des poids</strong>
Pendant l‚Äôentra√Ænement, les poids sont stock√©s en INT8 et quantifi√©s en 1-bit en utilisant la fonction <em>signum</em>.
Cette fonction permet simplement de centrer la distribution des poids en 0 et convertit tout ce qui est inf√©rieur √† 0 en -1 et tout ce qui est sup√©rieur √† 0 en 1.</p>
<p><img alt="Quantification des poids" src="../_images/weigthquanti.png" /></p>
<p>Une valeur <span class="math notranslate nohighlight">\(\beta\)</span> (valeur moyenne absolue) est √©galement extraite pour le processus de d√©quantification.</p>
<p><strong>Deuxi√®me √©tape : Quantification des activations</strong>
Pour les activations, la couche BitLinear utilise la quantification <em>absmax</em> pour convertir de FP16 √† INT8 et une valeur <span class="math notranslate nohighlight">\(\alpha\)</span> (valeur maximum absolue) est stock√©e pour la d√©quantification.</p>
<p><strong>Troisi√®me √©tape : D√©quantification</strong>
√Ä partir des <span class="math notranslate nohighlight">\(\alpha\)</span> et <span class="math notranslate nohighlight">\(\beta\)</span> que l‚Äôon a gard√©s, on peut utiliser ces valeurs pour d√©quantifier et repasser en pr√©cision FP16.</p>
<p>Et c‚Äôest tout, la proc√©dure est assez simple et permet au mod√®le d‚Äô√™tre repr√©sent√© avec uniquement des -1 et des 1.</p>
<p>Les auteurs du papier ont remarqu√© que, en utilisant cette technique, on obtient de bons r√©sultats sur des mod√®les assez profonds (plus de 30B), mais les r√©sultats sont assez moyens pour des mod√®les plus petits.</p>
</section>
<section id="bitnet-1-58-on-a-besoin-du-zero">
<h3>BitNet 1.58 : On a besoin du z√©ro !<a class="headerlink" href="#bitnet-1-58-on-a-besoin-du-zero" title="Link to this heading">#</a></h3>
<p>La m√©thode <a class="reference external" href="https://arxiv.org/pdf/2402.17764">BitNet1.58</a> a √©t√© introduite pour am√©liorer le mod√®le pr√©c√©dent, notamment pour le cas des mod√®les plus petits.
Dans cette m√©thode, les auteurs proposent d‚Äôajouter la valeur 0 en plus de -1 et 1. Cela ne para√Æt pas √™tre un gros changement, mais cette m√©thode permet d‚Äôam√©liorer grandement le mod√®le BitNet original.</p>
<p><strong>Note</strong> : Le mod√®le est surnomm√© 1.58 bits car <span class="math notranslate nohighlight">\(log_2(3)=1.58\)</span>, donc th√©oriquement, une repr√©sentation de 3 valeurs utilise 1.58 bits.</p>
<p>Mais alors pourquoi 0 est-il si utile ?
En fait, il faut simplement revenir aux bases et regarder la multiplication matricielle.
Une multiplication matricielle peut √™tre d√©compos√©e en deux op√©rations : la multiplication des poids deux par deux et la somme de l‚Äôensemble de ces poids.
Avec -1 et 1, lors de la somme, on pouvait d√©cider uniquement d‚Äôajouter la valeur ou de la soustraire. Avec l‚Äôajout du 0, on peut maintenant ignorer la valeur :</p>
<ul class="simple">
<li><p>1 : Je veux ajouter cette valeur</p></li>
<li><p>0 : Je veux ignorer cette valeur</p></li>
<li><p>-1 : Je veux soustraire cette valeur</p></li>
</ul>
<p>De cette mani√®re, on peut filtrer efficacement les valeurs, ce qui permet une bien meilleure repr√©sentation.</p>
<p>Pour r√©aliser la quantification en 1.58 bits, on utilise la quantification <em>absmean</em> qui est une variante de <em>absmax</em>. Au lieu de se baser sur le maximum, on se base sur la moyenne en valeur absolue <span class="math notranslate nohighlight">\(\alpha\)</span> et on arrondit ensuite les valeurs √† -1, 0 ou 1 :</p>
<p><img alt="BitNet 1.58" src="../_images/bitnet158.png" /></p>
<p>Et voil√†, c‚Äôest simplement ces deux techniques (repr√©sentation ternaire et quantification <em>absmean</em>) qui permettent d‚Äôam√©liorer drastiquement la m√©thode BitNet classique et de proposer des mod√®les extr√™mement quantifi√©s et encore performants.</p>
</section>
</section>
<section id="fine-tuning-des-modeles-de-langages">
<h2>Fine-Tuning des mod√®les de langages<a class="headerlink" href="#fine-tuning-des-modeles-de-langages" title="Link to this heading">#</a></h2>
<p>Lorsque nous avons calcul√© la VRAM n√©cessaire pour un mod√®le, nous avons regard√© uniquement pour l‚Äôinf√©rence. Si l‚Äôon souhaite entra√Æner le mod√®le, la VRAM n√©cessaire est beaucoup plus importante et va d√©pendre de l‚Äôoptimizer que l‚Äôon utilise (voir <a class="reference internal" href="05_Optimizer.html"><span class="std std-doc">cours sur les optimizers</span></a>). On peut alors imaginer que les LLM ont besoin d‚Äôune quantit√© √©norme de m√©moire pour √™tre entra√Æn√©s ou <em>fine-tun√©s</em>.</p>
<p>Pour r√©duire cette n√©cessit√© en m√©moire, des m√©thodes de <em>parameter efficient fine-tuning</em> (PEFT) ont √©t√© propos√©es et permettent de ne r√©entra√Æner qu‚Äôune partie du mod√®le. En plus de permettre de <em>fine-tuner</em> les mod√®les, cela a √©galement pour effet d‚Äô√©viter le <em>catastrophic forgetting</em> car on entra√Æne uniquement une petite partie des param√®tres totaux du mod√®le.</p>
<p>Il existe de nombreuses m√©thodes pour le PEFT : LoRA, <em>Adapter</em>, <em>Prefix Tuning</em>, <em>Prompt Tuning</em>, QLoRA, etc.</p>
<p>L‚Äôid√©e avec les m√©thodes de type <em>Adapter</em>, LoRA et QLoRA est d‚Äôajouter une couche entra√Ænable permettant d‚Äôadapter la valeur des poids (sans avoir besoin de r√©entra√Æner les couches de base du mod√®le).</p>
<section id="lora">
<h3>LoRA<a class="headerlink" href="#lora" title="Link to this heading">#</a></h3>
<p>La m√©thode <a class="reference external" href="https://arxiv.org/pdf/2106.09685">LoRA (low-rank adaptation of large language models)</a> est une technique de <em>fine-tuning</em> permettant d‚Äôadapter un LLM √† une t√¢che ou un domaine sp√©cifique. Cette m√©thode introduit des matrices entra√Ænables de d√©composition en rang √† chaque couche du transformer, ce qui r√©duit les param√®tres entra√Ænables du mod√®le car les couches de base sont <em>frozen</em>. La m√©thode peut potentiellement diminuer le nombre de param√®tres entra√Ænables d‚Äôun facteur 10 000 tout en r√©duisant la VRAM n√©cessaire pour l‚Äôentra√Ænement d‚Äôun facteur allant jusqu‚Äô√† 3. Les performances des mod√®les <em>fine-tun√©s</em> avec cette m√©thode sont √©quivalentes ou meilleures que les mod√®les <em>fine-tun√©s</em> de mani√®re classique sur de nombreuses t√¢ches.</p>
<p><img alt="LoRA" src="../_images/LoRA.webp" /></p>
<p>Au lieu de modifier la matrice <span class="math notranslate nohighlight">\(W\)</span> d‚Äôune couche, la m√©thode LoRA ajoute deux nouvelles matrices <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> dont le produit repr√©sente les modifications √† apporter √† la matrice <span class="math notranslate nohighlight">\(W\)</span>.
<span class="math notranslate nohighlight">\(Y=W+AB\)</span>
Si <span class="math notranslate nohighlight">\(W\)</span> est de taille <span class="math notranslate nohighlight">\(m \times n\)</span>, alors <span class="math notranslate nohighlight">\(A\)</span> est de taille <span class="math notranslate nohighlight">\(m \times r\)</span> et <span class="math notranslate nohighlight">\(B\)</span> de taille <span class="math notranslate nohighlight">\(r \times n\)</span>, o√π <span class="math notranslate nohighlight">\(r\)</span> est le rang qui est bien plus petit que <span class="math notranslate nohighlight">\(m\)</span> ou <span class="math notranslate nohighlight">\(n\)</span> (ce qui explique la diminution du nombre de param√®tres). Pendant l‚Äôentra√Ænement, seulement <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> sont modifi√©s, ce qui permet au mod√®le d‚Äôapprendre la t√¢che sp√©cifique.</p>
</section>
<section id="qlora">
<h3>QLoRA<a class="headerlink" href="#qlora" title="Link to this heading">#</a></h3>
<p>QLoRA est une version am√©lior√©e de LoRA qui permet d‚Äôajouter la quantification 4-bit pour les param√®tres du mod√®le pr√©-entra√Æn√©. Comme nous l‚Äôavons vu pr√©c√©demment, la quantification permet de r√©duire drastiquement la m√©moire n√©cessaire pour faire tourner le mod√®le. En combinant LoRA et la quantification, on peut maintenant imaginer entra√Æner un LLM sur un simple GPU grand public, ce qui paraissait impossible il y a encore quelques ann√©es.</p>
<p><strong>Note</strong> : QLoRA quantifie les poids en <em>Normal Float</em> 4 (NF4), qui est une m√©thode de quantification sp√©cifique aux mod√®les de deep learning. Pour en savoir plus, vous pouvez consulter cette <a class="reference external" href="https://www.youtube.com/watch?v=TPcXVJ1VSRI&amp;amp;t=563s">vid√©o</a> au temps indiqu√©. Le NF4 est con√ßu sp√©cifiquement pour repr√©senter des distributions gaussiennes (et les r√©seaux de neurones sont suppos√©s avoir des poids suivant une distribution gaussienne).</p>
<p>QLoRA est une version am√©lior√©e de LoRA qui permet d‚Äôajouter la quantization 4-bit pour les param√®tres du mod√®le pr√©-entrain√©. Comme nous l‚Äôavons vu pr√©c√©demment, la quantization permet de r√©duire drastiquement la m√©moire n√©cessaire pour faire tourner le mod√®le. En combinant LoRA et la quantization, on peut maintenant imaginer faire entra√Æner un LLM sur un simple GPU grand public ce qui paraissait impossible il y encore quelques ann√©es.</p>
<p><strong>Note</strong> : QLoRA quantize les poids en <em>Normal Float</em> 4 (NF4) qui est une m√©thode de quantization sp√©cifique aux mod√®les de deep learning. Pour en savoir plus, vous pouvez consulter cette <a class="reference external" href="https://www.youtube.com/watch?v=TPcXVJ1VSRI&amp;amp;t=563s">vid√©o</a> au temps indiqu√©. Le NF4 est con√ßu sp√©cifiquement pour repr√©senter des distributions gaussiennes (et les r√©seaux de neurones sont suppos√©s avoir des poids suivants une distribution gaussienne).</p>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Bonus_CoursSp√©cifiques"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="qcm_10_Tokenization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">QCM Interactif</p>
      </div>
    </a>
    <a class="right-next"
       href="qcm_11_Quantization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">QCM Interactif</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comment-representer-les-nombres-sur-un-ordinateur">Comment repr√©senter les nombres sur un ordinateur ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-a-la-quantification">Introduction √† la quantification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#point-rapide-sur-les-precisions-communes">Point rapide sur les pr√©cisions communes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-symetrique">Quantification sym√©trique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-asymetrique">Quantification asym√©trique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clipping-et-modification-de-range">Clipping et modification de range</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration">Calibration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#post-training-quantification-ptq">Post-Training Quantification (PTQ)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-dynamique">Quantification dynamique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-statique">Quantification statique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-entre-quantification-dynamique-et-statique">Diff√©rence entre quantification dynamique et statique</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ptq-la-quantification-en-4-bit">PTQ : la quantification en 4-bit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gguf">GGUF</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantification-aware-training-qat">Quantification Aware Training (QAT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bitnet-quantification-1-bit">BitNet : quantification 1-bit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bitnet-1-58-on-a-besoin-du-zero">BitNet 1.58 : On a besoin du z√©ro !</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-des-modeles-de-langages">Fine-Tuning des mod√®les de langages</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">LoRA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qlora">QLoRA</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Simon Thomine
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div id="language-switcher" style="text-align: center; margin-top: 20px; padding: 10px; border-top: 1px solid #eee;">
  <span style="margin-right: 10px;">üåê Language / Langue:</span>
  <a href="#" onclick="switchToEnglish()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #4CAF50; color: white; border-radius: 5px; font-weight: bold; transition: all 0.3s;">üá∫üá∏ English</a>
  <a href="#" onclick="switchToFrench()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #f0f0f0; border-radius: 5px; transition: all 0.3s;">üá´üá∑ Fran√ßais</a>
  <a href="#" onclick="switchToSpanish()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #ffd700; border-radius: 5px; transition: all 0.3s;">üá™üá∏ Espa√±ol</a>
  <a href="#" onclick="switchToChinese()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #ff4b4b; color: white; border-radius: 5px; transition: all 0.3s;">üá®üá≥ ‰∏≠Êñá</a>
</div>
<script>
function getLangMatch() {
  // Cherche /fr/, /en/, /es/, /zh/ comme segment de chemin
  return window.location.pathname.match(/\/(fr|en|es|zh)\//);
}

function getBaseUrl() {
  let origin = window.location.origin;
  let pathname = window.location.pathname;
  let match = getLangMatch();
  if (match) {
    // Prend tout avant le segment de langue
    return origin + pathname.substring(0, match.index + 1);
  }
  // Sinon, retourne le chemin courant
  return origin + pathname.substring(0, pathname.lastIndexOf('/') + 1);
}

function getCurrentPage() {
  let match = getLangMatch();
  if (match) {
    // Prend tout apr√®s le segment de langue
    return window.location.pathname.substring(match.index + match[0].length) || 'index.html';
  }
  return 'index.html';
}

function switchToEnglish() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  window.location.href = baseUrl + 'en/' + currentPage;
}

function switchToFrench() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  window.location.href = baseUrl + 'fr/' + currentPage;
}

function switchToSpanish() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  window.location.href = baseUrl + 'es/' + currentPage;
}

function switchToChinese() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  window.location.href = baseUrl + 'zh/' + currentPage;
}
</script>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>