
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bigrama &#8212; Deep Learning Course</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '05_NLP/02_bigramme';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Red neuronal completamente conectada" href="03_R%C3%A9seauFullyConnected.html" />
    <link rel="prev" title="Introducci√≥n al PLN" href="01_Introduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content"><span style="font-size:2em; font-weight:bold;">üöÄ Aprende Deep Learning desde cero üöÄ</span></div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning Course</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Curso de Deep Learning üöÄ
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üßÆ Fundamentos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html">Derivada y descenso del gradiente</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_Fondations/02_R%C3%A9gressionLogistique.html">Regresi√≥n Log√≠stica</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîó Redes totalmente conectadas</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html">Mi primer red neuronal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html">Introducci√≥n a PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html">T√©cnicas avanzadas</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üñºÔ∏è Redes convolucionales</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html">Las capas de convoluci√≥n</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/02_R%C3%A9seauConvolutif.html">Redes convolucionales</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/03_ConvImplementation.html">Implementaci√≥n de la capa de convoluci√≥n</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html">Redes convolucionales con PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html">Aplicaci√≥n en un conjunto de datos de im√°genes en color</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html">Aplicaci√≥n de la segmentaci√≥n</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîÑ Autoencoders</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/01_IntuitionEtPremierAE.html">Introducci√≥n a los autoencodificadores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_Autoencodeurs/02_DenoisingAE.html">Autoencoder para desruido</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üìù PLN</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_Introduction.html">Introducci√≥n al PLN</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Bigrama</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_R%C3%A9seauFullyConnected.html">Red neuronal completamente conectada</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_WaveNet.html">PyTorch y WaveNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Rnn.html">Redes neuronales recurrentes</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Lstm.html">Memoria a Corto-Largo Plazo (LSTM)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ó HuggingFace</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/01_introduction.html">Introducci√≥n a Hugging Face</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/02_ComputerVisionWithTransformers.html">Visi√≥n por computadora con Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/03_NlpWithTransformers.html">Procesamiento del lenguaje natural con Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/04_AudioWithTransformers.html">Procesamiento de audio con Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/05_ImageGenerationWithDiffusers.html">Generaci√≥n de im√°genes con Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_HuggingFace/06_DemoAvecGradio.html">Demostraci√≥n con Gradio</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚ö° Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/01_Introduction.html">Introducci√≥n a los transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/02_GptFromScratch.html">Construyamos un GPT desde cero</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/03_TrainingOurGpt.html">Entrenamiento de nuestro modelo GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html">Arquitectura y particularidades del <em>transformer</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/05_UtilisationsPossibles.html">Posibles usos de la arquitectura Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/06_VisionTransformerImplementation.html">Implementaci√≥n del Vision Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_Transformers/07_SwinTransformer.html">Swin Transformer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéØ Detecci√≥n y YOLO</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/01_Introduction.html">Introducci√≥n a la detecci√≥n de objetos en im√°genes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/02_YoloEnDetail.html">YOLO en detalle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_DetectionEtYolo/03_Ultralytics.html">Ultralytics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üîç Entrenamiento contrastivo</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/01_FaceVerification.html">Verificaci√≥n facial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09_EntrainementContrastif/02_NonSupervis%C3%A9.html">Aprendizaje contrastivo no supervisado</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéì Transfer Learning y Distillation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/01_TransferLearning.html">Aprendizaje por transferencia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/02_TransferLearningPytorch.html">Transfer Learning con PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/03_Distillation.html">La destilaci√≥n de conocimientos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/04_DistillationAnomalie.html">Destilaci√≥n de conocimientos para la detecci√≥n no supervisada de anomal√≠as</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/05_FineTuningLLM.html">Ajuste fino (Fine-Tuning) de los LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_TransferLearningEtDistillation/06_FineTuningBertHF.html">Ajuste fino de BERT con Hugging Face</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üé® Modelos generativos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/01_Introduction.html">Introducci√≥n a los modelos generativos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/02_GAN.html">Redes generativas antag√≥nicas (GAN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/03_GanImplementation.html">Implementaci√≥n de una GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/04_VAE.html">Autoencoders variacionales</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/05_VaeImplementation.html">Implementaci√≥n de un VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/06_NormalizingFlows.html">Flujos de normalizaci√≥n</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/07_DiffusionModels.html">Modelos de difusi√≥n</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_ModelesGeneratifs/08_DiffusionImplementation.html">Implementaci√≥n de un modelo de difusi√≥n</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üéÅ Bonus ‚Äì Cursos espec√≠ficos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html">Activaciones e inicializaciones</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html">Normalizaci√≥n por lotes (<em>Batch Normalization</em>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html">Aumento de datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html">Difusi√≥n (<em>Broadcasting</em>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html">Comprender los diferentes optimizadores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html">Regularizaci√≥n</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/07_ConnexionsResiduelles.html">Conexiones residuales</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html">Introducci√≥n a la validaci√≥n cruzada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html">M√©tricas de evaluaci√≥n de modelos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html">Introducci√≥n a la tokenizaci√≥n</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/11_Quantization.html">Cuantizaci√≥n</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/edit/main/en/05_NLP/02_bigramme.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/issues/new?title=Issue%20on%20page%20%2F05_NLP/02_bigramme.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/05_NLP/02_bigramme.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bigrama</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-del-conjunto-de-datos">An√°lisis del conjunto de datos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#que-es-un-bigrama">¬øQu√© es un bigrama?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metodo-por-conteo">M√©todo por conteo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matriz-de-ocurrencias">Matriz de ocurrencias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilidades">Probabilidades</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generacion">Generaci√≥n</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluacion-del-modelo">Evaluaci√≥n del modelo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maxima-verosimilitud-o-likelihood">M√°xima verosimilitud o likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">Log-likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#enfoque-mediante-redes-neuronales">Enfoque mediante redes neuronales</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problema-del-enfoque-de-conteo">Problema del enfoque de ‚Äúconteo‚Äù</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conjunto-de-datos-de-nuestra-red-neuronal">Conjunto de datos de nuestra red neuronal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nuestra-red-neuronal">Nuestra red neuronal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizacion">Optimizaci√≥n</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notas-adicionales">Notas adicionales</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bigrama">
<h1>Bigrama<a class="headerlink" href="#bigrama" title="Link to this heading">#</a></h1>
<section id="analisis-del-conjunto-de-datos">
<h2>An√°lisis del conjunto de datos<a class="headerlink" href="#analisis-del-conjunto-de-datos" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;prenoms.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les 5 pr√©noms les plus populaires : &#39;</span><span class="p">,</span><span class="n">words</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les 5 pr√©noms les moins populaires : &#39;</span><span class="p">,</span><span class="n">words</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le pr√©nom le plus long : &#39;</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Le pr√©nom le plus court : &#39;</span><span class="p">,</span><span class="nb">min</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les 5 pr√©noms les plus populaires :  [&#39;MARIE&#39;, &#39;JEAN&#39;, &#39;PIERRE&#39;, &#39;MICHEL&#39;, &#39;ANDR√â&#39;]
Les 5 pr√©noms les moins populaires :  [&#39;√âLOUEN&#39;, &#39;CHEYNA&#39;, &#39;BLONDIE&#39;, &#39;IMANN&#39;, &#39;GHILAIN&#39;]
Le pr√©nom le plus long :  GUILLAUME-ALEXANDRE
Le pr√©nom le plus court :  GUY
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unique_characters</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="c1"># Ajouter chaque caract√®re de la ligne √† l&#39;ensemble des caract√®res uniques</span>
  <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">word</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
    <span class="n">unique_characters</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Nombre de caract√®res uniques : &#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">unique_characters</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Caract√®res uniques : &#39;</span><span class="p">,</span><span class="n">unique_characters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nombre de caract√®res uniques :  45
Caract√®res uniques :  {&#39;√è&#39;, &#39;√ú&#39;, &#39;≈∏&#39;, &#39;U&#39;, &#39;√î&#39;, &#39;S&#39;, &#39;√Ü&#39;, &#39;√Ä&#39;, &#39;√à&#39;, &#39;-&#39;, &#39;W&#39;, &#39;H&#39;, &#39;√ä&#39;, &#39;√â&#39;, &#39;R&#39;, &#39;M&#39;, &#39;E&#39;, &#39;√ã&#39;, &#39;N&#39;, &#39;√é&#39;, &#39;X&#39;, &#39;√Ñ&#39;, &#39;F&#39;, &#39;√Ç&#39;, &#39;K&#39;, &#39;D&#39;, &#39;√ñ&#39;, &#39;I&#39;, &#39;J&#39;, &#39;Y&#39;, &#39;A&#39;, &#39;C&#39;, &#39;O&#39;, &#39;√õ&#39;, &#39;√ô&#39;, &#39;B&#39;, &#39;Z&#39;, &#39;P&#39;, &#39;T&#39;, &quot;&#39;&quot;, &#39;Q&#39;, &#39;√á&#39;, &#39;G&#39;, &#39;L&#39;, &#39;V&#39;}
</pre></div>
</div>
</div>
</div>
</section>
<section id="que-es-un-bigrama">
<h2>¬øQu√© es un bigrama?<a class="headerlink" href="#que-es-un-bigrama" title="Link to this heading">#</a></h2>
<p>Recordemos que el objetivo del proyecto es predecir el siguiente car√°cter a partir de los caracteres anteriores. En el modelo de <strong>bigramas</strong>, nos basamos √∫nicamente en el car√°cter anterior para predecir el car√°cter actual. Es la versi√≥n m√°s simple de este tipo de modelo.</p>
<p>Claro, para predecir un nombre, debemos empezar desde cero. Para predecir la primera letra, necesitamos conocer la probabilidad de que una letra sea la primera (y lo mismo para la √∫ltima letra). Por lo tanto, a√±adimos un car√°cter especial ‚Äò.‚Äô al inicio y al final de cada palabra antes de construir nuestros bigramas.</p>
<p>En cada nombre, tenemos varios ejemplos de bigramas (cada uno es independiente).
Tomemos el primer nombre y veamos cu√°ntos bigramas contiene:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
  <span class="n">bigram</span> <span class="o">=</span> <span class="p">(</span><span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">bigram</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;.&#39;, &#39;M&#39;)
(&#39;M&#39;, &#39;A&#39;)
(&#39;A&#39;, &#39;R&#39;)
(&#39;R&#39;, &#39;I&#39;)
(&#39;I&#39;, &#39;E&#39;)
(&#39;E&#39;, &#39;.&#39;)
</pre></div>
</div>
</div>
</div>
<p>El nombre ‚ÄúMarie‚Äù contiene 6 bigramas.</p>
</section>
<section id="metodo-por-conteo">
<h2>M√©todo por conteo<a class="headerlink" href="#metodo-por-conteo" title="Link to this heading">#</a></h2>
<p>Construyamos ahora un diccionario en Python que agrupe todos los bigramas del conjunto de datos contando sus ocurrencias.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">bigram</span> <span class="o">=</span> <span class="p">(</span><span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span><span class="p">)</span>
    <span class="n">b</span><span class="p">[</span><span class="n">bigram</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">kv</span><span class="p">:</span> <span class="o">-</span><span class="n">kv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Les 5 bigrammes les plus fr√©quents : &#39;</span><span class="p">,</span><span class="nb">sorted</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">kv</span><span class="p">:</span> <span class="o">-</span><span class="n">kv</span><span class="p">[</span><span class="mi">1</span><span class="p">])[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les 5 bigrammes les plus fr√©quents :  [((&#39;A&#39;, &#39;.&#39;), 7537), ((&#39;E&#39;, &#39;.&#39;), 6840), ((&#39;A&#39;, &#39;N&#39;), 6292), ((&#39;N&#39;, &#39;.&#39;), 3741), ((&#39;N&#39;, &#39;E&#39;), 3741)]
</pre></div>
</div>
</div>
</div>
<p>Ahora tenemos nuestro diccionario de frecuencias de bigramas en todo el conjunto de datos. Como podemos ver, es frecuente que los nombres terminen en A, E o N, y que las letras A y N se sucedan, as√≠ como las letras N y E.</p>
<section id="matriz-de-ocurrencias">
<h3>Matriz de ocurrencias<a class="headerlink" href="#matriz-de-ocurrencias" title="Link to this heading">#</a></h3>
<p>Es m√°s sencillo visualizar y procesar los datos en forma de matriz. Vamos a construir una matriz de tama√±o 46x46 (45 caracteres + el car√°cter especial ‚Äò.‚Äô) donde la fila corresponde a la primera letra y la columna a la segunda.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">46</span><span class="p">,</span> <span class="mi">46</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Vamos a ordenar nuestros caracteres y crear tablas de b√∫squeda (look-up tables) con el objeto diccionario de Python. Queremos poder convertir un car√°cter a un entero (para indexar en la matriz) y viceversa (para reconstruir los nombres a partir de enteros).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">))))</span>
<span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">stoi</span><span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">itos</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="n">stoi</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>
</div>
</div>
</div>
<p>Vamos a llenar ahora nuestra matriz:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">N</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora podemos mostrar la matriz (tabla de b√∫squeda).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Code pour dessiner une jolie matrice</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">46</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">46</span><span class="p">):</span>
    <span class="n">chstr</span> <span class="o">=</span> <span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">itos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">chstr</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;bottom&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/96ce12bf8300ffce2957c0bdc85dadbf28244c9c48c5c995ed2a013ee17708cf.png" src="../_images/96ce12bf8300ffce2957c0bdc85dadbf28244c9c48c5c995ed2a013ee17708cf.png" />
</div>
</div>
</section>
<section id="probabilidades">
<h3>Probabilidades<a class="headerlink" href="#probabilidades" title="Link to this heading">#</a></h3>
<p>Para conocer la probabilidad de que un nombre comience con una letra espec√≠fica, debemos observar la fila del car√°cter ‚Äò.‚Äô, es decir, la fila 0, y normalizar cada valor dividi√©ndolo por la suma de los valores de esa fila (para obtener valores entre 0 y 1 cuya suma sea igual a 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Compte de la premi√®re ligne : &quot;</span><span class="p">,</span><span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probabilit√©s : &quot;</span><span class="p">,</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compte de la premi√®re ligne :  tensor([   0,    0,    0, 3399,  825, 1483, 1208, 1400,  864,  907, 1039,  788,
        1352, 1503, 2108, 3606, 1501,  546,  620,   32, 1142, 2539, 1185,   72,
         329,  294,   29,  661,  393,    0,    2,    0,    0,    1,    2,  161,
           0,    0,    2,    2,    0,    5,    0,    0,    0,    0],
       dtype=torch.int32)
Probabilit√©s :  tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1330e-01, 2.7500e-02, 4.9433e-02,
        4.0267e-02, 4.6667e-02, 2.8800e-02, 3.0233e-02, 3.4633e-02, 2.6267e-02,
        4.5067e-02, 5.0100e-02, 7.0267e-02, 1.2020e-01, 5.0033e-02, 1.8200e-02,
        2.0667e-02, 1.0667e-03, 3.8067e-02, 8.4633e-02, 3.9500e-02, 2.4000e-03,
        1.0967e-02, 9.8000e-03, 9.6667e-04, 2.2033e-02, 1.3100e-02, 0.0000e+00,
        6.6667e-05, 0.0000e+00, 0.0000e+00, 3.3333e-05, 6.6667e-05, 5.3667e-03,
        0.0000e+00, 0.0000e+00, 6.6667e-05, 6.6667e-05, 0.0000e+00, 1.6667e-04,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])
</pre></div>
</div>
</div>
</div>
<p>Para generar nombres de manera aleatoria, no siempre queremos elegir la letra m√°s probable (ya que siempre generar√≠amos el mismo nombre). Queremos elegir una letra en funci√≥n de su probabilidad. Si la letra ‚Äòn‚Äô tiene una probabilidad de 0.1, queremos elegirla el 10% de las veces.
Para esto, utilizamos la funci√≥n <code class="docutils literal notranslate"><span class="pre">torch.multinomial</span></code> de PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">itos</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Z&#39;
</pre></div>
</div>
</div>
</div>
<p>En cada llamada, obtenemos una letra diferente en funci√≥n de su probabilidad de aparici√≥n en nuestro conjunto de datos de prueba.</p>
<p>Con todos estos elementos, ahora estamos listos para generar nombres a partir de nuestra matriz N. Lo ideal ser√≠a crear una matriz con las probabilidades directamente para evitar tener que renormalizar cada vez.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># On copie N et on la convertit en float</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">N</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="c1"># On normalise chaque ligne</span>
<span class="c1"># On somme sur la premi√®re dimension (les colonnes)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Somme des lignes : &quot;</span><span class="p">,</span><span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">P</span> <span class="o">/=</span> <span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># /= est un raccourci pour P = P / P.sum(1, keepdims=True)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matrice normalis√©e P est de taille : &quot;</span><span class="p">,</span><span class="n">P</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># On v√©rifie que la somme d&#39;une ligne est √©gale √† 1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Somme de la premi√®re ligne de P : &quot;</span><span class="p">,</span><span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Somme des lignes :  torch.Size([46, 1])
Matrice normalis√©e P est de taille :  torch.Size([46, 46])
Somme de la premi√®re ligne de P :  1.0
</pre></div>
</div>
</div>
</div>
<p><strong>Nota sobre la divisi√≥n de matrices de diferentes tama√±os</strong>: Como habr√°n notado, dividimos una matriz de tama√±o (46,46) por una matriz de tama√±o (46,1), lo cual parece imposible. Con PyTorch, existen <a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html">reglas de broadcasting</a>. Les recomiendo encarecidamente familiarizarse con este concepto, ya que es una fuente frecuente de errores. Para entender en detalle las reglas de broadcasting, pueden consultar el <a class="reference internal" href="../Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html"><span class="std std-doc">curso adicional</span></a>.
En la pr√°ctica, al dividir una matriz de tama√±o (46,46) por una matriz de tama√±o (46,1), se ‚Äúbroadcastea‚Äù la matriz (46,1) a (46,46) copiando 46 veces la matriz base. Esto permite realizar la operaci√≥n como se desea.</p>
</section>
<section id="generacion">
<h3>Generaci√≥n<a class="headerlink" href="#generacion" title="Link to this heading">#</a></h3>
<p>¬°Por fin es hora de generar nombres con nuestro m√©todo de bigramas!
Vamos a definir una funci√≥n para generar nombres:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">genName</span><span class="p">():</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># On commence par &#39;.&#39;</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span> <span class="c1"># Tant qu&#39;on n&#39;a pas g√©n√©r√© le caract√®re &#39;.&#39;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="c1"># On r√©cup√®re la distribution de probabilit√© de la ligne correspondant au caract√®re actuel</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># On tire un √©chantillon</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">itos</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span> <span class="c1"># On ajoute le caract√®re √† notre pr√©nom</span>
    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>
  <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">genName</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;MARAUSUR.&#39;
</pre></div>
</div>
</div>
</div>
<p>Por ejemplo, podemos generar 10 nombres aleatorios:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">genName</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DA.
TYEYSE-SSCL.
DE.
ANINEDANDVI.
SOKE.
RENNA.
FUXA.
EROA.
FA.
KALEN.
</pre></div>
</div>
</div>
</div>
<p>Como pueden ver, la generaci√≥n es bastante mala‚Ä¶
¬øPor qu√©? Porque el bigrama es un m√©todo muy limitado. Basarse √∫nicamente en el √∫ltimo car√°cter no permite tener suficiente conocimiento para generar nombres correctos.</p>
</section>
</section>
<section id="evaluacion-del-modelo">
<h2>Evaluaci√≥n del modelo<a class="headerlink" href="#evaluacion-del-modelo" title="Link to this heading">#</a></h2>
<section id="maxima-verosimilitud-o-likelihood">
<h3>M√°xima verosimilitud o likelihood<a class="headerlink" href="#maxima-verosimilitud-o-likelihood" title="Link to this heading">#</a></h3>
<p>Ahora queremos evaluar nuestro modelo en el conjunto de entrenamiento. Para esto, usamos la m√°xima verosimilitud, como en <a class="reference internal" href="../01_Fondations/02_R%C3%A9gressionLogistique.html"><span class="std std-doc">el segundo notebook del curso 1</span></a>.
La m√°xima verosimilitud o <em>likelihood</em> es una medida que corresponde al producto de las probabilidades de los eventos. Para tener un buen modelo, buscamos maximizar el <em>likelihood</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">productOfProbs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">2</span><span class="p">]:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="n">productOfProbs</span> <span class="o">*=</span> <span class="n">prob</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;La probabilit√© de </span><span class="si">{</span><span class="n">ch1</span><span class="si">}</span><span class="s2">-&gt;</span><span class="si">{</span><span class="n">ch2</span><span class="si">}</span><span class="s2"> est </span><span class="si">{</span><span class="n">prob</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Le produit des probabilit√©s est : &quot;</span><span class="p">,</span><span class="n">productOfProbs</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La probabilit√© de .-&gt;M est 0.120
La probabilit√© de M-&gt;A est 0.431
La probabilit√© de A-&gt;R est 0.084
La probabilit√© de R-&gt;I est 0.256
La probabilit√© de I-&gt;E est 0.119
La probabilit√© de E-&gt;. est 0.321
La probabilit√© de .-&gt;J est 0.045
La probabilit√© de J-&gt;E est 0.232
La probabilit√© de E-&gt;A est 0.024
La probabilit√© de A-&gt;N est 0.201
La probabilit√© de N-&gt;. est 0.212
Le produit des probabilit√©s est :  4.520583629652464e-10
</pre></div>
</div>
</div>
</div>
<p>Vemos r√°pidamente que multiplicar las probabilidades plantea un problema. Aqu√≠, las multiplicamos sobre 2 de los 30,000 elementos del conjunto de datos y obtenemos un valor muy bajo. Si las multiplicamos sobre todo el conjunto de datos, obtenemos un valor que no puede ser representado por una computadora.</p>
</section>
<section id="log-likelihood">
<h3>Log-likelihood<a class="headerlink" href="#log-likelihood" title="Link to this heading">#</a></h3>
<p>Para resolver este problema de precisi√≥n, usamos el logaritmo por varias razones:</p>
<ul class="simple">
<li><p>La funci√≥n log es mon√≥tona, es decir, si <span class="math notranslate nohighlight">\(a &gt; b\)</span>, entonces <span class="math notranslate nohighlight">\(log(a) &gt; log(b)\)</span>. Maximizar el <em>log-likelihood</em> es equivalente a maximizar el <em>likelihood</em> en un contexto de optimizaci√≥n.</p></li>
<li><p>Una propiedad interesante de los logaritmos (que explica por qu√© esta funci√≥n se usa a menudo en optimizaci√≥n y probabilidad) es la siguiente regla: <span class="math notranslate nohighlight">\(log(a \times b) = log(a) + log(b)\)</span>. Esto nos permite evitar multiplicar valores peque√±os que podr√≠an exceder la precisi√≥n de una computadora.</p></li>
</ul>
<p>Por lo tanto, podemos maximizar el <em>log-likelihood</em> en lugar del <em>likelihood</em>. Retomemos el bucle anterior y veamos qu√© obtenemos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sumOfLogs</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">2</span><span class="p">]:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="n">sumOfLogs</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La somme des log est : &quot;</span><span class="p">,</span><span class="n">sumOfLogs</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La somme des log est :  -21.517210006713867
</pre></div>
</div>
</div>
</div>
<p>Obtenemos un valor mucho m√°s razonable. Para los problemas de optimizaci√≥n, a menudo preferimos tener una funci√≥n que minimizar. En el caso de un modelo perfecto, cada probabilidad vale 1, por lo que cada log vale 0, y la suma de los logs vale 0. De lo contrario, obtenemos valores negativos, ya que una probabilidad siempre es menor que 1 y <span class="math notranslate nohighlight">\(log(a) &lt; 0 \text{ si } a &lt; 1\)</span>.
Para tener un problema de minimizaci√≥n, usamos el <em>negative log-likelihood</em>, que corresponde simplemente al opuesto del <em>log-likelihood</em>.</p>
<p>A menudo, tomamos el promedio en lugar de la suma, ya que es m√°s legible y equivalente en t√©rminos de optimizaci√≥n. Y lo calcularemos sobre el conjunto de nombres del conjunto de datos.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sumOfLogs</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">n</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="n">sumOfLogs</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="n">n</span><span class="o">+=</span><span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La somme des negative log est : &quot;</span><span class="p">,</span><span class="n">sumOfLogs</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La moyenne des negative log est : &quot;</span><span class="p">,</span><span class="n">sumOfLogs</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La somme des negative log est :  564925.125
La moyenne des negative log est :  2.4960792002651053
</pre></div>
</div>
</div>
</div>
<p>El <em>negative log-likelihood</em> del conjunto de datos es, por lo tanto, de 2.49.</p>
<p>Tambi√©n pueden ver si su nombre es com√∫n o poco com√∫n en comparaci√≥n con el promedio del conjunto de datos. Para esto, solo tienen que reemplazar mi nombre ‚ÄúSIMON‚Äù por el suyo (en may√∫sculas).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sumOfLogs</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">n</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="s2">&quot;SIMON&quot;</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="n">sumOfLogs</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="n">n</span><span class="o">+=</span><span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La moyenne des negative log est : &quot;</span><span class="p">,</span><span class="n">sumOfLogs</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La moyenne des negative log est :  2.598056602478027
</pre></div>
</div>
</div>
</div>
<p>Si el valor del <em>negative log-likelihood</em> correspondiente a su nombre es inferior al del conjunto de datos, su nombre es bastante com√∫n. De lo contrario, es m√°s bien poco com√∫n.</p>
</section>
</section>
<section id="enfoque-mediante-redes-neuronales">
<h2>Enfoque mediante redes neuronales<a class="headerlink" href="#enfoque-mediante-redes-neuronales" title="Link to this heading">#</a></h2>
<section id="problema-del-enfoque-de-conteo">
<h3>Problema del enfoque de ‚Äúconteo‚Äù<a class="headerlink" href="#problema-del-enfoque-de-conteo" title="Link to this heading">#</a></h3>
<p>Ahora vamos a intentar resolver el mismo problema de una manera diferente. Hemos resuelto este problema simplemente contando las ocurrencias de los bigramas y calculando la probabilidad en funci√≥n de esto. Este m√©todo funciona para bigramas, pero no funcionar√° para cosas m√°s complejas como los N-gramas.</p>
<p>De hecho, nuestra tabla de b√∫squeda es de tama√±o 46x46 para dos caracteres. Si consideramos N caracteres (por lo tanto, N-1 caracteres para predecir el N-√©simo), de inmediato tenemos muchas m√°s posibilidades. Podemos calcular simplemente que la tabla ser√° de tama√±o <span class="math notranslate nohighlight">\(46^N\)</span>. Para N=4, esto dar√≠a una tabla de tama√±o 4,477,456. Es decir, para valores de contexto importantes (los modelos actuales tienen un contexto de decenas de miles de tokens y hay m√°s de 46 posibilidades cada vez), este enfoque no funcionar√° en absoluto.</p>
<p>Es por eso que el enfoque mediante redes neuronales es muy interesante. En la continuaci√≥n del curso, mostraremos c√≥mo resolver este mismo problema con la ayuda de una red neuronal, lo que les dar√° una intuici√≥n sobre las capacidades de la red cuando el contexto aumenta.</p>
</section>
<section id="conjunto-de-datos-de-nuestra-red-neuronal">
<h3>Conjunto de datos de nuestra red neuronal<a class="headerlink" href="#conjunto-de-datos-de-nuestra-red-neuronal" title="Link to this heading">#</a></h3>
<p>Nuestra red neuronal recibir√° un car√°cter como entrada y deber√° predecir el car√°cter siguiente. Como funci√≥n de p√©rdida, podemos usar la funci√≥n <em>negative log-likelihood</em> para intentar acercarnos al valor del bigrama por ‚Äúconteo‚Äù.</p>
<p>Comencemos por crear nuestro conjunto de datos de entrenamiento. Retomamos el bucle de recorrido de los bigramas de la parte anterior y, esta vez, indexamos dos listas: <code class="docutils literal notranslate"><span class="pre">xs</span></code> para las entradas y <code class="docutils literal notranslate"><span class="pre">ys</span></code> para las etiquetas.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the training set of bigrams (x,y)</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span><span class="p">)</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>
    
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>. M
M A
A R
R I
I E
E .
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;valeurs d&#39;entr√©e : &quot;</span><span class="p">,</span><span class="n">xs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;valeurs de sortie : &quot;</span><span class="p">,</span><span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>valeurs d&#39;entr√©e :  tensor([ 0, 15,  3, 20, 11,  7])
valeurs de sortie :  tensor([15,  3, 20, 11,  7,  0])
</pre></div>
</div>
</div>
</div>
<p>Para el valor de entrada 0, que corresponde a ‚Äò.‚Äô, queremos predecir una etiqueta 15, que corresponde a ‚ÄòM‚Äô.</p>
<p>El problema de estas listas es que contienen enteros, y no es posible dar un entero como entrada a una red neuronal. En el campo del NLP, a menudo se usa el <em>one-hot encoding</em>, que consiste en convertir un √≠ndice en un vector de 0 con un 1 en la posici√≥n del √≠ndice. El tama√±o del vector corresponde al n√∫mero de clases posibles, por lo tanto, aqu√≠ 46.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="c1"># one-hot encoding</span>
<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">46</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># conversion en float pour le NN</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encodage one-hot des deux premiers caract√®res: &quot;</span><span class="p">,</span><span class="n">xenc</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encodage one-hot des deux premiers caract√®res:  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</pre></div>
</div>
</div>
</div>
<p>Como pueden ver, tenemos un 1 en la posici√≥n 0 del primer vector y un 1 en la posici√≥n 15 del segundo. Estos son los vectores que servir√°n como entrada a nuestra red neuronal. Podemos visualizar c√≥mo se ven estos vectores para tener una mejor intuici√≥n de lo que hace el <em>one-hot encoding</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Les 5 premiers vecteurs one-hot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">xenc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x784579d81f10&gt;
</pre></div>
</div>
<img alt="../_images/ff5371c82572c1f90d9fa93adee2bc0e1c73e24631447002e6deaa6c08c003db.png" src="../_images/ff5371c82572c1f90d9fa93adee2bc0e1c73e24631447002e6deaa6c08c003db.png" />
</div>
</div>
</section>
<section id="nuestra-red-neuronal">
<h3>Nuestra red neuronal<a class="headerlink" href="#nuestra-red-neuronal" title="Link to this heading">#</a></h3>
<p>Ahora vamos a crear nuestra red neuronal. Ser√° una red neuronal extremadamente simple que contiene una sola capa. Para el tama√±o de la capa, tomamos como entrada un vector de tama√±o <span class="math notranslate nohighlight">\(n \times 46\)</span>, por lo que necesitaremos una primera dimensi√≥n de tama√±o 46. En la salida, queremos una distribuci√≥n de probabilidad sobre el conjunto de caracteres. Nuestra capa de red ser√°, por lo tanto, de tama√±o <span class="math notranslate nohighlight">\(46 \times 46\)</span>.</p>
<p>Comencemos por inicializar nuestra capa con valores aleatorios:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># On met le param√®tre requires_grad √† True pour pouvoir optimiser la matrice par descente de gradient</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">46</span><span class="p">,</span> <span class="mi">46</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<p>El <em>forward</em> de nuestra red neuronal consistir√° simplemente en una multiplicaci√≥n matricial entre la entrada y la capa. Luego aplicaremos la funci√≥n <em>softmax</em> (ver curso sobre CNN) para obtener una distribuci√≥n de probabilidades.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># One hot encoding sur les entr√©es</span>
<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">46</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> 
<span class="c1"># Multiplication matricielle (forward pass)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>  <span class="c1"># @ est la multiplication matricielle</span>
<span class="c1">#Softmax pour obtenir des probabilit√©s</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> 
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([6, 46])
</pre></div>
</div>
</div>
</div>
<p>Obtenemos una distribuci√≥n de probabilidades para cada uno de nuestros 6 caracteres. Vamos a visualizar las salidas de nuestra red neuronal no entrenada y calcular el <em>negative log-likelihood</em> para ver d√≥nde nos encontramos en comparaci√≥n con nuestro modelo obtenido por ‚Äúconteo‚Äù.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nlls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># index de l&#39;entr√©e</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># index du label</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--------&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bigramme actuel </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">itos</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="si">}{</span><span class="n">itos</span><span class="p">[</span><span class="n">y</span><span class="p">]</span><span class="si">}</span><span class="s1"> (indexes </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;entr√©e du r√©seau de neurones :&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sortie du r√©seau (probabilit√©) :&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vrai label :&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;probabilit√© donn√© par le r√©seau sur le caract√®re r√©el :&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">logp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
  <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">logp</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;negative log likelihood:&#39;</span><span class="p">,</span> <span class="n">nll</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">nlls</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">nll</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=========&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;negative log likelihood moyen, i.e. loss =&#39;</span><span class="p">,</span> <span class="n">nlls</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--------
bigramme actuel 1: .M (indexes 0,15)
entr√©e du r√©seau de neurones : 0
sortie du r√©seau (probabilit√©) : tensor([0.0146, 0.0210, 0.0823, 0.0077, 0.0160, 0.0483, 0.0943, 0.0204, 0.0079,
        0.0112, 0.0085, 0.0179, 0.0188, 0.0292, 0.0022, 0.0092, 0.0200, 0.0094,
        0.0097, 0.0191, 0.1091, 0.0122, 0.0092, 0.0287, 0.0120, 0.0088, 0.0053,
        0.0217, 0.0177, 0.0050, 0.0038, 0.0483, 0.0320, 0.0441, 0.0105, 0.0126,
        0.0266, 0.0092, 0.0262, 0.0081, 0.0430, 0.0012, 0.0102, 0.0025, 0.0126,
        0.0116], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 15
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.009214116260409355
negative log likelihood: 4.687018394470215
--------
bigramme actuel 2: MA (indexes 15,3)
entr√©e du r√©seau de neurones : 15
sortie du r√©seau (probabilit√©) : tensor([0.0574, 0.1353, 0.0227, 0.0032, 0.1142, 0.0148, 0.1007, 0.0162, 0.0242,
        0.0089, 0.0040, 0.0459, 0.0023, 0.0081, 0.0064, 0.0124, 0.0083, 0.0112,
        0.0172, 0.0062, 0.0033, 0.0045, 0.0131, 0.0144, 0.0218, 0.0080, 0.0225,
        0.0097, 0.0164, 0.0074, 0.0165, 0.0091, 0.0412, 0.0087, 0.0100, 0.0039,
        0.0080, 0.0036, 0.0377, 0.0150, 0.0345, 0.0048, 0.0253, 0.0036, 0.0164,
        0.0210], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 3
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.0031920599285513163
negative log likelihood: 5.74708890914917
--------
bigramme actuel 3: AR (indexes 3,20)
entr√©e du r√©seau de neurones : 3
sortie du r√©seau (probabilit√©) : tensor([0.0199, 0.0169, 0.0239, 0.0122, 0.0174, 0.0203, 0.0043, 0.0822, 0.0517,
        0.0228, 0.0118, 0.0121, 0.0210, 0.0088, 0.0063, 0.0128, 0.1041, 0.0100,
        0.0338, 0.0772, 0.0056, 0.0565, 0.0134, 0.0032, 0.0253, 0.0120, 0.0337,
        0.0080, 0.0083, 0.0060, 0.0068, 0.0020, 0.0405, 0.0120, 0.0366, 0.0080,
        0.0111, 0.0135, 0.0164, 0.0038, 0.0133, 0.0029, 0.0094, 0.0047, 0.0504,
        0.0271], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 20
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.005596297327429056
negative log likelihood: 5.185649871826172
--------
bigramme actuel 4: RI (indexes 20,11)
entr√©e du r√©seau de neurones : 20
sortie du r√©seau (probabilit√©) : tensor([0.0030, 0.0300, 0.0056, 0.0311, 0.0361, 0.0294, 0.0462, 0.0163, 0.0369,
        0.0178, 0.0251, 0.0125, 0.0162, 0.0019, 0.0828, 0.0173, 0.0068, 0.0113,
        0.0204, 0.0124, 0.0653, 0.0059, 0.0038, 0.0075, 0.0165, 0.0332, 0.0065,
        0.0354, 0.0169, 0.0062, 0.0683, 0.0203, 0.0189, 0.0179, 0.0113, 0.0119,
        0.0549, 0.0035, 0.0051, 0.0061, 0.0569, 0.0268, 0.0164, 0.0021, 0.0146,
        0.0088], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 11
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.012452212162315845
negative log likelihood: 4.385857105255127
--------
bigramme actuel 5: IE (indexes 11,7)
entr√©e du r√©seau de neurones : 11
sortie du r√©seau (probabilit√©) : tensor([0.0265, 0.0211, 0.0312, 0.0235, 0.0020, 0.0151, 0.0145, 0.0083, 0.0141,
        0.0062, 0.0168, 0.0183, 0.0600, 0.0047, 0.0969, 0.0438, 0.0083, 0.0584,
        0.0572, 0.0061, 0.0159, 0.0475, 0.0079, 0.0116, 0.0331, 0.0043, 0.0049,
        0.0134, 0.0057, 0.0077, 0.0350, 0.0276, 0.0174, 0.0050, 0.0176, 0.0022,
        0.0169, 0.0029, 0.0281, 0.0115, 0.0291, 0.0250, 0.0071, 0.0126, 0.0277,
        0.0491], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 7
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.008320074528455734
negative log likelihood: 4.789083957672119
--------
bigramme actuel 6: E. (indexes 7,0)
entr√©e du r√©seau de neurones : 7
sortie du r√©seau (probabilit√©) : tensor([0.0397, 0.0266, 0.0185, 0.0024, 0.0054, 0.0061, 0.0143, 0.0269, 0.0398,
        0.0084, 0.0134, 0.0247, 0.1220, 0.0039, 0.0062, 0.0829, 0.0452, 0.0086,
        0.0062, 0.0130, 0.0106, 0.0137, 0.0073, 0.1132, 0.0146, 0.0252, 0.0112,
        0.0955, 0.0133, 0.0196, 0.0091, 0.0122, 0.0160, 0.0092, 0.0128, 0.0337,
        0.0058, 0.0112, 0.0070, 0.0029, 0.0033, 0.0073, 0.0052, 0.0049, 0.0125,
        0.0087], grad_fn=&lt;SelectBackward0&gt;)
vrai label : 0
probabilit√© donn√© par le r√©seau sur le caract√®re r√©el : 0.0397193469107151
negative log likelihood: 3.225916862487793
=========
negative log likelihood moyen, i.e. loss = 4.670102596282959
</pre></div>
</div>
</div>
</div>
<p>Para el c√°lculo de la p√©rdida, vamos a calcular el <em>negative log-likelihood</em> de la salida de nuestra red en relaci√≥n con la etiqueta de la siguiente manera:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calcul de la loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="c1"># On remet les gradients √† z√©ro (None est plus efficace)</span>
<span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span> 
<span class="c1"># Calcul des gradients automatique de pytorch</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4.670102596282959
tensor([[0.0024, 0.0035, 0.0137,  ..., 0.0004, 0.0021, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
</pre></div>
</div>
</div>
</div>
<p>Como pueden ver, hemos calculado los gradientes de nuestra matriz W con respecto a la <em>p√©rdida</em>. De la misma manera que en los cursos anteriores, podemos actualizar los pesos del modelo en la direcci√≥n del gradiente con un paso (el <em>learning_rate</em>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># avec un learning_rate de 0.1</span>
<span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="optimizacion">
<h3>Optimizaci√≥n<a class="headerlink" href="#optimizacion" title="Link to this heading">#</a></h3>
<p>A partir de todo lo que acabamos de ver, ahora podemos reunir los elementos y optimizar nuestro modelo.</p>
<p><strong>Creaci√≥n del conjunto de datos completo</strong>
Vamos a comenzar por crear nuestro conjunto de datos completo retomando el bucle anterior, pero recorriendo todo el conjunto de nombres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
  <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
<span class="n">num</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;number of examples: &#39;</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>number of examples:  226325
</pre></div>
</div>
</div>
</div>
<p><strong>Inicializaci√≥n del modelo</strong>
Ahora podemos inicializar nuestro modelo como antes, elegir el <em>learning_rate</em> y el n√∫mero de iteraciones.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">46</span><span class="p">,</span> <span class="mi">46</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lr</span><span class="o">=</span><span class="mi">50</span> <span class="c1"># en pratique, dans ce petit probl√®me, un learning rate de 50 fonctionne bien ce qui peut sembler √©tonnant</span>
<span class="n">iterations</span><span class="o">=</span><span class="mi">100</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Descenso del gradiente</strong>
Apliquemos ahora el algoritmo de descenso del gradiente a nuestro modelo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Descente du gradient</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
  
  <span class="c1"># forward pass</span>
  <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">46</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># transformation one hot sur les entr√©es</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
  <span class="n">probs</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># On applique le softmax</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># Calcul du negative log likelihood (loss)</span>
  <span class="k">if</span> <span class="n">k</span><span class="o">%</span><span class="k">10</span>==0:
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loss iteration &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39; : &#39;</span><span class="p">,</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  
  <span class="c1"># retropropagation</span>
  <span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># Remettre la gradient √† z√©ro √† chaque it√©ration (√† ne pas oublier !!!!)</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  
  <span class="c1"># Mise √† jour des poids</span>
  <span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">50</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss iteration 0 :  4.346113204956055
loss iteration 10 :  2.94492769241333
loss iteration 20 :  2.7590363025665283
loss iteration 30 :  2.6798315048217773
loss iteration 40 :  2.637108087539673
loss iteration 50 :  2.610524892807007
loss iteration 60 :  2.5923469066619873
loss iteration 70 :  2.5791807174682617
loss iteration 80 :  2.569261074066162
loss iteration 90 :  2.561541795730591
</pre></div>
</div>
</div>
</div>
<p>Despu√©s de 100 iteraciones, obtenemos un <em>negative log-likelihood</em> cercano al del modelo por ‚Äúconteo‚Äù. Esto es, de hecho, la capacidad m√°xima del modelo de bigramas en los datos de entrenamiento.</p>
<p><strong>Generaci√≥n de nombres con nuestro modelo</strong>
Ahora podemos generar nombres con nuestro modelo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">ix</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">46</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span> 
    <span class="c1"># Pr√©diction des probabilit√©s de la lettre suivante</span>
    <span class="n">p</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># On fait un tirage al√©atoire de la prochaine lettre en suivante la distribution p </span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="c1"># Conversion en lettre</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">itos</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>JE.
S.
ADJULA.
M.
LVERTY√úCI.
</pre></div>
</div>
</div>
</div>
</section>
<section id="notas-adicionales">
<h3>Notas adicionales<a class="headerlink" href="#notas-adicionales" title="Link to this heading">#</a></h3>
<p>La matriz de pesos <span class="math notranslate nohighlight">\(W\)</span> tiene el mismo tama√±o que la matriz <span class="math notranslate nohighlight">\(N\)</span> utilizada en el m√©todo de conteo. Lo que acabamos de hacer con el enfoque de red neuronal es, de hecho, aprender la matriz <span class="math notranslate nohighlight">\(N\)</span>.
Podemos confirmar esta intuici√≥n observando qu√© ocurre cuando realizamos la operaci√≥n <code class="docutils literal notranslate"><span class="pre">xenc</span> <span class="pre">&#64;</span> <span class="pre">W</span></code>. Se trata de una multiplicaci√≥n matricial de una matriz fila de tama√±o <span class="math notranslate nohighlight">\(1 \times 46\)</span> por una matriz cuadrada de tama√±o <span class="math notranslate nohighlight">\(46 \times 46\)</span>. Adem√°s, la matriz fila contiene solo ceros, excepto un 1 en el √≠ndice <span class="math notranslate nohighlight">\(i\)</span> de la letra. Esta multiplicaci√≥n matricial da como resultado la fila <span class="math notranslate nohighlight">\(i\)</span> de la matriz <span class="math notranslate nohighlight">\(W\)</span>.
Esto corresponde exactamente a lo que hac√≠amos en el m√©todo de conteo, donde recuper√°bamos las probabilidades de la fila <span class="math notranslate nohighlight">\(i\)</span> de <span class="math notranslate nohighlight">\(P\)</span>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./05_NLP"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_Introduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introducci√≥n al PLN</p>
      </div>
    </a>
    <a class="right-next"
       href="03_R%C3%A9seauFullyConnected.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Red neuronal completamente conectada</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-del-conjunto-de-datos">An√°lisis del conjunto de datos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#que-es-un-bigrama">¬øQu√© es un bigrama?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metodo-por-conteo">M√©todo por conteo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matriz-de-ocurrencias">Matriz de ocurrencias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilidades">Probabilidades</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generacion">Generaci√≥n</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluacion-del-modelo">Evaluaci√≥n del modelo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maxima-verosimilitud-o-likelihood">M√°xima verosimilitud o likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">Log-likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#enfoque-mediante-redes-neuronales">Enfoque mediante redes neuronales</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problema-del-enfoque-de-conteo">Problema del enfoque de ‚Äúconteo‚Äù</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conjunto-de-datos-de-nuestra-red-neuronal">Conjunto de datos de nuestra red neuronal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nuestra-red-neuronal">Nuestra red neuronal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizacion">Optimizaci√≥n</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notas-adicionales">Notas adicionales</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Simon Thomine
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div id="language-switcher" style="text-align: center; margin-top: 20px; padding: 10px; border-top: 1px solid #eee;">
  <span style="margin-right: 10px;">üåê Language / Langue:</span>
  <a href="#" onclick="switchToEnglish()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #4CAF50; color: white; border-radius: 5px; font-weight: bold; transition: all 0.3s;">üá∫üá∏ English</a>
  <a href="#" onclick="switchToFrench()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #f0f0f0; border-radius: 5px; transition: all 0.3s;">üá´üá∑ Fran√ßais</a>
  <a href="#" onclick="switchToSpanish()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #ffd700; border-radius: 5px; transition: all 0.3s;">üá™üá∏ Espa√±ol</a>
  <a href="#" onclick="switchToChinese()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #ff4b4b; color: white; border-radius: 5px; transition: all 0.3s;">üá®üá≥ ‰∏≠Êñá</a>
</div>
<script>
function getBaseUrl() {
  let baseUrl = window.location.origin;
  let pathname = window.location.pathname;
  if (pathname.includes('fr/')) {
    baseUrl += pathname.split('fr/')[0];
  } else if (pathname.includes('en/')) {
    baseUrl += pathname.split('en/')[0];
  } else if (pathname.includes('es/')) {
    baseUrl += pathname.split('es/')[0];
  } else if (pathname.includes('zh/')) {
    baseUrl += pathname.split('zh/')[0];
  } else {
    baseUrl += pathname.split('/').slice(0, -1).join('/') + '/';
  }
  return baseUrl;
}

function getCurrentPage() {
  let pathname = window.location.pathname;
  if (pathname.includes('fr/')) {
    return pathname.split('fr/')[1] || 'index.html';
  } else if (pathname.includes('en/')) {
    return pathname.split('en/')[1] || 'index.html';
  } else if (pathname.includes('es/')) {
    return pathname.split('es/')[1] || 'index.html';
  } else if (pathname.includes('zh/')) {
    return pathname.split('zh/')[1] || 'index.html';
  }
  return 'index.html';
}

function switchToEnglish() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'en/' + currentPage;
  window.location.href = newUrl;
}

function switchToFrench() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'fr/' + currentPage;
  window.location.href = newUrl;
}

function switchToSpanish() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'es/' + currentPage;
  window.location.href = newUrl;
}

function switchToChinese() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + 'zh/' + currentPage;
  window.location.href = newUrl;
}
</script>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>