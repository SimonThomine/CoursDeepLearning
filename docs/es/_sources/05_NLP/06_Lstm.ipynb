{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria a Corto-Largo Plazo (LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el cuaderno anterior, presentamos la capa clásica de una RNN. Desde su invención, se han creado muchas otras capas recurrentes.\n",
    "\n",
    "Aquí, estudiaremos la capa [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf) (*Long Short-Term Memory*), una alternativa a la capa RNN clásica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es una capa LSTM?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La capa LSTM está compuesta por una *unidad de memoria* con 4 capas completamente conectadas. Tres de estas capas se utilizan para seleccionar la información relevante de los pasos anteriores: la *puerta de olvido* (*forget gate*), la *puerta de entrada* (*input gate*) y la *puerta de salida* (*output gate*).\n",
    "\n",
    "- **Puerta de olvido** (*forget gate*): Elimina información de la memoria.\n",
    "- **Puerta de entrada** (*input gate*): Inserta información en la memoria.\n",
    "- **Puerta de salida** (*output gate*): Utiliza la información almacenada.\n",
    "\n",
    "La última capa completamente conectada genera una \"información candidata\" para la memoria de la capa LSTM.\n",
    "\n",
    "![LSTM](./images/lstm.png)\n",
    "\n",
    "Imagen extraída del [artículo del blog](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa en la figura, la capa LSTM recibe 3 vectores de entrada: $H_{t-1}$, $C_{t-1}$ y $X_{t}$. Los dos primeros provienen directamente de la capa LSTM, mientras que el tercero corresponde a la entrada en el tiempo $t$ (el carácter en nuestro caso).\n",
    "\n",
    "Para simplificar: $H_{t-1}$ contiene la memoria a corto plazo, y $C_{t-1}$ la memoria a largo plazo. Esto permite conservar la información importante en un contexto amplio sin descuidar el contexto local.\n",
    "\n",
    "La idea es resolver el problema de propagación de la información en secuencias largas que presentan las RNN clásicas.\n",
    "\n",
    "Para profundizar, puedes leer el [artículo](https://www.bioinf.jku.at/publications/older/2604.pdf) o consultar este [artículo del blog](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación en PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear el conjunto de datos, utilizamos nuevamente el archivo `moliere.txt` y reutilizamos el código del cuaderno anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducimos el número de elementos para un entrenamiento rápido (descomentar si se desea entrenar con todo el conjunto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  100000\n"
     ]
    }
   ],
   "source": [
    "text=text[:100000]\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJLMNOPQRSTUVYabcdefghijlmnopqrstuvxyz«»ÇÈÉÊàâæçèéêîïôùû\n",
      "Nombre de caractères différents :  76\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "División en conjuntos de entrenamiento y prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour le test\n",
    "train_data = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear el modelo, utilizamos directamente la implementación de PyTorch de la capa LSTM. A diferencia de las capas lineales o convolucionales, [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) permite apilar varias capas usando el parámetro *num_layers*. Si se desean definir una por una, se debe usar [nn.LSTMCell](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size,num_layers=1):\n",
    "        super(lstm, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # On utilise un embedding pour transformer les entiers(caractères) en vecteurs\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        # La couche LSTM peut prendre l'argument num_layers pour empiler plusieurs couches LSTM\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
    "        # Une dernière couche linéaire pour prédire le prochain caractère\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr=0.001\n",
    "hidden_dim=128\n",
    "seq_len=100\n",
    "num_layers=1\n",
    "model=lstm(vocab_size,hidden_dim,num_layers)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La capa LSTM recibe como entrada una secuencia y devuelve una secuencia del mismo tamaño. Esto acelera el entrenamiento, ya que se pueden procesar varios ejemplos al mismo tiempo.\n",
    "\n",
    "**Nota**: También se puede acelerar el entrenamiento procesando en *batch* varias secuencias en paralelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 2.17804336\n",
      "Epoch: 1 \t Loss: 1.76270216\n",
      "Epoch: 2 \t Loss: 1.62740668\n",
      "Epoch: 3 \t Loss: 1.54147145\n",
      "Epoch: 4 \t Loss: 1.47995140\n",
      "Epoch: 5 \t Loss: 1.43100239\n",
      "Epoch: 6 \t Loss: 1.39074463\n",
      "Epoch: 7 \t Loss: 1.35526441\n",
      "Epoch: 8 \t Loss: 1.32519794\n",
      "Epoch: 9 \t Loss: 1.29712536\n",
      "Epoch: 10 \t Loss: 1.27268774\n",
      "Epoch: 11 \t Loss: 1.24876227\n",
      "Epoch: 12 \t Loss: 1.22720749\n",
      "Epoch: 13 \t Loss: 1.20663312\n",
      "Epoch: 14 \t Loss: 1.18768359\n",
      "Epoch: 15 \t Loss: 1.16936996\n",
      "Epoch: 16 \t Loss: 1.15179397\n",
      "Epoch: 17 \t Loss: 1.13514291\n",
      "Epoch: 18 \t Loss: 1.11997525\n",
      "Epoch: 19 \t Loss: 1.10359089\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    state=None\n",
    "    running_loss = 0\n",
    "    n=0\n",
    "    data_ptr = torch.randint(100,(1,1)).item()\n",
    "    # On train sur des séquences de seq_len caractères et on break si on dépasse la taille du dataset\n",
    "    while True:\n",
    "        x = train_data[data_ptr : data_ptr+seq_len]\n",
    "        y = train_data[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        data_ptr+=seq_len\n",
    "        # Pour éviter de sortir de l'index du dataset\n",
    "        if data_ptr + seq_len + 1 > len(train_data):\n",
    "            break\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos evaluar la pérdida (*loss*) en los datos de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss de test: 1.51168611\n"
     ]
    }
   ],
   "source": [
    "state=None\n",
    "running_loss = 0\n",
    "n=0\n",
    "data_ptr = torch.randint(100,(1,1)).item()\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        x = test[data_ptr : data_ptr+seq_len]\n",
    "        y = test[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "    running_loss += loss.item()\n",
    "    n+=1\n",
    "    data_ptr+=seq_len\n",
    "    if data_ptr + seq_len + 1 > len(test):\n",
    "        break\n",
    "print(\"Loss de test: {0:.8f}\".format(running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo presenta un sobreajuste considerable... Intenta corregirlo por tu cuenta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ahora podemos probar la generación de texto!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Çà coeuse, et bon enfin l'avoir faire.\n",
      "\n",
      "MASCARILLE.\n",
      "\n",
      "En me donner d vous, Le pas.\n",
      "\n",
      "MASCARILLE, à dans un pour sûte matinix! cette ma foi.\n",
      "\n",
      "PANDOLFE.\n",
      "\n",
      "Ma foi, tu te le sy sois touves d'arrête sa bien sans les bonheur.\n",
      "\n",
      "MASCARILLE.\n",
      "\n",
      "Moi, je me suis to\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "moliere='.'\n",
    "sequence_length=250\n",
    "state=None\n",
    "for i in range(sequence_length):\n",
    "    x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()\n",
    "    y_pred,state = model.forward(x.unsqueeze(0),state)\n",
    "    probs=F.softmax(torch.squeeze(y_pred), dim=0)\n",
    "    sample=torch.multinomial(probs, 1)\n",
    "    moliere+=itos[sample.item()]\n",
    "print(moliere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La generación es un poco mejor que la del modelo RNN básico, pero aún no es convincente. Puedes intentar mejorar el rendimiento modificando los parámetros (número de capas en serie, dimensión oculta, etc.).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
