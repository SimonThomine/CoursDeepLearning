{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flujos de normalización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este curso, te presentamos los **flujos de normalización**, modelos generativos de *aprendizaje de representaciones*. Aunque son menos conocidos que los VAE, GAN o modelos de difusión, ofrecen varias ventajas significativas.\n",
    "\n",
    "Tanto los GAN como los VAE tienen limitaciones para evaluar con precisión la distribución de probabilidad. Los GAN no lo hacen en absoluto, mientras que los VAE emplean la [cota inferior variacional (ELBO)](https://deepgenerativemodels.github.io/notes/vae/). Esto genera problemas durante el entrenamiento: los VAE suelen producir imágenes borrosas, y los GAN pueden caer en el *colapso de modos* (*mode collapse*).\n",
    "\n",
    "Los flujos de normalización proporcionan una solución a estos inconvenientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo funcionan?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **flujo de normalización** es una serie de transformaciones *biyectivas*. Estas se utilizan para modelar distribuciones complejas, como las de las imágenes, transformándolas en una distribución simple, como una **gaussiana estándar** (media $0$ y varianza $1$).\n",
    "\n",
    "Para entrenarlos, se maximiza la **verosimilitud** de los datos. En la práctica, esto implica minimizar la *log-verosimilitud negativa* ($-\\log p(x)$) con respecto a la densidad de probabilidad real de los datos. Al ajustar los parámetros de las transformaciones, se logra que la distribución generada por el flujo se aproxime lo más posible a la distribución objetivo.\n",
    "\n",
    "![Esquema de un flujo de normalización](./images/NFlow.png)\n",
    "\n",
    "*Imagen extraída del [artículo](https://towardsdatascience.com/introduction-to-normalizing-flows-d002af262a4b).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas y desventajas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas principales de los flujos de normalización**:\n",
    "- Su entrenamiento es **muy estable**.\n",
    "- Convergen más fácilmente que los GAN o los VAE.\n",
    "- No requieren generar ruido para crear datos (a diferencia de los GAN o los VAE).\n",
    "\n",
    "**Desventajas**:\n",
    "- Son **menos expresivos** que los GAN o los VAE.\n",
    "- El **espacio latente** está limitado por la necesidad de funciones biyectivas y la preservación del volumen, lo que lo hace:\n",
    "  - Difícil de interpretar.\n",
    "  - De alta dimensionalidad.\n",
    "- Los resultados generados suelen ser **de menor calidad** que los de los GAN o los VAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Los flujos de normalización tienen un **fundamento teórico profundo**, pero no entraremos en detalles aquí. Si deseas profundizar, puedes consultar el curso **CS236 de Stanford**, disponible en este [enlace](https://deepgenerativemodels.github.io/notes/flow/).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
