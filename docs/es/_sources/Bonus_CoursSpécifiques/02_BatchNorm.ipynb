{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización por lotes (*Batch Normalization*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La *Normalización por Lotes* (*Batch Normalization*) fue introducida en 2015 en el artículo [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167). Tuvo un impacto significativo en el campo del *Deep Learning*. Actualmente, la normalización se utiliza casi sistemáticamente, ya sea *BatchNorm*, *LayerNorm* o *GroupNorm* (entre otras).\n",
    "\n",
    "La idea detrás de *BatchNorm* es simple y está relacionada con el cuaderno anterior. Buscamos obtener preactivaciones que sigan una distribución gaussiana en cada capa de la red. Hemos visto que una buena inicialización permite lograr este comportamiento, pero no siempre es evidente, especialmente con muchas capas diferentes.\n",
    "\n",
    "*BatchNorm* normaliza las preactivaciones con respecto a la dimensión del *batch* antes de pasarlas por las funciones de activación. Esto garantiza una distribución gaussiana en cada paso.\n",
    "\n",
    "Esta normalización no afecta la optimización, ya que se trata de una función derivable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reutilización del código\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a reutilizar el código del cuaderno anterior para implementar la *normalización por lotes*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../05_NLP/prenoms.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180834, 3]) torch.Size([180834])\n",
      "torch.Size([22852, 3]) torch.Size([22852])\n",
      "torch.Size([22639, 3]) torch.Size([22639])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # Contexte\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] \n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=10 # Dimension de l'embedding de C\n",
    "hidden_dim=200 # Dimension de la couche cachée\n",
    "\n",
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es nuestro código de propagación hacia adelante:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de *BatchNorm*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según el artículo, esta es la información:\n",
    "\n",
    "![Norm](./images/norm.png)\n",
    "\n",
    "En primer lugar, se trata de normalizar.\n",
    "\n",
    "Para ello, calculamos la media y la desviación estándar de *hpreact* y luego normalizamos con estos valores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-6\n",
    "hpreact_mean = hpreact.mean(dim=0, keepdim=True)\n",
    "hpreact_std= hpreact.std(dim=0, keepdim=True)\n",
    "hpreact_norm = (hpreact - hpreact_mean) / (hpreact_std+epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos integrar esta normalización en la propagación hacia adelante.\n",
    "\n",
    "Antes de eso, observemos que aún no hemos implementado la parte de *escala y desplazamiento* (*scale and shift*):\n",
    "\n",
    "![Scale and Shift](./images/scaleshift.png)\n",
    "\n",
    "**¿Para qué sirve?** La normalización confina los pesos a tomar valores de una gaussiana centrada y reducida. Esto limita la capacidad de expresión del modelo. Los parámetros aprendibles $\\gamma$ y $\\beta$ permiten evitar este problema al añadir un *desplazamiento* con $\\beta$ y una *escala* con $\\gamma$.\n",
    "\n",
    "Dado que se trata de parámetros aprendibles, también debemos agregarlos a los parámetros del modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, en la propagación hacia adelante tendremos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmean = hpreact.mean(0, keepdim=True)\n",
    "bnstd = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El problema de la *Normalización por Lotes*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al reflexionar un poco, podemos identificar problemas potenciales relacionados con *BatchNorm*:\n",
    "\n",
    "**Un ejemplo se ve afectado por otros elementos del *batch***: Normalizar según la dimensión del *batch* significa que los valores de cada ejemplo dentro del *batch* están influenciados por los otros ejemplos. Esto podría parecer problemático, pero en la práctica es algo positivo. El uso de *batches* aleatorios en cada época permite una regularización, lo que reduce el riesgo de *sobreajuste* (*overfit*) en los datos.\n",
    "Sin embargo, si queremos evitar este problema, podemos utilizar otros métodos de normalización que no normalicen según la dimensión del *batch*. En la práctica, la *BatchNorm* sigue siendo ampliamente utilizada porque funciona muy bien empíricamente.\n",
    "\n",
    "**Fase de prueba en un solo elemento**: Durante el entrenamiento, cada elemento está influenciado por los otros elementos de su *batch*. Sin embargo, en la fase de inferencia, cuando usamos el modelo en un solo elemento, ya no podemos aplicar *BatchNorm*. Esto es un problema, ya que queremos evitar un comportamiento diferente durante el entrenamiento y la inferencia.\n",
    "\n",
    "Para resolver este problema, tenemos dos soluciones:\n",
    "- Podemos calcular la media y la varianza sobre el conjunto de elementos al final del entrenamiento y usar estos valores. En la práctica, no queremos hacer una iteración adicional sobre todo el conjunto de datos solo para esto, por lo que nadie lo hace así.\n",
    "- Otra solución consiste en actualizar la media y la varianza a lo largo del entrenamiento mediante un EMA (*promedio móvil exponencial*). Al final del entrenamiento, tendremos una buena aproximación de la media y la varianza de todos los elementos de entrenamiento.\n",
    "\n",
    "En la práctica, podemos implementarlo en Python de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "bnmean_running = torch.zeros((1, hidden_dim))\n",
    "bnstd_running = torch.ones((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)\n",
    "bnstdi = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: En nuestra implementación, hemos elegido 0.001 para nuestro EMA. En la capa [*BatchNorm* de PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html), este parámetro está definido por *momentum* y su valor predeterminado es 0.1. En la práctica, la elección de este valor depende del tamaño del *batch* en relación con el tamaño del conjunto de datos de entrenamiento. Por ejemplo, para un *batch* grande con un conjunto de datos pequeño, podemos tomar 0.1. Para un *batch* pequeño con un conjunto de datos grande, es mejor elegir un valor más pequeño.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos ahora el entrenamiento de nuestro modelo para verificar que la capa funcione. Para este pequeño modelo, no habrá diferencia en el rendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.8241\n",
      "  10000/ 200000: 1.9756\n",
      "  20000/ 200000: 2.7151\n",
      "  30000/ 200000: 2.3287\n",
      "  40000/ 200000: 2.1411\n",
      "  50000/ 200000: 2.3207\n",
      "  60000/ 200000: 2.3250\n",
      "  70000/ 200000: 2.0320\n",
      "  80000/ 200000: 2.0615\n",
      "  90000/ 200000: 2.2468\n",
      " 100000/ 200000: 2.2081\n",
      " 110000/ 200000: 2.1418\n",
      " 120000/ 200000: 1.9665\n",
      " 130000/ 200000: 1.8572\n",
      " 140000/ 200000: 2.0577\n",
      " 150000/ 200000: 2.1804\n",
      " 160000/ 200000: 1.8604\n",
      " 170000/ 200000: 1.9810\n",
      " 180000/ 200000: 1.8228\n",
      " 190000/ 200000: 1.9977\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "max_steps = 200000\n",
    "\n",
    "for i in range(max_steps):\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "  emb = C[Xb] \n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1 \n",
    "  \n",
    "  # Batch normalization\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "      bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "      bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    \n",
    "  h = torch.tanh(hpreact) \n",
    "  logits = h @ W2 + b2 \n",
    "  loss = F.cross_entropy(logits, Yb)\n",
    "  \n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  lr = 0.1 if i < 100000 else 0.01 \n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "  if i % 10000 == 0:\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideraciones adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sesgo**: La *normalización por lotes* normaliza las preactivaciones de los pesos. Esta normalización anula el sesgo (ya que este desplaza la distribución, mientras que nosotros la recentramos). Cuando se utiliza *BatchNorm*, se puede prescindir del sesgo. En la práctica, si se deja un sesgo, no causa problemas, pero es un parámetro de la red que será inútil.\n",
    "\n",
    "**Ubicación de *BatchNorm***: Según lo que hemos visto, es lógico colocar *BatchNorm* antes de la función de activación. En la práctica, algunos prefieren colocarla después de la capa de activación, por lo que no se sorprenda si se encuentra con esto en la literatura o en algún código.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras normalizaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer un rápido repaso de las otras normalizaciones utilizadas para el entrenamiento de redes neuronales.\n",
    "\n",
    "![Tipos de Normalización](./images/types.png)\n",
    "\n",
    "Figura extraída del [artículo](https://arxiv.org/pdf/1803.08494).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalización por Capa (*Layer Normalization*)**: Esta capa de normalización también se utiliza con mucha frecuencia, especialmente en modelos de lenguaje (GPT, Llama). Consiste en normalizar sobre el conjunto de activaciones de la capa en lugar de sobre la dimensión del *batch*. En nuestra implementación, esto simplemente implicaría cambiar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)  \n",
    "bnstdi = hpreact.std(0, keepdim=True)   \n",
    "# Layer normalization\n",
    "bnmeani = hpreact.mean(1, keepdim=True)  \n",
    "bnstdi = hpreact.std(1, keepdim=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalización por Instancia (*Instance Normalization*)**: Esta capa normaliza las activaciones en cada canal de cada elemento de manera independiente.\n",
    "\n",
    "**Normalización por Grupo (*Group Normalization*)**: Esta capa es una especie de fusión entre *LayerNorm* e *InstanceNorm*, ya que se calcula la normalización sobre grupos de canales (si el tamaño de un grupo es 1, es *InstanceNorm*, y si el tamaño de un grupo es $C$, es *LayerNorm*)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
