{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posibles usos de la arquitectura Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las secciones anteriores, demostramos las capacidades de los *transformers* mediante una aplicación de predicción del siguiente *token* (como GPT). También abordamos la diferencia entre el codificador (*encoder*), el decodificador (*decoder*) y la arquitectura completa para las tareas de Procesamiento del Lenguaje Natural (NLP).\n",
    "\n",
    "Lo destacable de la arquitectura *transformer* es su gran versatilidad. Puede aplicarse a una amplia variedad de problemas, a diferencia de las capas convolucionales, que están sesgadas (lo que las hace muy eficientes rápidamente en el procesamiento de imágenes).\n",
    "\n",
    "En este curso, presentaremos brevemente algunas arquitecturas clásicas de *transformers* en diferentes dominios, principalmente en NLP y visión por computadora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El artículo [BERT: Pre-entrenamiento de Transformadores Bidireccionales Profundos para la Comprensión del Lenguaje](https://arxiv.org/pdf/1810.04805) propone un método para entrenar un modelo de lenguaje tipo *encoder* de manera no supervisada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sobre el entrenamiento no supervisado en NLP**: Una de las fortalezas de los modelos de lenguaje (LLM) como GPT y BERT es que pueden entrenarse con grandes cantidades de datos sin necesidad de anotarlos. En el caso de GPT, se toma un documento de texto, se oculta el final y se pide al modelo que lo genere. La función de pérdida (*loss*) se calcula comparando la generación del modelo con el texto original (como hicimos para generar texto al estilo de Molière). En BERT, el enfoque es ligeramente diferente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT es un modelo tipo *encoder*, lo que significa que considera el contexto de las palabras tanto a la izquierda como a la derecha (antes y después de la palabra actual). Para entrenarlo, no se puede proceder como con GPT, limitándose a predecir las palabras siguientes.\n",
    "\n",
    "**Modelo de Lenguaje Enmascarado (MLM)**: BERT es un *Masked Language Model* (MLM). Durante el entrenamiento, se enmascaran ciertas palabras de una frase (en posiciones aleatorias) y se pide al modelo que las prediga utilizando el contexto que rodea a la palabra enmascarada.\n",
    "\n",
    "![BERT](./images/bert.png)\n",
    "\n",
    "Figura extraída del [artículo](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicción de la Siguiente Frase (NSP)**: BERT también está preentrenado para determinar si una frase B sigue a una frase A en el texto, lo que ayuda al modelo a comprender las relaciones entre las frases.\n",
    "\n",
    "**Nota**: Para obtener más información sobre BERT y aprender a ajustarlo (*fine-tuning*), puedes consultar el [curso 10 sobre BERT](../10_TransferLearningEtDistillation/05_FineTuningLLM.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilidad de BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT y otros modelos de lenguaje tipo *encoder* (como RoBERTa, ALBERT, etc.) se utilizan como base para tareas más específicas. Luego, se realiza un ajuste fino (*fine-tuning*) para otras tareas, como las mencionadas en el cuaderno anterior (análisis de sentimientos, clasificación de texto, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Hemos visto cómo entrenar modelos *encoder* y *decoder* de manera no supervisada para tareas de NLP (BERT y GPT). También es posible entrenar un modelo completo (con *encoder*, *decoder* y atención cruzada) de manera no supervisada, como en el caso del modelo [T5](https://arxiv.org/pdf/1910.10683). No describimos su funcionamiento en este cuaderno, pero para obtener más información, puedes consultar este [artículo](https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers para procesamiento de imágenes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pocos años después del auge de los *transformers* en el campo del NLP, su uso en el ámbito de la visión por computadora también revolucionó este dominio.\n",
    "El artículo [Una Imagen Vale 16x16 Palabras: Transformadores para el Reconocimiento de Imágenes a Gran Escala](https://arxiv.org/pdf/2010.11929) presenta una aplicación de un *transformer* tipo *encoder* adaptado al procesamiento de imágenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT : Vision Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este artículo introduce el *Vision Transformer* (ViT), que se basa en dividir la imagen en *patches* (parches) que luego se ingresan al *transformer* como si fueran *tokens*.\n",
    "\n",
    "![ViT](./images/ViT.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en el lado derecho de la figura, la arquitectura corresponde a un tipo de *encoder* (la única diferencia con respecto a [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) es la aplicación de las normalizaciones antes de las capas en lugar de después).\n",
    "\n",
    "En el modelo *Vision Transformer* (ViT), cada imagen se divide en *patches* de tamaño fijo, por ejemplo, de $16 \\times 16$ píxeles. Cada *patch* se transforma en un vector al aplanarlo, y luego este vector se proyecta en un espacio de *embedding* mediante una capa de proyección lineal, similar a la utilizada en los modelos de procesamiento de texto como BERT o GPT (capa *Embedding*). Esta representación vectorial captura la información espacial y estructural de la imagen, al igual que los *embeddings* en los modelos de NLP capturan el significado y las relaciones entre palabras.\n",
    "El título del artículo \"Una Imagen Vale 16x16 Palabras\" refleja esta analogía: cada *patch* de la imagen se trata como una \"palabra\" proyectada en un espacio de *embedding* para permitir el aprendizaje con la arquitectura *transformer*.\n",
    "\n",
    "**Nota**:\n",
    "- El *Vision Transformer* del artículo original se entrena de manera supervisada en tareas de clasificación de objetos. Los resultados de este artículo son impresionantes y demuestran su capacidad para superar a los modelos convolucionales.\n",
    "- Una mejora notable de la arquitectura ViT para tareas de visión (con entrenamiento supervisado) es el [Swin Transformer](https://arxiv.org/abs/2103.14030). Este *transformer* tiene una arquitectura jerárquica (que puede recordar a las CNN) y permite capturar las relaciones espaciales de manera más eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje no supervisado para la visión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el campo del NLP, los modelos base (entrenados de manera no supervisada) han permitido avances espectaculares. Crear un modelo base para imágenes también es una tarea muy atractiva. Esto permitiría tener un modelo que pueda ajustarse fácilmente (*fine-tuning*) para tareas específicas y con buenos resultados. Para esta tarea, se han propuesto varias aproximaciones utilizando únicamente imágenes. A continuación, presentaremos dos de ellas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEIT**: [BEIT: Pre-entrenamiento tipo BERT para Transformadores de Imágenes](https://arxiv.org/pdf/2106.08254) propone utilizar el mismo método de entrenamiento que BERT, pero en el contexto de las imágenes. Esto consiste en enmascarar ciertos *patches* de la imagen y tratar de predecirlos durante el entrenamiento. Sin embargo, a diferencia de las palabras, las posibilidades de imágenes son casi infinitas (si queremos predecir una imagen RGB de tamaño $3 \\times 8 \\times 8$, hay $(256 \\times 256 \\times 256)^{8 \\times 8} = (16777216)^{64}$ posibilidades, lo que es más que el número de átomos en el universo). Por lo tanto, no se pueden predecir directamente los píxeles.\n",
    "Para solucionar este problema, se utiliza un [VQ-VAE](https://shashank7-iitd.medium.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a), que permite discretizar una representación de la imagen. Esta versión discreta corresponde a valores provenientes de un diccionario de tamaño fijo, y por lo tanto es posible predecir esta representación discreta.\n",
    "\n",
    "![BEIT](./images/beit.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image GPT**: El artículo [Pre-entrenamiento Generativo a partir de Píxeles](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) introduce un equivalente de GPT, pero para píxeles. Se trata de un modelo autorregresivo que genera los píxeles de una imagen uno por uno, al igual que un modelo autorregresivo de NLP con los *tokens*. Esto permite un entrenamiento no supervisado, pero presenta varios inconvenientes:\n",
    "- La generación toma mucho tiempo, ya que se genera un píxel a la vez. Por lo tanto, es necesario aplicar una reducción dimensional previa.\n",
    "- Generar de izquierda a derecha no tiene sentido para una imagen. ¿Por qué de izquierda a derecha y no de derecha a izquierda? ¿O comenzando desde el centro?\n",
    "\n",
    "![Image GPT](./images/imagegpt.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe d'autres façons d'entraîner des *transformers* de vision (ou d'autres modèles de vision) de manière non supervisée, comme les [Masked Autoencoders](https://arxiv.org/pdf/2111.06377) ou les modèles associant texte et image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers que combinan texto e imagen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos *Transformer* que combinan texto e imagen han demostrado ser de gran ayuda para crear modelos de base. Estos modelos suelen ser *subtituladores*, es decir, están entrenados para generar una descripción de una imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, presentaremos el funcionamiento del modelo [CLIP](https://openai.com/index/clip/), introducido en el artículo [Aprendizaje de Modelos Visuales Transferibles a partir de Supervisión con Lenguaje Natural](https://arxiv.org/pdf/2103.00020). También explicaremos la importancia de este tipo de modelo y sus capacidades en diversas tareas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons présenter le fonctionnement du modèle [CLIP](https://openai.com/index/clip/) introduit dans l'article [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020). Nous présenterons également l'intérêt de ce type de modèle et ses capacités dans le cadre de nombreuses tâches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arquitectura de CLIP**: El entrenamiento de CLIP se basa en un método contrastivo. Este método consiste en presentar al modelo dos ejemplos: uno positivo que corresponde a la etiqueta dada y uno negativo que no corresponde. El objetivo es que el modelo asocie correctamente el ejemplo positivo con la etiqueta, al tiempo que disocia el ejemplo negativo de la misma. Así, este enfoque permite definir una frontera clara entre lo que es relevante (positivo) y lo que no lo es (negativo), maximizando la separación entre ambos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica, CLIP utiliza tanto un codificador de texto (*encoder*) como un codificador de imágenes, ambos basados en arquitecturas de *transformers*. El modelo codifica descripciones textuales e imágenes para luego asociarlas correctamente durante el entrenamiento. El objetivo principal es maximizar la correlación entre las descripciones y las imágenes correspondientes, mientras se minimiza esta correlación para las parejas que no coinciden. Esto permite al modelo aprender a representar de manera eficiente las relaciones entre texto e imagen en un espacio de *embedding* común, facilitando así la comprensión y generación de texto a partir de imágenes, y viceversa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de la phase de test, on peut demander au modèle de générer une description adaptée pour notre image.\n",
    "\n",
    "![CLIP](./images/clip.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uso del modelo**: Más allá de ser un simple generador de subtítulos (*captioner*), CLIP también permite realizar clasificación *zero-shot*, es decir, clasificar una imagen sin haber entrenado el modelo específicamente para esa tarea. En el caso de CLIP, esto permite asignar una puntuación a cada descripción que se le proporciona. Por ejemplo, si se le dan dos descripciones como \"Una foto de un gato\" y \"Una foto de un perro\", devuelve puntuaciones de probabilidad que indican qué tan asociada está la imagen actual con cada una de las descripciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otros usos**: Este método de entrenamiento también ha permitido crear modelos de detección *zero-shot*, como [OWL-ViT](https://arxiv.org/pdf/2205.06230), modelos de transferencia de estilo e incluso modelos de generación de imágenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conjuntos de datos de imágenes con descripciones**: También podemos preguntarnos si una descripción de imagen no es equivalente a una etiqueta y si, por lo tanto, necesitaríamos una anotación laboriosa para entrenar este tipo de modelos (que requieren miles de millones de imágenes para ser eficientes). En realidad, es posible recolectar imágenes con descripciones de manera sencilla en internet gracias al atributo \"alt\" de las imágenes en el código HTML, que es una descripción que las personas añaden a sus imágenes.\n",
    "\n",
    "Por supuesto, estos datos no son necesariamente confiables, pero en este tipo de modelos la cantidad es más importante que la calidad.\n",
    "\n",
    "Además, actualmente existen bases de datos de código abierto que contienen miles de millones de pares imagen/descripción. La más conocida es [LAION-5B](https://laion.ai/blog/laion-5b/).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
