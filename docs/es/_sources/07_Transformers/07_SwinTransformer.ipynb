{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook analiza el artículo [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030). Propone una mejora de la arquitectura *transformer* con un diseño jerárquico adaptado a imágenes, similar a las redes neuronales convolucionales.\n",
    "La primera parte del notebook explica las propuestas del artículo una por una. La segunda parte presenta una implementación simplificada de la arquitectura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis del artículo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea principal del artículo es aplicar el mecanismo de *atención* de manera jerárquica sobre partes cada vez más grandes de la imagen. Este enfoque se basa en varios fundamentos:\n",
    "- El análisis de imágenes comienza con los detalles locales antes de considerar las relaciones entre todos los píxeles. Esto explica por qué las CNNs son tan eficientes.\n",
    "- El hecho de que los *tokens* (*patches*) no se comuniquen con todos los demás mejora el tiempo de cómputo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura jerárquica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura jerárquica del *swin transformer* se resume en la siguiente figura:\n",
    "\n",
    "![jerárquica](./images/hierarchique.png)\n",
    "\n",
    "En nuestra implementación, el modelo ViT convierte los *patches* en *tokens* y aplica un *transformer encoder* a todos los elementos. Es una arquitectura simple y sin sesgo en los datos, aplicable a diversos tipos de datos.\n",
    "\n",
    "La arquitectura *swin* añade un sesgo para mejorar su rendimiento con imágenes y acelerar el procesamiento. Como muestra la figura, la imagen se divide primero en pequeños *patches* (de tamaño $4 \\times 4$ en el artículo) agrupados en ventanas. La capa de atención se aplica solo a cada ventana de forma independiente. A medida que avanzamos en la red, la dimensión $C$ (tamaño de los *patches* relativo a la imagen) y el tamaño de las ventanas aumentan hasta cubrir toda la imagen, con el mismo número de *patches* que la arquitectura ViT. Al igual que una CNN, la red procesa primero la información local y, progresivamente (con el aumento del *campo receptivo*), información cada vez más global. Esto se logra aumentando el número de filtros y reduciendo la resolución de la imagen.\n",
    "\n",
    "Los nuevos *bloques* de *transformer* correspondientes se denominan *Window Multi-Head Self-Attention* (W-MSA en el artículo; nota: la M significa *Multi-Head*, no *Masked*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventana corredera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su analogía con las CNNs, los autores observaron que puede ser problemático dividir la imagen en ventanas con posiciones arbitrarias, ya que esto rompe la conexión entre píxeles vecinos ubicados en los bordes de las ventanas.\n",
    "\n",
    "Para solucionar este problema, los autores proponen usar un sistema de *ventanas desplazadas* (*shifting window*) en cada *bloque swin*. Los *bloques swin* se organizan en pares, como se describe en la figura al inicio del notebook.\n",
    "\n",
    "Así es como se ve una ventana desplazada:\n",
    "\n",
    "![desplazamiento](./images/shifting.png)\n",
    "\n",
    "Como puede observarse, con esta técnica se pasa de ventanas de $2 \\times 2$ *patches* a $3 \\times 3$ (en general, de $n \\times n$ a $(n+1) \\times (n+1)$). Esto plantea problemas para el procesamiento en la red, especialmente en modo *batch*.\n",
    "\n",
    "Los autores proponen incorporar un *desplazamiento cíclico* (*cyclic shift*), que consiste en aplicar esta operación a la imagen para permitir un procesamiento más eficiente:\n",
    "\n",
    "![cíclico](./images/cyclic.png)\n",
    "\n",
    "Tenga en cuenta que para usar este método es necesario enmascarar la información de los *patches* que no provienen de la misma parte de la imagen. Las partes blancas, amarillas, verdes y azules de la figura no se comunican entre sí gracias a una capa de *atención* enmascarada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sesgo de posición relativa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura ViT utilizaba un *position embedding* absoluto para añadir información de posición a los diferentes *patches*. El problema de este método es que no captura las relaciones entre los *patches*, por lo que es menos eficiente con imágenes de diferentes resoluciones.\n",
    "\n",
    "El *swin transformer* utiliza un *sesgo de posición relativa* para compensar esto. Este sesgo depende de la distancia relativa entre los diferentes *patches* y se añade cuando se calcula la atención entre dos *patches*. Su principal ventaja es mejorar la captura de relaciones espaciales y adaptarse a imágenes de diferentes resoluciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detalles complementarios de la arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como se observa en la primera figura del notebook, hay más capas en la etapa 3 del *swin transformer*. Al aumentar el número de capas de la red, solo se incrementan las capas de la etapa 3, mientras que las demás permanecen fijas. Esto permite aprovechar la arquitectura *swin* (como el *shifting*) y, al mismo tiempo, mantener una profundidad suficiente para ser eficiente en términos de tiempo de procesamiento.\n",
    "\n",
    "- Supongamos que cada ventana contiene *patches* de tamaño $M \\times M$. La complejidad computacional de una capa *multi-head self-attention* (MSA) y una capa *window multi-head self-attention* (W-MSA) para una imagen de $h \\times w$ *patches* es:\n",
    "$\\Omega(\\text{MSA}) = 4hwC^2 + 2(h w)^2 C$\n",
    "$\\Omega(\\text{W-MSA}) = 4hwC^2 + 2M^2hwC$\n",
    "La primera tiene una complejidad cuadrática, mientras que la segunda es lineal si $M$ es fijo. La arquitectura *swin* permite ganar velocidad de procesamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación simplificada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora pasaremos a la implementación en PyTorch del *swin transformer*. Algunas partes son bastante complejas de implementar y no las cubriremos aquí: la parte de *ventanas desplazadas* y el *sesgo de posición relativa*. Por lo tanto, nos limitaremos a implementar la arquitectura jerárquica.\n",
    "\n",
    "Si desea consultar la implementación completa del *swin transformer* realizada por los autores, puede visitar su [GitHub](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py). Nuestra implementación se inspira en el código de los autores y retoma nuestra implementación del ViT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detection automatique du GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión de imagen en parche\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la conversion de l'image en *patch*, nous reprenons notre fonction du notebook précédent :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    # On rajoute une dimension pour le batch\n",
    "    B,C,_,_ = image.shape\n",
    "    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(B,-1, C, patch_size, patch_size)\n",
    "    patches_flat = patches.flatten(2, 4)\n",
    "    return patches_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head self-attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la implementación del *swin*, la capa *multi-head self-attention* no cambia con respecto a la implementación del ViT. Básicamente es la misma capa, pero lo que cambia es la forma en que se utiliza en el *bloque swin*.\n",
    "\n",
    "Retomemos, entonces, nuestro código del notebook anterior:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head_enc(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "    def __init__(self, head_size,n_embd,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape   \n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # On a supprimer le masquage du futur\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head_enc de taille head_size\n",
    "        self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Si se desea implementar el *sesgo de posición relativa*, habría que modificar la función, ya que este sesgo se añade directamente durante el cálculo de la *atención* (consulte el [código fuente](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py) para más detalles).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mismo ocurre con la capa *feed forward*, que permanece igual:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del bloque swin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos implementando la función para dividir nuestra imagen en ventanas. Para ello, convertiremos nuestra $x$ a una dimensión $B \\times H \\times W \\times C$ en lugar de $B \\times T \\times C$. Luego, transformaremos nuestro tensor en múltiples ventanas que se procesarán en la dimensión *batch* (para tratar cada ventana de forma independiente).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size,input_resolution):\n",
    "    B,_,C = x.shape\n",
    "    H,W = input_resolution\n",
    "    x = x.view(B, H, W, C)\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que, como en la implementación del artículo, dividimos nuestra imagen de tamaño 224 en *patches* de $4 \\times 4$. Esto nos dará $224/4 \\times 224/4$ *patches*, es decir, 3136, que luego se proyectarán en una dimensión de *embedding* $C$ de tamaño 96 (para swin-T y swin-S). Vamos a separarlos en $M=7$ ventanas, lo que nos dará el siguiente tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 7, 7, 96])\n"
     ]
    }
   ],
   "source": [
    "# Pour un batch de taille 2\n",
    "window_size = 7\n",
    "n_embed = 96\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "windows=window_partition(dummy,window_size,(56,56))\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de pasarlo a la capa de *atención*, debemos convertirlo nuevamente a una dimensión $B \\times T \\times C$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 49, 96])\n"
     ]
    }
   ],
   "source": [
    "windows=windows.view(-1, window_size * window_size, n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podremos aplicar nuestra capa de *atención* para realizar el *self-attention* en todas las ventanas de forma independiente. Una vez hecho esto, hay que aplicar la transformación inversa para volver a un formato sin ventanas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 56, 56, 96])\n",
      "torch.Size([2, 3136, 96])\n"
     ]
    }
   ],
   "source": [
    "def window_reverse(windows, window_size,input_resolution):\n",
    "    H,W=input_resolution\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "windows=window_reverse(windows,window_size,(56,56))\n",
    "print(windows.shape)\n",
    "# et revenir en format BxTxC\n",
    "windows=windows.view(2,3136,n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acabamos de implementar los elementos fundamentales para el procesamiento por ventanas (jerarquía del *swin transformer*). Ahora podemos construir nuestro *bloque swin* que agrupa todas estas transformaciones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swinblock(nn.Module):\n",
    "  def __init__(self, n_embd,n_head,input_resolution,window_size,dropout=0.) -> None:\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)\n",
    "    self.ffwd = FeedFoward(n_embd,dropout)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "    self.input_resolution = input_resolution\n",
    "    self.window_size = window_size\n",
    "    self.n_embd = n_embd\n",
    "    \n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    x=window_partition(x, self.window_size,self.input_resolution)\n",
    "    x=self.ln1(x)\n",
    "    x=x.view(-1, self.window_size * self.window_size, self.n_embd)\n",
    "    x=self.sa(x)\n",
    "    x=window_reverse(x,self.window_size,self.input_resolution)\n",
    "    x=x.view(B,T,self.n_embd)\n",
    "    x=x+self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusión de parches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la arquitectura jerárquica del *swin transformer*, cada vez que aumentamos nuestro *campo receptivo* (reduciendo el número de ventanas), concatenamos los 4 *patches* adyacentes de tamaño $C$ en una dimensión $4C$ y luego aplicamos una capa lineal para reducirla a una dimensión más pequeña de $2C$. Esto reduce el número de *tokens* en un factor de 4 cada vez que disminuimos el número de ventanas.\n",
    "Podemos recuperar los *patches* adyacentes de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 96])\n"
     ]
    }
   ],
   "source": [
    "# Reprenons un exemple de nos 56x56 patchs\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "B,T,C = dummy.shape\n",
    "H,W=T**0.5,T**0.5\n",
    "dummy=dummy.view(2,56,56,n_embed)\n",
    "# En python, 0::2 prend un élément sur 2 à partir de 0, 1::2 prend un élément sur 2 à partir de 1\n",
    "# De cette manière, on peut récupérer les à intervalles réguliers \n",
    "dummy0 = dummy[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy1 = dummy[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy2 = dummy[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "dummy3 = dummy[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "print(dummy0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego concatenaremos nuestros *patches* adyacentes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 384])\n",
      "torch.Size([2, 784, 384])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.cat([dummy0, dummy1, dummy2, dummy3], -1)  # B H/2 W/2 4*C\n",
    "print(dummy.shape)\n",
    "# On repasse en BxTxC\n",
    "dummy = dummy.view(B, -1, 4 * C)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos dividido por cuatro el número de *patches* mientras aumentamos los canales en un factor de 4. Ahora aplicamos la capa lineal para reducir el número de canales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(4 * C, 2 * C, bias=False)\n",
    "dummy = layer(dummy)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y así, tenemos todos los elementos para construir nuestra capa de *merging*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, input_resolution, in_channels, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)\n",
    "        self.norm = norm_layer(4 * in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ya tenemos todos los elementos para construir nuestra capa de *merging*:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el *swin transformer*, es complicado añadir un *cls_token* en la implementación. Por eso usaremos el otro método mencionado en el notebook anterior: el *adaptive average pooling*. Esto nos permite tener una salida de tamaño fijo, independientemente del tamaño de la imagen de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 blocs de 2 couches au lieu de 4 car CIFAR-10 a de plus petites images\n",
    "class SwinTransformer(nn.Module):\n",
    "  def __init__(self,n_embed,patch_size,C,window_size,num_heads,img_dim=[16,8,4],depths=[2,2,2]) -> None:\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.proj_layer = nn.Linear(C*patch_size*patch_size, n_embed)\n",
    "    input_resolution = [(img_dim[0],img_dim[0]),(img_dim[1],img_dim[1]),(img_dim[2],img_dim[2])]\n",
    "    self.blocks1 = nn.Sequential(*[swinblock(n_embed,num_heads,input_resolution[0],window_size) for _ in range(depths[0])])\n",
    "    self.down1 = PatchMerging(input_resolution[0],in_channels=n_embed)\n",
    "    self.blocks2 = nn.Sequential(*[swinblock(n_embed*2,num_heads,input_resolution[1],window_size) for _ in range(depths[1])])\n",
    "    self.down2 = PatchMerging(input_resolution[1],in_channels=n_embed*2)\n",
    "    self.blocks3 = nn.Sequential(*[swinblock(n_embed*4,num_heads,input_resolution[2],window_size) for _ in range(depths[2])])\n",
    "    self.classi_head = nn.Linear(n_embed*4, 10)\n",
    "    self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = image_to_patches(x,self.patch_size)\n",
    "    x = self.proj_layer(x)\n",
    "    x = self.blocks1(x)\n",
    "    x = self.down1(x)\n",
    "    x = self.blocks2(x)\n",
    "    x = self.down2(x)\n",
    "    x = self.blocks3(x)\n",
    "    x = self.avgpool(x.transpose(1, 2)).flatten(1)\n",
    "    x = self.classi_head(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formación sobre Imagenette\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar nuestro modelo, utilizaremos nuevamente CIFAR-10, aunque el pequeño tamaño de las imágenes no se adapte necesariamente bien a la arquitectura jerárquica.\n",
    "\n",
    "**Nota**: Puede seleccionar una subparte del conjunto de datos para acelerar el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille d'une image :  torch.Size([3, 32, 32])\n",
      "taille du train dataset :  40000\n",
      "taille du val dataset :  10000\n",
      "taille du test dataset :  10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# Transformation des données, normalisation et transformation en tensor pytorch\n",
    "transform = T.Compose([T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = datasets.CIFAR10(root='./../data', train=True,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(dataset))[:5000]\n",
    "# dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "testdataset = datasets.CIFAR10(root='./../data', train=False,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(testdataset))[:1000]\n",
    "# testdataset = torch.utils.data.Subset(testdataset, indices)\n",
    "print(\"taille d'une image : \",dataset[0][0].shape)\n",
    "\n",
    "\n",
    "#Création des dataloaders pour le train, validation et test\n",
    "train_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\n",
    "print(\"taille du train dataset : \",len(train_dataset))\n",
    "print(\"taille du val dataset : \",len(val_dataset))\n",
    "print(\"taille du test dataset : \",len(testdataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "val_loader= DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 2\n",
    "n_embed = 24\n",
    "n_head = 4\n",
    "C=3 \n",
    "window_size = 4\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.0001 #1e-3\n",
    "\n",
    "model = SwinTransformer(n_embed,patch_size,C,window_size,n_head,img_dim=[16,8,4],depths=[2,2,2]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss train 1.9195597559928894, loss val 1.803518475151062,précision 33.94\n",
      "Epoch 1, loss train 1.7417401003360748, loss val 1.6992134885787964,précision 37.84\n",
      "Epoch 2, loss train 1.651085284280777, loss val 1.6203388486862182,précision 40.53\n",
      "Epoch 3, loss train 1.5808091670751572, loss val 1.5558069843292237,précision 43.03\n",
      "Epoch 4, loss train 1.522760990524292, loss val 1.5169190183639527,précision 44.3\n",
      "Epoch 5, loss train 1.4789127678394318, loss val 1.4665142657279968,précision 47.02\n",
      "Epoch 6, loss train 1.4392719486951828, loss val 1.4568698994636535,précision 47.65\n",
      "Epoch 7, loss train 1.4014943064451217, loss val 1.4456377569198609,précision 48.14\n",
      "Epoch 8, loss train 1.3745941290140151, loss val 1.4345624563694,précision 48.38\n",
      "Epoch 9, loss train 1.3492228104948998, loss val 1.398228020954132,précision 50.04\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss_val += F.cross_entropy(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, loss train {loss_train/len(train_loader)}, loss val {loss_val/len(val_loader)},précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El entrenamiento ha finalizado; obtenemos una precisión del 50% en los datos de validación.\n",
    "\n",
    "Veamos ahora los resultados en nuestros datos de prueba:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision 49.6\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡La precisión es aproximadamente similar a la de los datos de validación!\n",
    "\n",
    "**Nota**: Los resultados no son muy buenos por varias razones. En primer lugar, estamos procesando imágenes pequeñas, mientras que la arquitectura jerárquica del *swin transformer* está diseñada para manejar imágenes de mayores dimensiones. Además, nuestra implementación es bastante minimalista, ya que faltan dos elementos clave de la arquitectura *swin*: la parte de *ventanas desplazadas* y el *sesgo de posición relativa*. El objetivo de este notebook era dar una intuición sobre el funcionamiento de la arquitectura *swin* y no proponer una implementación perfecta ;)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
