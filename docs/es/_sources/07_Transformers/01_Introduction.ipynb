{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a los transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el capítulo anterior, descubrimos múltiples aplicaciones de la librería *Transformers* de Hugging Face. Como su nombre indica, esta librería gestiona modelos *transformers*. Pero, ¿qué es exactamente un modelo *transformer*?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿De dónde proviene el *transformer*?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta 2017, la mayoría de las redes neuronales para el procesamiento del lenguaje natural (NLP) utilizaban RNN (*Recurrent Neural Networks*). En ese año, investigadores de Google publicaron un artículo que revolucionó el campo del NLP y, más tarde, otras áreas del *deep learning* (visión, audio, etc.). Introdujeron la arquitectura *transformer* en su trabajo [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762).\n",
    "\n",
    "Esta es la apariencia de la arquitectura del *transformer*:\n",
    "\n",
    "![Transformer](./images/transformer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primera vista, parece bastante complejo. La parte izquierda se denomina **codificador** (*encoder*), mientras que la derecha es el **decodificador** (*decoder*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido del curso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera parte: construyendo GPT desde cero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera parte de este curso está fuertemente inspirada en el video [\"Let's build GPT: from scratch, in code, spelled out.\"](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1806s&ab_channel=AndrejKarpathy) de Andrej Karpathy. Aquí implementamos un modelo que predice el siguiente carácter basándose en los caracteres anteriores (similar al curso 5 sobre NLP). Esta sección nos ayudará a entender la importancia de la arquitectura *transformer*, especialmente desde la perspectiva del decodificador.\n",
    "\n",
    "En esta parte, entrenaremos un modelo para generar texto al estilo de \"Molière\" de forma automática.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda parte: teoría y codificador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda parte aborda conceptos más matemáticos y también presenta el decodificador de la arquitectura *transformer*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tercera parte: ViT, BERT y otras arquitecturas destacadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tercera parte presenta brevemente adaptaciones de la arquitectura *transformer* para tareas distintas a las de GPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuarta parte: implementación del *Vision Transformer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la cuarta parte, implementamos el *Vision Transformer* basado en el artículo [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) y lo entrenamos con el conjunto de datos CIFAR-10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Quinta parte: implementación del *Swin Transformer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta quinta y última parte ofrece una explicación del artículo [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030), junto con una implementación simplificada.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
