{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînons notre modèle GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook s'appuie sur le notebook précédent. Il est fortement conseillé de faire le notebook précédent avant celui-ci. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "block_size = 128 # Longueur du contexte \n",
    "max_iters = 5000 # Nombre d'itérations d'entraînement\n",
    "eval_interval = 500 # Intervalle pour l'évaluation sur les données de validation\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128 # Dimension de la couche d'attention\n",
    "n_head = 4 # Nombre de head d'attention (pour la multi-head attention)\n",
    "n_layer = 3 # Nombre de couches d'attention\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour la validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "def get_batch(split):\n",
    "    # On genere un batch de données (sur train ou val)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    #On génére batch_size indice de début de séquence pris au hasard dans le dataset\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # On stocke dans notre tenseur torch\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # On met les sur le GPU si on en a un \n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "# Fonction pour estimer le loss plus précisement\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couches Head, Multi-Head, Feed Forward et TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Ajout de dropout pour la regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head de taille head_size\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Couche pour Linear (voir schema) après concatenation\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # Dropout si besoin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 4*n_embd comme dans le papier\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GeLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Block transformer\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # x+ car c'est une connexion résiduelle\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation de notre modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps d'implémenter notre propre GPT. Dans l'idée, il s'agit d'un agencement de block *transformer* avec également des matrices d'*embeddings* pour convertir les *tokens* en *embeddings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Chaque token recupere son embedding à partir d'une look-up table (entrainable)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # Agencement de n_layer TransformerBlock de taille n_embed avec n_head heads.\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # Une manière optimile d'initialiser les poids (à regarder plus en détails si ça vous interesse)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx et targets sont des tenseurs d'entier de taille (B,T)\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        # Position embedding pour ajouter une information spatiale sur les éléments\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # Fonction pour générer du texte : très proche de celle utilisée dans le modèle bigramme du notebook précédent\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx contient des entiers dans un tenseur de taille (B, T), c'est le contexte actuel\n",
    "        for _ in range(max_new_tokens):\n",
    "            # On limite le contexte à block_size (maximum que le réseau peut prendre)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # On calcule la prédictions du prochain token\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # On sample depuis les probabilités obtenues avec le softmax\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # On ajouter l'élément sample à notre texte\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant temps d'entraîner le modèle. L'entraînement peut être assez long selon votre ordinateur et les hyperparamètres que vous avez défini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres du modèle :  632149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4823, val loss 4.4809\n",
      "step 500: train loss 1.9838, val loss 2.0682\n",
      "step 1000: train loss 1.6177, val loss 1.7505\n",
      "step 1500: train loss 1.4865, val loss 1.6591\n",
      "step 2000: train loss 1.4180, val loss 1.6133\n",
      "step 2500: train loss 1.3637, val loss 1.5702\n",
      "step 3000: train loss 1.3256, val loss 1.5459\n",
      "step 3500: train loss 1.3022, val loss 1.5306\n",
      "step 4000: train loss 1.2784, val loss 1.5076\n",
      "step 4500: train loss 1.2622, val loss 1.4943\n",
      "step 4999: train loss 1.2467, val loss 1.4835\n"
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(\"Nombre de paramètres du modèle : \",sum(p.numel() for p in m.parameters()))\n",
    "\n",
    "# Optimizer AdamW\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # On evalue le loss sur train et validation de temps en temps (tous les eval_interval itérations)\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # On recupère un batch aléatoire du dataset\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Calcul du loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant générer du \"Molière\" automatiquement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FMEROT.\n",
      "\n",
      "Il a rien qu'il mieux a plus que me le sortir engage.\n",
      "\n",
      "DONE ELVIRE.\n",
      "\n",
      "Ah! ot, et vous bons aime aussi seule abuer?\n",
      "\n",
      "SGANARELLE.\n",
      "\n",
      "Vous plaissez. Vous savez non pas!\n",
      "\n",
      "LE MONCTAGNE.\n",
      "\n",
      "Vous êtes, cet autres que changus!\n",
      "\n",
      "DON JUAN.\n",
      "\n",
      "Ne sont point tout ce qui ne vous désie.\n",
      "\n",
      "VALÈRE.\n",
      "\n",
      "Une son méclai que expère\n",
      "Que je suis sous-moi qu'en le consie,\n",
      "Que je tiens peurté aun la régle, que je suis en me ducens mois.\n",
      "Au comoqu'on disant fait ce le vite avisenve\n",
      "Qui voir de cettte réjoustoisse dis, et \n"
     ]
    }
   ],
   "source": [
    "# Generation d'un texte par le modèle\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous le voyez, ça ressemble (de loin) à un texte de Molière. En augmentant le nombre de paramètres du modèle, on peut obtenir beaucoup mieux mais le temps de traitement sera très lent (surtout si vous n'avez pas de GPU)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
