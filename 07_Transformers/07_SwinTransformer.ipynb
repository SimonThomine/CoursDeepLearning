{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook analyse le papier [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030) qui propose une amélioration de l'archicture *transformer* avec un design hierarchique spécifique aux images pouvant rappeler les réseaux de neurones convolutifs.   \n",
    "La première partie du notebook explique les propositions de l'article une par une et la seconde partie est une implémentation simplifiée de l'architecture.\n",
    "\n",
    "<img src=\"images/swin.png\" alt=\"swin\" width=\"700\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'article "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée principale du papier est d'appliquer l'*attention* de manière hierarchique sur des parties de plus en plus grande de l'image. Cette approche a plusieurs fondements : Tout d'abord, l'analyse des images en regardant d'abord les détails locaux avant de regarder la relation entre tous les pixels de l'image est intuitivement logique (c'est pourquoi les CNNs sont si performants). Ensuite, le fait que les *tokens* (*patch*) ne communique pas avec tous les autres permet d'améliorer le temps de calcul.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture hierarchique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture hierarchique du *swin transformer* est résumée dans cette figure :  \n",
    "\n",
    "<img src=\"images/hierarchique.png\" alt=\"hierarchique\" width=\"500\"/> \n",
    "\n",
    "Nous avons vu, dans notre implémentation, que le modèle ViT convertit les *patchs* en *tokens* et applique simplement un *transformer encoder* sur tous les éléments. C'est une architecture très simple et sans aucun biais sur les données (c'est l'architecture de base du *transformer* qui peut s'appliquer sur un peu tous les types de données).\n",
    "\n",
    "L'architecture *swin* ajoute un biais destinée à la rendre plus performante sur les images et plus rapide en terme de traitement. Comme on le voit dans la figure, l'image est d'abord séparé en très petits *patchs* (taille $4 \\times 4$ dans le papier) et les *patchs* sont regroupés en fenêtre. La couche d'attention est ensuite appliquée uniquement sur chaque fenêtre de manière indépendante. Plus on va profond dans le réseau, plus la dimension C, taille des *patchs* (relative à l'image) et des fenêtres augmente jusqu'à avoir une fenêtre de toute l'image et le même nombre de *patchs* que l'architecture ViT. A la manière d'un CNN, le réseau traîte d'abord les informations locales puis au fur et à mesure (avec l'augmentation du *receptive field*) des informations de plus en plus globales. Cela se fait en augmentant le nombre de filtres et en diminuant la résolution de l'image. \n",
    "\n",
    "Les nouveaux *blocks* de *transformer* correspondants sont appelés *Window Multi-Head Self-Attention* (W-MSA dans le papier, attention le M signifie *Multi-Head* et pas *Masked*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fenêtre glissante "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans leur analogie avec le CNN, les auteurs se sont rendu compte qu'il peut être problématique de séparer en fenêtres à des positions arbitraires car cela brise la connexion entre des pixels voisins situés à des extremités de fenêtres.   \n",
    "\n",
    "Pour corriger ce problème, les auteurs proposent d'utiliser un système de fenêtre glissante (*shifting window*) dans chaque *block swin*. Les *blocks swin* sont agencés par paires comme décrit dans la figure du début du notebook.  \n",
    "\n",
    "Voici à quoi ressemble la fenêtre glissante : \n",
    "\n",
    "<img src=\"images/shifting.png\" alt=\"shifting\" width=\"500\"/>\n",
    "\n",
    "Comme vous pouvez le constater, avec cette technique, on passe de $2 \\times 2$ patchs à $3 \\times 3$ patchs (de manière générale de $n \\times n$ patchs à $(n+1) \\times (n+1)$) ce qui est problématique pour le traitement par le réseau en particulier en *batch*.  \n",
    "\n",
    "Les auteurs proposent d'incorporer un *cyclic shift* qui consiste à faire cette opération sur l'image pour permettre un traitement plus efficace : \n",
    "\n",
    "<img src=\"images/cyclic.png\" alt=\"cyclic\" width=\"500\"/>\n",
    "\n",
    "A noter que pour utiliser cette méthode, il est nécessaire de masquer les informations de *patchs* ne provenant pas d'une même partie de l'image. Les parties blanches, jaunes, vertes et bleues de la figure ne communique pas ensemble grâce à une couche d'*attention* masquée. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative position bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture ViT utilisait un *position embedding* absolu pour ajouter une information de position sur les différents *patchs*. Le problème de *position embedding* est qu'il ne capture par les relations entre les *patchs* et est donc moins performant si on donne au *transformer* des images de résolutions différentes.\n",
    "\n",
    "Le *swin tranformer* utilise un biais de positions relative pour compenser cela. Ce biais va dépendre de la distance relative entre les différents patchs. Ce biais est ajouté lorsque l'attention est calculée entre deux *patchs*. Ce biais a plusieurs effets mais son principal intêret est qu'il améliorer la capture des relations spatiales et qu'il permet de s'adapter à des images de résolutions différentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détails supplémentaires sur l'architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comme on le voit sur la première figure du notebook, il y a plus de couches dans le stage 3 du *swin transformer*. Lorsqu'on augmente le nombre de couches du réseaux, c'est uniquement les couches du stage 3 qui vont être augmentées, les autres couches restent fixes. Cela permet de béneficier de l'architecture *swin* (*shifting* etc ...) tout en étant suffisament profond et performant en terme de temps de traitement.\n",
    "\n",
    "- Supposons que chaque fenêtre contienne des *patchs* de $M \\times M$. La complexité computationnelle d'une couche *multi-head self-attention* (MSA) et celle d'une couche *window multi-head self-attention* (W-MSA) une image de $h \\times w$ *patchs* sont:   \n",
    "$\\Omega(\\text{MSA}) = 4hwC^2 + 2(h w)^2 C$    \n",
    "$\\Omega(\\text{W-MSA}) = 4hwC^2 + 2M^2hwC$   \n",
    "Le premier est de complexité quadratique tandis que le second est linéaire si $M$ est fixe. L'architecture *swin* permet de gagner en vitesse de traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation simplifiée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant à l'implémentation en pytorch du *swin transformer*. Certaines parties sont assez complexes en terme d'implémentation et nous n'allons pas les couvrir dans cette partie : la partie fenêtre glissante et la partie *relative position bias*. Nous allons donc nous contenter d'implémenter l'architecture hierarchique. \n",
    "\n",
    "Si vous souhaitez regarder l'implémentation complète du *swin transformer* par les auteurs, vous pouvez aller voir leur [github](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py). Notre implémentation s'inspire du code des auteurs et reprend notre implémentation du ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detection automatique du GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion de l'image en patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la conversion de l'image en *patch*, on reprend notre fonction du notebook précédent : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    # On rajoute une dimension pour le batch\n",
    "    B,C,_,_ = image.shape\n",
    "    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(B,-1, C, patch_size, patch_size)\n",
    "    patches_flat = patches.flatten(2, 4)\n",
    "    return patches_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'implémentation du *swin*, la *couche multi-head self-attention* ne change pas par rapport à l'implémentation du ViT. C'est essentiellement la même couche et ce qui va changer c'est la manière de l'utiliser dans le *swin block*.   \n",
    "Reprenons donc notre code du notebook précédent : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head_enc(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "    def __init__(self, head_size,n_embd,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape   \n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # On a supprimer le masquage du futur\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head_enc de taille head_size\n",
    "        self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Si on voulait implémenter le *relative position bias*, on aurait besoin de modifier la fonction car ce *bias* s'ajoute directement lors du calcul de l'*attention* (voir [code source](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py) pour aller plus loin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed foward layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est pareil pour la *feed forward layer* qui reste la même :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation du swin block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par implémenter la fonction pour partionner notre image en fenêtre. Pour cela, on va reconvertir notre x en dimension $B \\times H \\times W \\times C$ plutôt que $B \\times T \\times C$. Puis on va ensuite transformer notre tenseur en plusieurs fenêtres qui passeront dans la dimension *batch* (pour traîter chaque fenêtre indépendamment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size,input_resolution):\n",
    "    B,_,C = x.shape\n",
    "    H,W = input_resolution\n",
    "    x = x.view(B, H, W, C)\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'exemple, supposons que, comme dans l'implémentation du papier, on divise notre image de taille 224 et *patchs* de taille $4 \\times 4$. Cela nous donnera $224/4 \\times 224/4$ *patchs* donc 3136 qui seront ensuite projeté dans une dimension d'embedding $C$ de taille 96 (pour swin-T et swin-S). On va séparer en $M=7$ fenêtres ce qui nous donnera ce tenseur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 7, 7, 96])\n"
     ]
    }
   ],
   "source": [
    "# Pour un batch de taille 2\n",
    "window_size = 7\n",
    "n_embed = 96\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "windows=window_partition(dummy,window_size,(56,56))\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de le passer à la couche d'*attention*, on va devoir le remettre dans une dimension $B \\times T \\times C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 49, 96])\n"
     ]
    }
   ],
   "source": [
    "windows=windows.view(-1, window_size * window_size, n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra ensuite appliquer notre couche d'*attention* pour effectuer le *self-attention* sur toutes les fenêtres indépendamment. Une fois que c'est fait, il faut appliquer la transformée inverse pour revenir dans un format sans fenêtres : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 56, 56, 96])\n",
      "torch.Size([2, 3136, 96])\n"
     ]
    }
   ],
   "source": [
    "def window_reverse(windows, window_size,input_resolution):\n",
    "    H,W=input_resolution\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "windows=window_reverse(windows,window_size,(56,56))\n",
    "print(windows.shape)\n",
    "# et revenir en format BxTxC\n",
    "windows=windows.view(2,3136,n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous venons d'implémenter les éléments fondamentaux pour le traitement en fenêtre (hierarchique du *swin transformer*). On va maintenant pouvoir construire notre *block swin* regroupant toutes ces transformations : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swinblock(nn.Module):\n",
    "  def __init__(self, n_embd,n_head,input_resolution,window_size,dropout=0.) -> None:\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)\n",
    "    self.ffwd = FeedFoward(n_embd,dropout)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "    self.input_resolution = input_resolution\n",
    "    self.window_size = window_size\n",
    "    self.n_embd = n_embd\n",
    "    \n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    x=window_partition(x, self.window_size,self.input_resolution)\n",
    "    x=self.ln1(x)\n",
    "    x=x.view(-1, self.window_size * self.window_size, self.n_embd)\n",
    "    x=self.sa(x)\n",
    "    x=window_reverse(x,self.window_size,self.input_resolution)\n",
    "    x=x.view(B,T,self.n_embd)\n",
    "    x=x+self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'architecture hierarchique du *swin transformer*, à chaque fois l'on augmente notre *receptive field* donc que l'on diminue le nombre de fenêtres, on va concatener les 4 *patchs* adjacents de taille $C$ en dimension $4C$ puis appliquer une couche linéaire pour revenir à une dimension plus petite de $2C$. Cela va réduire le nombre de *tokens* par 4 à chaque fois que l'on réduit le nombre de fenêtres.  \n",
    "On peut récuperer les *patchs* adjacents de cette manière : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 96])\n"
     ]
    }
   ],
   "source": [
    "# Reprenons un exemple de nos 56x56 patchs\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "B,T,C = dummy.shape\n",
    "H,W=T**0.5,T**0.5\n",
    "dummy=dummy.view(2,56,56,n_embed)\n",
    "# En python, 0::2 prend un élément sur 2 à partir de 0, 1::2 prend un élément sur 2 à partir de 1\n",
    "# De cette manière, on peut récupérer les à intervalles réguliers \n",
    "dummy0 = dummy[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy1 = dummy[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy2 = dummy[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "dummy3 = dummy[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "print(dummy0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ensuite concatener nos *patchs* adjacents : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 384])\n",
      "torch.Size([2, 784, 384])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.cat([dummy0, dummy1, dummy2, dummy3], -1)  # B H/2 W/2 4*C\n",
    "print(dummy.shape)\n",
    "# On repasse en BxTxC\n",
    "dummy = dummy.view(B, -1, 4 * C)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien divisé par quatre le nombre de *patchs* tout en augmentant les channels par 4. On applique maintenant la couche linéaire pour diminuer le nombre de channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(4 * C, 2 * C, bias=False)\n",
    "dummy = layer(dummy)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà, on a tous les éléments pour construire notre couche de *merging* : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, input_resolution, in_channels, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)\n",
    "        self.norm = norm_layer(4 * in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du modèle swin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le *swin transformer*, il est compliqué d'ajouter un *cls_token* dans l'implémentation. C'est pourquoi nous allons utiliser l'autre méthode mentionnée dans le notebook précédent c'est à dire l'*adaptive average pooling*. Cela nous permet d'avoir une sortie de taille fixe peu importe la taille de l'image d'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 blocs de 2 couches au lieu de 4 car CIFAR-10 a de plus petites images\n",
    "class SwinTransformer(nn.Module):\n",
    "  def __init__(self,n_embed,patch_size,C,window_size,num_heads,img_dim=[16,8,4],depths=[2,2,2]) -> None:\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.proj_layer = nn.Linear(C*patch_size*patch_size, n_embed)\n",
    "    input_resolution = [(img_dim[0],img_dim[0]),(img_dim[1],img_dim[1]),(img_dim[2],img_dim[2])]\n",
    "    self.blocks1 = nn.Sequential(*[swinblock(n_embed,num_heads,input_resolution[0],window_size) for _ in range(depths[0])])\n",
    "    self.down1 = PatchMerging(input_resolution[0],in_channels=n_embed)\n",
    "    self.blocks2 = nn.Sequential(*[swinblock(n_embed*2,num_heads,input_resolution[1],window_size) for _ in range(depths[1])])\n",
    "    self.down2 = PatchMerging(input_resolution[1],in_channels=n_embed*2)\n",
    "    self.blocks3 = nn.Sequential(*[swinblock(n_embed*4,num_heads,input_resolution[2],window_size) for _ in range(depths[2])])\n",
    "    self.classi_head = nn.Linear(n_embed*4, 10)\n",
    "    self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = image_to_patches(x,self.patch_size)\n",
    "    x = self.proj_layer(x)\n",
    "    x = self.blocks1(x)\n",
    "    x = self.down1(x)\n",
    "    x = self.blocks2(x)\n",
    "    x = self.down2(x)\n",
    "    x = self.blocks3(x)\n",
    "    x = self.avgpool(x.transpose(1, 2)).flatten(1)\n",
    "    x = self.classi_head(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement sur Imagenette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour tester notre modèle, nous allons à nouveau utiliser CIFAR-10 même si la petite taille des images ne se prête pas forcément bien à l'architecture hierarchique. \n",
    "\n",
    "**Note** : Vous pouvez séléctionner une sous-partie du dataset pour accélerer l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille d'une image :  torch.Size([3, 32, 32])\n",
      "taille du train dataset :  40000\n",
      "taille du val dataset :  10000\n",
      "taille du test dataset :  10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# Transformation des données, normalisation et transformation en tensor pytorch\n",
    "transform = T.Compose([T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = datasets.CIFAR10(root='./../data', train=True,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(dataset))[:5000]\n",
    "# dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "testdataset = datasets.CIFAR10(root='./../data', train=False,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(testdataset))[:1000]\n",
    "# testdataset = torch.utils.data.Subset(testdataset, indices)\n",
    "print(\"taille d'une image : \",dataset[0][0].shape)\n",
    "\n",
    "\n",
    "#Création des dataloaders pour le train, validation et test\n",
    "train_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\n",
    "print(\"taille du train dataset : \",len(train_dataset))\n",
    "print(\"taille du val dataset : \",len(val_dataset))\n",
    "print(\"taille du test dataset : \",len(testdataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "val_loader= DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 2\n",
    "n_embed = 24\n",
    "n_head = 4\n",
    "C=3 \n",
    "window_size = 4\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.0001 #1e-3\n",
    "\n",
    "model = SwinTransformer(n_embed,patch_size,C,window_size,n_head,img_dim=[16,8,4],depths=[2,2,2]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss train 1.9195597559928894, loss val 1.803518475151062,précision 33.94\n",
      "Epoch 1, loss train 1.7417401003360748, loss val 1.6992134885787964,précision 37.84\n",
      "Epoch 2, loss train 1.651085284280777, loss val 1.6203388486862182,précision 40.53\n",
      "Epoch 3, loss train 1.5808091670751572, loss val 1.5558069843292237,précision 43.03\n",
      "Epoch 4, loss train 1.522760990524292, loss val 1.5169190183639527,précision 44.3\n",
      "Epoch 5, loss train 1.4789127678394318, loss val 1.4665142657279968,précision 47.02\n",
      "Epoch 6, loss train 1.4392719486951828, loss val 1.4568698994636535,précision 47.65\n",
      "Epoch 7, loss train 1.4014943064451217, loss val 1.4456377569198609,précision 48.14\n",
      "Epoch 8, loss train 1.3745941290140151, loss val 1.4345624563694,précision 48.38\n",
      "Epoch 9, loss train 1.3492228104948998, loss val 1.398228020954132,précision 50.04\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss_val += F.cross_entropy(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, loss train {loss_train/len(train_loader)}, loss val {loss_val/len(val_loader)},précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entraînement est terminé, on obtient une précision de 50% sur les données de validation.  \n",
    "\n",
    "Regardons maintenant sur nos données de test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision 49.6\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La précision est à peu près similaire que sur les données de validation ! \n",
    "\n",
    "**Note** : Les résultats ne sont pas très bons pour plusieurs raisons. Tout d'abord, nous traitons des petites images et l'architecture hierarchique du *swin transformer* est plutôt conçu pour traiter des images de plus grandes dimensions. Ensuite, notre implémentation est vraiment minimaliste puisqu'il manque deux éléments clés de l'architecture *swin* : la partie fenêtre glissante et la partie *relative position bias*. Le but de ce notebook était de vous donner une intuition sur le fonctionnement de l'architecture *swin* et pas de vous proposer une implémentation parfaite ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
