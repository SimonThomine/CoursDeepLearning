{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisations possibles de l'architecture Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les parties précédentes, nous avons demontré les capacités des *transformers* par une application de prédiction du prochain *token* (GPT). Nous avons également mentionné la différence entre encodeur, décodeur et architecture complète pour les tâches de NLP.  \n",
    "\n",
    "Ce qui est génial avec l'architecture *transformers* est que celle-ci est très généraliste, c'est à dire qu'elle peut s'appliquer à énormement de problèmes différents. Ce n'est, par exemple, pas le cas avec les couches de convolutions qui sont biaisées (ce qui les rend aussi très performantes rapidement sur les images).\n",
    "\n",
    "Dans ce cours, nous allons présenter rapidement quelques architectures classiques de *transformers* dans différents domaines (NLP et Vision principalement). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le papier [BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805) propose une façon d'entraîner un modèle de langage de type encodeur de manière non supervisée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point sur l'entraînement non supervisé pour les NLP** : Une des forces des modèles de langage (LLM) comme GPT et BERT, c'est qu'on peut les entraîner sur des grosses quantités de données sans avoir à fournir une annotation de ces données. Pour GPT, il faut imaginer qu'on prend un document texte, qu'on cache la fin du document à notre modèle GPT et qu'on lui demande de générer la fin. Pour calculer le *loss*, on va comparer la génération de notre modèle au texte original (c'est ce que nous avons fait pour générer du Molière). Pour BERT, l'approche est un peu différente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT est un modèle encodeur, c'est à dire qu'il prend en compte le contexte des mots à la fois à droite et à gauche (avant et après le mot actuel). Pour l'entraîner, on ne peut pas faire comme GPT et se contenter de prédire les mots suivants.  \n",
    "\n",
    "**Masked Langage Model (MLM)** : BERT est ce qu'on appelle un *Masked Langage Model* (MLM), pendant l'entraînement, on va masquer certains mots d'une phrase (à des positions aléatoires) et on va demander au modèle de les prédire en se servant du contexte autour du mot masqué. \n",
    "\n",
    "<img src=\"images/bert.png\" alt=\"transformer\" width=\"500\"/> \n",
    "\n",
    "Figure extraite de [blogpost](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Sentence Prediction (NSP)** : BERT est également pré-entraîné à si une phrase B suit une phrase A dans le texte, ce qui aide le modèle à comprendre les relations entre les phrases. \n",
    "\n",
    "**Note** : Pour en savoir plus sur BERT et pour apprendre à le *finetune* vous pouvez consulter le [cours 10 sur BERT](../10_TransferLearningEtDistillation/05_FineTuningLLM.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilité de BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT et les autres modèles de langage encodeur (RoBERTa, ALBERT etc ...) sont utilisés comme base pour des tâches plus précises. On va ensuite les *finetune* pour d'autres tâches et en particulier les tâches évoquées dans le notebook précédent (analyse de sentiments, classification de texte etc ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : On va vu comment entraîner les modèles encodeur et décodeur de manière non supervisée pour les tâches de NLP (BERT et GPT). Il est aussi possible d'entraîner un modèle complet (encodeur, décodeur et cross attention) de manière non supervisée. C'est le cas du modèle [T5](https://arxiv.org/pdf/1910.10683). Nous n'allons pas décrire le fonctionnement dans ce notebook mais pour en savoir plus, vous pouvez consulter le [blogpost](https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers pour le traîtement d'images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques années après le BOOM des *transformers* dans le domaine du NLP, l'utilisation de cette architecture dans le domaine de la vision par ordinateur a également bouleversé le domaine.   \n",
    "Le papier [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
    "](https://arxiv.org/pdf/2010.11929) introduit une application d'un *transformer* de type encodeur adapté au traitement des images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT : Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce papier introduit le *Vision Transformer* (ViT) qui se base sur une découpe de l'image en *patch* qui vont être ensuite donné en entrée au *transformer* comme des *tokens*.  \n",
    "\n",
    "<img src=\"images/ViT.png\" alt=\"transformer\" width=\"700\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir à droite de la figure, l'architecture correspond à une architecture de type encodeur (la seule chose qui change par rapport à [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) est l'application des normes avant les couches plutôt qu'après). \n",
    "\n",
    "Dans le modèle *Vision Transformer* (ViT), chaque image est découpée en *patchs* de taille fixe, par exemple 16x16 pixels. Chaque *patch* est transformé en un vecteur en l'aplatissant, puis ce vecteur est projeté dans un espace d'*embedding* à l'aide d'une couche de projection linéaire, similaire à celle utilisée dans les modèles de traitement de texte comme BERT ou GPT (couche *Embedding*). Cette représentation vectorielle capture les informations spatiales et structurelles de l'image, tout comme les *embeddings* dans les modèles NLP capturent le sens et les relations entre les mots.\n",
    "Le titre de l'article \"An Image is Worth 16x16 Words\" reflète cette analogie : chaque *patch* d'image est traité comme un \"mot\" projeté dans un espace d'*embedding* pour permettre l'apprentissage par avec l'architecture *transformer*.  \n",
    "\n",
    "**Note** : \n",
    "- Le *Vision Transformer* du papier original est entraîné de manière supervisé sur des tâches de classification d'objets. Les résultats de ce papier sont impressionnants et démontrent sa capacité à surpasser les modèle convolutifs.   \n",
    "- Une amélioration notable de l'architecture ViT pour les tâches de vision (avec entraînement supervisé) est le [Swin Transformer](https://arxiv.org/abs/2103.14030). Ce transformer a une architecture hierarchique (pouvant rappeler les CNN) permettant de capturer les relations spatiales plus efficacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage non supervisé pour la vision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le domaine du NLP, les modèles de fondations (entrainé de manière non supervisée) ont permis des avancées spectaculaires. Créer un modèle de fondation pour les images est aussi une tâche très attrayantes. Cela permettrait d'avoir un modèle que l'on peut *finetune* simplement sur des tâches précises  et avec de bons résultats. Pour cette tâche, plusieurs approches ont été proposées à partir d'images uniquement. Nous allons en présenter deux dans la suite de cette partie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEIT** : [BEIT: BERT Pre-Training of Image Transformers](https://arxiv.org/pdf/2106.08254) propose d'utiliser le même mode d'entraînement que BERT mais dans le contexte des images. Cela va consister à masquer certains *patchs* de l'image que l'on va essayer de prédire pendant l'entraînement. Cependant, à l'inverse des mots, les possibilités d'images sont presque infinie (si on veut prédire une image RGB de taille $3 \\times 8 \\times 8$, il y a $(256 \\times 256 \\times 256)^{8 \\times 8} = (16777216)^{64}$ de possiblités ce qui est plus que le nombre d'atomes dans l'univers) donc on ne peut pas directement prédire les pixels.  \n",
    "Pour remedier à ce problème, on utilise un [VQ-VAE](https://shashank7-iitd.medium.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a) qui permet de discrétiser une representation de l'image. Cette version discrète correspond à des valeurs issues d'un dictionnaire de taille fixe et il est donc possible de prédire cette representation discrète.   \n",
    "\n",
    "<img src=\"images/beit.png\" alt=\"transformer\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image GPT** : L'article [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) introduit un équivalent de GPT mais pour les pixels. Il s'agit d'un modèle autoregressif qui va générer les pixels d'une image un par un comme le fait un modèle autoregressif de NLP avec les *tokens*. Cela permet d'avoir un entraînement non supervisé mais il y a quand même de nombreux défauts : \n",
    "- La génération prend énormement de temps car on génére un pixel à la fois. On doit donc appliquer une réduction de dimension au préalable.\n",
    "- Générer de gauche à droite n'a pas de sens pour une image, pourquoi de gauche à droite et pas de droite à gauche ? Ou en commençant du milieu ?  \n",
    "\n",
    "<img src=\"images/imagegpt.png\" alt=\"transformer\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe d'autres façons d'entraîner des *transformers* de vision (ou autre modèle de vision) de manière non supervisée comme les [Masked Autoencoders](https://arxiv.org/pdf/2111.06377) ou les modèles associant texte et image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers associant texte et image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modèles *transformer* associants texte et image se sont revélés d'une grande aide pour la création de modèle de fondations. Ces modèles sont souvent des *captionners*, c'est-à-dire qu'on les entraîne à générer la description d'une image.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP : Connecter images et texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons présenter le fonctionnement du modèle [CLIP](https://openai.com/index/clip/) introduit dans le papier [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020). Nous présenterons également l'intêret de ce type de modèle et ses capacités dans le cadre de nombreuses tâches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture de CLIP** : L'entraînement de CLIP repose sur une méthode contrastive. Cette méthode d'entraînement consiste à présenter au modèle deux exemples : un exemple positif qui correspond au label donné et un exemple négatif qui ne correspond pas au label. Le but est de pousser le modèle à associer correctement l'exemple positif au label tout en dissociant l'exemple négatif du label. Ainsi, cette approche permet de définir une frontière claire entre ce qui est pertinent (positif) et ce qui ne l'est pas (négatif), en maximisant la séparation entre les deux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, CLIP utilise à la fois un encodeur textuel et un encodeur d'image, tous deux basés sur des architectures de *transformers*. Le modèle encode des descriptions textuelles et des images pour ensuite les associer de manière correcte pendant l'entraînement. L'objectif principal est de maximiser la corrélation entre les descriptions et les images correspondantes, tout en minimisant cette corrélation pour les paires qui ne correspondent pas. Cela permet au modèle d'apprendre à représenter efficacement les relations entre texte et image dans un espace d'*embedding* commun, facilitant ainsi la compréhension et la génération de texte à partir d'images, et vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de la phase de test, on peut demander au modèle de générer une description adaptée pour notre image.\n",
    "\n",
    "<img src=\"images/clip.png\" alt=\"transformer\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilisation du modèle** : Au délà d'être un simple captionner, CLIP permet aussi de faire de la classification *zero-shot*, c'est à dire qu'on va pouvoir classifier une image sans avoir entraîné le modèle spécifiquement sur cette tâche. Dans le cas de CLIP, cela va permettre de donner un score à chaque description qu'on lui fournit. On va lui donner deux descriptions \"A photo of a cat\" et \"A photo of a dog\" et il va nous renvoyer des scores de probabilités d'association de notre image actuelle avec chacune des deux descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autres utilisations** : Cette méthode d'entraînement a également permis de créer des modèles de détection *zero-shot* comme [OWL-ViT](https://arxiv.org/pdf/2205.06230), des modèles de transfert de style ou encore des modèles de générations d'images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset d'images avec description** : On peut également se demander si une description d'image n'est pas équivalente à un label et qu'on aurait donc besoin d'une annotation laborieuse pour entraîner ce type de modèle (qui demandent des milliards d'images pour être performants). En réalité, il est possible de récolter des images avec description assez simplement sur internet grâce au \"alt\" de l'image en code HTML. Il s'agit d'une description de l'image que les gens ajoutent à leur image dans le code HTML.   \n",
    "Bien sûr, ces données ne sont pas forcément fiables mais la quantité est plus intéressante que la qualité dans ce type de modèle.  \n",
    "\n",
    "De plus il existe maintenant des bases de données open-source contenant plusieurs milliards de paires image/description. Le plus connu étant [LAION-5B](https://laion.ai/blog/laion-5b/)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
