{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construisons GPT à partir de rien "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook va présenter la création à partir de rien d'un modèle de langage pour prédire le prochain caractère qui se base sur l'architecture du transformers (encodeur en particulier).  \n",
    "Pour cela, nous utilons un fichier texte moliere.txt qui regroupe l'intégralité des dialogues des pièces de Molière.   \n",
    "Ce dataset a été crée à partir des oeuvres complètes de Molière disponibles sur le site [Gutenberg.org](Gutenberg.org). J'ai nettoyé un peu les données pour ne garder que les dialogues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par ouvrir et par visualiser un peu ce que contient notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons les 250 premiers caractères : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALÈRE.\n",
      "\n",
      "Eh bien, Sabine, quel conseil me donnes-tu?\n",
      "\n",
      "SABINE.\n",
      "\n",
      "Vraiment, il y a bien des nouvelles. Mon oncle veut résolûment que ma\n",
      "cousine épouse Villebrequin, et les affaires sont tellement avancées,\n",
      "que je crois qu'ils eussent été mariés dès aujo\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons set() pour récuperer les caractères uniques présent dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijlmnopqrstuvxyz«»ÇÈÉÊÏàâæçèéêëìîïòôùûŒœ\n",
      "Nombre de caractères différents :  85\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de notre dataset d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans le cours 5, nous allons créer un maping pour passer de caractères à entier. Le mapping que nous faisons ici est une forme de tokenization la plus simple possible.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point rapide sur la tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La tokenization, qu'est ce que c'est ?** : La tokenization est le processus de conversion d'un texte en séquence d'entier où chaque entier peut correspondre à un caractère, un groupe de caractère ou un mot selon les méthodes employées.   \n",
    "\n",
    "**Balance entre Vocabulaire et taille de séquence** : Un bon tokenizer trouve une balance entre la taille du vocabulaire (26 pour toutes les lettres de l'alphabet et ~100 000 pour les nombre de mots de la langue française). Plus on a une taille de vocabulaire petite, plus les séquences seront longues (le mot \"Bonjour\" est encodé par 7 tokens si notre vocabulaire est au niveau du caractère et un seul token si notre vocabulaire regroupe tous les mots de la langue française) et inversement. En pratique, les deux extrèmes sont problémiques et on cherche le juste milieu.  \n",
    "\n",
    "**Tokenizer de la littérature** : Les tokenizers sont une part importante du bon fonctionnement d'un modèle de langage. \n",
    "La façon de créer un bon tokenizer dépend de la méthode et des données d'entraînement. Parmi les tokenizers les plus utilisés, on retrouve [SentencePiece](https://github.com/google/sentencepiece) de Google et [tiktoken](https://github.com/openai/tiktoken) de OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 50, 49, 46, 50, 56, 53, 1, 68, 1, 55, 50, 56, 54]\n",
      "Bonjour à Tous\n"
     ]
    }
   ],
   "source": [
    "# Creation d'un mapping de caractère à entiers et inversement\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "\n",
    "print(encode(\"Bonjour à tous\"))\n",
    "print(decode(encode(\"Bonjour à Tous\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va transformer notre dataset en séquence d'entier et le stocker sous forme de tenseur pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33, 12, 23, 64, 29, 16,  8,  0,  0, 16, 44,  1, 38, 45, 41, 49,  6,  1,\n",
      "        30, 37, 38, 45, 49, 41,  6,  1, 52, 56, 41, 47,  1, 39, 50, 49, 54, 41,\n",
      "        45, 47,  1, 48, 41,  1, 40, 50, 49, 49, 41, 54,  7, 55, 56, 11,  0,  0,\n",
      "        30, 12, 13, 20, 25, 16,  8,  0,  0, 33, 53, 37, 45, 48, 41, 49, 55,  6,\n",
      "         1, 45, 47,  1, 59,  1, 37,  1, 38, 45, 41, 49,  1, 40, 41, 54,  1, 49,\n",
      "        50, 56, 57, 41, 47, 47, 41, 54,  8,  1, 24, 50, 49,  1, 50, 49, 39, 47,\n",
      "        41,  1, 57, 41, 56, 55,  1, 53, 73, 54, 50, 47, 82, 48, 41, 49, 55,  1,\n",
      "        52, 56, 41,  1, 48, 37,  0, 39, 50, 56, 54, 45, 49, 41,  1, 73, 51, 50,\n",
      "        56, 54, 41,  1, 33, 45, 47, 47, 41, 38, 53, 41, 52, 56, 45, 49,  6,  1,\n",
      "        41, 55,  1, 47, 41, 54,  1, 37, 42, 42, 37, 45, 53, 41, 54,  1, 54, 50,\n",
      "        49, 55,  1, 55, 41, 47, 47, 41, 48, 41, 49, 55,  1, 37, 57, 37, 49, 39,\n",
      "        73, 41, 54,  6,  0, 52, 56, 41,  1, 46, 41,  1, 39, 53, 50, 45, 54,  1,\n",
      "        52, 56,  3, 45, 47, 54,  1, 41, 56, 54, 54, 41, 49, 55,  1, 73, 55, 73,\n",
      "         1, 48, 37, 53, 45, 73, 54,  1, 40, 72, 54,  1, 37, 56, 46, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:250]) # Les 250 premiers caractères encodé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant découper notre texte en une partie training et une partie validation. Prenons un ratio de 0.9-0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour la validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour notre modèle de langage, on va également définir une taille de contexte \"block_size\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33, 12, 23, 64, 29, 16,  8,  0,  0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, les 8 premiers caractères represente le contexte et le 9ème est le label. Nous verrons ensuite que ce simple exemple regroupe en fait une multitude d'exemples.   \n",
    "En effet, notre modèle doit être capable de prédire le prochain caractère peu importe le contexte qu'il a en amont. On a donc, dans cette liste, 8 exemples qui sont les suivants :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quand l'entrée est [33] le label est : 12\n",
      "Quand l'entrée est [33 12] le label est : 23\n",
      "Quand l'entrée est [33 12 23] le label est : 64\n",
      "Quand l'entrée est [33 12 23 64] le label est : 29\n",
      "Quand l'entrée est [33 12 23 64 29] le label est : 16\n",
      "Quand l'entrée est [33 12 23 64 29 16] le label est : 8\n",
      "Quand l'entrée est [33 12 23 64 29 16  8] le label est : 0\n",
      "Quand l'entrée est [33 12 23 64 29 16  8  0] le label est : 0\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Quand l'entrée est {context.numpy()} le label est : {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sait maintenant comme créer un ensemble de entrée/label à partir d'un seul exemple. Adaptons cette méthode pour un traitement en batch : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrée : \n",
      "torch.Size([4, 8])\n",
      "tensor([[ 0,  0, 30, 18, 12, 25, 12, 29],\n",
      "        [40, 37, 49, 55,  1, 52, 56, 41],\n",
      "        [56, 53,  1, 12, 47, 38, 41, 53],\n",
      "        [37, 45, 54,  1, 47, 68,  1, 40]])\n",
      "Labels :\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 0, 30, 18, 12, 25, 12, 29, 16],\n",
      "        [37, 49, 55,  1, 52, 56, 41,  1],\n",
      "        [53,  1, 12, 47, 38, 41, 53, 55],\n",
      "        [45, 54,  1, 47, 68,  1, 40, 41]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # La taille de batch (les séquences calculés en parallèles)\n",
    "block_size = 8 # La taille de contexte maximale pour une prédiction du modèle\n",
    "\n",
    "def get_batch(split):\n",
    "    # On genere un batch de données (sur train ou val)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    #On génére batch_size indice de début de séquence pris au hasard dans le dataset\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # On stocke dans notre tenseur torch\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('Entrée : ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('Labels :')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chacun de ces 4 exemples regroupe 8 exemples distincts (comme expliqué précedemment), cela fait donc un total de 32 exemples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance du bigramme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cours 5 sur les NLP, nous avons vu le bigramme qui peut être considéré comme le modèle de langage le plus simple et qui consiste à prédire la prochain caractère à partir d'un unique caractère de contexte. Notons $B$ pour la batch_size, $T$ pour le block_size et $C$ pour le vocab_size.    \n",
    "Pour voir sa performance sur le dataset moliere.txt, implémentons le rapidement en pytorch :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 85])\n",
      "tensor(5.1318, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # Chaque token va directement lire la valeur du prochain à partir d'une look-up table entrainé\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # Taille (B,T)\n",
    "    logits = self.token_embedding_table(idx) \n",
    "    # Taille (B,T,C)\n",
    "    \n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx est de la taille (B,T) avec T le contexte actuel\n",
    "    for _ in range(max_new_tokens):\n",
    "      # Forward du modèle pour récuperer les prédictions\n",
    "      logits, loss = self(idx)\n",
    "      # On prend uniquement le dernier caractère\n",
    "      logits = logits[:, -1, :] # devient (B, C)\n",
    "      # On applique la softmax pour récuperer les probabilités\n",
    "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "      # On sample avec torch.multinomial\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # devient (B, 1)\n",
    "      # On ajouter l'élément sample à la séquence actuelle\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle est implementé mais non entraîné, si on le teste comme ça on obtient des résultats catastrophiques : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ScgI:MœUSÊgtrYZc(-DêDm)(Œ'T;jàSduÏâTMtUæòUHjOôbMrVEmà\n",
      "h'sfîRH»Dîxd;ŒhZê(âMoJjVoŒMmdZùï.tgîlDpr» CæŒK\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est tout simplement aléatoire et c'est logique car le modèle est initialisé aléatoirement.  \n",
    "On va maintenant entrainer le modèle :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.342592477798462\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "steps=10000\n",
    "for step in range(steps): # Nombre d'étape d'entraînement (élements traités = steps*batch_size)\n",
    "\n",
    "    # On récupère un batch de données aléatoires\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # On calcule le loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Retropropagation\n",
    "    loss.backward()\n",
    "    # Mise à jour des poids du modèle\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Générons à partir de notre modèle entrainé : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "t t joieçolèr TTE.\n",
      "\n",
      "DIluptrr dias me men, qul e? l feut an, à pastirou GOtrs ve ames à p\n",
      "ELE.\n",
      "Pas atesunt-vofroitur j'y trte delace ét mplant ce padi quendene t, apagi mittere côte cl de che.\n",
      "\n",
      "JUn eloin jeuene me IRSONEtren.\n",
      "Qu pun di asque e,\n",
      "\n",
      "ARÉLVaiens l pçuntonceuem'euss vonde.\n",
      "Ahoil'ablà au'avo\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate une amélioration dans la structuration des données et certains mots semblent presque correct mais ça reste catastrophique. En soit, on s'attendait à ce résultat car le bigramme est un modèle trop simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
