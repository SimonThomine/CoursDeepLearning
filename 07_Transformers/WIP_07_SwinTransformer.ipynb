{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook analyse le papier [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030) proposant une amélioration de l'archicture transformer avec un design hierarchique spécifique aux images pouvant rappeler les réseaux de neurones convolutifs.   \n",
    "La première partie du notebook explique les propositions de l'article une par une et la seconde partie est une implémentation simplifiée de l'architecture.\n",
    "\n",
    "<img src=\"images/swin.png\" alt=\"swin\" width=\"700\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'article "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée principale du papier est d'appliquer l'attention de manière hierarchique sur des parties de plus en plus grande de l'image. Cette approche a plusieurs fondements : Tout d'abord, l'analyse des images en regardant d'abord les détails locaux avant de regarder la relation entre tous les pixels de l'image est intuitivement logique (c'est pourquoi les CNNs sont si performants). Ensuite, le fait que les tokens (patch) ne communique pas avec tous les autres permet d'améliorer le temps de calcul.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture hierarchique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture hierarchique du swin transformer est résumée dans cette figure :  \n",
    "\n",
    "<img src=\"images/hierarchique.png\" alt=\"hierarchique\" width=\"500\"/> \n",
    "\n",
    "Nous avons vu, dans notre implémentation, que le modèle ViT convertit les patchs en \"tokens\" et applique simplement un transformer encoder sur tous les éléments. C'est une architecture très simple et sans aucun biais sur les données (c'est l'architecture de base du transformer qui peut s'appliquer sur un peu tous les types de données).\n",
    "\n",
    "L'architecture swin ajoute un biais destinée à la rendre plus performante sur les images et plus rapide en terme de traitement. Comme on le voit dans la figure, l'image est d'abord séparé en très petits patchs (taille $4 \\times 4$ dans le papier) et les patchs sont regroupés en fenêtre. La couche d'attention est ensuite appliquée uniquement sur chaque fenêtre de manière indépendante. Plus on va profond dans le réseau, plus la dimension C, taille des patchs (relative à l'image) et des fenêtres augmente jusqu'à avoir une fenêtre de toute l'image et le même nombre de patchs que l'architecture ViT. A la manière d'un CNN, le réseau traîte d'abord les informations locales puis au fur et à mesure (avec l'augmentation du receptive field) des informations de plus en plus globales. Cela se fait en augmentant le nombre de filtres et en diminuant la résolution de l'image. \n",
    "\n",
    "Les nouveaux blocks de transformer correspondants sont appelés \"Window Multi-Head Self-Attention\" (W-MSA dans le papier, attention le M signifie Multi-Head et pas Masked).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fenêtre glissante "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans leur analogie avec le CNN, les auteurs se sont rendu compte qu'il peut être problématique de séparer en fenêtres à des positions arbitraires car cela brise la connexion entre des pixels voisins situés à des extremités de fenêtres.   \n",
    "\n",
    "Pour corriger ce problème, les auteurs proposent d'utiliser un système de fenêtre glissante (shifting window) dans chaque block swin. Les blocks swin sont agencés par paires comme décrit dans la figure du début du notebook.  \n",
    "\n",
    "Voici à quoi ressemble la fenêtre glissante : \n",
    "\n",
    "<img src=\"images/shifting.png\" alt=\"shifting\" width=\"500\"/>\n",
    "\n",
    "Comme vous pouvez le constater, avec cette technique, on passe de $2 \\times 2$ patchs à $3 \\times 3$ patchs (de manière générale de $n \\times n$ patchs à $(n+1) \\times (n+1)$) ce qui est problématique pour le traitement par le réseau en particulier en batch.  \n",
    "\n",
    "Les auteurs proposent d'incorporer un cyclic shift qui consiste à faire cette opération sur l'image pour permettre un traitement plus efficace : \n",
    "\n",
    "<img src=\"images/cyclic.png\" alt=\"cyclic\" width=\"500\"/>\n",
    "\n",
    "A noter que pour utiliser cette méthode, il est nécessaire de masquer les informations de patchs ne provenant pas d'une même partie de l'image. Les parties blanches, jaunes, vertes et bleues de la figure ne communique pas ensemble grâce à une couche d'attention masquée. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative position bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture ViT utilisait un position embedding absolu pour ajouter une information de position sur les différents patchs. Le problème de position embedding est qu'il ne capture par les relations entre les patchs et est donc moins performant si on donne au transformer des images de résolutions différentes.\n",
    "\n",
    "Le swin tranformer utilise un biais de positions relative pour compenser cela. Ce biais va dépendre de la distance relative entre les différents patchs. Ce biais est ajouté lorsque l'attention est calculée entre deux patchs. Ce biais a plusieurs effets mais son principal intêret est qu'il améliorer la capture des relations spatiales et qu'il permet de s'adapter à des images de résolutions différentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détails supplémentaires sur l'architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comme on le voit sur la première figure du notebook, il y a plus de couches dans le stage 3 du swin transformer. Lorsqu'on augmente le nombre de couches du réseaux, c'est uniquement les couches du stage 3 qui vont être augmentée, les autres couches restent fixes. Cela permet de béneficier de l'archicture swin (shifting etc ...) tout en étant suffisament profond et performant en terme de temps de traitement.\n",
    "\n",
    "- Supposons que chaque fenêtre contienne des patchs de $M \\times M$. La complexité computationnelle d'une couche multi-head self-attention (MSA) et celle d'une couche window multi-head self-attention (W-MSA) une image de $h \\times w$ patchs sont:   \n",
    "$\\Omega(\\text{MSA}) = 4hwC^2 + 2(h w)^2 C$    \n",
    "$\\Omega(\\text{W-MSA}) = 4hwC^2 + 2M^2hwC$   \n",
    "Le premier est de complexité quadratique tandis que le second est linéaire si $M$ est fixe. L'architecture swin permet de gagner en vitesse de traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation simplifiée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant à l'implémentation en pytorch du swin transformer. Certaines parties sont assez complexes en terme d'implémentation et nous n'allons pas les couvrir dans cette partie : la partie fenêtre glissante et la partie relative position bias. Nous allons donc nous contenter d'implémenter l'architecture hierarchique. \n",
    "\n",
    "Si vous souhaitez regarder l'implémentation complète du swin transformer par les auteurs, vous pouvez aller voir leur [github](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py). Notre implémentation s'inspire du code des auteurs et reprend notre implémentation du ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detection automatique du GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion de l'image en patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la conversion de l'image en patch, on reprend notre fonction du notebook précédent : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    # On rajoute une dimension pour le batch\n",
    "    B,C,_,_ = image.shape\n",
    "    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(B,-1, C, patch_size, patch_size)\n",
    "    patches_flat = patches.flatten(2, 4)\n",
    "    return patches_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'implémentation du swin, la couche multi-head self-attention ne change pas par rapport à l'implémentation du ViT. C'est essentiellement la même couche et ce qui va changer c'est la manière de l'utiliser dans le swin block.   \n",
    "Reprenons donc notre code du notebook précédent : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head_enc(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "    def __init__(self, head_size,n_embd,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape   \n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # On a supprimer le masquage du futur\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head_enc de taille head_size\n",
    "        self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Si on voulait implémenter le relative position bias, on aurait besoin de modifier la fonction car ce bias s'ajoute directement lors du calcul de l'attention (voir [code source](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py) pour aller plus loin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed foward layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est pareil pour la feed forward layer qui reste la même :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation du swin block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par implémenter la fonction pour partionner notre image en fenêtre. Pour cela, on va reconvertir notre x en dimension $B \\times H \\times W \\times C$ plutôt que $B \\times T \\times C$. Puis on va ensuite transformer notre tenseur en plusieurs fenêtres qui passeront dans la dimension batch (pour traîter chaque fenêtre indépendamment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size,input_resolution):\n",
    "    B,_,C = x.shape\n",
    "    H,W = input_resolution\n",
    "    x = x.view(B, H, W, C)\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'exemple, supposons que, comme dans l'implémentation du papier, on divise notre image de taille 224 et patchs de taille $4 \\times 4$. Cela nous donnera $224/4 \\times 224/4$ patchs donc 3136 qui seront ensuite projeté dans une dimension d'embedding $C$ de taille 96 (pour swin-T et swin-S). On va séparer en $M=7$ fenêtres ce qui nous donnera ce tenseur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 7, 7, 96])\n"
     ]
    }
   ],
   "source": [
    "# Pour un batch de taille 2\n",
    "window_size = 7\n",
    "n_embed = 96\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "windows=window_partition(dummy,window_size,(56,56))\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de le passer à la couche d'attention, on va devoir le remettre dans une dimension $B \\times T \\times C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 49, 96])\n"
     ]
    }
   ],
   "source": [
    "windows=windows.view(-1, window_size * window_size, n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra ensuite appliquer notre couche d'attention pour effectuer le self-attention sur toutes les fenêtres indépendamment. Une fois que c'est fait, il faut appliquer la transformée inversion pour revenir dans un format sans fenêtres : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 56, 56, 96])\n",
      "torch.Size([2, 3136, 96])\n"
     ]
    }
   ],
   "source": [
    "def window_reverse(windows, window_size,input_resolution):\n",
    "    H,W=input_resolution\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "windows=window_reverse(windows,window_size,(56,56))\n",
    "print(windows.shape)\n",
    "# et revenir en format BxTxC\n",
    "windows=windows.view(2,3136,n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous venons d'implémenter les éléments fondamentaux pour le traitement en fenêtre (hierarchique du swin transformer). On va maintenant pouvoir construire notre block swin regroupant toutes ces transformations : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swinblock(nn.Module):\n",
    "  def __init__(self, n_embd,n_head,input_resolution,window_size,dropout=0.) -> None:\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)\n",
    "    self.ffwd = FeedFoward(n_embd,dropout)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "    self.input_resolution = input_resolution\n",
    "    self.window_size = window_size\n",
    "    self.n_embd = n_embd\n",
    "    \n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    x=window_partition(x, self.window_size,self.input_resolution)\n",
    "    x=self.ln1(x)\n",
    "    x=x.view(-1, window_size * window_size, self.n_embd)\n",
    "    x=self.sa(x)\n",
    "    x=window_reverse(x,self.window_size,self.input_resolution)\n",
    "    x=x.view(B,T,self.n_embd)\n",
    "    x=x+self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'architecture hierarchique du swin transformer, à chaque fois l'on augmente notre \"receptive field\" donc que l'on diminue le nombre de fenêtres, on va concatener les 4 patchs adjacents de taille $C$ en dimension $4C$ puis appliquer une couche linéaire pour revenir à une dimension plus petite de $2C$. Cela va réduire le nombre de tokens par 4 à chaque fois que l'on réduit le nombre de fenêtres.  \n",
    "On peut récuperer les patchs adjacents de cette manière : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 96])\n"
     ]
    }
   ],
   "source": [
    "# Reprenons un exemple de nos 56x56 patchs\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "B,T,C = dummy.shape\n",
    "H,W=T**0.5,T**0.5\n",
    "dummy=dummy.view(2,56,56,n_embed)\n",
    "# En python, 0::2 prend un élément sur 2 à partir de 0, 1::2 prend un élément sur 2 à partir de 1\n",
    "# De cette manière, on peut récupérer les à intervalles réguliers \n",
    "dummy0 = dummy[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy1 = dummy[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy2 = dummy[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "dummy3 = dummy[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "print(dummy0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ensuite concatener nos patchs adjacents : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 384])\n",
      "torch.Size([2, 784, 384])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.cat([dummy0, dummy1, dummy2, dummy3], -1)  # B H/2 W/2 4*C\n",
    "print(dummy.shape)\n",
    "# On repasse en BxTxC\n",
    "dummy = dummy.view(B, -1, 4 * C)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien divisé par quatre le nombre de patchs tout en augmentant les channels par 4. On applique maintenant la couche linéaire pour diminuer le nombre de channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(4 * C, 2 * C, bias=False)\n",
    "dummy = layer(dummy)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà, on a tous les éléments pour construire notre couche de merging : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, input_resolution, in_channels, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)\n",
    "        self.norm = norm_layer(4 * in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du modèle swin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le swin transformer, il est compliqué d'ajouter un cls_token dans l'implémentation. C'est pourquoi nous allons utiliser l'autre méthode mentionnée dans le notebook précédent c'est à dire l'adaptive average pooling. Cela nous permet d'avoir une sortie de taille fixe peu importe la taille de l'image d'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTrasformer(nn.Module):\n",
    "  def __init__(self,n_embed,patch_size,C,window_size,depths,num_heads) -> None:\n",
    "    super().__init__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
