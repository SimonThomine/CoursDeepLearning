{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning BERT avec Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La library transformers de Hugging Face propose des outils pour *finetune* les mod√®les de mani√®re simple et efficace. Dans ce notebook, nous allons montrer l'utilisation de Hugging Face pour *finetune* BERT sur deux t√¢ches : *named-entity recognition* (*token-level classification*) et analyse de sentiment (*sentence-level classification*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commen√ßons par faire de la *token-level classification* sur une t√¢che de NER. \n",
    "Pour cela, nous utilisons le dataset [CONLL](https://arxiv.org/pdf/cs/0306050v1). Pour l'exemple, nous allons prendre uniquement 1000 √©l√©ments de ce dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments,AutoModelForTokenClassification,AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorForTokenClassification,DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'Neither', 'the', 'National', 'Socialists', '(', 'Nazis', ')', 'nor', 'the', 'communists', 'dared', 'to', 'kidnap', 'an', 'American', 'citizen', ',', '\"', 'he', 'shouted', ',', 'in', 'an', 'oblique', 'reference', 'to', 'his', 'extradition', 'to', 'Germany', 'from', 'Denmark', '.', '\"']\n",
      "[0, 0, 0, 7, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"eriktks/conll2003\",trust_remote_code=True)\n",
    "\n",
    "# 1000 √©l√©ments pour l'entra√Ænement\n",
    "sub_train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# 500 √©l√©ments pour l'√©valuation\n",
    "sub_val_dataset = dataset['validation'].shuffle(seed=42).select(range(500)) # 500 examples for evaluation\n",
    "\n",
    "print(sub_train_dataset['tokens'][0])\n",
    "print(sub_train_dataset['ner_tags'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a notre s√©quence de mot et la s√©quence de labels correspondantes.    \n",
    "Maintenant, associons nos labels aux classes : En NER, si plusieurs mots appartiennent √† la m√™me entit√©, le premier mot aura la classe \"B-XXX\" et les mots suivants de l'entit√© la classe \"I-XXX\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "# On va associer les labels √† des entiers\n",
    "itos={0: 'O', 1:'B-PER', 2:'I-PER',  3:'B-ORG',  4:'I-ORG',  5:'B-LOC',  6:'I-LOC', 7:'B-MISC', 8:'I-MISC'}\n",
    "stoi = {v: k for k, v in itos.items()}\n",
    "print(stoi)\n",
    "print(itos)\n",
    "label_names=list(itos.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant charger le *tokenizer* de BERT. C'est la classe qui permettra de convertir notre phrase en s√©quence de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tokenizer s'occupe de transformer la phrase en token mais il faut aussi adapter les labels. Chaque token doit avoir le bon label.   \n",
    "Cette fonction permet d'associer les labels aux tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "  new_labels = []\n",
    "  current_word = None\n",
    "  for word_id in word_ids:\n",
    "    if word_id != current_word:\n",
    "      # D√©but d'un nouveau mot\n",
    "      current_word = word_id\n",
    "      # -100 pour les tokens sp√©ciaux\n",
    "      label = -100 if word_id is None else labels[word_id]\n",
    "      new_labels.append(label)\n",
    "    elif word_id is None:\n",
    "      # -100 pour les tokens sp√©ciaux\n",
    "      new_labels.append(-100)\n",
    "    else:\n",
    "      # Les tokens du m√™me mot ont le m√™me label (sauf le premier)\n",
    "      label = labels[word_id]\n",
    "      # B pour le premier token du mot, I pour les suivants (cf itos)\n",
    "      if label % 2 == 1:\n",
    "        label += 1\n",
    "      new_labels.append(label)\n",
    "  return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant transformer notre s√©quence en tokens et avoir les labels correspondants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 12651.62 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 11565.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "  # On tokenise les phrases\n",
    "  tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True )\n",
    "  all_labels = examples[\"ner_tags\"]\n",
    "  new_labels = []\n",
    "  # On aligne les labels avec les tokens\n",
    "  for i, labels in enumerate(all_labels):\n",
    "    word_ids = tokenized_inputs.word_ids(i)\n",
    "    new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "  tokenized_inputs[\"labels\"] = new_labels\n",
    "  return tokenized_inputs\n",
    "\n",
    "# On applique la fonction sur les donn√©es de train et de validation\n",
    "train_tokenized_datasets = sub_train_dataset.map(\n",
    "  tokenize_and_align_labels,\n",
    "  batched=True,\n",
    ")\n",
    "val_tokenized_datasets = sub_val_dataset.map(\n",
    "  tokenize_and_align_labels,\n",
    "  batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©ons notre mod√®le BERT. HuggingFace permet de cr√©er directement un mod√®le pour la *token-level classification* avec *AutoModelForTokenClassification*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-uncased\",id2label=itos, label2id=stoi) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D√©finissons aussi une fonction nous permettant de calculer l'*accuracy* et le *f1-score* sur nos donn√©es de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # On supprime les labels -100\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est pr√™t √† entra√Æner notre mod√®le ! Pour cela on va utiliser le trainer de Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Pour param√©trer l'entra√Ænement, on peut changer tout un tas de param√®tres mais ceux par d√©faut sont souvent suffisants\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./models\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 501/625 [03:08<00:46,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1273, 'grad_norm': 11.809627532958984, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [03:55<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 235.0171, 'train_samples_per_second': 21.275, 'train_steps_per_second': 2.659, 'train_loss': 0.10341672458648682, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.10341672458648682, metrics={'train_runtime': 235.0171, 'train_samples_per_second': 21.275, 'train_steps_per_second': 2.659, 'total_flos': 106538246287344.0, 'train_loss': 0.10341672458648682, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# la fonction DataCollatorForTokenClassification permet de rajouter du padding pour que les s√©quences du batch aient la m√™me taille\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=args,\n",
    "    train_dataset=train_tokenized_datasets, # Dataset d'entra√Ænement\n",
    "    eval_dataset=val_tokenized_datasets, # Dataset d'√©valuation\n",
    "    compute_metrics=compute_metrics, \n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entra√Ænement est termin√©, on peut √©valuer notre mod√®le sur les donn√©es de validation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:06<00:00, 10.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.10586605966091156,\n",
       " 'eval_accuracy': 0.9793857803954564,\n",
       " 'eval_f1': 0.902547065337763,\n",
       " 'eval_runtime': 6.1292,\n",
       " 'eval_samples_per_second': 81.577,\n",
       " 'eval_steps_per_second': 10.279,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient de tr√®s bon scores : *accuracy* de 0.98 et *f1-score* de 0.90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de sentiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, passons √† une t√¢che de type *sentence-level classification* : l'analyse de sentiment. Pour cela, nous utilisons le dataset [imdb](https://ai.stanford.edu/~amaas/data/sentiment/). C'est un dataset qui regroupe des critiques positives ou n√©gatives de films. Le but du mod√®le va √™tre de determiner sur la critique est positive ou n√©gative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/imdb\",trust_remote_code=True)\n",
    "\n",
    "# 1000 √©l√©ments pour l'entra√Ænement\n",
    "sub_train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# 500 √©l√©ments pour l'√©valuation\n",
    "sub_val_dataset = dataset['test'].shuffle(seed=42).select(range(500)) # 500 examples for evaluation\n",
    "\n",
    "print(sub_train_dataset['text'][0])\n",
    "print(sub_train_dataset['label'][0])\n",
    "\n",
    "itos={0: 'neg', 1:'pos'}\n",
    "stoi = {v: k for k, v in itos.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut utiliser le m√™me tokenizer que pour la partie pr√©c√©dente pour extraire les tokens de notre texte. Ici, pas besoin de faire correspondre le label √† chaque token parce que le label concerne la phrase enti√®re."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 4040.25 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 5157.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2045, 2003, 2053, 7189, 2012, 2035, 2090, 3481, 3771, 1998, 6337, 2099, 2021, 1996, 2755, 2008, 2119, 2024, 2610, 2186, 2055, 6355, 6997, 1012, 6337, 2099, 3504, 15594, 2100, 1010, 3481, 3771, 3504, 4438, 1012, 6337, 2099, 14811, 2024, 3243, 3722, 1012, 3481, 3771, 1005, 1055, 5436, 2024, 2521, 2062, 8552, 1012, 1012, 1012, 3481, 3771, 3504, 2062, 2066, 3539, 8343, 1010, 2065, 2057, 2031, 2000, 3962, 12319, 1012, 1012, 1012, 1996, 2364, 2839, 2003, 5410, 1998, 6881, 2080, 1010, 2021, 2031, 1000, 17936, 6767, 7054, 3401, 1000, 1012, 2111, 2066, 2000, 12826, 1010, 2000, 3648, 1010, 2000, 16157, 1012, 2129, 2055, 2074, 9107, 1029, 6057, 2518, 2205, 1010, 2111, 3015, 3481, 3771, 3504, 2137, 2021, 1010, 2006, 1996, 2060, 2192, 1010, 9177, 2027, 9544, 2137, 2186, 1006, 999, 999, 999, 1007, 1012, 2672, 2009, 1005, 1055, 1996, 2653, 1010, 2030, 1996, 4382, 1010, 2021, 1045, 2228, 2023, 2186, 2003, 2062, 2394, 2084, 2137, 1012, 2011, 1996, 2126, 1010, 1996, 5889, 2024, 2428, 2204, 1998, 6057, 1012, 1996, 3772, 2003, 2025, 23105, 2012, 2035, 1012, 1012, 1012, 102]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True,is_split_into_words=False) \n",
    "tokenized_train_dataset = sub_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val_dataset = sub_val_dataset.map(preprocess_function, batched=True)\n",
    "print(tokenized_train_dataset['input_ids'][0])\n",
    "print(tokenized_train_dataset['label'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut cr√©er notre mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2, id2label=itos, label2id=stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant cr√©er notre fonction de calcul des performances : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels,average=\"macro\")\n",
    "    return {\n",
    "        \"f1\": f1_score[\"f1\"],\n",
    "        \"accuracy\": accuracy_score[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et entrainer le mod√®le : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 500/625 [12:05<02:58,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2295, 'grad_norm': 0.022515051066875458, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [15:06<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 906.0393, 'train_samples_per_second': 5.519, 'train_steps_per_second': 0.69, 'train_loss': 0.1885655658721924, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.1885655658721924, metrics={'train_runtime': 906.0393, 'train_samples_per_second': 5.519, 'train_steps_per_second': 0.69, 'total_flos': 613576571755968.0, 'train_loss': 0.1885655658721924, 'epoch': 5.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "# Pad the inputs to the maximum length in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluons notre mod√®le : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:33<00:00,  1.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.565979540348053,\n",
       " 'eval_f1': 0.8879354508196722,\n",
       " 'eval_accuracy': 0.888,\n",
       " 'eval_runtime': 34.4579,\n",
       " 'eval_samples_per_second': 14.51,\n",
       " 'eval_steps_per_second': 1.828,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient des bons scores : *accuracy* de 0.89 et *f1-score* de 0.89 √©galement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
