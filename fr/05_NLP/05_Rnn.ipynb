{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux de neurones récurrents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, on va découvrir les réseaux de neurones récurrents (RNN) pour prédire le prochain caractère. On se base sur l'architecture décrite dans l'article [Recurrent neural network based language model](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf), qui propose une version simple de RNN pour cette tâche.\n",
    "\n",
    "L'avantage des RNN est qu'ils n'ont pas besoin d'une taille de contexte fixe, contrairement aux modèles basés sur des réseaux fully connected vus précédemment.\n",
    "\n",
    "Les RNN gardent en mémoire le contexte, quelle que soit la longueur de la séquence. C'est une idée cool en théorie, mais on verra à la fin du cours qu'ils ont leurs limites.\n",
    "\n",
    "![RNN](./images/rnn.png)\n",
    "\n",
    "Figure extraite de l'article original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment fonctionne un RNN ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les RNN fonctionnent de manière séquentielle : les caractères sont traités un par un. Le caractère suivant dépend à la fois de l'élément actuel et de l'état (*state*) mémorisé, qui contient les infos des caractères précédents.\n",
    "\n",
    "Mathématiquement, un RNN a 3 composantes :\n",
    "- L'entrée (*input*) $x$\n",
    "- L'état caché (*state*) $s$\n",
    "- La sortie (*output*) $y$\n",
    "\n",
    "On ajoute aussi le temps $t$ pour gérer la séquence.\n",
    "\n",
    "L'entrée à l'instant $t$ est donnée par :\n",
    "$x(t) = w(t) + s(t-1)$\n",
    "où $w()$ est l'encodage one-hot et $s(t-1)$ est l'état précédent.\n",
    "\n",
    "Ensuite, on calcule l'état et la sortie :\n",
    "$s(t) = sigmoid(x(t))$\n",
    "$y(t) = softmax(s(t))$\n",
    "\n",
    "Le seul paramètre à ajuster est la taille de la couche cachée $s$.\n",
    "\n",
    "Pour l'initialisation, $s(0)$ peut être un petit vecteur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passage à la pratique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Générer des prénoms avec un RNN n'est pas super utile, car les prénoms sont courts et le contexte limité. Pour des tâches plus intéressantes, on a besoin d'un dataset avec un contexte plus large.\n",
    "\n",
    "On utilise donc un fichier texte contenant les dialogues de Molière. Ce dataset a été créé à partir des œuvres complètes disponibles sur [Gutenberg.org](https://www.gutenberg.org/). J'ai nettoyé les données pour ne garder que les dialogues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme le dataset est volumineux, on prend seulement une partie (par exemple les 50 000 premiers caractères) pour un traitement plus rapide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  50000\n"
     ]
    }
   ],
   "source": [
    "text=text[:50000]\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les 250 premiers caractères :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALÈRE.\n",
      "\n",
      "Eh bien, Sabine, quel conseil me donnes-tu?\n",
      "\n",
      "SABINE.\n",
      "\n",
      "Vraiment, il y a bien des nouvelles. Mon oncle veut résolûment que ma\n",
      "cousine épouse Villebrequin, et les affaires sont tellement avancées,\n",
      "que je crois qu'ils eussent été mariés dès aujo\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le nombre de caractères uniques :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJLMNOPQRSTUVYabcdefghijlmnopqrstuvxyzÇÈÉàâæçèéêîïôùû\n",
      "Nombre de caractères différents :  73\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée un mapping entre caractères et entiers (et inversement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On encode le dataset en convertissant les chaînes de caractères en entiers, puis en tenseurs PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 12, 22, 59, 28, 16,  8,  0,  0, 16, 41,  1, 35, 42, 38, 46,  6,  1,\n",
      "        29, 34, 35, 42, 46, 38,  6,  1, 49, 53, 38, 44,  1, 36, 47, 46, 51, 38,\n",
      "        42, 44,  1, 45, 38,  1, 37, 47, 46, 46, 38, 51,  7, 52, 53, 11,  0,  0,\n",
      "        29, 12, 13, 20, 24, 16,  8,  0,  0, 32, 50, 34, 42, 45, 38, 46, 52,  6,\n",
      "         1, 42, 44,  1, 56,  1, 34,  1, 35, 42, 38, 46,  1, 37, 38, 51,  1, 46,\n",
      "        47, 53, 54, 38, 44, 44, 38, 51,  8,  1, 23, 47, 46,  1, 47, 46, 36, 44,\n",
      "        38,  1, 54, 38, 53, 52,  1, 50, 66, 51, 47, 44, 72, 45, 38, 46, 52,  1,\n",
      "        49, 53, 38,  1, 45, 34,  0, 36, 47, 53, 51, 42, 46, 38,  1, 66, 48, 47,\n",
      "        53, 51, 38,  1, 32, 42, 44, 44, 38, 35, 50, 38, 49, 53, 42, 46,  6,  1,\n",
      "        38, 52,  1, 44, 38, 51,  1, 34, 39, 39, 34, 42, 50, 38, 51,  1, 51, 47,\n",
      "        46, 52,  1, 52, 38, 44, 44, 38, 45, 38, 46, 52,  1, 34, 54, 34, 46, 36,\n",
      "        66, 38, 51,  6,  0, 49, 53, 38,  1, 43, 38,  1, 36, 50, 47, 42, 51,  1,\n",
      "        49, 53,  3, 42, 44, 51,  1, 38, 53, 51, 51, 38, 46, 52,  1, 66, 52, 66,\n",
      "         1, 45, 34, 50, 42, 66, 51,  1, 37, 65, 51,  1, 34, 53, 43, 47])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:250]) # Les 250 premiers caractères encodé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sépare les données en ensembles d'entraînement et de test :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour le test\n",
    "train_data = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : À chaque itération, on parcourt tout le dataset de manière séquentielle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant construire le modèle !\n",
    "\n",
    "Comme indiqué dans l'article, l'entrée (le caractère) est encodée en one-hot, puis additionnée avec l'état précédent. On a donc besoin de deux couches fully connected :\n",
    "- La première transforme l'entrée $x(t)$ en état $s(t)$\n",
    "- La seconde transforme $s(t)$ en prédiction $y(t)$\n",
    "\n",
    "![RNN](./images/rnn_math.png)\n",
    "\n",
    "Équation tirée de l'article. $f$ est la fonction sigmoid et $g$ la softmax.\n",
    "\n",
    "**Note** : L'[article](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) est clair et concis, je vous conseille de le lire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(nn.Module): \n",
    "  def __init__(self,hidden_dim,vocab_size) -> None:\n",
    "    super(rnn, self).__init__()\n",
    "    self.hidden_to_hidden=nn.Linear(hidden_dim+vocab_size, hidden_dim)\n",
    "    self.hidden_to_output=nn.Linear(hidden_dim, vocab_size)\n",
    "    self.vocab_size=vocab_size\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.sigmoid=nn.Sigmoid() \n",
    "    \n",
    "  # Le réseau prend en entrée le caractère actuel et le state précédent\n",
    "  def forward(self, x,state):\n",
    "    # On one-hot encode le caractère\n",
    "    x = torch.nn.functional.one_hot(x, self.vocab_size).float()\n",
    "    if state is None:\n",
    "      # Si on a pas de state (début de la séquence), on initialise le state avec des petites valeurs aléatoires\n",
    "      state = torch.randn(self.hidden_dim) * 0.1\n",
    "    x = torch.cat((x, state), dim=-1)  # Concaténation de x et du state\n",
    "    state = self.sigmoid(self.hidden_to_hidden(x)) # Calcul du nouveau state\n",
    "    output = self.hidden_to_output(state) # Calcul de l'output\n",
    "    # On renvoie l'output et le state pour le prochain pas de temps\n",
    "    return output, state.detach() # detach() pour éviter de propager le gradient dans le state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les paramètres d'entraînement :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr=0.1\n",
    "hidden_dim=128\n",
    "model=rnn(hidden_dim,vocab_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraîne maintenant le modèle !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 2.63949568\n",
      "Epoch: 1 \t Loss: 2.16456994\n",
      "Epoch: 2 \t Loss: 2.00850788\n",
      "Epoch: 3 \t Loss: 1.91673251\n",
      "Epoch: 4 \t Loss: 1.84440742\n",
      "Epoch: 5 \t Loss: 1.78986003\n",
      "Epoch: 6 \t Loss: 1.74923073\n",
      "Epoch: 7 \t Loss: 1.71709289\n",
      "Epoch: 8 \t Loss: 1.68791167\n",
      "Epoch: 9 \t Loss: 1.66215199\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    state=None\n",
    "    running_loss = 0\n",
    "    n=0\n",
    "    for i in range(len(train_data)-1):\n",
    "        x = train_data[i]\n",
    "        y = train_data[i+1]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste maintenant le modèle sur les données de test :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.77312289\n"
     ]
    }
   ],
   "source": [
    "state=None\n",
    "running_loss = 0\n",
    "n=0\n",
    "for i in range(len(train_data)-1):\n",
    "    with torch.no_grad():\n",
    "        x = train_data[i]\n",
    "        y = train_data[i+1]\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "print(\"Loss: {0:.8f}\".format(running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *loss* sur les données de test est légèrement plus élevé qu'en entraînement. Le modèle a un peu *overfitté*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération de texte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle étant entraîné, on peut maintenant générer du texte façon Molière !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "VARDILE.\n",
      "\n",
      "Vout on est nt, jes l'un ouint; sabhil.\n",
      "\n",
      "LE DOCTE.\n",
      "\n",
      "Si vous dicefalassîntes\n",
      "GIRGIB.\n",
      "\n",
      "MARGRIILÉ.\n",
      "\n",
      "LE DOCTE. Jort; et\n",
      "; bieu,\n",
      "et je mu tu d'ais d'ai coupce!\n",
      "\n",
      "SGÉLLÉ.\n",
      "\n",
      "Il Sgnous elli massit que\n",
      "Suis pluagil dés.\n",
      "Cais téscompas: y totte demes\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "moliere='.'\n",
    "sequence_length=250\n",
    "state=None\n",
    "for i in range(sequence_length):\n",
    "    x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()\n",
    "    y_pred,state = model.forward(x,state)\n",
    "    probs=F.softmax(torch.squeeze(y_pred), dim=0)\n",
    "    sample=torch.multinomial(probs, 1)\n",
    "    moliere+=itos[sample.item()]\n",
    "print(moliere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat n'est pas parfait, mais on reconnaît quelques mots et une structure de phrases proche du fichier \"moliere.txt\". Pas mal pour un RNN à une seule couche !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment améliorer les résultats ?** Voici quelques pistes :\n",
    "- Augmenter le nombre de couches ou la taille de la couche cachée\n",
    "- Utiliser un *embedding* au lieu du *one-hot encoding*\n",
    "- Tester d'autres variantes de RNN comme [LSTM](https://arxiv.org/pdf/1308.0850) ou [GRU](https://arxiv.org/abs/1409.1259)\n",
    "- ~~Utiliser une architecture *transformer*~~ (spoiler !)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les limites des RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longtemps au cœur de la recherche en NLP et en deep learning, les RNN ont plusieurs limites qui les rendent peu pratiques pour les gros modèles :\n",
    "- Leur architecture permet un contexte infini en théorie, mais la structure séquentielle complique la propagation de l'info sur de longues séquences.\n",
    "- Le *vanishing gradient* sur les longues séquences est un vrai problème.\n",
    "- La structure séquentielle rend la parallélisation difficile, alors que les GPU sont optimisés pour les calculs parallèles. L'entraînement est donc plus lent.\n",
    "- La structure fixe n'est pas toujours adaptée pour capturer les relations complexes.\n",
    "\n",
    "Depuis l'arrivée des [transformers](https://arxiv.org/pdf/1706.03762), les RNN sont de moins en moins utilisés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
