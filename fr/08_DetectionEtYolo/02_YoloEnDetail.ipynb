{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO en détail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, on va explorer en détail le fonctionnement du modèle YOLO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection d'objets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histoire de la détection d'objets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peu après la publication du célèbre [papier](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) qui a popularisé le deep learning, les chercheurs en traitement d'images se sont mis à développer des modèles de deep learning.\n",
    "\n",
    "Comme expliqué dans le notebook précédent, il existe trois grandes catégories d'algorithmes de traitement d'images : la classification, la détection et la segmentation.\n",
    "\n",
    "La classification est assez simple à mettre en œuvre avec un modèle de deep learning profond, à condition d'avoir les ressources nécessaires. En revanche, la détection demande plus d'ingéniosité.\n",
    "\n",
    "En 2014, un groupe de chercheurs a proposé le papier [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/pdf/1311.2524), plus connu sous le nom de R-CNN. Ce papier, très influent, introduit une architecture en deux étapes pour la détection d'objets et offre des performances remarquables. Le principal problème de cette approche est son temps de traitement trop lent, qui ne permet pas une détection en temps réel. De nombreuses méthodes ont tenté de résoudre ce problème en proposant des architectures différentes, comme [fast R-CNN](https://arxiv.org/pdf/1504.08083), [faster R-CNN](https://arxiv.org/pdf/1506.01497) et [mask R-CNN](https://arxiv.org/pdf/1703.06870). Ces méthodes améliorent considérablement le R-CNN de base, mais elles ne suffisent pas pour une détection en temps réel dans la plupart des cas.\n",
    "\n",
    "En 2015, un article a provoqué un bouleversement majeur dans le domaine de la détection d'objets. Ce papier est [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Only Look Once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les approches précédentes utilisaient une proposition de régions suivie d'une classification. En d'autres termes, elles exploitaient des classifieurs puissants pour la détection d'objets.\n",
    "\n",
    "Le papier You Only Look Once (YOLO) propose de prédire les *bounding box* et les probabilités d'appartenance à une classe directement grâce à un unique réseau de neurones. Cette architecture est beaucoup plus rapide et permet d'atteindre des vitesses de traitement allant jusqu'à 45 images par seconde.\n",
    "\n",
    "C'est une révolution dans le domaine de la détection d'objets !\n",
    "\n",
    "Mais alors, comment ça marche ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO : Comment ça marche ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie décrit l'architecture de YOLO en s'inspirant du [blogpost](https://medium.com/analytics-vidhya/yolo-explained-5b6f4564f31). Je vous invite à le consulter. Les images utilisées sont tirées du blogpost ou du papier original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation en grille\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le principe de base de YOLO est de diviser l'image en plus petites parties à l'aide d'une grille de dimension $S \\times S$ comme ceci :\n",
    "\n",
    "![yolo](./images/grid.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule contenant le centre d'un objet (par exemple un chien ou un vélo) est responsable de sa détection (pour le calcul du *loss*). Chaque cellule de la grille prédit $B$ *bounding boxes* (paramétrable, 2 dans le papier original) et un score de confiance pour chacune. La *bounding box* prédite contient les valeurs $x,y,w,h,c$, où $(x,y)$ est la position du centre dans la grille, $(w,h)$ sont les dimensions de la *box* en pourcentage de l'image entière et $c$ est la confiance du modèle (probabilité).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer la précision de notre *bounding box* lors de l'entraînement (composante du *loss*), on utilise l'*intersection over union*, définie comme :\n",
    "\n",
    "$\\frac{pred_{box}\\cap label_{box}}{pred_{box} \\cup label_{box}}$\n",
    "\n",
    "![iou](./images/iou.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En plus de prédire la *bounding box* et la confiance, chaque cellule prédit aussi la classe de l'objet. Cette classe est représentée par un vecteur *one_hot* (qui contient uniquement des 0 sauf un 1 dans la bonne classe) dans les annotations. Il est important de noter que chaque cellule peut prédire plusieurs *bounding boxes* mais une seule classe. C'est une des limitations de l'algorithme : si plusieurs objets sont dans la même cellule, le modèle ne pourra pas les prédire correctement.\n",
    "\n",
    "Maintenant qu'on a toutes les informations, on peut calculer la dimension de sortie du réseau. On a $S \\times S$ cellules, chaque cellule prédit $B$ *bounding boxes* et $C$ probabilités (avec $C$ le nombre de classes).\n",
    "\n",
    "La prédiction du modèle est donc de taille : $S \\times S \\times (C + B \\times 5)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela nous amène à la figure suivante :\n",
    "\n",
    "![yolo](./images/yolo.png)\n",
    "\n",
    "La figure du centre en haut montre les *bounding boxes* prédites par le modèle (celles avec un trait plus épais ont des scores de confiance élevés). La figure du centre en bas montre la classe prédite dans chaque cellule (en bleu la classe chien, en jaune la classe vélo et en rose la classe voiture).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture du modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture du modèle YOLO, en termes d'agencement des couches, est aussi particulière. Elle comprend trois composants principaux : *head*, *neck* et *backbone*.\n",
    "\n",
    "- **Backbone** : C'est la partie la plus importante du réseau, composée d'une série de convolutions pour détecter les *features* les plus importantes. Cette partie est souvent pré-entraînée sur un dataset de classification.\n",
    "- **Neck and Head** : Ces couches traitent l'*output* des couches de convolutions pour produire une prédiction de taille $S \\times S \\times (C + B \\times 5)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le papier original de YOLO, la grille est de taille 7x7, il y a 20 classes ([Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)) et on prédit deux *bounding boxes* par cellule. Cela donne une prédiction de taille :\n",
    "\n",
    "$7 \\times 7 \\times (20 + 2 \\times 5) = 1470$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs d'entraînement du modèle (taille d'image, *epochs*, nombre de couches, *batch_size*, etc.) sont détaillées dans le papier original et nous n'allons pas entrer dans les détails ici.\n",
    "\n",
    "En revanche, il est intéressant de s'attarder un peu sur la fonction de *loss*. L'idée logique de base serait d'utiliser simplement le *loss* MSE entre nos prédictions et les labels. Cependant, ça ne fonctionne pas tel quel car le modèle donnerait une importance similaire à la qualité de la localisation et à la précision de la prédiction. En pratique, on utilise une pondération sur les *loss* $\\lambda_{coord}$ et $\\lambda_{noobj}$. Les valeurs du papier original sont définies à 5 pour $\\lambda_{coord}$ et 0.5 pour $\\lambda_{noobj}$. À noter que $\\lambda_{noobj}$ est utilisé uniquement sur les cellules où il n'y a pas d'objets pour éviter que son score de confiance, proche de 0, n'impacte trop les cellules contenant des objets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations de YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons déjà évoqué sa principale limitation : ne prédire qu'un nombre limité de *bounding boxes* par cellule et ne pas permettre la détection d'objets de différentes catégories dans la même cellule. Cela pose problème lorsque l'on veut détecter des personnes dans une foule, par exemple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amélioration de YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu que YOLO est un modèle très performant et rapide pour la détection d'objets dans les images. C'est pourquoi, de nombreux chercheurs ont cherché à l'améliorer en proposant diverses optimisations. Aujourd'hui encore, de nouvelles versions de YOLO sortent régulièrement.\n",
    "\n",
    "Cette partie présente chronologiquement les différentes versions de YOLO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv2 (2017) - aussi connu sous le nom de YOLO9000\n",
    "**Papier : [YOLO9000: Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242)**\n",
    "\n",
    "**Innovations :**\n",
    "  - Introduction de l'idée d'ancres (*anchors*) pour améliorer la précision des prédictions de boîte.\n",
    "  - Passage de la résolution d'entrée de 224x224 à 416x416 pour améliorer la détection d'objets de petite taille.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv3 (2018)\n",
    "**Papier : [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767)**\n",
    "\n",
    "**Innovations :**\n",
    "  - Utilisation d'un modèle plus profond avec une architecture Darknet-53, un réseau de neurones convolutifs résiduel.\n",
    "  - Détection multi-échelle, avec des prédictions faites à trois niveaux de granularité différents (*feature maps* de différentes tailles).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv4 (2020)\n",
    "**Papier : [YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/abs/2004.10934)**\n",
    "\n",
    "**Innovations :**\n",
    "  - Utilisation du *backbone* CSPDarknet53 pour une meilleure performance.\n",
    "  - Améliorations des têtes de détection avec PANet (Path Aggregation Network) pour améliorer les flux d'informations.\n",
    "  - Introduction du concept de *Mosaic Data Augmentation* pour enrichir la diversité des données d'entraînement.\n",
    "  - Ajout de diverses techniques modernes comme *DropBlock*, *Mish activation*, et SPP (*Spatial Pyramid Pooling*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv5 (2020)\n",
    "**Développé par Ultralytics**\n",
    "\n",
    "**Innovations :**\n",
    "  - Pas de papier officiel, mais des améliorations pratiques dans l'implémentation et la performance.\n",
    "  - Modèle plus léger et plus facile à entraîner avec une meilleure gestion des dépendances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv6 (2022)\n",
    "\n",
    "**Innovations :**\n",
    "  - Nouveau *backbone* YOLOv6S, optimisé pour les performances en temps réel.\n",
    "  - Techniques avancées de réduction de la latence.\n",
    "  - Améliorations dans les méthodes d'augmentation des données et l'optimisation des hyperparamètres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv7 (2022)\n",
    "**Papier : [YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors](https://arxiv.org/abs/2207.02696)**\n",
    "\n",
    "**Innovations :**\n",
    "  - Intégration de *bag of freebies* pour améliorer la précision sans augmenter le temps d'inférence.\n",
    "  - Architecture optimisée pour un compromis optimal entre vitesse et précision.\n",
    "  - Ajout de diverses techniques de régularisation pour améliorer la performance générale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv8 (2023)\n",
    "**Développé par Ultralytics**\n",
    "\n",
    "**Innovations :**\n",
    "  - Encore plus optimisé pour les performances en temps réel et l'intégration mobile.\n",
    "  - Architecture flexible permettant des ajustements pour divers cas d'utilisation, y compris la détection, la segmentation et la classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO-World (2024)\n",
    "**Papier : [YOLO-World: Real-Time Open-Vocabulary Object Detection](https://arxiv.org/pdf/2401.17270)**\n",
    "\n",
    "**Innovations :**\n",
    "  - Utilisation d'un *transformer encoder* pour le texte permettant la détection *open-vocabulary*."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
