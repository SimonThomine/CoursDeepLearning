# ü§ù Transfer learning et distillation ü§ù
Ce cours pr√©sente deux concepts majeurs en deep learning : le transfer learning et la distillation des connaissances. La premi√®re partie de ce cours pr√©sente le transfer learning dans sa globalit√© puis propose une impl√©mentation pratique. La seconde partie pr√©sente le concept de distillation des connaissances et ses variantes puis propose un cas d'application de la distillation des connaissances pour la d√©tection d'anomalies non supervis√©e.

## Notebook 1Ô∏è‚É£ : [Transfer learning](01_TransferLearning.ipynb)
Ce notebook introduit le concept de transfer learning et les utilisations possibles du transfer learning.

## Notebook 2Ô∏è‚É£ : [Transfer learning pytorch](02_TransferLearningPytorch.ipynb)
Ce notebook pr√©sente un exemple de transfer learning pour la classification √† partir d'un dataset contenant un nombre limit√© d'images.

## Notebook 3Ô∏è‚É£ : [Distillation](03_Distillation.ipynb)
Ce notebook introduit le concept de distillation des conaissances en deep learning ainsi que les utilisations possibles de cette approche.

## Notebook 4Ô∏è‚É£ : [DistillationAnomalie](04_DistillationAnomalie.ipynb)
Ce notebook montre un exemple de distillation des connaissances pour la d√©tection d'anomalies non supervis√©e.

## Notebook 5Ô∏è‚É£ : [Fine Tuning LLM](05_FineTuningLLM.ipynb)
Ce notebook pr√©sente en d√©tail l'architecture de BERT, un mod√®le con√ßu pour √™tre *finetune* sur des t√¢ches annexes.

## Notebook 6Ô∏è‚É£ : [Fine Tuning Bert HF](06_FineTuningBertHF.ipynb)
Ce notebook propose des exemples de *finetuning* du mod√®le BERT avec Hugging Face pour des t√¢ches de *token-level prediction* et *sentence-level prediction*.