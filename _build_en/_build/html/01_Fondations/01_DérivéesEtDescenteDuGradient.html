
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>ANGLAISSSSSS &#8212; Deep Learning Course</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '01_Fondations/01_DérivéesEtDescenteDuGradient';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">🚀 Learn Deep Learning from scratch 🚀</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning Course</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Deep Learning Course 🚀
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">🧮 Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_DerivativesAndGradientDescent.html">Derivatives and Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_LogisticRegression.html">Logistic Regression</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/edit/main/en/01_Fondations/01_DérivéesEtDescenteDuGradient.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/SimonThomine/CoursDeepLearning/issues/new?title=Issue%20on%20page%20%2F01_Fondations/01_DérivéesEtDescenteDuGradient.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/01_Fondations/01_DérivéesEtDescenteDuGradient.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>ANGLAISSSSSS</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comprehension-intuitive-de-la-derivee">Compréhension intuitive de la dérivée</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#les-bases-de-l-optimisation-par-descente-de-gradient">Les bases de l’optimisation par descente de gradient</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-regle-de-la-chaine">La règle de la chaîne</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimisation-de-plusieurs-variables">Optimisation de plusieurs variables</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="anglaissssss">
<h1>ANGLAISSSSSS<a class="headerlink" href="#anglaissssss" title="Link to this heading">#</a></h1>
<p>Ce cours a pour but de présenter l’algorithme de descente du gradient qui est un des piliers du deep learning. Pour cela, il est nécessaire de faire quelques rappels sur la dérivée.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span> 
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<section id="comprehension-intuitive-de-la-derivee">
<h2>Compréhension intuitive de la dérivée<a class="headerlink" href="#comprehension-intuitive-de-la-derivee" title="Link to this heading">#</a></h2>
<p>Définissons une fonction : <span class="math notranslate nohighlight">\(f(x) = 2x^2 - 3x + 4\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mi">4</span>
<span class="n">f</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>13
</pre></div>
</div>
</div>
</div>
<p>Traçons cette fonction sur un graphe avec matplotlib.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7ae9dcdf63d0&gt;]
</pre></div>
</div>
<img alt="../_images/8ae53ec1c1a7a83c40158a7e0d6aeac20e0c0089b96c23bc11822f2fad3e62d5.png" src="../_images/8ae53ec1c1a7a83c40158a7e0d6aeac20e0c0089b96c23bc11822f2fad3e62d5.png" />
</div>
</div>
<p>Le calcul de la dérivée permet de connaître la pente de la tangente à la courbe en un point.<br />
Pour calculer la dérivée d’une fonction, on utilise la définition de la dérivée :<br />
<span class="math notranslate nohighlight">\(f'(x) = \lim_{h \to 0} \frac{f(x+h)-h}{h}\)</span><br />
En prenant un h suffisamment petit, on peut estimer la pente de la courbe numériquement.</p>
<p><strong>Note</strong> : La pente de la courbe correspond au rapport entre la variation de <span class="math notranslate nohighlight">\(y\)</span> et de <span class="math notranslate nohighlight">\(x\)</span>. Si quand on augmente <span class="math notranslate nohighlight">\(x\)</span> de 1, <span class="math notranslate nohighlight">\(y\)</span> augmente de 2, alors la pente de la courbe vaut 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span><span class="o">=</span><span class="mf">0.0001</span>
<span class="n">x</span><span class="o">=-</span><span class="mf">1.0</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dérivée en x=-1 : &quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span><span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">h</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="mf">2.0</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dérivée en x=2 : &quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span><span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dérivée en x=-1 :  -6.999800000002665
Dérivée en x=2 :  5.000200000004895
</pre></div>
</div>
</div>
</div>
<p>D’après le graphique, on voit bien que la pente de la courbe est négative en <span class="math notranslate nohighlight">\(x=-1\)</span>  et positive en <span class="math notranslate nohighlight">\(x=2\)</span>.<br />
Le <strong>signe</strong> de la dérivée permet de connaître le sens de la pente et la <strong>valeur</strong> de la dérivée permet de connaître l’intensité de la pente.<br />
On peut vérifier nos résultats à l’aide des <a class="reference external" href="https://fr.wikipedia.org/wiki/Table_de_d%C3%A9riv%C3%A9es_usuelles">dérivées usuelles</a>  : Si <span class="math notranslate nohighlight">\(f(x)=2x²-3x+4\)</span> alors <span class="math notranslate nohighlight">\(f'(x)=4x-3\)</span><br />
On retrouve <span class="math notranslate nohighlight">\(f'(-1)=4 \times -1-3=-7 \approx -6.999800000002665\)</span><br />
et <span class="math notranslate nohighlight">\(f'(2)=4 \times 2-3=5 \approx 5.000200000004895\)</span><br />
Les résultats ne sont pas exacts car h n’est pas infiniment petit dans notre calcul numérique.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># On définit deriv_f = f&#39;(x)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">deriv_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="les-bases-de-l-optimisation-par-descente-de-gradient">
<h2>Les bases de l’optimisation par descente de gradient<a class="headerlink" href="#les-bases-de-l-optimisation-par-descente-de-gradient" title="Link to this heading">#</a></h2>
<p>Le but de l’optimisation va être de minimiser ou maximiser une fonction objectif qui correspond à l’objectif que l’on s’est fixé.<br />
Si notre but est de trouver le minimum de la fonction, il y a plusieurs façons de faire.<br />
Une première façon est de résoudre l’équation <span class="math notranslate nohighlight">\(f'(x)=0\)</span> :<br />
<span class="math notranslate nohighlight">\(4x-3=0\)</span><br />
<span class="math notranslate nohighlight">\(4x=3\)</span><br />
<span class="math notranslate nohighlight">\(x=\frac{3}{4}\)</span><br />
Ce calcul nous permet de trouver le minimum de <span class="math notranslate nohighlight">\(f(x)\)</span> dans notre cas. Cependant, dans un cas général, résoudre <span class="math notranslate nohighlight">\(f'(x)=0\)</span> permet de trouver les optimums (maximums et minimums) et pas forcément le minimum global.</p>
<p>Une seconde façon est d’utiliser une méthode d’optimisation que l’on appelle la <strong>descente du gradient</strong>. Pour appliquer cette méthode, on commence par se placer en un point quelconque, par exemple <span class="math notranslate nohighlight">\(x=2\)</span>.<br />
On calcule la dérivée <span class="math notranslate nohighlight">\(f'(2)=5\)</span>. La pente est positive c’est-à-dire que si <span class="math notranslate nohighlight">\(x\)</span> augmente alors <span class="math notranslate nohighlight">\(f(x)\)</span> augmente et si <span class="math notranslate nohighlight">\(x\)</span> diminue alors <span class="math notranslate nohighlight">\(f(x)\)</span> diminue.<br />
Notre but est de trouver le minimum de <span class="math notranslate nohighlight">\(f(x)\)</span>, on va donc changer notre x en fonction de la pente et d’un facteur <span class="math notranslate nohighlight">\(\alpha\)</span> = 0,1 (<em>learning rate</em>).<br />
On se retrouve avec <span class="math notranslate nohighlight">\(x_{new}=x - pente \times \alpha= 2-0.5=1.5\)</span> et on peut recalculer la dérivée à notre nouveau point <span class="math notranslate nohighlight">\(f'(1.5)=4 \times 1.5-3=3\)</span>, la dérivée est encore positive, il faut donc à nouveau diminuer la valeur de <span class="math notranslate nohighlight">\(x\)</span>.<br />
La méthode d’optimisation de la descente du gradient consiste à créer une boucle qui va modifier la valeur de <span class="math notranslate nohighlight">\(x\)</span> jusqu’à atteindre un minimum en se basant sur un facteur <span class="math notranslate nohighlight">\(\alpha\)</span> et le signe de la dérivée. A noter que l’on tient compte de l’intensité de la pente dans notre calcul.<br />
On peut coder la descente du gradient de cette manière.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Descente du gradient</span>
<span class="n">x</span><span class="o">=</span><span class="mf">2.0</span> <span class="c1"># valeur aléatoire de x</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span> <span class="c1"># pas</span>
<span class="n">iterations</span><span class="o">=</span><span class="mi">250</span> <span class="c1"># nombre d&#39;itérations</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
  <span class="n">grad</span><span class="o">=</span><span class="n">deriv_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">grad</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">-</span><span class="n">alpha</span>
  <span class="k">elif</span><span class="p">(</span><span class="n">grad</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">+</span><span class="n">alpha</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;minimum found YAY, x = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;approximate minimum found YAY, x = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>approximate minimum found YAY, x =  0.7599999999999989
</pre></div>
</div>
</div>
</div>
<p>On retrouve <span class="math notranslate nohighlight">\(x \approx \frac{3}{4}\)</span>. Avec un pas plus faible (<span class="math notranslate nohighlight">\(\alpha\)</span>) et plus d’itérations, on peut retrouver une valeur encore plus précise.</p>
</section>
<section id="la-regle-de-la-chaine">
<h2>La règle de la chaîne<a class="headerlink" href="#la-regle-de-la-chaine" title="Link to this heading">#</a></h2>
<p>Avant de passer aux choses sérieuses, il est nécessaire de faire un dernier rappel mathématique d’une importance capitale pour l’apprentissage profond. C’est cette règle qui permet l’entraînement des paramètres des couches “cachées” du réseau.<br />
Il s’agit du théorème de dérivation des fonctions composées (plus souvent appelé règle de la chaîne ou chain-rule en anglais) qui énonce le principe suivant :<br />
Si une variable y dépend d’une seconde variable u, qui dépend à son tour d’une variable x, alors :
<span class="math notranslate nohighlight">\(\frac{dy}{dx}=\frac{dy}{du}\cdot\frac{du}{dx}\)</span></p>
<p>Reprenons un exemple de descente du gradient sur des fonctions qui dépendent les unes des autres.<br />
<span class="math notranslate nohighlight">\(u=2x²-x-2\)</span><br />
<span class="math notranslate nohighlight">\(y=3u+1\)</span><br />
<span class="math notranslate nohighlight">\(\frac{dy}{dx}=\frac{dy}{du}\cdot\frac{du}{dx}\)</span> avec <span class="math notranslate nohighlight">\(\frac{dy}{du}=3\)</span> et <span class="math notranslate nohighlight">\(\frac{du}{dx}=2x-1\)</span><br />
<span class="math notranslate nohighlight">\(\frac{dy}{dx}=3(2x-1)\)</span><br />
<span class="math notranslate nohighlight">\(\frac{dy}{dx}=6x-3\)</span><br />
On sait maintenant comment la variation de <span class="math notranslate nohighlight">\(x\)</span> impacte <span class="math notranslate nohighlight">\(y\)</span> et on peut appliquer notre algorithme de descente du gradient.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">=</span><span class="mf">2.0</span>
<span class="k">def</span><span class="w"> </span><span class="nf">deriv_y_x</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">6</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
  <span class="n">grad</span><span class="o">=</span><span class="n">deriv_y_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">grad</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">-</span><span class="n">alpha</span>
  <span class="k">elif</span><span class="p">(</span><span class="n">grad</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">+</span><span class="n">alpha</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;minimum found YAY, x = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;approximate minimum found YAY, x = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>approximate minimum found YAY, x =  0.49999999999999867
</pre></div>
</div>
</div>
</div>
</section>
<section id="optimisation-de-plusieurs-variables">
<h2>Optimisation de plusieurs variables<a class="headerlink" href="#optimisation-de-plusieurs-variables" title="Link to this heading">#</a></h2>
<p>Jusqu’à présent, on s’est contenté de trouver le minimum d’une fonction contenant une seule variable <span class="math notranslate nohighlight">\(x\)</span>.<br />
Un avantage des méthodes d’optimisation est que l’on peut optimiser plusieurs variables simultanément avec la descente du gradient. Pour cela, il faut calculer la dérivée par rapport à chacune des variables.</p>
<p>Pour notre exemple, prenons 3 variables a, b et c dans le modèle suivant :<br />
<span class="math notranslate nohighlight">\(u=3a²-2a+b²+1\)</span><br />
<span class="math notranslate nohighlight">\(y=2u+c\)</span><br />
Pour pouvoir appliquer la descente du gradient, on doit calculer <span class="math notranslate nohighlight">\(\frac{dy}{da}\)</span>, <span class="math notranslate nohighlight">\(\frac{dy}{db}\)</span> et <span class="math notranslate nohighlight">\(\frac{dy}{dc}\)</span><br />
Pour calculer les deux premiers, on utilise la règle de la chaîne ce qui nous donne :</p>
<ul class="simple">
<li><p>Pour la variable a :<br />
<span class="math notranslate nohighlight">\(\frac{dy}{da} = \frac{dy}{du}\cdot\frac{du}{da}\)</span><br />
<span class="math notranslate nohighlight">\(\frac{dy}{da} = 2(6a-2) = 12a-4\)</span></p></li>
<li><p>Pour la variable b :<br />
<span class="math notranslate nohighlight">\(\frac{dy}{da} = \frac{dy}{du}\cdot\frac{du}{db}\)</span>
<span class="math notranslate nohighlight">\(\frac{dy}{da} = 2(2b) = 4b\)</span></p></li>
<li><p>Pour la varible c :<br />
<span class="math notranslate nohighlight">\(\frac{dy}{dc}=1\)</span><br />
À partir de ces équations, on peut appliquer la descente du gradient.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">deriv_y_a</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">12</span><span class="o">*</span><span class="n">a</span><span class="o">-</span><span class="mi">4</span>
<span class="k">def</span><span class="w"> </span><span class="nf">deriv_y_b</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">4</span><span class="o">*</span><span class="n">b</span>
<span class="k">def</span><span class="w"> </span><span class="nf">deriv_y_c</span><span class="p">(</span><span class="n">c</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">1</span>

<span class="n">a</span><span class="o">=</span><span class="mf">1.0</span>
<span class="n">b</span><span class="o">=</span><span class="mf">1.0</span>
<span class="n">c</span><span class="o">=</span><span class="mf">1.0</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span>
<span class="k">def</span><span class="w"> </span><span class="nf">deriv_y_x</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">6</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
  <span class="n">grad_a</span><span class="o">=</span><span class="n">deriv_y_a</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
  <span class="n">grad_b</span><span class="o">=</span><span class="n">deriv_y_b</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
  <span class="n">grad_c</span><span class="o">=</span><span class="n">deriv_y_c</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">grad_a</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="o">-</span><span class="n">alpha</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="o">+</span><span class="n">alpha</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">grad_b</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="o">-</span><span class="n">alpha</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="o">+</span><span class="n">alpha</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">grad_c</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="o">-</span><span class="n">alpha</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="o">+</span><span class="n">alpha</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;approximate minimum found YAY, a = &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot; b = &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot; c = &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>approximate minimum found YAY, a = 0.29999999999999966 b = -3.191891195797325e-16 c = -11.50000000000003
</pre></div>
</div>
</div>
</div>
<p>On a pu trouver des valeurs qui minimisent la valeur de <span class="math notranslate nohighlight">\(y\)</span>. La paramètre c va tendre vers moins l’infini avec beaucoup d’itérations tandis que le paramètre b va valoir 0 et le paramètre a va valoir 0.3.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./01_Fondations"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comprehension-intuitive-de-la-derivee">Compréhension intuitive de la dérivée</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#les-bases-de-l-optimisation-par-descente-de-gradient">Les bases de l’optimisation par descente de gradient</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-regle-de-la-chaine">La règle de la chaîne</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimisation-de-plusieurs-variables">Optimisation de plusieurs variables</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Simon Thomine
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div id="language-switcher" style="text-align: center; margin-top: 20px; padding: 10px; border-top: 1px solid #eee;">
  <span style="margin-right: 10px;">🌐 Language / Langue:</span>
  <a href="#" onclick="switchToEnglish()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #4CAF50; color: white; border-radius: 5px; font-weight: bold; transition: all 0.3s;">🇺🇸 English</a>
  <a href="#" onclick="switchToFrench()" style="text-decoration: none; margin: 0 5px; padding: 5px 10px; background: #f0f0f0; border-radius: 5px; transition: all 0.3s;">🇫🇷 Français</a>
</div>
<script>
function getBaseUrl() {
  // Get the base URL without the build path
  let baseUrl = window.location.origin;
  let pathname = window.location.pathname;
  
  // Remove the build-specific parts
  if (pathname.includes('_build_fr/_build/html/')) {
    baseUrl += pathname.split('_build_fr/_build/html/')[0];
  } else if (pathname.includes('_build_en/_build/html/')) {
    baseUrl += pathname.split('_build_en/_build/html/')[0];
  } else {
    baseUrl += pathname.split('/').slice(0, -1).join('/') + '/';
  }
  
  return baseUrl;
}

function getCurrentPage() {
  let pathname = window.location.pathname;
  if (pathname.includes('_build_fr/_build/html/')) {
    return pathname.split('_build_fr/_build/html/')[1] || 'index.html';
  } else if (pathname.includes('_build_en/_build/html/')) {
    return pathname.split('_build_en/_build/html/')[1] || 'index.html';
  }
  return 'index.html';
}

function switchToEnglish() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + '_build_en/_build/html/' + currentPage;
  console.log('Switching to English:', newUrl);
  window.location.href = newUrl;
}

function switchToFrench() {
  const baseUrl = getBaseUrl();
  const currentPage = getCurrentPage();
  const newUrl = baseUrl + '_build_fr/_build/html/' + currentPage;
  console.log('Switching to French:', newUrl);
  window.location.href = newUrl;
}
</script>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>