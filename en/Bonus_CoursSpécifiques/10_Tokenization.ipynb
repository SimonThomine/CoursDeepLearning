{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key element of language models (LLMs) is *tokenization*. This is the first step in a transformer network, which involves converting text into a sequence of integers. This course is largely inspired by Andrej Karpathy's video, [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE&ab_channel=AndrejKarpathy).\n",
    "\n",
    "When we implemented our GPT, we used a very simple *tokenizer* that encodes each character with a different integer. In practice, we prefer to encode *chunks* of characters, i.e., groupings of characters.\n",
    "\n",
    "Understanding how a tokenizer works is essential to grasp how a language model works.\n",
    "\n",
    "By the end of this course, we will be able to answer these questions:\n",
    "- Why do LLMs struggle with spelling words?\n",
    "- Why do LLMs struggle with simple string operations (like reversing a string)?\n",
    "- Why are LLMs better in English?\n",
    "- Why are LLMs bad at arithmetic?\n",
    "- Why is GPT-2 not very good at Python?\n",
    "- Why does my LLM stop immediately if I send it the string \"<endoftext>\"?\n",
    "- Why does the LLM break when I talk to it about SolidGoldMagiKarp?\n",
    "- Why is it preferable to use YAML rather than JSON with LLMs?\n",
    "\n",
    "**Note**: The tokenizer is a completely separate part of the LLM, with its own training dataset and trained differently.\n",
    "\n",
    "![Tokenizer](./images/tokenizer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by analyzing GPT-2 tokenization via the [Tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2) site to understand what can go wrong. The GPT-2 tokenizer has a vocabulary of about 50,000 words, meaning 50,000 distinct tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, if we look at the arithmetic part, we quickly notice that numbers can be split into tokens in a somewhat arbitrary way.\n",
    "For example:\n",
    "\n",
    "![Arithmetic](./images/arith.png)\n",
    "\n",
    "998 is a standalone token, but 9988 is split into two tokens: 99 and 88.\n",
    "It's easy to imagine that counting becomes complicated for the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identical Words, Different Tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For identical words, depending on how they are written, we get different tokens.\n",
    "For example:\n",
    "![Same1](./images/same1.png)\n",
    "![Same2](./images/same2.png)\n",
    "\n",
    "The 4 identical words are represented by different tokens (token 198 corresponds to a newline). The model will therefore need to learn that these tokens are almost identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same sentence in different languages, the number of tokens used is not the same:\n",
    "\n",
    "![Language](./images/langage.png)\n",
    "\n",
    "This is because the GPT-2 tokenizer is trained primarily on English data.\n",
    "In practice, this reduces the model's capabilities in other languages, as the context is no longer the same in terms of information. You can insert much longer text in English than in Japanese.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe how the tokenizer behaves with Python code:\n",
    "\n",
    "![Python](./images/python.png)\n",
    "\n",
    "Each space in the indentation is counted as a token. If the code contains many conditions or loops, the context increases rapidly, making the model less effective.\n",
    "\n",
    "**Note**: This issue was fixed in later versions of GPT (3 and 4), for example, a 4-tab indentation is a single token.\n",
    "\n",
    "![Python2](./images/python2.png)\n",
    "\n",
    "**Note 2**: The configuration of our code editor (2 or 4 spaces for Python indentation) can also influence tokenization.\n",
    "\n",
    "**Note 3**: An LLM specialized in code will also have a specialized tokenizer, which improves performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Own Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our own tokenizer, let's start by seeing how to convert strings into integers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible method is to use [Unicode](https://fr.wikipedia.org/wiki/Unicode). This allows converting each character into an integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 101, 32, 99, 111, 117, 114, 115, 32, 100, 101, 32, 100, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 101, 115, 116, 32, 103, 233, 110, 105, 97, 108]\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Ce cours de deep learning est génial !\"\n",
    "# ord() permet de récupérer le code unicode d'un caractère\n",
    "unicode=[ord(char) for char in sentence]\n",
    "print(unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we cannot use this method for several reasons:\n",
    "- To date, there are almost 150,000 characters, which is too large a vocabulary size.\n",
    "- There are regular updates (once a year), which would make a Unicode-based tokenizer obsolete after a year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTF-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is to use UTF-8 *encoding* (16 or 32 bits would also be possible, but less practical), which allows encoding Unicode in 4 to 8 bits. By doing this, our base vocabulary size will be 256.\n",
    "\n",
    "We keep the idea of UTF-8, but we want to increase the vocabulary size, as 256 is too small and would force LLMs to have enormous context sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66, 111, 110, 106, 111, 117, 114]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"Bonjour\"\n",
    "list(sentence.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase our vocabulary size, we use the *byte-pair encoding* algorithm.\n",
    "The operation of this algorithm is simple: iteratively, we find the most frequent byte pair and replace it with a new token (which increases the vocabulary by 1).\n",
    "For example, take the sequence:\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "In the first iteration, we see that the pair \"aa\" is the most frequent, so we replace it with the token Z:\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "In the second iteration, we replace the pair \"ab\" with Y:\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "Finally, in the third iteration, we can replace ZY with X:\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "We have thus increased the vocabulary while reducing the sequence size (and therefore the context needed to process it).\n",
    "\n",
    "**Note**: The choice of training data has a crucial impact on the tokenizer. It must be chosen based on our objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this algorithm is that we can apply it as many times as necessary until we achieve a context size that satisfies us.\n",
    "\n",
    "**Note**: The choice of training data has a crucial impact on the tokenizer. It must be chosen based on our objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Byte-Pair Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of *byte-pair encoding*, let's take a large piece of text and count the pairs. For this, we'll use the first chapter of the first volume of *La Comédie humaine* by Balzac. The text was retrieved from [Gutenberg](https://www.gutenberg.org/ebooks/41211).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Au milieu de la rue Saint-Denis, presque au coin de la rue du\n",
      "Petit-Lion, existait naguère une de ces maisons précieuses qui donnent\n",
      "aux historiens la facilité de reconstruire par analogie l'ancien Paris.\n",
      "Les murs menaçants de cette bicoque semblaient avoir été bariolés\n",
      "d'hiéroglyphes. Quel autre nom le flâneur pouvait-il donner aux X et aux\n",
      "V que traçaient sur la façade les pièces de bois transversales ou\n",
      "diagonales dessinées dans le badigeon par de petites lézardes\n",
      "parallèles? Évidemment, au passage de toutes les voitures, chacune de\n",
      "ces solives s'agitait dans sa mortaise. Ce vénérable édifice était\n",
      "surmonté d'un toit triangulaire dont aucun modèle ne se verra bientôt\n",
      "plus à Paris. Cette couverture, tordue par les intempéries du climat\n",
      "parisien, s'avançait de trois pieds sur la rue, autant pour garantir des\n",
      "eaux pluviales le seuil de la porte, que pour abriter le mur d'un\n",
      "grenier et sa lucarne sans appui. Ce dernier étage était construit en\n",
      "planches clouées l'une sur l'autre comme de\n",
      "[65, 117, 32, 109, 105, 108, 105, 101, 117, 32, 100, 101, 32, 108, 97, 32, 114, 117, 101, 32, 83, 97, 105, 110, 116, 45, 68, 101, 110, 105, 115, 44, 32, 112, 114, 101, 115, 113, 117, 101, 32, 97, 117, 32, 99, 111, 105, 110, 32, 100, 101, 32, 108, 97, 32, 114, 117, 101, 32, 100, 117, 10, 80, 101, 116, 105, 116, 45, 76, 105, 111, 110, 44, 32, 101, 120, 105, 115, 116, 97, 105, 116, 32, 110, 97, 103, 117, 195, 168, 114, 101, 32, 117, 110, 101, 32, 100, 101, 32, 99, 101, 115, 32, 109, 97, 105, 115, 111, 110, 115, 32, 112, 114, 195, 169, 99, 105, 101, 117, 115, 101, 115, 32, 113, 117, 105, 32, 100, 111, 110, 110, 101, 110, 116, 10, 97, 117, 120, 32, 104, 105, 115, 116, 111, 114, 105, 101, 110, 115, 32, 108, 97, 32, 102, 97, 99, 105, 108, 105, 116, 195, 169, 32, 100, 101, 32, 114, 101, 99, 111, 110, 115, 116, 114, 117, 105, 114, 101, 32, 112, 97, 114, 32, 97, 110, 97, 108, 111, 103, 105, 101, 32, 108, 39, 97, 110, 99, 105, 101, 110, 32, 80, 97, 114, 105, 115, 46, 10, 76, 101, 115, 32, 109, 117, 114, 115, 32, 109, 101, 110, 97, 195, 167, 97, 110, 116, 115, 32, 100, 101, 32, 99, 101, 116, 116, 101, 32, 98, 105, 99, 111, 113, 117, 101, 32, 115, 101, 109, 98, 108, 97, 105, 101, 110, 116, 32, 97, 118, 111, 105, 114, 32, 195, 169, 116, 195, 169, 32, 98, 97, 114, 105, 111, 108, 195, 169, 115, 10, 100, 39, 104, 105, 195, 169, 114, 111, 103, 108, 121, 112, 104, 101, 115, 46, 32, 81, 117, 101, 108, 32, 97, 117, 116, 114, 101, 32, 110, 111, 109, 32, 108, 101, 32, 102, 108, 195, 162, 110, 101, 117, 114, 32, 112, 111, 117, 118, 97, 105, 116, 45, 105, 108, 32, 100, 111, 110, 110, 101, 114, 32, 97, 117, 120, 32, 88, 32, 101, 116, 32, 97, 117, 120, 10, 86, 32, 113, 117, 101, 32, 116, 114, 97, 195, 167, 97, 105, 101, 110, 116, 32, 115, 117, 114, 32, 108, 97, 32, 102, 97, 195, 167, 97, 100, 101, 32, 108, 101, 115, 32, 112, 105, 195, 168, 99, 101, 115, 32, 100, 101, 32, 98, 111, 105, 115, 32, 116, 114, 97, 110, 115, 118, 101, 114, 115, 97, 108, 101, 115, 32, 111, 117, 10, 100, 105, 97, 103, 111, 110, 97, 108, 101, 115, 32, 100, 101, 115, 115, 105, 110, 195, 169, 101, 115, 32, 100, 97, 110, 115, 32, 108, 101, 32, 98, 97, 100, 105, 103, 101, 111, 110, 32, 112, 97, 114, 32, 100, 101, 32, 112, 101, 116, 105, 116, 101, 115, 32, 108, 195, 169, 122, 97, 114, 100, 101, 115, 10, 112, 97, 114, 97, 108, 108, 195, 168, 108, 101, 115, 63, 32, 195, 137, 118, 105, 100, 101, 109, 109, 101, 110, 116, 44, 32, 97, 117, 32, 112, 97, 115, 115, 97, 103, 101, 32, 100, 101, 32, 116, 111, 117, 116, 101, 115, 32, 108, 101, 115, 32, 118, 111, 105, 116, 117, 114, 101, 115, 44, 32, 99, 104, 97, 99, 117, 110, 101, 32, 100, 101, 10, 99, 101, 115, 32, 115, 111, 108, 105, 118, 101, 115, 32, 115, 39, 97, 103, 105, 116, 97, 105, 116, 32, 100, 97, 110, 115, 32, 115, 97, 32, 109, 111, 114, 116, 97, 105, 115, 101, 46, 32, 67, 101, 32, 118, 195, 169, 110, 195, 169, 114, 97, 98, 108, 101, 32, 195, 169, 100, 105, 102, 105, 99, 101, 32, 195, 169, 116, 97, 105, 116, 10, 115, 117, 114, 109, 111, 110, 116, 195, 169, 32, 100, 39, 117, 110, 32, 116, 111, 105, 116, 32, 116, 114, 105, 97, 110, 103, 117, 108, 97, 105, 114, 101, 32, 100, 111, 110, 116, 32, 97, 117, 99, 117, 110, 32, 109, 111, 100, 195, 168, 108, 101, 32, 110, 101, 32, 115, 101, 32, 118, 101, 114, 114, 97, 32, 98, 105, 101, 110, 116, 195, 180, 116, 10, 112, 108, 117, 115, 32, 195, 160, 32, 80, 97, 114, 105, 115, 46, 32, 67, 101, 116, 116, 101, 32, 99, 111, 117, 118, 101, 114, 116, 117, 114, 101, 44, 32, 116, 111, 114, 100, 117, 101, 32, 112, 97, 114, 32, 108, 101, 115, 32, 105, 110, 116, 101, 109, 112, 195, 169, 114, 105, 101, 115, 32, 100, 117, 32, 99, 108, 105, 109, 97, 116, 10, 112, 97, 114, 105, 115, 105, 101, 110, 44, 32, 115, 39, 97, 118, 97, 110, 195, 167, 97, 105, 116, 32, 100, 101, 32, 116, 114, 111, 105, 115, 32, 112, 105, 101, 100, 115, 32, 115, 117, 114, 32, 108, 97, 32, 114, 117, 101, 44, 32, 97, 117, 116, 97, 110, 116, 32, 112, 111, 117, 114, 32, 103, 97, 114, 97, 110, 116, 105, 114, 32, 100, 101, 115, 10, 101, 97, 117, 120, 32, 112, 108, 117, 118, 105, 97, 108, 101, 115, 32, 108, 101, 32, 115, 101, 117, 105, 108, 32, 100, 101, 32, 108, 97, 32, 112, 111, 114, 116, 101, 44, 32, 113, 117, 101, 32, 112, 111, 117, 114, 32, 97, 98, 114, 105, 116, 101, 114, 32, 108, 101, 32, 109, 117, 114, 32, 100, 39, 117, 110, 10, 103, 114, 101, 110, 105, 101, 114, 32, 101, 116, 32, 115, 97, 32, 108, 117, 99, 97, 114, 110, 101, 32, 115, 97, 110, 115, 32, 97, 112, 112, 117, 105, 46, 32, 67, 101, 32, 100, 101, 114, 110, 105, 101, 114, 32, 195, 169, 116, 97, 103, 101, 32, 195, 169, 116, 97, 105, 116, 32, 99, 111, 110, 115, 116, 114, 117, 105, 116, 32, 101, 110, 10, 112, 108, 97, 110, 99, 104, 101, 115, 32, 99, 108, 111, 117, 195, 169]\n"
     ]
    }
   ],
   "source": [
    "with open('balzac.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()\n",
    "print(text[:1000])\n",
    "\n",
    "tokens = list(map(int, text.encode('utf-8')))\n",
    "print(list(tokens[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the pairs now:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 5 paires les plus fréquentes :  [(5025, (101, 32)), (2954, (115, 32)), (2429, (32, 100)), (2332, (116, 32)), (2192, (101, 115))]\n",
      "La paire la plus fréquente est :  (101, 32)\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): \n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(\"Les 5 paires les plus fréquentes : \",sorted(((v,k) for k,v in stats.items()), reverse=True)[:5])\n",
    "\n",
    "top_pair = max(stats, key=stats.get)\n",
    "print(\"La paire la plus fréquente est : \", top_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a function to merge the most frequent pairs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n",
      "taille du texte avant : 128987\n",
      "[65, 117, 32, 109, 105, 108, 105, 101, 117, 32, 100, 256, 108, 97, 32, 114, 117, 256, 83, 97, 105, 110, 116, 45, 68, 101, 110, 105, 115, 44, 32, 112, 114, 101, 115, 113, 117, 256, 97, 117, 32, 99, 111, 105, 110, 32, 100, 256, 108, 97, 32, 114, 117, 256, 100, 117, 10, 80, 101, 116, 105, 116, 45, 76, 105, 111, 110, 44, 32, 101, 120, 105, 115, 116, 97, 105, 116, 32, 110, 97, 103, 117, 195, 168, 114, 256, 117, 110, 256, 100, 256, 99, 101, 115, 32, 109, 97, 105, 115, 111]\n",
      "taille du texte après : 123962\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour fusionner les paires les plus fréquentes, on donne en entrée la liste des tokens, la paire à fusionner et le nouvel index\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # Si on est pas à la dernière position et que la paire correspond, on la remplace\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# Test de la fonction merge\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "\n",
    "print(\"taille du texte avant :\", len(tokens))\n",
    "# On fusionne la paire la plus fréquente et on lui donne un nouvel index (256 car on a déjà les caractères de 0 à 255)\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2[:100])\n",
    "print(\"taille du texte après :\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a single merge, we have already significantly reduced the text encoding size.\n",
    "Now, we will define the desired vocabulary size and merge as many times as needed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (115, 32) into a new token 257\n",
      "merging (116, 32) into a new token 258\n",
      "merging (195, 169) into a new token 259\n",
      "merging (101, 110) into a new token 260\n",
      "merging (97, 105) into a new token 261\n",
      "merging (44, 32) into a new token 262\n",
      "merging (111, 110) into a new token 263\n",
      "merging (101, 257) into a new token 264\n",
      "merging (111, 117) into a new token 265\n",
      "merging (114, 32) into a new token 266\n",
      "merging (97, 110) into a new token 267\n",
      "merging (113, 117) into a new token 268\n",
      "merging (100, 256) into a new token 269\n",
      "merging (97, 32) into a new token 270\n",
      "merging (101, 117) into a new token 271\n",
      "merging (101, 115) into a new token 272\n",
      "merging (108, 256) into a new token 273\n",
      "merging (105, 110) into a new token 274\n",
      "merging (46, 32) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276 # La taille du vocabulaire que l'on souhaite\n",
    "num_merges = vocab_size - 256\n",
    "tokens_merged=tokens\n",
    "\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(tokens_merged)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  tokens_merged = merge(tokens_merged, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the difference between the two token sequences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de base: 128987\n",
      "Taille après merge: 98587\n",
      "compression ratio: 1.31X\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille de base:\", len(tokens))\n",
    "print(\"Taille après merge:\", len(tokens_merged))\n",
    "print(f\"compression ratio: {len(tokens) / len(tokens_merged):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have effectively compressed the sequence size while increasing the vocabulary by only 20.\n",
    "GPT-2 increases the vocabulary to 50,000, so you can imagine that this drastically reduces the sequence size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding/Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built our tokenizer, we want to be able to convert between integers (tokens) and our text.\n",
    "\n",
    "For this, let's first build the *decoding* function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "# Fonction pour décoder les ids en texte, prend en entrée une liste d'entiers et retourne une chaine de caractères\n",
    "def decode(ids):\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\") # errors=\"replace\" permet de remplacer les caractères non reconnus par le caractére spécial �\n",
    "  return text\n",
    "\n",
    "print(decode([87]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the *encoding* function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 263, 106, 265, 114]\n",
      "Bonjour\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour encoder le texte en ids, prend en entrée une chaine de caractères et retourne une liste d'entiers \n",
    "def encode(text):\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break \n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"Bonjour\"))\n",
    "\n",
    "# On eut véifier que l'encodage et le décodage fonctionne correctement\n",
    "print(decode(encode(\"Bonjour\")))\n",
    "\n",
    "# Et sur le text en entier\n",
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT series uses *regex patterns* to split the text before creating the vocabulary. This allows for more control over the type of tokens generated (e.g., avoiding different tokens for \"dog\", \"dog!\", and \"dog?\"). In the source code of Tiktoken (GPT tokenizer), we can find the following pattern: **'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+**.\n",
    "\n",
    "The syntax is quite complex, but we will break it down to understand what it does:\n",
    "- **'s|'t|'re|'ve|'m|'ll|'d**: Matches English contractions like \"is\", \"it\", \"are\", \"have\", \"am\", \"will\", and \"had\". These tokens are often important to isolate in natural language processing.\n",
    "- **?\\p{L}+**: Matches words composed of letters. The \"?\" at the beginning means the word can be preceded by a space, allowing capturing words with or without an initial space.\n",
    "- **?\\p{N}+**: Matches sequences of digits (numbers). Similarly, an optional space can precede the digit sequence.\n",
    "- **?[^\\s\\p{L}\\p{N}]+**: Matches one or more characters that are neither spaces, letters, nor digits. This captures symbols and punctuation, with an optional space at the beginning.\n",
    "- **\\s+(?!\\S)**: Matches one or more spaces followed only by spaces (i.e., a sequence of spaces at the end of a string or before a line break).\n",
    "- **\\s+**: Matches one or more spaces. This is a generic match for multiple spaces between words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text has been split according to the conditions described in the *regex pattern*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens are also added for training and *fine-tuning*:\n",
    "- **<|endoftext|>**: This token is used to delimit the separation between different documents in the training data.\n",
    "- **<|im_start|>** and **<|im_end|>**: These tokens delimit the beginning and end of a user message for a chatbot, for example.\n",
    "\n",
    "**Note**: During *fine-tuning*, it is possible to add tokens to the tokenizer (such as **<|im_start|>** and **<|im_end|>**, for example) specific to the task you want to perform. Of course, this will require modifying the embedding matrix and retraining it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Types of Tokenizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer we implemented is based on OpenAI's [tiktoken](https://github.com/openai/tiktoken), used on GPT models. Another popular tokenizer is [sentencepiece](https://github.com/google/sentencepiece), used on Google and Meta models, for example.\n",
    "\n",
    "**Note**: Sentencepiece is much more complex than tiktoken and has many parameters to adjust. In practice, it is likely used because the code is open source (while the training code of tiktoken is not open-source, we only have access to the code for encoding and decoding).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization on Other Modalities?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to perform *multimodal* processing (which is trendy right now), we need to produce tokens from modalities other than text, such as sound or images.\n",
    "Ideally, we would transform our sound or image into tokens to give to the transformer as if it were text.\n",
    "\n",
    "For images, we can use a [VQVAE](https://arxiv.org/pdf/1711.00937) or a [VQGAN](https://arxiv.org/pdf/2012.09841). The idea is to use a VAE or GAN to generate discrete values in a latent space. These discrete values are then used as tokens.\n",
    "\n",
    "![VQGAN](./images/VQGAN.png)\n",
    "\n",
    "Figure extracted from the [article](https://arxiv.org/pdf/2012.09841).\n",
    "\n",
    "OpenAI's SORA model does something similar, but for videos:\n",
    "\n",
    "![SORA](./images/SORA.png)\n",
    "\n",
    "Figure extracted from the [article](https://arxiv.org/pdf/2402.17177)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to the Initial Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now answer the questions asked at the beginning of the course using what we have learned:\n",
    "- **Why do LLMs struggle with spelling words?**\n",
    "Token splitting means that each word is not split into all its characters, but rather into *chunks* of characters. This makes it difficult for the model to decompose them.\n",
    "\n",
    "- **Why do LLMs struggle with simple string operations (like reversing a string)?**\n",
    "This is roughly for the same reason as the previous question. To reverse a word, it is not enough to reverse the tokens representing that word.\n",
    "\n",
    "- **Why are LLMs better in English?**\n",
    "There are several reasons for this: the transformer's training data and the tokenizer's training data. For the transformer, more English data allows it to better learn the language and its nuances. For the tokenizer, if it is trained on English data, the generated tokens will be primarily adapted for English words, so we will need less context than for other languages.\n",
    "\n",
    "- **Why are LLMs bad at arithmetic?**\n",
    "Numbers are represented quite arbitrarily depending on the training data. Performing operations on these tokens is not an easy task for the LLM.\n",
    "\n",
    "- **Why is GPT-2 not very good at Python?**\n",
    "As we saw in this course, the GPT-2 tokenizer converts a simple space into a token. In Python, with indentation and multiple conditions/loops, there are quickly many spaces, which significantly impacts the context.\n",
    "\n",
    "- **Why does my LLM stop immediately if I send it the string \"<endoftext>\"?**\n",
    "This is a special token added in the training data to separate text. When the LLM encounters it, it must stop its generation.\n",
    "\n",
    "- **Why does the LLM break when I talk to it about SolidGoldMagiKarp?**\n",
    "This question is a bit less obvious, and I recommend reading the excellent [blogpost](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation). In simple terms, if words are present in the tokenizer's training data but not in the LLM's training data, then the embedding of this token will not be trained at all, and the LLM will behave randomly when it encounters this token. SolidGoldMagiKarp is a Reddit user who must have appeared regularly in the tokenizer's training data but not in the transformer's training data.\n",
    "\n",
    "- **Why is it preferable to use YAML rather than JSON with LLMs?**\n",
    "This is a bit like with Python. The GPT-2 tokenizer (and most models, for that matter) converts a JSON document into more tokens than its YAML equivalent. Switching from JSON to YAML thus reduces the context needed to process the document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
