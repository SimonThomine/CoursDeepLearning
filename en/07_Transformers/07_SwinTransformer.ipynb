{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook analyzes the article [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030). It proposes an improvement to the *transformer* architecture with a hierarchical design tailored for images, reminiscent of convolutional neural networks.\n",
    "The first part of the notebook explains the article's proposals one by one. The second part presents a simplified implementation of the architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of the article is to apply *attention* hierarchically to increasingly larger parts of the image. This approach is based on several foundations:\n",
    "- Image analysis begins with local details before considering relationships between all pixels. This is why CNNs are so effective.\n",
    "- The fact that *tokens* (*patches*) do not communicate with all others improves computation time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical architecture of the *swin transformer* is summarized in this figure:\n",
    "\n",
    "![hierarchique](./images/hierarchique.png)\n",
    "\n",
    "In our implementation, the ViT model converts *patches* into *tokens* and applies a *transformer encoder* to all elements. This is a simple architecture without data bias, applicable to various types of data.\n",
    "\n",
    "The *swin* architecture adds a bias to make it more performant on images and faster in processing. As shown in the figure, the image is first divided into small *patches* (size $4 \\times 4$ in the article) grouped into windows. The attention layer is then applied only to each window independently. As you go deeper into the network, the dimension C (size of *patches* relative to the image) and the window size increase until they cover the entire image, with the same number of *patches* as the ViT architecture. Like a CNN, the network first processes local information, then, gradually (with the increase of the *receptive field*), increasingly global information. This is done by increasing the number of filters and decreasing the image resolution.\n",
    "\n",
    "The corresponding new *transformer* blocks are called *Window Multi-Head Self-Attention* (W-MSA in the article, note that M stands for *Multi-Head* and not *Masked*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their analogy with CNN, the authors noted that it can be problematic to separate the image into windows at arbitrary positions. This breaks the connection between neighboring pixels located at the edges of the windows.\n",
    "\n",
    "To correct this problem, the authors propose using a sliding window system (*shifting window*) in each *swin block*. The *swin blocks* are arranged in pairs as described in the figure at the beginning of the notebook.\n",
    "\n",
    "Here is what the sliding window looks like:\n",
    "\n",
    "![shifting](./images/shifting.png)\n",
    "\n",
    "As you can see, with this technique, we go from $2 \\times 2$ *patches* to $3 \\times 3$ *patches* (generally from $n \\times n$ *patches* to $(n+1) \\times (n+1)$). This poses a problem for processing by the network, particularly in *batch*.\n",
    "\n",
    "The authors propose incorporating a *cyclic shift* which consists of performing this operation on the image to allow more efficient processing:\n",
    "\n",
    "![cyclic](./images/cyclic.png)\n",
    "\n",
    "Note that to use this method, it is necessary to mask the information of *patches* not coming from the same part of the image. The white, yellow, green, and blue parts of the figure do not communicate with each other thanks to a masked *attention* layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Position Bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ViT architecture used an absolute *position embedding* to add position information to the different *patches*. The problem with *position embedding* is that it does not capture the relationships between *patches* and is therefore less effective with images of different resolutions.\n",
    "\n",
    "The *swin transformer* uses a relative position bias to compensate for this. This bias depends on the relative distance between the different *patches*. It is added when attention is calculated between two *patches*. Its main interest is to improve the capture of spatial relationships and adapt to images of different resolutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Details on the Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As seen in the first figure of the notebook, there are more layers in stage 3 of the *swin transformer*. When increasing the number of layers in the network, only the layers of stage 3 are increased, the others remain fixed. This allows benefiting from the *swin* architecture (*shifting*, etc.) while being deep enough and performant in terms of processing time.\n",
    "\n",
    "- Assume that each window contains *patches* of size $M \\times M$. The computational complexity of a *multi-head self-attention* (MSA) layer and that of a *window multi-head self-attention* (W-MSA) layer for an image of $h \\times w$ *patches* are:\n",
    "$\\Omega(\\text{MSA}) = 4hwC^2 + 2(h w)^2 C$\n",
    "$\\Omega(\\text{W-MSA}) = 4hwC^2 + 2M^2hwC$\n",
    "The first is of quadratic complexity while the second is linear if $M$ is fixed. The *swin* architecture allows for faster processing speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the PyTorch implementation of the *swin transformer*. Some parts are quite complex in terms of implementation and we will not cover them here: the sliding window part and the *relative position bias*. We will therefore limit ourselves to implementing the hierarchical architecture.\n",
    "\n",
    "If you wish to consult the complete implementation of the *swin transformer* by the authors, you can check out their [github](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py). Our implementation is inspired by the authors' code and builds on our implementation of the ViT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detection automatique du GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Patch Conversion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For converting the image into *patches*, we reuse our function from the previous notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    # On rajoute une dimension pour le batch\n",
    "    B,C,_,_ = image.shape\n",
    "    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(B,-1, C, patch_size, patch_size)\n",
    "    patches_flat = patches.flatten(2, 4)\n",
    "    return patches_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Self-Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation of the *swin*, the *multi-head self-attention* layer does not change compared to the implementation of the ViT. It is essentially the same layer, but what changes is how it is used in the *swin block*.\n",
    "\n",
    "Let's reuse our code from the previous notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head_enc(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "    def __init__(self, head_size,n_embd,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape   \n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # On a supprimer le masquage du futur\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head_enc de taille head_size\n",
    "        self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If we wanted to implement the *relative position bias*, we would need to modify the function as this *bias* is added directly during the calculation of the *attention* (see [source code](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py) for more details).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same goes for the *feed forward layer*, which remains the same:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Swin Block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by implementing the function to partition our image into windows. To do this, we will reconvert our $x$ into the dimension $B \\times H \\times W \\times C$ rather than $B \\times T \\times C$. Then, we will transform our tensor into multiple windows that will pass into the *batch* dimension (to process each window independently).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size,input_resolution):\n",
    "    B,_,C = x.shape\n",
    "    H,W = input_resolution\n",
    "    x = x.view(B, H, W, C)\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's assume that, as in the article's implementation, we divide our 224-sized image into $4 \\times 4$ *patches*. This will give us $224/4 \\times 224/4$ *patches*, i.e., 3136, which will then be projected into an embedding dimension $C$ of size 96 (for swin-T and swin-S). We will separate into $M=7$ windows, which will give us this tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 7, 7, 96])\n"
     ]
    }
   ],
   "source": [
    "# Pour un batch de taille 2\n",
    "window_size = 7\n",
    "n_embed = 96\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "windows=window_partition(dummy,window_size,(56,56))\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing it to the *attention* layer, we need to put it back into a $B \\times T \\times C$ dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 49, 96])\n"
     ]
    }
   ],
   "source": [
    "windows=windows.view(-1, window_size * window_size, n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply our *attention* layer to perform *self-attention* on all windows independently. Once that's done, we need to apply the inverse transform to return to a windowless format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 56, 56, 96])\n",
      "torch.Size([2, 3136, 96])\n"
     ]
    }
   ],
   "source": [
    "def window_reverse(windows, window_size,input_resolution):\n",
    "    H,W=input_resolution\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "windows=window_reverse(windows,window_size,(56,56))\n",
    "print(windows.shape)\n",
    "# et revenir en format BxTxC\n",
    "windows=windows.view(2,3136,n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just implemented the fundamental elements for window-based processing (hierarchical *swin transformer*). We can now build our *swin block* that groups all these transformations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swinblock(nn.Module):\n",
    "  def __init__(self, n_embd,n_head,input_resolution,window_size,dropout=0.) -> None:\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)\n",
    "    self.ffwd = FeedFoward(n_embd,dropout)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "    self.input_resolution = input_resolution\n",
    "    self.window_size = window_size\n",
    "    self.n_embd = n_embd\n",
    "    \n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    x=window_partition(x, self.window_size,self.input_resolution)\n",
    "    x=self.ln1(x)\n",
    "    x=x.view(-1, self.window_size * self.window_size, self.n_embd)\n",
    "    x=self.sa(x)\n",
    "    x=window_reverse(x,self.window_size,self.input_resolution)\n",
    "    x=x.view(B,T,self.n_embd)\n",
    "    x=x+self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Merging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the hierarchical architecture of the *swin transformer*, whenever we increase our *receptive field* (by decreasing the number of windows), we will concatenate the 4 adjacent *patches* of size $C$ into a dimension of $4C$, then apply a linear layer to return to a smaller dimension of $2C$. This reduces the number of *tokens* by 4 each time we decrease the number of windows.\n",
    "We can retrieve the adjacent *patches* in this way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 96])\n"
     ]
    }
   ],
   "source": [
    "# Reprenons un exemple de nos 56x56 patchs\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "B,T,C = dummy.shape\n",
    "H,W=T**0.5,T**0.5\n",
    "dummy=dummy.view(2,56,56,n_embed)\n",
    "# En python, 0::2 prend un élément sur 2 à partir de 0, 1::2 prend un élément sur 2 à partir de 1\n",
    "# De cette manière, on peut récupérer les à intervalles réguliers \n",
    "dummy0 = dummy[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy1 = dummy[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy2 = dummy[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "dummy3 = dummy[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "print(dummy0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then concatenate our adjacent *patches*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 384])\n",
      "torch.Size([2, 784, 384])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.cat([dummy0, dummy1, dummy2, dummy3], -1)  # B H/2 W/2 4*C\n",
    "print(dummy.shape)\n",
    "# On repasse en BxTxC\n",
    "dummy = dummy.view(B, -1, 4 * C)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have indeed divided the number of *patches* by four while increasing the channels by 4. We are now applying the linear layer to reduce the number of channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(4 * C, 2 * C, bias=False)\n",
    "dummy = layer(dummy)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it, we have all the elements to build our *merging* layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, input_resolution, in_channels, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)\n",
    "        self.norm = norm_layer(4 * in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Swin Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the *swin transformer*, it is complicated to add a *cls_token* in the implementation. This is why we will use the other method mentioned in the previous notebook, namely *adaptive average pooling*. This allows us to have a fixed-size output, regardless of the size of the input image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 blocs de 2 couches au lieu de 4 car CIFAR-10 a de plus petites images\n",
    "class SwinTransformer(nn.Module):\n",
    "  def __init__(self,n_embed,patch_size,C,window_size,num_heads,img_dim=[16,8,4],depths=[2,2,2]) -> None:\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.proj_layer = nn.Linear(C*patch_size*patch_size, n_embed)\n",
    "    input_resolution = [(img_dim[0],img_dim[0]),(img_dim[1],img_dim[1]),(img_dim[2],img_dim[2])]\n",
    "    self.blocks1 = nn.Sequential(*[swinblock(n_embed,num_heads,input_resolution[0],window_size) for _ in range(depths[0])])\n",
    "    self.down1 = PatchMerging(input_resolution[0],in_channels=n_embed)\n",
    "    self.blocks2 = nn.Sequential(*[swinblock(n_embed*2,num_heads,input_resolution[1],window_size) for _ in range(depths[1])])\n",
    "    self.down2 = PatchMerging(input_resolution[1],in_channels=n_embed*2)\n",
    "    self.blocks3 = nn.Sequential(*[swinblock(n_embed*4,num_heads,input_resolution[2],window_size) for _ in range(depths[2])])\n",
    "    self.classi_head = nn.Linear(n_embed*4, 10)\n",
    "    self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = image_to_patches(x,self.patch_size)\n",
    "    x = self.proj_layer(x)\n",
    "    x = self.blocks1(x)\n",
    "    x = self.down1(x)\n",
    "    x = self.blocks2(x)\n",
    "    x = self.down2(x)\n",
    "    x = self.blocks3(x)\n",
    "    x = self.avgpool(x.transpose(1, 2)).flatten(1)\n",
    "    x = self.classi_head(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Imagenette\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our model, we will once again use CIFAR-10, even though the small size of the images may not be well suited to the hierarchical architecture.\n",
    "\n",
    "**Note**: You can select a subset of the dataset to speed up training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille d'une image :  torch.Size([3, 32, 32])\n",
      "taille du train dataset :  40000\n",
      "taille du val dataset :  10000\n",
      "taille du test dataset :  10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# Transformation des données, normalisation et transformation en tensor pytorch\n",
    "transform = T.Compose([T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = datasets.CIFAR10(root='./../data', train=True,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(dataset))[:5000]\n",
    "# dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "testdataset = datasets.CIFAR10(root='./../data', train=False,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(testdataset))[:1000]\n",
    "# testdataset = torch.utils.data.Subset(testdataset, indices)\n",
    "print(\"taille d'une image : \",dataset[0][0].shape)\n",
    "\n",
    "\n",
    "#Création des dataloaders pour le train, validation et test\n",
    "train_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\n",
    "print(\"taille du train dataset : \",len(train_dataset))\n",
    "print(\"taille du val dataset : \",len(val_dataset))\n",
    "print(\"taille du test dataset : \",len(testdataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "val_loader= DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 2\n",
    "n_embed = 24\n",
    "n_head = 4\n",
    "C=3 \n",
    "window_size = 4\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.0001 #1e-3\n",
    "\n",
    "model = SwinTransformer(n_embed,patch_size,C,window_size,n_head,img_dim=[16,8,4],depths=[2,2,2]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss train 1.9195597559928894, loss val 1.803518475151062,précision 33.94\n",
      "Epoch 1, loss train 1.7417401003360748, loss val 1.6992134885787964,précision 37.84\n",
      "Epoch 2, loss train 1.651085284280777, loss val 1.6203388486862182,précision 40.53\n",
      "Epoch 3, loss train 1.5808091670751572, loss val 1.5558069843292237,précision 43.03\n",
      "Epoch 4, loss train 1.522760990524292, loss val 1.5169190183639527,précision 44.3\n",
      "Epoch 5, loss train 1.4789127678394318, loss val 1.4665142657279968,précision 47.02\n",
      "Epoch 6, loss train 1.4392719486951828, loss val 1.4568698994636535,précision 47.65\n",
      "Epoch 7, loss train 1.4014943064451217, loss val 1.4456377569198609,précision 48.14\n",
      "Epoch 8, loss train 1.3745941290140151, loss val 1.4345624563694,précision 48.38\n",
      "Epoch 9, loss train 1.3492228104948998, loss val 1.398228020954132,précision 50.04\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss_val += F.cross_entropy(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, loss train {loss_train/len(train_loader)}, loss val {loss_val/len(val_loader)},précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is complete, we achieve an accuracy of 50% on the validation data.\n",
    "\n",
    "Let's now look at our test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision 49.6\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is roughly similar to that of the validation data!\n",
    "\n",
    "**Note**: The results are not very good for several reasons. First, we are processing small images and the hierarchical architecture of the *swin transformer* is rather designed to process larger images. Secondly, our implementation is really minimalist as it lacks two key elements of the *swin* architecture: the sliding window part and the *relative position bias*. The goal of this notebook was to give you an intuition about how the *swin* architecture works and not to propose a perfect implementation;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
