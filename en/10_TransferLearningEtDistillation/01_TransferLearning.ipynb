{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a common technique in deep learning. It involves reusing the weights of a pre-trained network as a base to train a new model.\n",
    "\n",
    "Here are its main advantages:\n",
    "- Training is faster if the tasks are similar.\n",
    "- Performance is better than with a model trained from scratch.\n",
    "- Less data is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transferlearning](./images/transferlearning.png)\n",
    "\n",
    "Figure from this [article](https://www.techtarget.com/searchcio/definition/transfer-learning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning or Fine-Tuning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two terms are often confused as they are very similar. In practice, fine-tuning is a form of transfer learning that involves re-training only part of the layers of the reused model.\n",
    "\n",
    "Here are the clear definitions:\n",
    "- **Transfer Learning**: Training a model using the weights of a pre-trained model on another task (you can re-train the entire model or just some layers).\n",
    "- **Fine-Tuning**: Re-training certain layers (often the last ones) of a pre-trained model to adapt it to a new task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use Fine-Tuning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning involves re-training certain layers of a pre-trained model to adapt it to a new task. Therefore, you need to choose the number of layers to re-train.\n",
    "\n",
    "How to choose this number of layers? There is no fixed formula. We generally rely on our intuition and these rules:\n",
    "- The fewer data you have, the fewer layers you re-train (little data = only the last layer; a lot of data = almost all layers).\n",
    "- The more similar the tasks are, the fewer layers you re-train (e.g., detecting hamsters in addition to cats, dogs, and rabbits; detecting diseases from a cat/dog/rabbit model is very different).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Transfer Learning or Fine-Tuning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, using a pre-trained model as a base is always beneficial (unless the domains are very different). I recommend using it whenever possible.\n",
    "\n",
    "However, this imposes some constraints:\n",
    "- The model architecture can no longer be modified as desired (the non-re-trained layers).\n",
    "- You need to have the weights of a pre-trained model (many are available online, see [course 6 on HuggingFace](../06_HuggingFace/README.md)).\n",
    "\n",
    "**Note**: For image classification, a model pre-trained on [ImageNet](https://www.image-net.org/) is often used because its 1000 classes make the model quite generalist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset: How to Proceed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fine-tuning a model, you can have two objectives:\n",
    "- **Case 1**: Train a model on a completely different task from the one it was pre-trained for (e.g., classifying dinosaurs when the model is trained on mammals). In this case, the model can \"forget\" its original task without any issues.\n",
    "- **Case 2**: Train a model on a complementary task to the one it was pre-trained for (e.g., detecting birds while remaining performant on mammals). In this case, you want the model to remain performant on the original task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the case, different data is used for training. For **Case 1**, the dataset contains only the new images to classify (e.g., only dinosaurs). For **Case 2**, data from both the old and new datasets is included so that the model remains performant on the old data. Generally, a 50/50 split is used, but this can vary (e.g., half mammals, half birds).\n",
    "\n",
    "**Note**: According to this principle, true open-source means making the code, weights, and training data of the model accessible. Without these three elements, you cannot effectively fine-tune the model. This is particularly true for language models LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base models are trained on large amounts of data (often unlabeled) and serve as a basis for fine-tuning or transfer learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Models for NLP**: For NLP, there are many base models such as GPT, BLOOM, Llama, Gemini, etc. These models are fine-tuned for various tasks. For example, chatGPT is a fine-tuned version of GPT for chatbot-type conversations.\n",
    "\n",
    "**Base Models for Images**: For images, the term base model is debated as it is not as clear-cut as for NLP. Examples include ViT, DINO, CLIP, etc.\n",
    "\n",
    "**Base Models for Sound**: For sound, the CLAP model is an example of a base model.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
