{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Object Detection in Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image processing is divided into three main categories:\n",
    "- **Classification**: Determines if an object is present in the image (example: is this a picture of a dog?).\n",
    "- **Detection**: Locates the position of an object in the image (example: where is the dog?).\n",
    "- **Segmentation**: Identifies the pixels belonging to an object (example: which pixels belong to the dog?).\n",
    "\n",
    "![ClassDetSeg](./images/ClassDetSeg.jpeg)\n",
    "\n",
    "Image extracted from this [site](https://docs.ultralytics.com/fr/guides/steps-of-a-cv-project/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course on CNNs, we covered classification problems with a classic CNN architecture ending with a Fully Connected layer, as well as segmentation problems with the U-Net model.\n",
    "\n",
    "Object detection is more complex to explain, so this course focuses on existing methods and a detailed description of the [YOLO](https://arxiv.org/pdf/1506.02640) model.\n",
    "\n",
    "We will first explain the differences between the two main categories of detectors:\n",
    "- **Two-Stage Detectors**: These include the family of [RCNN (Region-based Convolutional Neural Networks)](https://arxiv.org/pdf/1311.2524).\n",
    "- **Single-Stage Detectors**: These include the family of [YOLO (You Only Look Once)](https://arxiv.org/pdf/1506.02640).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Detectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As its name suggests, a *two-stage detector* follows two steps to detect objects:\n",
    "- **First step**: Proposing regions (*region proposal*) where objects of interest might be located.\n",
    "- **Second step**: Refining the detection, i.e., associating the object class and the precision of the *bounding box* (if an object is present).\n",
    "\n",
    "![rcnn](./images/rcnn.png)\n",
    "\n",
    "Image extracted from the [article](https://arxiv.org/pdf/1311.2524).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, *two-stage detectors* are very accurate and allow for complex detections, but they are quite slow and do not support real-time processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most well-known *two-stage* networks are the family of RCNNs. For more information, check out this [blogpost](https://blog.roboflow.com/what-is-r-cnn/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Stage Detectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *one-stage detector* only requires a single step to generate the *bounding boxes* with the corresponding labels. The network divides the image into a grid and predicts several *bounding boxes* and their probabilities for each cell of the grid.\n",
    "\n",
    "![yolo](./images/yolo.png)\n",
    "\n",
    "Figure extracted from the [article](https://arxiv.org/pdf/1506.02640).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One-stage detectors* are generally less accurate than *two-stage detectors*, but they are much faster and allow for real-time processing. This is the most widely used family of detectors today.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Maximum Suppression and Anchors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS (Non-Maximum Suppression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When detecting objects with our model, the architecture does not prevent multiple *bounding boxes* from overlapping on the same object. Before transmitting the detections to the user, we want to have one detection per object, the most relevant possible.\n",
    "\n",
    "This is where *non-maximum suppression* comes into play. The algorithm will not be detailed in this course, but you can consult the following resources for more details: [blogpost](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c) and [site](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/).\n",
    "\n",
    "![nms](./images/nms.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors (Anchor boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchors are predefined *bounding boxes* placed on a regular grid covering the image. They can have different aspect ratios (length/height) and variable sizes to cover as many possible object sizes as possible. Anchors reduce the number of positions to study for the model. With anchors, the model predicts the offset from the pre-generated anchor and the probability of belonging to an object.\n",
    "\n",
    "This method improves the quality of detections. For more information, check out the [blogpost](https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9).\n",
    "\n",
    "In practice, there are often many anchors. The following figure shows 1% of the anchors from the [retinaNet](https://arxiv.org/pdf/1708.02002) model:\n",
    "\n",
    "![anchor](./images/anchors.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Object Detection with the Transformer Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, the *transformer* architecture has been adapted for object detection. The [DETR](https://arxiv.org/pdf/2005.12872) model uses a CNN model to extract visual features. These *features* are then passed through a *transformer encoder* (with a *positional embedding*) to determine spatial relationships between features using the *attention* mechanism. A *transformer decoder* (different from the one used in NLP) takes as input the output of the *encoder* (*keys* and *values*) and *embeddings* of object labels (*queries*), converting these *embeddings* into predictions. Finally, a final linear layer processes the decoder output to predict the labels and *bounding boxes*.\n",
    "\n",
    "For more information, check out the [article](https://arxiv.org/pdf/2005.12872) or this [blogpost](https://medium.com/visionwizard/detr-b677c7016a47).\n",
    "\n",
    "![detr](./images/detr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method offers several advantages:\n",
    "- No need for NMS, anchors, or *region proposal*, which simplifies the architecture and training pipeline.\n",
    "- The model has a better overall understanding of the scene thanks to the *attention* mechanism.\n",
    "\n",
    "However, it also has some drawbacks:\n",
    "- *Transformers* are computationally intensive, so this model is slower than a *one-stage detector* like YOLO.\n",
    "- Learning is often longer than for a detector based solely on a CNN.\n",
    "\n",
    "**Note**: *Transformers* used in vision often have longer training times than CNNs. A possible explanation is that CNNs have a bias that makes them particularly suitable for images, requiring shorter training times. *Transformers*, being general-purpose models without bias, have to learn from scratch.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
