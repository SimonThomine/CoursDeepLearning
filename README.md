<p align="center">
  <h1><center> 	ğŸš€ Apprendre le Deep Learning Ã  partir de zÃ©ro ğŸš€</h1>
</p>

# ğŸ“š Description
Ce repository propose des cours d'initiation au deep learning se basant sur des notebooks.
Pour un dÃ©butant, les cours sont Ã  faire dans l'ordre pour une meilleur comprÃ©hension globale. 

## ğŸ› ï¸ Installation de l'environnement de travail 
L'ensemble des library nÃ©cessaires pour le cours sont disponibles dans requirements.txt, vous pouvez choisir d'installer tout d'un coup ou au fur et Ã  mesure de votre avancement dans le cours.   
Il est conseillÃ© d'utiliser un environnement de travail conda pour Ã©viter tout conflit avec des library dÃ©jÃ  installÃ© sur votre ordinateur.  

```
`pip install -r requirements.txt`
```

# ğŸ—ºï¸ Plan du cours
## 1. ğŸ—ï¸ Fondations
Le premier cours "Fondations" introduit les bases de l'optimisation par descente du gradient avec une comprÃ©hension intuitive. La rÃ¨gle de la chaÃ®ne est introduite puis un premier exemple de regression logistique est prÃ©sentÃ©. 
<!-- Lorsque le premier cours est bien compris, il est recommandÃ© de faire la partie exercice avant de passer aux cours suivants.  -->

## 2. ğŸ§  RÃ©seau Fully Connected
Le deuxiÃ¨me cours "RÃ©seauFullyConnected" introduit le fonctionnement d'un rÃ©seau de neurones avec d'abord un exemple d'un rÃ©seau codÃ© avec [micrograd](https://github.com/karpathy/micrograd/tree/master) pour permettre d'explorer cette library pour bien comprendre le fonctionnement. Une version franÃ§aise MicrogradFR est disponible dans ce repository.   
Ensuite, pour introduire la library pytorch, le mÃªme exemple est reconstruit mais en utilisant pytorch au lieu de micrograd.  
Le dernier notebook de cette partie introduit des techniques avancÃ©es d'entraÃ®nement de rÃ©seau de neurones qu'il est utile de connaÃ®tre pour amÃ©liorer les performances de nos rÃ©seaux. 

## 3. ğŸ–¼ï¸ RÃ©seaux convolutifs
Le troisiÃ¨me cours "RÃ©seauConvolutifs" aborde tout d'abord le principe de fonctionnement des couches de convolution puis montre comment on les utilise au sein d'un rÃ©seau de neurones. Plusieurs exemples sont ensuite abordÃ©s pour montrer les capacitÃ©s d'un rÃ©seau convolutif : classification sur MNIST, classification sur CIFAR-10 et segmentation sur "Oxford-IIIT Pet Dataset". 

## 4. ğŸ”„ Autoencodeurs
Le quatriÃ¨me cours "Autoencodeurs" aborde la notion d'entraÃ®nement non supervisÃ© en prÃ©sentant les diffÃ©rences entre supervisÃ© et non supervisÃ©. L'exemple de l'autoencodeur est ensuite abordÃ© ainsi que son application pour la dÃ©tection d'anomalies non supervisÃ©e. Pour finir, un notebook montre le potentiel de l'autoencodeur pour le problÃ¨me du "denoising". 

## 5. ğŸ—¨ï¸ NLP
Le cinquiÃ¨me cours "NLP" est grandement inspirÃ© de la sÃ©rie de vidÃ©o de Andrej Karpathy ["Building makemore"](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) qui traÃ®te les NLP avec une approche de prÃ©diction du prochain token. Le cours aborde d'abord des modÃ¨les trÃ¨s simples pour avoir une intuition sur le traÃ®tement de donnÃ©es discrÃ¨tes avec un rÃ©seau neurones puis les modÃ¨les se complexifient petit Ã  petit. 

## 6. ğŸ¤— Hugging Face
Le sixiÃ¨me cours "HuggingFace" est dÃ©diÃ© Ã  une exploration des librarys, des modÃ¨les, des datasets et autres de [Hugging Face](https://huggingface.co/). C'est une plateforme regroupant Ã©normement des modÃ¨les open source pour une grande variÃ©tÃ© de tÃ¢ches avec une library pour les implÃ©menter rapidement et efficacement en python. Le cours prÃ©sente d'abord le site de Hugging Face pour ensuite prÃ©senter les fonctionnalitÃ©s des diffÃ©rentes librarys (transformers et diffusers principalement) sur diffÃ©rents cas d'usage. Le dernier notebook prÃ©sente briÃ¨vement gradio, une library pour crÃ©er des interfaces simples de dÃ©mo.

# ğŸ“Œ TODO
 - [x] Cours sur les fondations
 - [x] Cours sur les rÃ©seau fully connected 
 - [x] Cours sur les CNN  
 - [x] Cours sur les AutoEncoders 
 - [ ] Cours sur le NLP (Karpathy makemore)
 - [ ] Cours sur Hugging Face
 - [ ] Cours sur les RNN 
 - [ ] Cours sur les Transformers (Concept et applications sur NLP)
 - [ ] Cours sur le transfer learning (fine tuning surtout)
 - [ ] Cours spÃ©cifiques sur Regularization (intuition etc)  
 - [ ] Cours spÃ©cifiques sur Dropout (intuition etc)  
 - [ ] Cours spÃ©cifiques sur BatchNorm (intuition etc)  
 <!-- - [ ] Exercices d'entraÃ®nement Fondations  
 - [ ] Exercices d'entraÃ®nement RÃ©seauFullyConnected   -->
