<p align="center">
  <h1><center> 	üöÄ Apprendre le Deep Learning √† partir de z√©ro üöÄ</h1>
</p>

# üìö Description
Ce repository propose des cours d'initiation au deep learning se basant sur des notebooks.
Pour un d√©butant, les cours sont √† faire dans l'ordre pour une meilleur compr√©hension globale. 

## üõ†Ô∏è Installation de l'environnement de travail 
L'ensemble des library n√©cessaires pour le cours sont disponibles dans requirements.txt, vous pouvez choisir d'installer tout d'un coup ou au fur et √† mesure de votre avancement dans le cours.   
Il est conseill√© d'utiliser un environnement de travail conda pour √©viter tout conflit avec des library d√©j√† install√© sur votre ordinateur.  

```
`pip install -r requirements.txt`
```

# üó∫Ô∏è Plan du cours
## 1. üèóÔ∏è Fondations
Le premier cours "Fondations" introduit les bases de l'optimisation par descente du gradient avec une compr√©hension intuitive. La r√®gle de la cha√Æne est introduite puis un premier exemple de regression logistique est pr√©sent√©. 

## 2. üß† R√©seau Fully Connected
Le deuxi√®me cours "R√©seauFullyConnected" introduit le fonctionnement d'un r√©seau de neurones avec d'abord un exemple d'un r√©seau cod√© avec [micrograd](https://github.com/karpathy/micrograd/tree/master) pour permettre d'explorer cette library pour bien comprendre le fonctionnement. Une version fran√ßaise MicrogradFR est disponible dans ce repository.   
Ensuite, pour introduire la library pytorch, le m√™me exemple est reconstruit mais en utilisant pytorch au lieu de micrograd.  
Le dernier notebook de cette partie introduit des techniques avanc√©es d'entra√Ænement de r√©seau de neurones qu'il est utile de conna√Ætre pour am√©liorer les performances de nos r√©seaux. 

## 3. üñºÔ∏è R√©seaux convolutifs
Le troisi√®me cours "R√©seauConvolutifs" aborde tout d'abord le principe de fonctionnement des couches de convolution puis montre comment on les utilise au sein d'un r√©seau de neurones. Plusieurs exemples sont ensuite abord√©s pour montrer les capacit√©s d'un r√©seau convolutif : classification sur MNIST, classification sur CIFAR-10 et segmentation sur "Oxford-IIIT Pet Dataset". 

## 4. üîÑ Autoencodeurs
Le quatri√®me cours "Autoencodeurs" aborde la notion d'entra√Ænement non supervis√© en pr√©sentant les diff√©rences entre supervis√© et non supervis√©. L'exemple de l'autoencodeur est ensuite abord√© ainsi que son application pour la d√©tection d'anomalies non supervis√©e. Pour finir, un notebook montre le potentiel de l'autoencodeur pour le probl√®me du "denoising". 

## 5. üó®Ô∏è NLP
Le cinqui√®me cours "NLP" est grandement inspir√© de la s√©rie de vid√©o de Andrej Karpathy ["Building makemore"](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) qui tra√Æte les NLP avec une approche de pr√©diction du prochain token. Le cours aborde d'abord des mod√®les tr√®s simples pour avoir une intuition sur le tra√Ætement de donn√©es discr√®tes avec un r√©seau neurones puis les mod√®les se complexifient petit √† petit. 

## 6. ü§ó Hugging Face
Le sixi√®me cours "HuggingFace" est d√©di√© √† une exploration des librarys, des mod√®les, des datasets et autres de [Hugging Face](https://huggingface.co/). C'est une plateforme regroupant √©normement des mod√®les open source pour une grande vari√©t√© de t√¢ches avec une library pour les impl√©menter rapidement et efficacement en python. Le cours pr√©sente d'abord le site de Hugging Face pour ensuite pr√©senter les fonctionnalit√©s des diff√©rentes librarys (transformers et diffusers principalement) sur diff√©rents cas d'usage. Le dernier notebook pr√©sente bri√®vement gradio, une library pour cr√©er des interfaces simples de d√©mo.

## 7. ü§ñ Transformers
Le septi√®me cours "Transformers" est d√©di√© √† l'architecture du transformers. Apr√®s avoir vu ses applications dans le cours pr√©c√©dent. Nous allons entrer dans le d√©tail de l'architecture pour en comprendre les m√©canismes. Le premier notebook est grandement inspir√© de la vid√©o [Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1806s&ab_channel=AndrejKarpathy) de Andrej Karpathy et propose une impl√©mentation pas √† pas d'un encodeur transformers. Le but de ce notebook sera de cr√©er un mod√®le capable de g√©n√©rer du "Moli√®re" automatiquement. La seconde partie est une approche plus math√©matique et la pr√©sentation de la partie encodeur du transformers. La troisi√®me partie pr√©sente des architectures de mod√®le reposant sur la couche transformers pour de nombreux cas d'applications (Vision, traduction etc ...). Enfin, une derni√®re partie propose une impl√©mentation du vision transformer √† partir de l'article original.

## 8. üîç Detection
Le huiti√®me cours "Detection" pr√©sente le fonctionnement de la d√©tection d'objets sur des images. L'introduction pr√©sente ce qu'est la d√©tection et les deux m√©thodes classiques (two-stage et one-stage). Le notebook suivant propose une description pr√©cise du fonctionnement de [YOLO](https://arxiv.org/pdf/1506.02640) et le dernier notebook pr√©sente la library [ultralytics](https://www.ultralytics.com/) qui permet d'acc√®der aux mod√®les YOLO tr√®s simplement.

## 9. üéØ Entrainement contrastif
Le neuvi√®me cours "Entrainement contrastif" pr√©sente le concept de l'entra√Ænement contrastif. Un premier notebook pr√©sente ce qu'est l'entra√Ænement contrastif en se basant sur l'impl√©mentation d'un article de "face verification". Le second notebook pr√©sente la place de l'entra√Ænment contrastif dans le deep learning r√©cent et notamment son int√™ret pour l'entrainement non supervis√©. 

## 10. ü§ù Transfer learning et distillation
Le dixi√®me cours "Transfer learning et distillation" pr√©sente deux concepts majeurs en deep learning : le transfer learning et la distillation des connaissances. La premi√®re partie de ce cours pr√©sente le transfer learning dans sa globalit√© puis propose une impl√©mentation pratique. La seconde partie pr√©sente le concept de distillation des connaissances et ses variantes puis propose un cas d'application de la distillation des connaissances pour la d√©tection d'anomalies non supervis√©e.

## Bonus üåü Cours sp√©cifiques
Ce cours pr√©sente des concepts tr√®s int√©ressant √† comprendre mais non essentiels dans une pratique courante du deep learning. Si vous √™tes int√©ress√© par comprendre le fonctionnement d'un r√©seau de neurones de mani√®re plus approfondie et de d√©couvrir la raison de l'utilisation de techniques comme la BatchNorm, les connexions r√©siduelles, les optimizers, le dropout, la data augmentation etc ..., ce cours est fait pour vous ! 

# üìå TODO
 - [x] Cours sur les fondations
 - [x] Cours sur les r√©seau fully connected 
 - [x] Cours sur les CNN  
 - [x] Cours sur les AutoEncoders 
 - [x] Cours sur le NLP (Karpathy makemore)
 - [x] Cours sur Hugging Face
 - [x] Cours sur les Transformers (Concept et applications sur NLP + vid√©o karpathy)
 - [x] Cours sur la d√©tection d'objets(Yolo principalement)
 - [x] Cours sur les RNN (inclu dans NLP)
 - [x] Cours sur le transfer learning et distillation 
 - [x] Cours sur le contrastive training (siamese nets, triplet loss, entrainement non supervis√©)
 - [x] Cours sp√©cifique sur l'initialisation 
 - [x] Cours sp√©cifique sur BatchNorm 
 - [x] Cours sp√©cifique sur Regularization (l2 et dropout)
 - [x] Cours sp√©cifique sur les connections r√©siduelles 
 - [x] Cours sp√©cifique sur les optimizers
 - [x] Cours sp√©cifique sur la data augmentation
 - [ ] Cours sur la g√©n√©ration d'images


**License**

Ce travail est mis √† disposition selon les termes de la licence MIT