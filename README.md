<p align="center">
  <h1><center> 	üöÄ Apprendre le Deep Learning √† partir de z√©ro üöÄ</h1>
</p>

# üìö Description
Ce repository propose des cours d'initiation au deep learning se basant sur des notebooks.
Pour un d√©butant, les cours sont √† faire dans l'ordre pour une meilleur compr√©hension globale. 

## üõ†Ô∏è Installation de l'environnement de travail 
L'ensemble des library n√©cessaires pour le cours sont disponibles dans requirements.txt, vous pouvez choisir d'installer tout d'un coup ou au fur et √† mesure de votre avancement dans le cours.   
Il est conseill√© d'utiliser un environnement de travail conda pour √©viter tout conflit avec des library d√©j√† install√© sur votre ordinateur.  

```
`pip install -r requirements.txt`
```

# üó∫Ô∏è Plan du cours
## 1. üèóÔ∏è Fondations
Le premier cours "Fondations" introduit les bases de l'optimisation par descente du gradient avec une compr√©hension intuitive. La r√®gle de la cha√Æne est introduite puis un premier exemple de regression logistique est pr√©sent√©. 
<!-- Lorsque le premier cours est bien compris, il est recommand√© de faire la partie exercice avant de passer aux cours suivants.  -->

## 2. üß† R√©seau Fully Connected
Le deuxi√®me cours "R√©seauFullyConnected" introduit le fonctionnement d'un r√©seau de neurones avec d'abord un exemple d'un r√©seau cod√© avec [micrograd](https://github.com/karpathy/micrograd/tree/master) pour permettre d'explorer cette library pour bien comprendre le fonctionnement. Une version fran√ßaise MicrogradFR est disponible dans ce repository.   
Ensuite, pour introduire la library pytorch, le m√™me exemple est reconstruit mais en utilisant pytorch au lieu de micrograd.  
Le dernier notebook de cette partie introduit des techniques avanc√©es d'entra√Ænement de r√©seau de neurones qu'il est utile de conna√Ætre pour am√©liorer les performances de nos r√©seaux. 

## 3. üñºÔ∏è R√©seaux convolutifs
Le troisi√®me cours "R√©seauConvolutifs" aborde tout d'abord le principe de fonctionnement des couches de convolution puis montre comment on les utilise au sein d'un r√©seau de neurones. Plusieurs exemples sont ensuite abord√©s pour montrer les capacit√©s d'un r√©seau convolutif : classification sur MNIST, classification sur CIFAR-10 et segmentation sur "Oxford-IIIT Pet Dataset". 

## 4. üîÑ Autoencodeurs
Le quatri√®me cours "Autoencodeurs" aborde la notion d'entra√Ænement non supervis√© en pr√©sentant les diff√©rences entre supervis√© et non supervis√©. L'exemple de l'autoencodeur est ensuite abord√© ainsi que son application pour la d√©tection d'anomalies non supervis√©e. Pour finir, un notebook montre le potentiel de l'autoencodeur pour le probl√®me du "denoising". 

## 5. üó®Ô∏è NLP
Le cinqui√®me cours "NLP" est grandement inspir√© de la s√©rie de vid√©o de Andrej Karpathy ["Building makemore"](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) qui tra√Æte les NLP avec une approche de pr√©diction du prochain token. Le cours aborde d'abord des mod√®les tr√®s simples pour avoir une intuition sur le tra√Ætement de donn√©es discr√®tes avec un r√©seau neurones puis les mod√®les se complexifient petit √† petit. 

## 6. ü§ó Hugging Face
Le sixi√®me cours "HuggingFace" est d√©di√© √† une exploration des librarys, des mod√®les, des datasets et autres de [Hugging Face](https://huggingface.co/). C'est une plateforme regroupant √©normement des mod√®les open source pour une grande vari√©t√© de t√¢ches avec une library pour les impl√©menter rapidement et efficacement en python. Le cours pr√©sente d'abord le site de Hugging Face pour ensuite pr√©senter les fonctionnalit√©s des diff√©rentes librarys (transformers et diffusers principalement) sur diff√©rents cas d'usage. Le dernier notebook pr√©sente bri√®vement gradio, une library pour cr√©er des interfaces simples de d√©mo.

## 7. ü§ñ Transformers
Le septi√®me cours "Transformers" est d√©di√© √† l'architecture du transformers. Apr√®s avoir vu ses applications dans le cours pr√©c√©dent. Nous allons entrer dans le d√©tail de l'architecture pour en comprendre les m√©canismes. Le premier notebook est grandement inspir√© de la vid√©o [Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1806s&ab_channel=AndrejKarpathy) de Andrej Karpathy et propose une impl√©mentation pas √† pas d'un encodeur transformers. Le but de ce notebook sera de cr√©er un mod√®le capable de g√©n√©rer du "Moli√®re" automatiquement. La seconde partie est une approche plus math√©matique et la pr√©sentation de la partie encodeur du transformers. La derni√®re partie pr√©sente des architectures de mod√®le reposant sur la couche transformers pour de nombreux cas d'applications (Vision, traduction etc ...).

## 8. üîç Detection
Le huiti√®me cours "Detection" pr√©sente le fonctionnement de la d√©tection d'objets sur des images. L'introduction pr√©sente ce qu'est la d√©tection et les deux m√©thodes classiques (two-stage et one-stage). Le notebook suivant propose une impl√©mentation from scratch de YOLO et le dernier notebook pr√©sente la library [ultralytics](https://www.ultralytics.com/) qui permet d'acc√®der aux mod√®les YOLO tr√®s simplement. Dans ce dernier notebook, on pr√©sente √©galement les diff√©rences cl√©s entre les mod√®les YOLO.

# üìå TODO
 - [x] Cours sur les fondations
 - [x] Cours sur les r√©seau fully connected 
 - [x] Cours sur les CNN  
 - [x] Cours sur les AutoEncoders 
 - [x] Cours sur le NLP (Karpathy makemore)
 - [x] Cours sur Hugging Face
 - [x] Cours sur les Transformers (Concept et applications sur NLP + vid√©o karpathy)
 - [ ] Cours sur les RNN 
 - [ ] Cours sur la d√©tection d'objets(Yolo principalement)
 - [ ] Cours sur le transfer learning (fine tuning surtout)
 - [ ] Cours sp√©cifiques sur Regularization (intuition etc)  
 - [ ] Cours sp√©cifiques sur Dropout (intuition etc)  
 - [ ] Cours sp√©cifiques sur BatchNorm (intuition etc)  


**License**

Ce travail est mis √† disposition selon les termes de la licence MIT