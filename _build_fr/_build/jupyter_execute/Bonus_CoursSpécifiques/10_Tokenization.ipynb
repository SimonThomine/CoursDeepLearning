{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à la tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un élément clé des modèles de langages (LLM) est la *tokenization*. Il s'agit de la toute première étape d'un réseau transformer et cela consiste à transformer un texte en une séquence d'entiers. Ce cours est grandement inspiré de la vidéo de Andrej Karpathy [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE&ab_channel=AndrejKarpathy). \n",
    "\n",
    "Quand nous avons implementé notre GPT, nous avons utilisé un *tokenizer* très simple qui consiste à encoder chaque caractère avec un entier différent. En pratique, on préfere encoder des *chunks* de caractères c'est à dire des groupements de caractères.   \n",
    "\n",
    "Il est important de comprendre comment fonctionne un tokenizer pour comprendre complétement le fonctionnement d'un modèle de langage.   \n",
    "\n",
    "A la fin du cours, nous serons en mesure de répondre à ces questions : \n",
    "- Pourquoi les LLM ont du mal a épeler les mots ?\n",
    "- Pourquoi les LLM ont du mal à faire des opérations simples sur les string (comme inverser un string) ?\n",
    "- Pourquoi les LLM sont meilleurs sur l'anglais ? \n",
    "- Pourquoi les LLM sont mauvais en arithmétique ?\n",
    "- Pourquoi GPT-2 n'est pas très bon en python ? \n",
    "- Pourquoi mon LLM s'arrête directement si je lui envoie le string \"<endoftext>\" ?\n",
    "- Pourquoi le LLM se casse quand je lui parle de SolidGoldMagiKarp ?\n",
    "- Pourquoi il faut mieux utiliser YAML plutôt que JSON avec les LLM ?\n",
    "\n",
    "**Note** : Le tokenizer est une partie complétement séparée du LLM avec son propre dataset d'entrainement et qui est entrainé différemment : \n",
    "\n",
    "<img src=\"images/tokenizer.png\" alt=\"tokenizer\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer de GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par analyser la tokenization de GPT-2 via le site internet [Tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2) et essayons de comprendre ce qui peut poser problème. Le tokenizer de GPT-2 a un vocabulaire d'environ 50 000 mots (donc 50 000 tokens distincts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmétique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, si on regarde pour la partie arithmétique, on se rend vite compte que les nombres peuvent être séparer en token d'une manière qui semble assez arbitraire.   \n",
    "Par exemple : \n",
    "\n",
    "<img src=\"images/arith.png\" alt=\"classi\" width=\"100\"/>\n",
    "\n",
    "998 est un token à part entière mais 9988 est séparé en deux tokens 99 et 88.  \n",
    "On peut facilement imaginer que pour le LLM, ça va être compliqué de compter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Même mots, différents tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour des mots identiques et en fonction de la manière dont ils sont écrit, on a des tokens différents.   \n",
    "Par exemple :    \n",
    "<img src=\"images/same1.png\" alt=\"classi\" width=\"100\"/>   \n",
    "<img src=\"images/same2.png\" alt=\"classi\" width=\"500\"/>\n",
    "\n",
    "Les 4 mots identiques sont representées par des tokens différents (le token 198 correspond au retour à la ligne). Le modèle va donc devoir apprendre que ces tokens sont presques les mêmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autres langues "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la même phrase dans différentes langues, le nombre de tokens utilisés n'est pas le même : \n",
    "\n",
    "<img src=\"images/langage.png\" alt=\"classi\" width=\"300\"/>   \n",
    "\n",
    "Cela est du au fait que le tokenizer de GPT-2 est entrainé majoritairement sur des données en langue anglaise.    \n",
    "En pratique, cela diminue les capacités du modèle dans les autres langues car le contexte n'est plus le même en terme d'information. On peut mettre un texte beaucoup plus long en anglais qu'en japonais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder comment le tokenizer se comporte pour le code python : \n",
    "\n",
    "<img src=\"images/python.png\" alt=\"classi\" width=\"200\"/>   \n",
    "\n",
    "Chaque espace de l'indentation est compté comme un token... Si on a un code avec de nombreuses conditions ou boucles, le contexte va augmenter de manière très rapide ce qui rendra le modèle assez peu performant. \n",
    "\n",
    "**Note** : Ce défaut a été corrigé dans les versions suivantes de GPT (3 et 4), une indentation de 4 tab sera un unique token par exemple.\n",
    "\n",
    "<img src=\"images/python2.png\" alt=\"classi\" width=\"200\"/>\n",
    "\n",
    "**Note 2** : La façon dont est configuré notre éditeur de code (2 ou 4 espaces pour l'indentation en python) peut également avoir un impact sur la tokenization.\n",
    "\n",
    "**Note 3** : Un LLM spécialisé dans le code aura aussi un tokenizer spécialisé dans le code ce qui n'est pas négligeable en terme de gain de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construisons notre propre tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour construire notre propre tokenizer, commençons par voir comment on peut convertir nos string en entier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une méthode possible de d'utiliser le [unicode](https://fr.wikipedia.org/wiki/Unicode). Cela va permettre de convertir chaque caractère en un entier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 101, 32, 99, 111, 117, 114, 115, 32, 100, 101, 32, 100, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 101, 115, 116, 32, 103, 233, 110, 105, 97, 108]\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Ce cours de deep learning est génial !\"\n",
    "# ord() permet de récupérer le code unicode d'un caractère\n",
    "unicode=[ord(char) for char in sentence]\n",
    "print(unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, on ne peut pas utiliser ça pour plusieurs raisons : \n",
    "- A ce jour, il y a presque 150 000 caractères ce qui est trop important comme taille de vocabulaire.\n",
    "- Il y a régulierement des mise à jours (une par an) ce qui fait qu'un tokenizer basé sur ça deviendrait obsolète au bout d'un an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre possibilité est d'utiliser l'*encoding* utf-8 (16 ou 32 serait aussi possible mais moins pratique) qui permet d'encoder l'unicode en 4 à 8 bits. En faisant cela, notre taille de vocabulaire de base sera de 256. \n",
    "\n",
    "On va garder l'idée d'utf-8 mais on souhaiterait augmenter la taille du vocabulaire car 256 c'est très petits et ça demanderait aux LLMs d'avoir des tailles de contexte énormes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66, 111, 110, 106, 111, 117, 114]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"Bonjour\"\n",
    "list(sentence.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-pair encoding algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour augmenter notre taille de vocabulaire, on va utiliser l'algorithme *byte-pair encoding*.    \n",
    "Le fonctionnement de cet algorithme est très simple : On va, de manière itérative, trouver la paire de byte la plus commune et la remplacer avec un nouveau token (ce qui augmente le vocabulaire de 1).   \n",
    "Par exemple, prenons la séquence :  \n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "A la première itération, on voit que la paire \"aa\" est la plus fréquente, on va donc la remplacer par le token Z :   \n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "Puis à la seconde itération, c'est la paire \"ab\" que l'on va remplacer avec Y :  \n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "Et enfin, à la troisième itération, on peut remplacer ZY par X : \n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "On aura donc augmenté le vocabulaire tout en réduisant la taille de la séquence (et donc le contexte nécessaire pour traiter la séquence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'avantage de cet algorithme, c'est qu'on peut le faire autant de fois que l'on veut jusqu'à ce qu'on obtienne une taille de contexte qui nous satisfait.\n",
    "\n",
    "**Note** : Le choix des données d'entraînement a un impact crucial sur le tokenizer, il faut veiller à les choisir en fonction de nos objectifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-pair encoding application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour montrer l'utilisation du *byte-pair encoding*, prenons un gros morceau de texte et comptons les paires. Pour cela, utilisons le premier chapitre du premier volume de la comédie humaine de Balzac. Le texte a été récuperé sur [Gutenberg](https://www.gutenberg.org/ebooks/41211)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Au milieu de la rue Saint-Denis, presque au coin de la rue du\n",
      "Petit-Lion, existait naguère une de ces maisons précieuses qui donnent\n",
      "aux historiens la facilité de reconstruire par analogie l'ancien Paris.\n",
      "Les murs menaçants de cette bicoque semblaient avoir été bariolés\n",
      "d'hiéroglyphes. Quel autre nom le flâneur pouvait-il donner aux X et aux\n",
      "V que traçaient sur la façade les pièces de bois transversales ou\n",
      "diagonales dessinées dans le badigeon par de petites lézardes\n",
      "parallèles? Évidemment, au passage de toutes les voitures, chacune de\n",
      "ces solives s'agitait dans sa mortaise. Ce vénérable édifice était\n",
      "surmonté d'un toit triangulaire dont aucun modèle ne se verra bientôt\n",
      "plus à Paris. Cette couverture, tordue par les intempéries du climat\n",
      "parisien, s'avançait de trois pieds sur la rue, autant pour garantir des\n",
      "eaux pluviales le seuil de la porte, que pour abriter le mur d'un\n",
      "grenier et sa lucarne sans appui. Ce dernier étage était construit en\n",
      "planches clouées l'une sur l'autre comme de\n",
      "[65, 117, 32, 109, 105, 108, 105, 101, 117, 32, 100, 101, 32, 108, 97, 32, 114, 117, 101, 32, 83, 97, 105, 110, 116, 45, 68, 101, 110, 105, 115, 44, 32, 112, 114, 101, 115, 113, 117, 101, 32, 97, 117, 32, 99, 111, 105, 110, 32, 100, 101, 32, 108, 97, 32, 114, 117, 101, 32, 100, 117, 10, 80, 101, 116, 105, 116, 45, 76, 105, 111, 110, 44, 32, 101, 120, 105, 115, 116, 97, 105, 116, 32, 110, 97, 103, 117, 195, 168, 114, 101, 32, 117, 110, 101, 32, 100, 101, 32, 99, 101, 115, 32, 109, 97, 105, 115, 111, 110, 115, 32, 112, 114, 195, 169, 99, 105, 101, 117, 115, 101, 115, 32, 113, 117, 105, 32, 100, 111, 110, 110, 101, 110, 116, 10, 97, 117, 120, 32, 104, 105, 115, 116, 111, 114, 105, 101, 110, 115, 32, 108, 97, 32, 102, 97, 99, 105, 108, 105, 116, 195, 169, 32, 100, 101, 32, 114, 101, 99, 111, 110, 115, 116, 114, 117, 105, 114, 101, 32, 112, 97, 114, 32, 97, 110, 97, 108, 111, 103, 105, 101, 32, 108, 39, 97, 110, 99, 105, 101, 110, 32, 80, 97, 114, 105, 115, 46, 10, 76, 101, 115, 32, 109, 117, 114, 115, 32, 109, 101, 110, 97, 195, 167, 97, 110, 116, 115, 32, 100, 101, 32, 99, 101, 116, 116, 101, 32, 98, 105, 99, 111, 113, 117, 101, 32, 115, 101, 109, 98, 108, 97, 105, 101, 110, 116, 32, 97, 118, 111, 105, 114, 32, 195, 169, 116, 195, 169, 32, 98, 97, 114, 105, 111, 108, 195, 169, 115, 10, 100, 39, 104, 105, 195, 169, 114, 111, 103, 108, 121, 112, 104, 101, 115, 46, 32, 81, 117, 101, 108, 32, 97, 117, 116, 114, 101, 32, 110, 111, 109, 32, 108, 101, 32, 102, 108, 195, 162, 110, 101, 117, 114, 32, 112, 111, 117, 118, 97, 105, 116, 45, 105, 108, 32, 100, 111, 110, 110, 101, 114, 32, 97, 117, 120, 32, 88, 32, 101, 116, 32, 97, 117, 120, 10, 86, 32, 113, 117, 101, 32, 116, 114, 97, 195, 167, 97, 105, 101, 110, 116, 32, 115, 117, 114, 32, 108, 97, 32, 102, 97, 195, 167, 97, 100, 101, 32, 108, 101, 115, 32, 112, 105, 195, 168, 99, 101, 115, 32, 100, 101, 32, 98, 111, 105, 115, 32, 116, 114, 97, 110, 115, 118, 101, 114, 115, 97, 108, 101, 115, 32, 111, 117, 10, 100, 105, 97, 103, 111, 110, 97, 108, 101, 115, 32, 100, 101, 115, 115, 105, 110, 195, 169, 101, 115, 32, 100, 97, 110, 115, 32, 108, 101, 32, 98, 97, 100, 105, 103, 101, 111, 110, 32, 112, 97, 114, 32, 100, 101, 32, 112, 101, 116, 105, 116, 101, 115, 32, 108, 195, 169, 122, 97, 114, 100, 101, 115, 10, 112, 97, 114, 97, 108, 108, 195, 168, 108, 101, 115, 63, 32, 195, 137, 118, 105, 100, 101, 109, 109, 101, 110, 116, 44, 32, 97, 117, 32, 112, 97, 115, 115, 97, 103, 101, 32, 100, 101, 32, 116, 111, 117, 116, 101, 115, 32, 108, 101, 115, 32, 118, 111, 105, 116, 117, 114, 101, 115, 44, 32, 99, 104, 97, 99, 117, 110, 101, 32, 100, 101, 10, 99, 101, 115, 32, 115, 111, 108, 105, 118, 101, 115, 32, 115, 39, 97, 103, 105, 116, 97, 105, 116, 32, 100, 97, 110, 115, 32, 115, 97, 32, 109, 111, 114, 116, 97, 105, 115, 101, 46, 32, 67, 101, 32, 118, 195, 169, 110, 195, 169, 114, 97, 98, 108, 101, 32, 195, 169, 100, 105, 102, 105, 99, 101, 32, 195, 169, 116, 97, 105, 116, 10, 115, 117, 114, 109, 111, 110, 116, 195, 169, 32, 100, 39, 117, 110, 32, 116, 111, 105, 116, 32, 116, 114, 105, 97, 110, 103, 117, 108, 97, 105, 114, 101, 32, 100, 111, 110, 116, 32, 97, 117, 99, 117, 110, 32, 109, 111, 100, 195, 168, 108, 101, 32, 110, 101, 32, 115, 101, 32, 118, 101, 114, 114, 97, 32, 98, 105, 101, 110, 116, 195, 180, 116, 10, 112, 108, 117, 115, 32, 195, 160, 32, 80, 97, 114, 105, 115, 46, 32, 67, 101, 116, 116, 101, 32, 99, 111, 117, 118, 101, 114, 116, 117, 114, 101, 44, 32, 116, 111, 114, 100, 117, 101, 32, 112, 97, 114, 32, 108, 101, 115, 32, 105, 110, 116, 101, 109, 112, 195, 169, 114, 105, 101, 115, 32, 100, 117, 32, 99, 108, 105, 109, 97, 116, 10, 112, 97, 114, 105, 115, 105, 101, 110, 44, 32, 115, 39, 97, 118, 97, 110, 195, 167, 97, 105, 116, 32, 100, 101, 32, 116, 114, 111, 105, 115, 32, 112, 105, 101, 100, 115, 32, 115, 117, 114, 32, 108, 97, 32, 114, 117, 101, 44, 32, 97, 117, 116, 97, 110, 116, 32, 112, 111, 117, 114, 32, 103, 97, 114, 97, 110, 116, 105, 114, 32, 100, 101, 115, 10, 101, 97, 117, 120, 32, 112, 108, 117, 118, 105, 97, 108, 101, 115, 32, 108, 101, 32, 115, 101, 117, 105, 108, 32, 100, 101, 32, 108, 97, 32, 112, 111, 114, 116, 101, 44, 32, 113, 117, 101, 32, 112, 111, 117, 114, 32, 97, 98, 114, 105, 116, 101, 114, 32, 108, 101, 32, 109, 117, 114, 32, 100, 39, 117, 110, 10, 103, 114, 101, 110, 105, 101, 114, 32, 101, 116, 32, 115, 97, 32, 108, 117, 99, 97, 114, 110, 101, 32, 115, 97, 110, 115, 32, 97, 112, 112, 117, 105, 46, 32, 67, 101, 32, 100, 101, 114, 110, 105, 101, 114, 32, 195, 169, 116, 97, 103, 101, 32, 195, 169, 116, 97, 105, 116, 32, 99, 111, 110, 115, 116, 114, 117, 105, 116, 32, 101, 110, 10, 112, 108, 97, 110, 99, 104, 101, 115, 32, 99, 108, 111, 117, 195, 169]\n"
     ]
    }
   ],
   "source": [
    "with open('balzac.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()\n",
    "print(text[:1000])\n",
    "\n",
    "tokens = list(map(int, text.encode('utf-8')))\n",
    "print(list(tokens[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comptons maintenant les paires : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 5 paires les plus fréquentes :  [(5025, (101, 32)), (2954, (115, 32)), (2429, (32, 100)), (2332, (116, 32)), (2192, (101, 115))]\n",
      "La paire la plus fréquente est :  (101, 32)\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): \n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(\"Les 5 paires les plus fréquentes : \",sorted(((v,k) for k,v in stats.items()), reverse=True)[:5])\n",
    "\n",
    "top_pair = max(stats, key=stats.get)\n",
    "print(\"La paire la plus fréquente est : \", top_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons maintenant une fonction pour merge les paires les plus fréquentes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n",
      "taille du texte avant : 128987\n",
      "[65, 117, 32, 109, 105, 108, 105, 101, 117, 32, 100, 256, 108, 97, 32, 114, 117, 256, 83, 97, 105, 110, 116, 45, 68, 101, 110, 105, 115, 44, 32, 112, 114, 101, 115, 113, 117, 256, 97, 117, 32, 99, 111, 105, 110, 32, 100, 256, 108, 97, 32, 114, 117, 256, 100, 117, 10, 80, 101, 116, 105, 116, 45, 76, 105, 111, 110, 44, 32, 101, 120, 105, 115, 116, 97, 105, 116, 32, 110, 97, 103, 117, 195, 168, 114, 256, 117, 110, 256, 100, 256, 99, 101, 115, 32, 109, 97, 105, 115, 111]\n",
      "taille du texte après : 123962\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour fusionner les paires les plus fréquentes, on donne en entrée la liste des tokens, la paire à fusionner et le nouvel index\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # Si on est pas à la dernière position et que la paire correspond, on la remplace\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# Test de la fonction merge\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "\n",
    "print(\"taille du texte avant :\", len(tokens))\n",
    "# On fusionne la paire la plus fréquente et on lui donne un nouvel index (256 car on a déjà les caractères de 0 à 255)\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2[:100])\n",
    "print(\"taille du texte après :\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec une seule merge, on a déjà bien réduit la taille de l'encoding du texte.  \n",
    "Maintenant, on va définir la taille de vocabulaire que l'on veut et on va merger autant de fois que nécessaire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (115, 32) into a new token 257\n",
      "merging (116, 32) into a new token 258\n",
      "merging (195, 169) into a new token 259\n",
      "merging (101, 110) into a new token 260\n",
      "merging (97, 105) into a new token 261\n",
      "merging (44, 32) into a new token 262\n",
      "merging (111, 110) into a new token 263\n",
      "merging (101, 257) into a new token 264\n",
      "merging (111, 117) into a new token 265\n",
      "merging (114, 32) into a new token 266\n",
      "merging (97, 110) into a new token 267\n",
      "merging (113, 117) into a new token 268\n",
      "merging (100, 256) into a new token 269\n",
      "merging (97, 32) into a new token 270\n",
      "merging (101, 117) into a new token 271\n",
      "merging (101, 115) into a new token 272\n",
      "merging (108, 256) into a new token 273\n",
      "merging (105, 110) into a new token 274\n",
      "merging (46, 32) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276 # La taille du vocabulaire que l'on souhaite\n",
    "num_merges = vocab_size - 256\n",
    "tokens_merged=tokens\n",
    "\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(tokens_merged)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  tokens_merged = merge(tokens_merged, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant voir la différence entre les deux séquences de tokens : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de base: 128987\n",
      "Taille après merge: 98587\n",
      "compression ratio: 1.31X\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille de base:\", len(tokens))\n",
    "print(\"Taille après merge:\", len(tokens_merged))\n",
    "print(f\"compression ratio: {len(tokens) / len(tokens_merged):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien compressé la taille de la séquence alors que l'on a augmenté le vocabulaire de seulement 20.\n",
    "GPT-2 augmente le vocabulaire de 50 000 donc vous imaginez bien que ça réduit drastiquement la taille des séquences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding/Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que l'on a construit notre tokenizer, on veut pouvoir passer des entiers (tokens) à notre texte et inversement.  \n",
    "\n",
    "Pour cela, construisons d'abord la fonction de *decoding* : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "# Fonction pour décoder les ids en texte, prend en entrée une liste d'entiers et retourne une chaine de caractères\n",
    "def decode(ids):\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\") # errors=\"replace\" permet de remplacer les caractères non reconnus par le caractére spécial �\n",
    "  return text\n",
    "\n",
    "print(decode([87]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et la fonction d'*encoding* : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 263, 106, 265, 114]\n",
      "Bonjour\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour encoder le texte en ids, prend en entrée une chaine de caractères et retourne une liste d'entiers \n",
    "def encode(text):\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break \n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"Bonjour\"))\n",
    "\n",
    "# On eut véifier que l'encodage et le décodage fonctionne correctement\n",
    "print(decode(encode(\"Bonjour\")))\n",
    "\n",
    "# Et sur le text en entier\n",
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La série des GPT utilise des *regex patterns* pour séparer le texte avant de créer le vocabulaire. Cela va permettre d'avoir plus de controle sur le type de tokens qui vont être générés (on va par exemple éviter d'avoir différents tokens pour \"chien\", \"chien!\" et \"chien?\").   \n",
    "Dans le code source de Tiktoken (tokenizer de GPT), on peut retrouver le *pattern* suivant **'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+**.   \n",
    " La syntaxe est assez compliquée mais nous allons le décomposer petit à petit pour comprendre ce que ça fait : \n",
    "- **'s|'t|'re|'ve|'m|'ll|'d**  : Correspond aux contractions anglaises comme \"is\", \"it\", \"are\", \"have\", \"am\", \"will\" et \"had\". Ces tokens sont souvent importants à isoler dans le traitement du langage naturel.\n",
    "- **?\\p{L}+** : Correspond aux mots constitués de lettres. Le \"?\" en début signifie que le mot peut être précédé d'un espace, ce qui permet de capturer des mots avec ou sans espace initial.\n",
    "- **?\\p{N}+** : Correspond aux séquences de chiffres (nombres). De la même manière que précédemment, un espace optionnel peut précéder la séquence de chiffres.\n",
    "-  **?[^\\s\\p{L}\\p{N}]+** : Correspond à un ou plusieurs caractères qui ne sont ni des espaces, ni des lettres, ni des chiffres. Cela capture des symboles et des ponctuations, avec un espace optionnel au début.\n",
    "- **\\s+(?!\\S)** : Correspond à un ou plusieurs espaces suivies uniquement d'espaces (donc une séquence d'espaces en fin de chaîne ou avant une rupture de ligne).\n",
    "- **\\s+** : Correspond à une ou plusieurs espaces. C'est une correspondance générique pour les espaces multiples entre les mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le texte a été séparé avec les conditions décrites dans le *regex pattern*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des tokens spéciaux sont également ajouté pour l'entraînement et pour le *finetuning* :\n",
    "- **<|endoftext|>** : Ce token est utilisé pour délimiter la séparation entre des documents différents dans les données d'entrainement.\n",
    "- **<|im_start|>** et **<|im_end|>** : Ces tokens délimitent le début et la fin d'un message de l'utilisateur pour un chatbot par exemple.\n",
    "\n",
    "**Note** : Lors du finetuning, il est possible d'ajouter des tokens au tokenizer (comme **<|im_start|>** et **<|im_end|>** par exemple) spécifiques à la tâche que l'on souhaite réaliser. Bien sur, cela demandera de modifier la matrice d'embedding et de la réentrainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autres types de tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tokenizer que nous avons implementé se calque sur le tokenizer [tiktoken](https://github.com/openai/tiktoken) de OpenAI qui est utilisé sur les modèles GPT. Un autre tokenizer répandu est [sentencepiece](https://github.com/google/sentencepiece) qui est utilisé sur les modèles de google et de meta par exemple.\n",
    "\n",
    "**Note** : Sentencepiece est bien plus complexe que tiktoken et dispose d'énormement de paramètres à régler. En pratique, il est utilisé sans doute parce que le code est open source (alors que le code d'entrainement de tiktoken n'est pas open-source, on a juste accès au code pour encoder et décoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization sur d'autres modalités ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'on va vouloir faire du traitement *multimodal* (c'est à la mode en ce moment), il va falloir produire des tokens à partir de modalités différentes du texte comme du son ou des images.  \n",
    "Ce qu'on voudrait idéalement c'est transformer notre son ou image en tokens et ensuite les donner au transformer comme s'il s'agissait de texte.  \n",
    "\n",
    "Pour les images, on peut faire ça avec un [VQVAE](https://arxiv.org/pdf/1711.00937) ou un [VQGAN](https://arxiv.org/pdf/2012.09841). L'idée est d'utiliser un VAE ou GAN pour générer des valeurs discrètes dans un espace latent. Ces valeurs discrètes vont alors être utilisées comme tokens.  \n",
    "\n",
    "<img src=\"images/VQGAN.png\" alt=\"VQGAN\" width=\"800\"/>\n",
    "\n",
    "Figure extraite de l'[article](https://arxiv.org/pdf/2012.09841).\n",
    "\n",
    "Le modèle SORA de OpenAi fait un peu la même chose mais sur des videos :\n",
    "\n",
    "<img src=\"images/SORA.png\" alt=\"SORA\" width=\"800\"/>\n",
    "\n",
    "Figure extraite de l'[article](https://arxiv.org/pdf/2402.17177)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réponses aux questions du début"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant pouvoir répondre aux questions du début du cours à l'aide de ce que nous avons appris : \n",
    "- **Pourquoi les LLM ont du mal a épeler les mots ?**   \n",
    "La séparation en tokens fait que chaque mot n'est pas séparé en tous ses caractères mais plutôt en *chunks* de caractère. Il est alors compliqué pour le modèle de les décomposer.\n",
    "\n",
    "- **Pourquoi les LLM ont du mal à faire des opérations simples sur les string (comme inverser un string) ?**   \n",
    "C'est à peu près pour la même raison que la question précédente. Pour inverser un mot, il ne suffit pas d'inverser les tokens representant ce mot. \n",
    "\n",
    "\n",
    "- **Pourquoi les LLM sont meilleurs sur l'anglais ?**    \n",
    "Il y a plusieurs raisons à cela : les données d'entraînement du transformer et les données d'entraînement du tokenizer. Pour le transformer, plus de données en anglais va lui permettre d'apprendre mieux la langue et ses subtilités. Pour le tokenizer, si on l'entraine sur des données en anglais, les tokens générés vont principalement être adapté pour des mots anglais donc on aura besoin de moins de contexte que pour les autres langues. \n",
    "\n",
    "\n",
    "- **Pourquoi les LLM sont mauvais en arithmétique ?**    \n",
    "Les nombres sont representés assez arbitrairement en fonction des données d'entraînement. Réaliser des opérations sur ces tokens n'est pas une chose aisée pour le LLM.\n",
    "\n",
    "- **Pourquoi GPT-2 n'est pas très bon en python ?**    \n",
    "Comme nous l'avons vu dans ce cours, le tokenizer de GPT-2 transforme un simple espace en un token. En python, avec l'indentation et de multiples conditions/boucles, il y a rapidement beaucoup d'espace et cela impact fortement le contexte.\n",
    "\n",
    "- **Pourquoi mon LLM s'arrête directement si je lui envoie le string \"<endoftext>\" ?**   \n",
    "Il s'agit d'un token spécial ajouté dans les données d'entraînement pour séparer le texte. Lorsque le LLM le rencontre, il doit arrêter sa génération.\n",
    "\n",
    "- **Pourquoi le LLM se casse quand je lui parle de SolidGoldMagiKarp ?**    \n",
    "Celle-ci est un peu moins évidente et je vous conseille de lire l'excellent [blogpost](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation). En expliquant simplement, si des mots sont présents dans les données d'entrainement du tokenizer mais pas dans les données d'entrainement du LLM, alors l'embedding de ce token ne sera pas du tout entrainé et le LLM va se comporter de manière aléatoire lorsqu'il rencontre ce token. SolidGoldMagiKarp est un utilisateur reddit qui devait apparaitre régulièrement dans les données d'entrainement du tokenizer mais pas dans les données d'entrainement du transformer.   \n",
    "\n",
    "- **Pourquoi il faut mieux utiliser YAML plutôt que JSON avec les LLM ?**    \n",
    "C'est un peu la même idée que pour python. Le tokenizer de GPT-2 (et de la plupart des modèles à vrai dire) transforme un document json en plus de tokens que son équivalent yaml. Passer de json à yaml va donc réduire le contexte nécessaire pour traiter le document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}