# 🏗️ Fondations 🏗️ 
Ce cours introduit introduit les bases de l'optimisation par descente du gradient avec une compréhension intuitive. La règle de la chaîne est introduite puis un premier exemple de regression logistique est présenté. 

## Notebook 1️⃣ : [Dérivées de descente du gradient](01_DérivéesEtDescenteDuGradient.ipynb)
Ce notebook propose des rappels sur la dérivée d'une fonction pour ensuite introduire l'algorithme de descente du gradient et la règle de la chaîne.

## Notebook 2️⃣ : [Régression logistique](02_RégressionLogistique.ipynb)
Ce notebook est dédié à une explication et une implémentation de la reégression logistique. Les fonctions d'activation et les fonctions de coûts (*loss*) sont également introduites.