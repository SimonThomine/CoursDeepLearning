{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construisons GPT à partir de rien "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook va présenter la création, à partir de zéro, d'un modèle de langage pour prédire le prochain caractère qui se base sur l'architecture du *transformer* (décodeur en particulier).  \n",
    "Pour cela, nous utilons un fichier texte moliere.txt qui regroupe l'intégralité des dialogues des pièces de Molière.   \n",
    "Ce dataset a été crée à partir des oeuvres complètes de Molière disponibles sur le site [Gutenberg.org](https://www.gutenberg.org/). J'ai nettoyé un peu les données pour ne garder que les dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Pour utiliser le GPU automatiquement si vous en avez un \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par ouvrir et par visualiser un peu ce que contient notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons les 250 premiers caractères : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALÈRE.\n",
      "\n",
      "Eh bien, Sabine, quel conseil me donnes-tu?\n",
      "\n",
      "SABINE.\n",
      "\n",
      "Vraiment, il y a bien des nouvelles. Mon oncle veut résolûment que ma\n",
      "cousine épouse Villebrequin, et les affaires sont tellement avancées,\n",
      "que je crois qu'ils eussent été mariés dès aujo\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons *set()* pour récuperer les caractères uniques présent dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijlmnopqrstuvxyz«»ÇÈÉÊÏàâæçèéêëìîïòôùûŒœ\n",
      "Nombre de caractères différents :  85\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de notre dataset d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans le cours 5, nous allons créer un *mapping* pour passer de caractères à entier. Le *mapping* que nous faisons ici est une forme de *tokenization* la plus simple possible.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point rapide sur la tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La tokenization, qu'est ce que c'est ?** : La *tokenization* est le processus de conversion d'un texte en séquence d'entier où chaque entier peut correspondre à un caractère, un groupe de caractère ou un mot selon les méthodes employées.   \n",
    "\n",
    "**Balance entre Vocabulaire et taille de séquence** : Un bon *tokenizer* trouve une balance entre la taille du vocabulaire (26 pour toutes les lettres de l'alphabet et ~100 000 pour les nombre de mots de la langue française). Plus on a une taille de vocabulaire petite, plus les séquences seront longues (le mot \"Bonjour\" est encodé par 7 *tokens* si notre vocabulaire est au niveau du caractère et un seul *token* si notre vocabulaire regroupe tous les mots de la langue française) et inversement. En pratique, les deux extrèmes sont problématiques et on cherche le juste milieu.  \n",
    "\n",
    "**Tokenizer de la littérature** : Les *tokenizers* sont une part importante du bon fonctionnement d'un modèle de langage. \n",
    "La façon de créer un bon *tokenizer* dépend de la méthode et des données d'entraînement. Parmi les *tokenizers* les plus utilisés, on retrouve [SentencePiece](https://github.com/google/sentencepiece) de Google et [tiktoken](https://github.com/openai/tiktoken) de OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 50, 49, 46, 50, 56, 53, 1, 68, 1, 55, 50, 56, 54]\n",
      "Bonjour à Tous\n"
     ]
    }
   ],
   "source": [
    "# Creation d'un mapping de caractère à entiers et inversement\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "\n",
    "print(encode(\"Bonjour à tous\"))\n",
    "print(decode(encode(\"Bonjour à Tous\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va transformer notre dataset en séquence d'entier et le stocker sous forme de tenseur pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33, 12, 23, 64, 29, 16,  8,  0,  0, 16, 44,  1, 38, 45, 41, 49,  6,  1,\n",
      "        30, 37, 38, 45, 49, 41,  6,  1, 52, 56, 41, 47,  1, 39, 50, 49, 54, 41,\n",
      "        45, 47,  1, 48, 41,  1, 40, 50, 49, 49, 41, 54,  7, 55, 56, 11,  0,  0,\n",
      "        30, 12, 13, 20, 25, 16,  8,  0,  0, 33, 53, 37, 45, 48, 41, 49, 55,  6,\n",
      "         1, 45, 47,  1, 59,  1, 37,  1, 38, 45, 41, 49,  1, 40, 41, 54,  1, 49,\n",
      "        50, 56, 57, 41, 47, 47, 41, 54,  8,  1, 24, 50, 49,  1, 50, 49, 39, 47,\n",
      "        41,  1, 57, 41, 56, 55,  1, 53, 73, 54, 50, 47, 82, 48, 41, 49, 55,  1,\n",
      "        52, 56, 41,  1, 48, 37,  0, 39, 50, 56, 54, 45, 49, 41,  1, 73, 51, 50,\n",
      "        56, 54, 41,  1, 33, 45, 47, 47, 41, 38, 53, 41, 52, 56, 45, 49,  6,  1,\n",
      "        41, 55,  1, 47, 41, 54,  1, 37, 42, 42, 37, 45, 53, 41, 54,  1, 54, 50,\n",
      "        49, 55,  1, 55, 41, 47, 47, 41, 48, 41, 49, 55,  1, 37, 57, 37, 49, 39,\n",
      "        73, 41, 54,  6,  0, 52, 56, 41,  1, 46, 41,  1, 39, 53, 50, 45, 54,  1,\n",
      "        52, 56,  3, 45, 47, 54,  1, 41, 56, 54, 54, 41, 49, 55,  1, 73, 55, 73,\n",
      "         1, 48, 37, 53, 45, 73, 54,  1, 40, 72, 54,  1, 37, 56, 46, 50])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:250]) # Les 250 premiers caractères encodé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant découper notre texte en une partie training et une partie validation. Prenons un ratio de 0.9-0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour la validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour notre modèle de langage, on va également définir une taille de contexte *block_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33, 12, 23, 64, 29, 16,  8,  0,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, les 8 premiers caractères represente le contexte et le 9ème est le label. Ce simple exemple regroupe en fait une multitude d'exemples car notre modèle doit être capable de prédire le prochain caractère peu importe le contexte qu'il a en amont. Dans cette liste, on a donc 8 exemples qui sont les suivants :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quand l'entrée est [33] le label est : 12\n",
      "Quand l'entrée est [33 12] le label est : 23\n",
      "Quand l'entrée est [33 12 23] le label est : 64\n",
      "Quand l'entrée est [33 12 23 64] le label est : 29\n",
      "Quand l'entrée est [33 12 23 64 29] le label est : 16\n",
      "Quand l'entrée est [33 12 23 64 29 16] le label est : 8\n",
      "Quand l'entrée est [33 12 23 64 29 16  8] le label est : 0\n",
      "Quand l'entrée est [33 12 23 64 29 16  8  0] le label est : 0\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Quand l'entrée est {context.numpy()} le label est : {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sait maintenant comme créer un ensemble de entrée/label à partir d'un seul exemple.   \n",
    "Adaptons cette méthode pour un traitement en *batch* : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrée : \n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 69, 39, 41,  2,  0,  0, 27],\n",
      "        [53,  1, 56, 49,  1, 39, 84, 56],\n",
      "        [54, 11,  0,  0, 24, 12, 30, 14],\n",
      "        [ 1, 51, 72, 53, 41,  8,  0,  0]], device='cuda:0')\n",
      "Labels :\n",
      "torch.Size([4, 8])\n",
      "tensor([[69, 39, 41,  2,  0,  0, 27, 19],\n",
      "        [ 1, 56, 49,  1, 39, 84, 56, 53],\n",
      "        [11,  0,  0, 24, 12, 30, 14, 12],\n",
      "        [51, 72, 53, 41,  8,  0,  0, 33]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # La taille de batch (les séquences calculés en parallèles)\n",
    "block_size = 8 # La taille de contexte maximale pour une prédiction du modèle\n",
    "\n",
    "def get_batch(split):\n",
    "    # On genere un batch de données (sur train ou val)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    #On génére batch_size indice de début de séquence pris au hasard dans le dataset\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # On stocke dans notre tenseur torch\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # On met les sur le GPU si on en a un \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('Entrée : ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('Labels :')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chacun de ces 4 exemples regroupe 8 exemples distincts (comme expliqué précedemment), cela fait donc un total de 32 exemples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle bigramme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cours 5 sur les NLP, nous avons vu le bigramme qui peut être considéré comme le modèle de langage le plus simple et qui consiste à prédire la prochain caractère à partir d'un unique caractère de contexte. Notons $B$ pour le *batch_size*, $T$ pour le *block_size* et $C$ pour le *vocab_size*.    \n",
    "Pour voir sa performance sur le dataset moliere.txt, implémentons le rapidement en pytorch :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 85])\n",
      "tensor(4.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # Chaque token va directement lire la valeur du prochain à partir d'une look-up table entrainé\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # Taille (B,T)\n",
    "    logits = self.token_embedding_table(idx) \n",
    "    # Taille (B,T,C)\n",
    "    \n",
    "    # Pour gérer le cas de la génération (pas de target)\n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else: # Cas de l'entraînement\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx est de la taille (B,T) avec T le contexte actuel\n",
    "    for _ in range(max_new_tokens):\n",
    "      # Forward du modèle pour récuperer les prédictions\n",
    "      logits, _ = self(idx)\n",
    "      # On prend uniquement le dernier caractère\n",
    "      logits = logits[:, -1, :] # devient (B, C)\n",
    "      # On applique la softmax pour récuperer les probabilités\n",
    "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "      # On sample avec torch.multinomial\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # devient (B, 1)\n",
    "      # On ajouter l'élément sample à la séquence actuelle\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle est implementé mais non entraîné, si on le teste comme ça on obtient des résultats catastrophiques : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CZjb!DzPGŒR?'hô.ù\n",
      "cddhhf,séÇqmp.ÉMjôCùÊF:TAFYèL  àP;zbVmëtuPipL.ôHtSEé,t:æéÉYÈìïë?VGYxoùyçnï'lpôHà!ô\n"
     ]
    }
   ],
   "source": [
    "base=torch.zeros((1, 1), dtype=torch.long).to(device) # Le premier élément est un 0 (token de retour à la ligne)\n",
    "# On génère 100 éléments\n",
    "print(decode(m.generate(idx = base , max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est tout simplement aléatoire et c'est logique car le modèle est initialisé aléatoirement. \n",
    " \n",
    "On va maintenant entrainer le modèle :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2493152618408203\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "steps=10000\n",
    "for step in range(steps): # Nombre d'étape d'entraînement (élements traités = steps*batch_size)\n",
    "\n",
    "    # On récupère un batch de données aléatoires\n",
    "    xb, yb = get_batch('train')\n",
    "    # On calcule le loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Retropropagation\n",
    "    loss.backward()\n",
    "    # Mise à jour des poids du modèle\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Générons à partir de notre modèle entrainé : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "ELASGOXûÏï!\n",
      "ANDann donde se ns ntrar pous fa àTEn!.\n",
      "\n",
      "TELITEL'enomouvûûKbeue\n",
      "SGAvore oue mesontre\n",
      "t de pou n qur quvabou qude dente je père e em'eni\n",
      "\n",
      "La d'euhèmpon, j'es en paiqus de rau plenoilà jonont DARLysontausqus es ei voisangur s ve.\n",
      "\n",
      "\n",
      "\n",
      "DO lar dire tré quseuqu'arme à ai? t pe ne ndome l pa, \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate une amélioration dans la structuration des données et certains mots semblent presque correct mais ça reste catastrophique. En soit, on s'attendait à ce résultat car le bigramme est un modèle trop simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant présenter pas à pas le concept de *self-attention* qui est un concept clé de l'architecture d'un transformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est ce que que l'on veut faire ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va commencer par une idée simple. On a un tenseur de taille $(B,T,C)$, on veut que chaque élément T soit la moyenne de l'élément actuel et des éléments précédents mais sans tenir compte des éléments suivants. C'est la façon la plus triviale de donner une importance aux éléments précédents pour prédire la valeur actuelle (ce qui est l'idée derrière le mécanisme d'attention)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En python, on peut implémenter l'idée de cette manière : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création de notre tenseur random\n",
    "B,T,C = 4,4,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5023, -0.5911],\n",
      "        [ 1.0199, -0.2976],\n",
      "        [-1.7581,  0.0969],\n",
      "        [ 0.7444, -0.3360]])\n",
      "tensor([[ 1.5023, -0.5911],\n",
      "        [ 1.2611, -0.4443],\n",
      "        [ 0.2547, -0.2639],\n",
      "        [ 0.3771, -0.2819]])\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne des éléments précédents (incluant l'élément actuel) pour chaque valeur.\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien ce qu'on voulait, si vous faites les calculs chaque élément correspond aux à la moyenne de l'élément actuel avec les éléments précédents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par contre, on sait que les boucles for sont inefficaces lors du calcul. On voudrait plutôt une opération matricielle pour effectuer la même opération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappel sur la multiplication entre deux matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication Matricielle : Matrice $(3 \\times 3)$ par Matrice $(3 \\times 2)$\n",
    "Matrices de départ\n",
    "\n",
    "Soit la matrice $A$ de dimensions $(3 \\times 3)$ :\n",
    "\n",
    "$A =\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "et la matrice $B$ de dimensions $(3 \\times 2)$ :\n",
    "\n",
    "$B =\n",
    "\\begin{pmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "b_{31} & b_{32}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "La multiplication matricielle $C = A \\times B$ donne une matrice $C$ de dimensions $(3 \\times 2)$ :\n",
    "\n",
    "$C =\n",
    "\\begin{pmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22} \\\\\n",
    "c_{31} & c_{32}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "où chaque élément $c_{ij}$ est calculé comme suit :\n",
    "\n",
    "$c_{ij} = \\sum_{k=1}^{3} a_{ik} \\cdot b_{kj}$\n",
    "\n",
    "C'est-à-dire :\n",
    "\n",
    "- $c_{11} = a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}$\n",
    "- $c_{12} = a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32}$\n",
    "- $c_{21} = a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31}$\n",
    "- $c_{22} = a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}$\n",
    "- $c_{31} = a_{31}b_{11} + a_{32}b_{21} + a_{33}b_{31}$\n",
    "- $c_{32} = a_{31}b_{12} + a_{32}b_{22} + a_{33}b_{32}$\n",
    "\n",
    "Voici un exemple en python qui illustre cela : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[7., 6.],\n",
      "        [5., 0.],\n",
      "        [1., 8.]])\n",
      "--\n",
      "c=\n",
      "tensor([[13., 14.],\n",
      "        [13., 14.],\n",
      "        [13., 14.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'astuce mathématique pour le self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est maitenant que la magie opére. Lorsque, au lieu d'une matrice de 1, on prend une matrice triangulaire inférieure et qu'on refait le calcul : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[1., 2.],\n",
      "        [1., 4.],\n",
      "        [6., 6.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 2.,  6.],\n",
      "        [ 8., 12.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque valeur de la matrice est la somme de la valeur actuelle et des valeurs précédentes. C'est presque ce que l'on veut ! Il suffit alors de normaliser selon les lignes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[1., 2.],\n",
      "        [8., 6.],\n",
      "        [9., 8.]])\n",
      "--\n",
      "c=\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [4.5000, 4.0000],\n",
      "        [6.0000, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà, le tour est joué ! On a remplacé notre double boucle for par une simple multiplication matricielle et une normalisation des valeurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant l'utiliser pour calculer *xbow* et comparer sa valeur avec la valeur que l'on avait calculé avec notre double boucle : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C) fonctionne grâce au broadcasting de pytorch\n",
    "torch.allclose(xbow, xbow2) # Vérifie que tous les éléments sont identiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la place de la normalisation, on peut utiliser la fonction *softmax*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "# On met toutes les valeurs égales à 0 à la valeur -inf\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant appliquer la *softmax* sur la matrice et TADAAA : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, la version avec *softmax* est utilisée pour la couche *self-attention*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention : le coeur du transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actuellement, la matrice $wei$ contient des valeurs uniformes sur chaque ligne ce qui ne donne aucune réelle information sur l'importance des informations précédente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est là que le concept de *self-attention* intervient. Ce qu'on voudrait, c'est une matrice $wei$ que l'on peut entraîner. \n",
    "\n",
    "On va créer 3 valeurs à partir de notre valeur de $x$ :  \n",
    "\n",
    "**query** : *Qu'est ce que je recherche ?* Cette valeur représente ce que chaque position de la séquence essaye de trouver dans les autres positions.\n",
    "\n",
    "**key** : *Qu'est ce que je contiens ?* Cette valeur représente ce que chaque position de la séquence contient comme information qui pourrait être pertinente pour d'autres positions.   \n",
    "\n",
    "**value** : *Quelle est ma valeur ?* Cette valeur représente l'information réelle à extraire de chaque position de la séquence si elle est jugée pertinente.    \n",
    "\n",
    "Pour extraire les valeurs *query*, *key* et *value*, on utilise une couche linéaire qui projette l'entrée dans une dimension *head_size*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer l'importance d'un éléments précédent de la séquence par rapport à un l'élément actuel. On effectue le produit scalaire entre les *query* Q et les *key* K(tranposée) :   \n",
    "$wei = QK^T$  \n",
    "\n",
    "Pour obtenir des poids d'attention (somme égale à 1), on applique la *softmax* et on multiplie par les *value* V :   \n",
    "$Output = \\text{softmax}\\left(wei\\right) \\cdot V$   \n",
    "\n",
    "<img src=\"images/attention2.png\" alt=\"transformer\" width=\"150\"/>   \n",
    "\n",
    "En python, on l'implémente de cette manière :   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "\n",
    "head_size = 16 # Valeur de head_size (projection de x)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # Pour appliquer le softmax, il faut des valeurs -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre matrice $wei$ est donc maintenant entièrement entraînable et il est donc possible d'utiliser cette couche pour l'entraînement d'un réseau de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes sur la couche *self-attention*** :  \n",
    "- L'attention est un mécanisme de communication qui peut être vu comme un graphe avec des connexions entre les noeuds (dans notre cas, les noeuds de fin sont connectés à l'ensemble des noeuds précédents).  \n",
    "- Dans la couche d'attention, il n'y a aucune notion de la position des éléments les uns par rapport aux autres. Pour combler ce problème, il faudra rajouter un *positionnal_embedding* (voir suite du cours).   \n",
    "- Pour précision, il n'y a aucune interaction le long de la dimension *batch*, chaque élément du *batch* est traité indépendamment des autres. C'est un peu comme si on avait *batch_size* graphes indépendants.   \n",
    "- Ce *block* d'attention est appelé *decoder block*. Il a la particularité que chaque élément ne communique qu'avec le passé (grâce à la matrice triangulaire inférieure). Cependant, il existe d'autres couches d'attention (*encoder*) qui permettent la communication de tous les éléments les uns avec les autres (pour la traduction, l'analyse de sentiments ou encore le traitement d'images)  \n",
    "- On parle de *self-attention* parce que les *query*, *key* and *value* viennent de la même source. Il est possible d'avoir des *query*, *key* et *value* qui proviennent de sources différentes, on parle alors de *cross-attention*.   \n",
    "- Si vous lisez le papier [Attention is all you need](https://arxiv.org/pdf/1706.03762), vous constaterez qu'il y a une normalisation par la racine de la *head_size* :   \n",
    "<img src=\"images/attention.png\" alt=\"transformer\" width=\"300\"/>   \n",
    "Cela permet une stabilité de la fonction *softmax* lors de l'initialisation des poids en particulier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentons maintenant une classe *head* qui va effectuer les opération de la *self-attention*. C'est simplement ce que l'on a vu au dessus sous forme de classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "\n",
    "    def __init__(self, head_size,n_embd,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Ajout de dropout pour la regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la papier [Attention is all you need](https://arxiv.org/pdf/1706.03762), une variante de la *self-attention* est proposée. Cette variante se nomme *multi-head attention* et consiste simplement à avoir plusieurs couches de *self-attention* en parallèle. Le but de cette couche est de paralléliser le traitement pour que celui-ci soit plus rapide sur GPU. \n",
    "\n",
    "<img src=\"images/multihead.png\" alt=\"transformer\" width=\"200\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'implémentation est assez simple puisqu'il s'agit juste de plusieurs couches *head*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head de taille head_size\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Couche pour Linear (voir schema) après concatenation\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # Dropout si besoin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un dernier élément du *transformer* que l'on peut voir dans le papier [Attention is all you need](https://arxiv.org/pdf/1706.03762) est la couche *Feed Forward* qui est simplement un petit fully connected network.  \n",
    "\n",
    "On l'implémente en python comme cela :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 4*n_embd comme dans le papier\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couche transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a maitenant tous les éléments pour implémenter notre couche *transformer* qui va utiliser *multi-head attention* et *feed forward*. Sur la figure principale du papier, on remarque également qu'il a des connexions résiduelles entre l'*input* et l'*output* des couches d'*attention* et de *feed forward*. Ces connexions permettent de faciliter l'entraînement d'un modèle profond (plus de détails dans la papier [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)). On va donc également implémenter ces connexions résiduelles. Pour ce qui est de la *layer norm*, nous n'allons pas entrer dans les détails ici mais on peut comparer son utilité à une couche de *batch norm* (plus de détails dans ce [blogpost](https://medium.com/@hunter-j-phillips/layer-normalization-e9ae93eb3c9c)). Nous utilisons donc simplement l'implémentation pytorch de la [layer norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).   \n",
    "\n",
    "Voici l'implémentation python :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Block transformer\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # x+ car c'est une connexion résiduelle\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : On applique la *layer norm* avant les couches (contrairement au papier). C'est la seule partie du *transformer* qui a été modifiée depuis la publication du papier et qui améliorer les performances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour plus de clarté, nous allons créer notre modèle et l'optimiser dans le notebook suivant. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
