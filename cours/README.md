<p align="center">
  <h1><center> 	üöÄ Apprendre le Deep Learning √† partir de z√©ro üöÄ</h1>
</p>

<img src="images/banner_comic.webp" alt="banner" width="800" style="display: block; margin-left: auto; margin-right: auto;" />

## üìö Description

Ce repository propose des cours d'initiation au deep learning se basant sur des notebooks.
Pour un d√©butant, les cours sont √† faire dans l'ordre pour une meilleur compr√©hension globale. 

Un site internet du cours est disponible pour naviguer plus facilement : [**üåê Website üåê**](https://simonthomine.github.io/CoursDeepLearning/)

### üõ†Ô∏è Installation de l'environnement de travail 
L'ensemble des library n√©cessaires pour le cours sont disponibles dans requirements.txt, vous pouvez choisir d'installer tout d'un coup ou au fur et √† mesure de votre avancement dans le cours.   
Il est conseill√© d'utiliser un environnement de travail conda pour √©viter tout conflit avec des library d√©j√† install√© sur votre ordinateur.  

```
`pip install -r requirements.txt`
```

## üó∫Ô∏è Plan du cours
### 1. üèóÔ∏è [Fondations](./01_Fondations/README.md)
Le premier cours "Fondations" introduit les bases de l'optimisation par descente du gradient avec une compr√©hension intuitive. La r√®gle de la cha√Æne est introduite puis un premier exemple de regression logistique est pr√©sent√©. 

### 2. üß† [R√©seau Fully Connected](./02_R√©seauFullyConnected/README.md)
Le deuxi√®me cours "R√©seauFullyConnected" introduit le fonctionnement d'un r√©seau de neurones avec d'abord un exemple d'un r√©seau cod√© avec [micrograd](https://github.com/karpathy/micrograd/tree/master) pour permettre d'explorer cette library pour bien comprendre le fonctionnement. Une version fran√ßaise [MicrogradFR](./02_R√©seauFullyConnected/MicrogradFR/README.md) est disponible dans le cours.   
Ensuite, pour introduire la library pytorch, le m√™me exemple est reconstruit mais en utilisant pytorch au lieu de micrograd.  
Le dernier notebook de cette partie introduit des techniques avanc√©es d'entra√Ænement de r√©seau de neurones qu'il est utile de conna√Ætre pour am√©liorer les performances de nos r√©seaux. 

### 3. üñºÔ∏è [R√©seaux convolutifs](./03_R√©seauConvolutifs/README.md)
Le troisi√®me cours "R√©seauConvolutifs" aborde tout d'abord le principe de fonctionnement des couches de convolution puis montre comment on les utilise au sein d'un r√©seau de neurones. Plusieurs exemples sont ensuite abord√©s pour montrer les capacit√©s d'un r√©seau convolutif : classification sur MNIST, classification sur CIFAR-10 et segmentation sur "Oxford-IIIT Pet Dataset". 

### 4. üîÑ [Autoencodeurs](./04_Autoencodeurs/README.md)
Le quatri√®me cours "Autoencodeurs" aborde la notion d'entra√Ænement non supervis√© en pr√©sentant les diff√©rences entre supervis√© et non supervis√©. L'exemple de l'autoencodeur est ensuite abord√© ainsi que son application pour la d√©tection d'anomalies non supervis√©e. Pour finir, un notebook montre le potentiel de l'autoencodeur pour le probl√®me du "denoising". 

### 5. üó®Ô∏è [NLP](./05_NLP/README.md)
Le cinqui√®me cours "NLP" est grandement inspir√© de la s√©rie de vid√©o de Andrej Karpathy ["Building makemore"](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) qui tra√Æte les NLP avec une approche de pr√©diction du prochain token. Le cours aborde d'abord des mod√®les tr√®s simples pour avoir une intuition sur le tra√Ætement de donn√©es discr√®tes avec un r√©seau neurones puis les mod√®les se complexifient petit √† petit. 

### 6. ü§ó [Hugging Face](./06_HuggingFace/README.md)
Le sixi√®me cours "HuggingFace" est d√©di√© √† une exploration des librarys, des mod√®les, des datasets et autres de [Hugging Face](https://huggingface.co/). C'est une plateforme regroupant √©normement des mod√®les open source pour une grande vari√©t√© de t√¢ches avec une library pour les impl√©menter rapidement et efficacement en python. Le cours pr√©sente d'abord le site de Hugging Face pour ensuite pr√©senter les fonctionnalit√©s des diff√©rentes librarys (transformers et diffusers principalement) sur diff√©rents cas d'usage. Le dernier notebook pr√©sente bri√®vement gradio, une library pour cr√©er des interfaces simples de d√©mo.

### 7. ü§ñ [Transformers](./07_Transformers/README.md)
Le septi√®me cours "Transformers" est d√©di√© √† l'architecture du transformers. Apr√®s avoir vu ses applications dans le cours pr√©c√©dent. Nous allons entrer dans le d√©tail de l'architecture pour en comprendre les m√©canismes. Le premier notebook est grandement inspir√© de la vid√©o [Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1806s&ab_channel=AndrejKarpathy) de Andrej Karpathy et propose une impl√©mentation pas √† pas d'un encodeur transformers. Le but de ce notebook sera de cr√©er un mod√®le capable de g√©n√©rer du "Moli√®re" automatiquement. La seconde partie est une approche plus math√©matique et la pr√©sentation de la partie encodeur du transformers. La troisi√®me partie pr√©sente des architectures de mod√®le reposant sur la couche transformers pour de nombreux cas d'applications (Vision, traduction etc ...). Enfin, une derni√®re partie propose une impl√©mentation du vision transformer √† partir de l'article original.

### 8. üîç [Detection](./08_DetectionEtYolo/README.md)
Le huiti√®me cours "Detection" pr√©sente le fonctionnement de la d√©tection d'objets sur des images. L'introduction pr√©sente ce qu'est la d√©tection et les deux m√©thodes classiques (two-stage et one-stage). Le notebook suivant propose une description pr√©cise du fonctionnement de [YOLO](https://arxiv.org/pdf/1506.02640) et le dernier notebook pr√©sente la library [ultralytics](https://www.ultralytics.com/) qui permet d'acc√®der aux mod√®les YOLO tr√®s simplement.

### 9. üéØ [Entrainement contrastif](./09_EntrainementContrastif/README.md)
Le neuvi√®me cours "Entrainement contrastif" pr√©sente le concept de l'entra√Ænement contrastif. Un premier notebook pr√©sente ce qu'est l'entra√Ænement contrastif en se basant sur l'impl√©mentation d'un article de "face verification". Le second notebook pr√©sente la place de l'entra√Ænment contrastif dans le deep learning r√©cent et notamment son int√™ret pour l'entrainement non supervis√©. 

### 10. ü§ù [Transfer learning et distillation](./10_TransferLearningEtDistillation/README.md)
Le dixi√®me cours "Transfer learning et distillation" pr√©sente deux concepts majeurs en deep learning : le transfer learning et la distillation des connaissances. La premi√®re partie de ce cours pr√©sente le transfer learning dans sa globalit√© puis propose une impl√©mentation pratique. La seconde partie pr√©sente le concept de distillation des connaissances et ses variantes puis propose un cas d'application de la distillation des connaissances pour la d√©tection d'anomalies non supervis√©e. Enfin, une derni√®re partie parle du *finetuning* sur les LLM en introduisant l'architecture de BERT puis en montrant des exemples de *finetuning* avec transformers du [Hugging Face](https://huggingface.co/).

### 11. üåÄ [Mod√®les g√©n√©ratifs](./11_ModelesGeneratifs/README.md)
Le onzi√®me cours "Mod√®les g√©n√©ratifs" introduit le principe de mod√®les g√©n√©ratifs par opposition aux mod√®les discriminatifs. Les 4 grandes familles de mod√®les g√©n√©ratifs sont pr√©sent√©es et certaines sont impl√©ment√©es : les GAN, les VAE, les normalizing flow et les mod√®les de diffusion. Les mod√®les autoregressifs ne sont pas abord√©s car ceux-ci ont √©t√© d√©crits dans le cours NLP et Transformers.

### Bonus üåü [Cours sp√©cifiques](./Bonus_CoursSp√©cifiques/README.md)
Ce cours pr√©sente des concepts tr√®s int√©ressant √† comprendre mais non essentiels dans une pratique courante du deep learning. Si vous √™tes int√©ress√© par comprendre le fonctionnement d'un r√©seau de neurones de mani√®re plus approfondie et de d√©couvrir la raison de l'utilisation de techniques comme la BatchNorm, les connexions r√©siduelles, les optimizers, le dropout, la data augmentation etc ..., ce cours est fait pour vous ! 

**License**

Ce travail est mis √† disposition selon les termes de la licence MIT