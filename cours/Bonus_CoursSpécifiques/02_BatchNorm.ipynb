{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La *Batch Normalization* (ou normalisation par lot) a été introduite en 2015 dans l'article [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167) et a eu un impact majeur dans le milieu du Deep Learning. Aujourd'hui, la normalisation est utilisée presque systématiquement qu'il s'agisse de *BatchNorm*, *LayerNorm* ou *GroupNorm* (et d'autres). \n",
    "\n",
    "L'idée de la *BatchNorm* est assez simple et est en rapport direct avec le notebook précédent. On veut avoir des preactivations suivant une distribution à peu près gaussienne à chaque couche de notre réseau. On va vu qu'avec une bonne initialisation, cela permettait d'avoir ce comportement mais une bonne initialisation n'est pas toujours évidente surtout quand on utilise beaucoup de couches différentes. \n",
    "\n",
    "La *BatchNorm* consiste à normaliser les preactivations par rapport à la dimension du *batch* avant de les passer dans les fonctions d'activations. Cela va garantir qu'on a une distribution environ gausienne à chaque étape.  \n",
    "\n",
    "Cette normalisation n'a pas d'impact pour l'optimisation car il s'agit d'une fonction dérivable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprise du code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va à nouveau reprendre le code du notebook précédent pour créer notre *batch normalization*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../05_NLP/prenoms.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180834, 3]) torch.Size([180834])\n",
      "torch.Size([22852, 3]) torch.Size([22852])\n",
      "torch.Size([22639, 3]) torch.Size([22639])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # Contexte\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] \n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=10 # Dimension de l'embedding de C\n",
    "hidden_dim=200 # Dimension de la couche cachée\n",
    "\n",
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici notre code de propagation avant : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on reprend l'article, on a les informations suivantes : \n",
    "\n",
    "<img src=\"images/norm.png\" alt=\"norm\" width=\"400\"/> \n",
    "\n",
    "Dans un premier temps, il s'agit simplement de normaliser. \n",
    "\n",
    "Pour cela, on va calculer la moyenne et l'écart type de hpreact puis normaliser grâce à ces valeurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-6\n",
    "hpreact_mean = hpreact.mean(dim=0, keepdim=True)\n",
    "hpreact_std= hpreact.std(dim=0, keepdim=True)\n",
    "hpreact_norm = (hpreact - hpreact_mean) / (hpreact_std+epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant pouvoir l'intégrer à notre propagation avant.  \n",
    "\n",
    "Mais avant cela, on peut remarquer que l'on a pas implementé la partie *scale and shift* : \n",
    "\n",
    "<img src=\"images/scaleshift.png\" alt=\"scaleshift\" width=\"400\"/> \n",
    "\n",
    "**A quoi ça sert ?** : En appliquant la normalisation, on confine les poids à ne prendre que des valeurs d'une gaussienne centrée réduite. Cela va réduire les capacités d'expression de notre modèle. Les paramètres apprenables $\\gamma$ et $\\beta$ permettent de contourner ce problème en ajoutant d'un part un *shift* avec $\\beta$ et un *scale* avec $\\gamma$.  \n",
    "\n",
    "Comme il s'agit de paramètres apprenables, on doit aussi les ajouter dans les paramètres du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et en propagation avant, on aura donc : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmean = hpreact.mean(0, keepdim=True)\n",
    "bnstd = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le problème de la Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En y refléchissant un peu, on peut vite trouver des problèmes potentiels dû à la *BatchNorm* : \n",
    "\n",
    "**Un exemple est impacté par les autres éléments du *batch*** :  Le fait de normaliser selon la dimension du *batch* fait que les valeurs de chaque exemple au sein du *batch* sont impactées par les autres exemples du *batch*. Cela pourrait sembler catastrophique mais en pratique, c'est plutôt une bonne chose. Le fait d'avoir des *batchs* aléatoires à chaque epo*ch permet une sorte de régularisation ce qui va permettre au modèle d'être moins enclin à *overfit* sur les données.   \n",
    "Néanmoins, si on veut éviter ce problème, on peut utiser d'autres méthodes de normalisation qui ne normalisent pas selon la dimension du *batch*. En pratique, la *BatchNorm* est encore énormement utilisée car elle fonctionne très bien empiriquement. \n",
    "\n",
    "**Phase de test sur un seul élément** : Pendant l'entraînement, chaque élément est impacté par les éléments de son *batch* mais lorsqu'on est en phase d'inférence et que l'on veut utiliser notre modèle sur un seul élément, on ne peut plus faire la *BatchNorm*. C'est un problème car on ne veut pas avoir un comportement différent pendant l'entraînement et pendant l'inférence. \n",
    "\n",
    "Pour palier à ce problème, on a deux solutions : \n",
    "- On peut calculer la moyenne et la variance sur l'ensemble des éléments  à la fin de l'entraînement et utiliser ces valeurs. En pratique, on ne veut pas faire une itération supplémentaire sur l'ensemble du dataset juste pour ça donc personne ne fait comme ça. \n",
    "- Une autre solution consiste à mettre à jour notre moyenne et vaiance tout au long de l'entraînement grâce à un EMA (*exponential moving average*). A la fin de l'entraînement, on aura une bonne approximation de la moyenne et de la variance de l'ensemble des éléments d'entraînement.\n",
    "\n",
    "En pratique, on peut l'implementer comme ça en python : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "bnmean_running = torch.zeros((1, hidden_dim))\n",
    "bnstd_running = torch.ones((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)\n",
    "bnstdi = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Dans notre implémentation, on a choisi de prendre 0.001 pour notre EMA. Dans la couche [*BatchNorm* de pytorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html), ce paramètre est défini par *momentum* et sa valeur par défaut est 0.1. En pratique, le choix de cette valeur va dépendre de la taille de votre *batch* par rapport à la taille du jeu de données d'entraînement. Pour un gros *batch* avec un petit jeu de données, on va prendre 0.1 par exemple et pour un petit *batch* avec un gros jeu de données, on prendre plutôt une plus petite valeur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons maintenant l'entraînement de notre modèle pour vérifier que le couche fonctionne. Pour ce petit modèle, on aura pas de différence de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.8241\n",
      "  10000/ 200000: 1.9756\n",
      "  20000/ 200000: 2.7151\n",
      "  30000/ 200000: 2.3287\n",
      "  40000/ 200000: 2.1411\n",
      "  50000/ 200000: 2.3207\n",
      "  60000/ 200000: 2.3250\n",
      "  70000/ 200000: 2.0320\n",
      "  80000/ 200000: 2.0615\n",
      "  90000/ 200000: 2.2468\n",
      " 100000/ 200000: 2.2081\n",
      " 110000/ 200000: 2.1418\n",
      " 120000/ 200000: 1.9665\n",
      " 130000/ 200000: 1.8572\n",
      " 140000/ 200000: 2.0577\n",
      " 150000/ 200000: 2.1804\n",
      " 160000/ 200000: 1.8604\n",
      " 170000/ 200000: 1.9810\n",
      " 180000/ 200000: 1.8228\n",
      " 190000/ 200000: 1.9977\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "max_steps = 200000\n",
    "\n",
    "for i in range(max_steps):\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "  emb = C[Xb] \n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1 \n",
    "  \n",
    "  # Batch normalization\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "      bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "      bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    \n",
    "  h = torch.tanh(hpreact) \n",
    "  logits = h @ W2 + b2 \n",
    "  loss = F.cross_entropy(logits, Yb)\n",
    "  \n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  lr = 0.1 if i < 100000 else 0.01 \n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "  if i % 10000 == 0:\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considérations supplémentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Biais** : La *batch norm* a pour effet de normaliser les preactivations des poids. Cette normalisation va nullifier le biais (car celui-ci décale la distribution et nous on la recentre). Lorsqu'on utilise la *BatchNorm*, on peut se passer du biais. En pratique, si on laisse un biais ça ne pose pas de problème mais c'est un paramètre du réseau qui sera inutile. \n",
    "\n",
    "**Placement de la BatchNorm** : D'après ce qu'on a vu, il est logique de placer la *BatchNorm* avant la fonction d'activation. En pratique, certains préferent la placer après la couche d'activation donc ne soyez pas étonné si vous tombez sur ça dans la littérature ou dans un code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autres normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons faire un tour rapide des autres normalisation qui sont utilisé pour l'entraînement des réseaux de neurones. \n",
    "\n",
    "<img src=\"images/types.png\" alt=\"typesofnorm\" width=\"800\"/> \n",
    "\n",
    "Figure extraite de l'[article](https://arxiv.org/pdf/1803.08494)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer Normalization** : Cette couche de normalisation est également très fréquemment utilisée notamment dans les modèles de langages (GPT, Llama). Il s'agit simplement de normaliser sur l'ensemble des activations de la couche plutôt que sur l'axe du *batch*. Dans notre implémentation, cela reviendrait simplement à changer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)  \n",
    "bnstdi = hpreact.std(0, keepdim=True)   \n",
    "# Layer normalization\n",
    "bnmeani = hpreact.mean(1, keepdim=True)  \n",
    "bnstdi = hpreact.std(1, keepdim=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instance Normalization** : Cette couche va normaliser les activations sur chaque canal de chaque élément indépendamment.\n",
    "\n",
    "**Group Normalization** : Cette couche est une sorte de fusion entre la *LayerNorm* et l'*InstanceNorm* puisqu'on va calculer la normalisation sur des groupes de canaux (si la taille d'un groupe vaut 1, c'est l'*InstanceNorm* et si la taille d'un groupe vaut C, c'est la *LayerNorm*)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
