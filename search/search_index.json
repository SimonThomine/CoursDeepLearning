{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"\ud83c\udfe0 Accueil","text":"<p>   \ud83d\ude80 Apprendre le Deep Learning \u00e0 partir de z\u00e9ro \ud83d\ude80 <p></p>"},{"location":"index.html#description","title":"\ud83d\udcda Description","text":"<p>Ce repository propose des cours d'initiation au deep learning se basant sur des notebooks. Pour un d\u00e9butant, les cours sont \u00e0 faire dans l'ordre pour une meilleur compr\u00e9hension globale. </p> <p>Un site internet du cours est disponible pour naviguer plus facilement : \ud83c\udf10 Website \ud83c\udf10</p>"},{"location":"index.html#installation-de-lenvironnement-de-travail","title":"\ud83d\udee0\ufe0f Installation de l'environnement de travail","text":"<p>L'ensemble des library n\u00e9cessaires pour le cours sont disponibles dans requirements.txt, vous pouvez choisir d'installer tout d'un coup ou au fur et \u00e0 mesure de votre avancement dans le cours.  Il est conseill\u00e9 d'utiliser un environnement de travail conda pour \u00e9viter tout conflit avec des library d\u00e9j\u00e0 install\u00e9 sur votre ordinateur.  </p> <pre><code>`pip install -r requirements.txt`\n</code></pre>"},{"location":"index.html#plan-du-cours","title":"\ud83d\uddfa\ufe0f Plan du cours","text":""},{"location":"index.html#1-fondations","title":"1. \ud83c\udfd7\ufe0f Fondations","text":"<p>Le premier cours \"Fondations\" introduit les bases de l'optimisation par descente du gradient avec une compr\u00e9hension intuitive. La r\u00e8gle de la cha\u00eene est introduite puis un premier exemple de regression logistique est pr\u00e9sent\u00e9. </p>"},{"location":"index.html#2-reseau-fully-connected","title":"2. \ud83e\udde0 R\u00e9seau Fully Connected","text":"<p>Le deuxi\u00e8me cours \"R\u00e9seauFullyConnected\" introduit le fonctionnement d'un r\u00e9seau de neurones avec d'abord un exemple d'un r\u00e9seau cod\u00e9 avec micrograd pour permettre d'explorer cette library pour bien comprendre le fonctionnement. Une version fran\u00e7aise MicrogradFR est disponible dans le cours.  Ensuite, pour introduire la library pytorch, le m\u00eame exemple est reconstruit mais en utilisant pytorch au lieu de micrograd. Le dernier notebook de cette partie introduit des techniques avanc\u00e9es d'entra\u00eenement de r\u00e9seau de neurones qu'il est utile de conna\u00eetre pour am\u00e9liorer les performances de nos r\u00e9seaux. </p>"},{"location":"index.html#3-reseaux-convolutifs","title":"3. \ud83d\uddbc\ufe0f R\u00e9seaux convolutifs","text":"<p>Le troisi\u00e8me cours \"R\u00e9seauConvolutifs\" aborde tout d'abord le principe de fonctionnement des couches de convolution puis montre comment on les utilise au sein d'un r\u00e9seau de neurones. Plusieurs exemples sont ensuite abord\u00e9s pour montrer les capacit\u00e9s d'un r\u00e9seau convolutif : classification sur MNIST, classification sur CIFAR-10 et segmentation sur \"Oxford-IIIT Pet Dataset\". </p>"},{"location":"index.html#4-autoencodeurs","title":"4. \ud83d\udd04 Autoencodeurs","text":"<p>Le quatri\u00e8me cours \"Autoencodeurs\" aborde la notion d'entra\u00eenement non supervis\u00e9 en pr\u00e9sentant les diff\u00e9rences entre supervis\u00e9 et non supervis\u00e9. L'exemple de l'autoencodeur est ensuite abord\u00e9 ainsi que son application pour la d\u00e9tection d'anomalies non supervis\u00e9e. Pour finir, un notebook montre le potentiel de l'autoencodeur pour le probl\u00e8me du \"denoising\". </p>"},{"location":"index.html#5-nlp","title":"5. \ud83d\udde8\ufe0f NLP","text":"<p>Le cinqui\u00e8me cours \"NLP\" est grandement inspir\u00e9 de la s\u00e9rie de vid\u00e9o de Andrej Karpathy \"Building makemore\" qui tra\u00eete les NLP avec une approche de pr\u00e9diction du prochain token. Le cours aborde d'abord des mod\u00e8les tr\u00e8s simples pour avoir une intuition sur le tra\u00eetement de donn\u00e9es discr\u00e8tes avec un r\u00e9seau neurones puis les mod\u00e8les se complexifient petit \u00e0 petit. </p>"},{"location":"index.html#6-hugging-face","title":"6. \ud83e\udd17 Hugging Face","text":"<p>Le sixi\u00e8me cours \"HuggingFace\" est d\u00e9di\u00e9 \u00e0 une exploration des librarys, des mod\u00e8les, des datasets et autres de Hugging Face. C'est une plateforme regroupant \u00e9normement des mod\u00e8les open source pour une grande vari\u00e9t\u00e9 de t\u00e2ches avec une library pour les impl\u00e9menter rapidement et efficacement en python. Le cours pr\u00e9sente d'abord le site de Hugging Face pour ensuite pr\u00e9senter les fonctionnalit\u00e9s des diff\u00e9rentes librarys (transformers et diffusers principalement) sur diff\u00e9rents cas d'usage. Le dernier notebook pr\u00e9sente bri\u00e8vement gradio, une library pour cr\u00e9er des interfaces simples de d\u00e9mo.</p>"},{"location":"index.html#7-transformers","title":"7. \ud83e\udd16 Transformers","text":"<p>Le septi\u00e8me cours \"Transformers\" est d\u00e9di\u00e9 \u00e0 l'architecture du transformers. Apr\u00e8s avoir vu ses applications dans le cours pr\u00e9c\u00e9dent. Nous allons entrer dans le d\u00e9tail de l'architecture pour en comprendre les m\u00e9canismes. Le premier notebook est grandement inspir\u00e9 de la vid\u00e9o Let's build GPT de Andrej Karpathy et propose une impl\u00e9mentation pas \u00e0 pas d'un encodeur transformers. Le but de ce notebook sera de cr\u00e9er un mod\u00e8le capable de g\u00e9n\u00e9rer du \"Moli\u00e8re\" automatiquement. La seconde partie est une approche plus math\u00e9matique et la pr\u00e9sentation de la partie encodeur du transformers. La troisi\u00e8me partie pr\u00e9sente des architectures de mod\u00e8le reposant sur la couche transformers pour de nombreux cas d'applications (Vision, traduction etc ...). Enfin, une derni\u00e8re partie propose une impl\u00e9mentation du vision transformer \u00e0 partir de l'article original.</p>"},{"location":"index.html#8-detection","title":"8. \ud83d\udd0d Detection","text":"<p>Le huiti\u00e8me cours \"Detection\" pr\u00e9sente le fonctionnement de la d\u00e9tection d'objets sur des images. L'introduction pr\u00e9sente ce qu'est la d\u00e9tection et les deux m\u00e9thodes classiques (two-stage et one-stage). Le notebook suivant propose une description pr\u00e9cise du fonctionnement de YOLO et le dernier notebook pr\u00e9sente la library ultralytics qui permet d'acc\u00e8der aux mod\u00e8les YOLO tr\u00e8s simplement.</p>"},{"location":"index.html#9-entrainement-contrastif","title":"9. \ud83c\udfaf Entrainement contrastif","text":"<p>Le neuvi\u00e8me cours \"Entrainement contrastif\" pr\u00e9sente le concept de l'entra\u00eenement contrastif. Un premier notebook pr\u00e9sente ce qu'est l'entra\u00eenement contrastif en se basant sur l'impl\u00e9mentation d'un article de \"face verification\". Le second notebook pr\u00e9sente la place de l'entra\u00eenment contrastif dans le deep learning r\u00e9cent et notamment son int\u00earet pour l'entrainement non supervis\u00e9. </p>"},{"location":"index.html#10-transfer-learning-et-distillation","title":"10. \ud83e\udd1d Transfer learning et distillation","text":"<p>Le dixi\u00e8me cours \"Transfer learning et distillation\" pr\u00e9sente deux concepts majeurs en deep learning : le transfer learning et la distillation des connaissances. La premi\u00e8re partie de ce cours pr\u00e9sente le transfer learning dans sa globalit\u00e9 puis propose une impl\u00e9mentation pratique. La seconde partie pr\u00e9sente le concept de distillation des connaissances et ses variantes puis propose un cas d'application de la distillation des connaissances pour la d\u00e9tection d'anomalies non supervis\u00e9e. Enfin, une derni\u00e8re partie parle du finetuning sur les LLM en introduisant l'architecture de BERT puis en montrant des exemples de finetuning avec transformers du Hugging Face.</p>"},{"location":"index.html#11-modeles-generatifs","title":"11. \ud83c\udf00 Mod\u00e8les g\u00e9n\u00e9ratifs","text":"<p>Le onzi\u00e8me cours \"Mod\u00e8les g\u00e9n\u00e9ratifs\" introduit le principe de mod\u00e8les g\u00e9n\u00e9ratifs par opposition aux mod\u00e8les discriminatifs. Les 4 grandes familles de mod\u00e8les g\u00e9n\u00e9ratifs sont pr\u00e9sent\u00e9es et certaines sont impl\u00e9ment\u00e9es : les GAN, les VAE, les normalizing flow et les mod\u00e8les de diffusion. Les mod\u00e8les autoregressifs ne sont pas abord\u00e9s car ceux-ci ont \u00e9t\u00e9 d\u00e9crits dans le cours NLP et Transformers.</p>"},{"location":"index.html#bonus-cours-specifiques","title":"Bonus \ud83c\udf1f Cours sp\u00e9cifiques","text":"<p>Ce cours pr\u00e9sente des concepts tr\u00e8s int\u00e9ressant \u00e0 comprendre mais non essentiels dans une pratique courante du deep learning. Si vous \u00eates int\u00e9ress\u00e9 par comprendre le fonctionnement d'un r\u00e9seau de neurones de mani\u00e8re plus approfondie et de d\u00e9couvrir la raison de l'utilisation de techniques comme la BatchNorm, les connexions r\u00e9siduelles, les optimizers, le dropout, la data augmentation etc ..., ce cours est fait pour vous ! </p> <p>License</p> <p>Ce travail est mis \u00e0 disposition selon les termes de la licence MIT</p>"},{"location":"01_Fondations/index.html","title":"\ud83c\udfd7\ufe0f Fondations \ud83c\udfd7\ufe0f","text":"<p>Ce cours introduit introduit les bases de l'optimisation par descente du gradient avec une compr\u00e9hension intuitive. La r\u00e8gle de la cha\u00eene est introduite puis un premier exemple de regression logistique est pr\u00e9sent\u00e9. </p>"},{"location":"01_Fondations/index.html#notebook-1-derivees-de-descente-du-gradient","title":"Notebook 1\ufe0f\u20e3 : D\u00e9riv\u00e9es de descente du gradient","text":"<p>Ce notebook propose des rappels sur la d\u00e9riv\u00e9e d'une fonction pour ensuite introduire l'algorithme de descente du gradient et la r\u00e8gle de la cha\u00eene.</p>"},{"location":"01_Fondations/index.html#notebook-2-regression-logistique","title":"Notebook 2\ufe0f\u20e3 : R\u00e9gression logistique","text":"<p>Ce notebook est d\u00e9di\u00e9 \u00e0 une explication et une impl\u00e9mentation de la re\u00e9gression logistique. Les fonctions d'activation et les fonctions de co\u00fbts (loss) sont \u00e9galement introduites.</p>"},{"location":"01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html","title":"D\u00e9riv\u00e9e et descente du gradient","text":"<p>Ce cours a pour but de pr\u00e9senter l'algorithme de descente du gradient qui est un des piliers du deep learning. Pour cela, il est n\u00e9cessaire de faire quelques rappels sur la d\u00e9riv\u00e9e.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt \nimport numpy as np\n</pre> import matplotlib.pyplot as plt  import numpy as np <p>D\u00e9finissons une fonction : $f(x) = 2x^2 - 3x + 4$</p> In\u00a0[3]: Copied! <pre>def f(x):\n  return 2*x**2-3*x+4\nf(3)\n</pre> def f(x):   return 2*x**2-3*x+4 f(3) Out[3]: <pre>13</pre> <p>Tra\u00e7ons cette fonction sur un graphe avec matplotlib.</p> In\u00a0[4]: Copied! <pre>xs = np.arange(-5, 5, 0.25)\nys = f(xs)\nplt.plot(xs, ys)\n</pre> xs = np.arange(-5, 5, 0.25) ys = f(xs) plt.plot(xs, ys) Out[4]: <pre>[&lt;matplotlib.lines.Line2D at 0x7ae9dcdf63d0&gt;]</pre> <p>Le calcul de la d\u00e9riv\u00e9e permet de conna\u00eetre la pente de la tangente \u00e0 la courbe en un point. Pour calculer la d\u00e9riv\u00e9e d'une fonction, on utilise la d\u00e9finition de la d\u00e9riv\u00e9e : $f'(x) = \\lim_{h \\to 0} \\frac{f(x+h)-h}{h}$ En prenant un h suffisamment petit, on peut estimer la pente de la courbe num\u00e9riquement.</p> <p>Note : La pente de la courbe correspond au rapport entre la variation de $y$ et de $x$. Si quand on augmente $x$ de 1, $y$ augmente de 2, alors la pente de la courbe vaut 2.</p> In\u00a0[4]: Copied! <pre>h=0.0001\nx=-1.0\nprint(\"D\u00e9riv\u00e9e en x=-1 : \", (f(x+h)-f(x))/h)\nx=2.0\nprint(\"D\u00e9riv\u00e9e en x=2 : \", (f(x+h)-f(x))/h)\n</pre> h=0.0001 x=-1.0 print(\"D\u00e9riv\u00e9e en x=-1 : \", (f(x+h)-f(x))/h) x=2.0 print(\"D\u00e9riv\u00e9e en x=2 : \", (f(x+h)-f(x))/h) <pre>D\u00e9riv\u00e9e en x=-1 :  -6.999800000002665\nD\u00e9riv\u00e9e en x=2 :  5.000200000004895\n</pre> <p>D'apr\u00e8s le graphique, on voit bien que la pente de la courbe est n\u00e9gative en $x=-1$  et positive en $x=2$. Le signe de la d\u00e9riv\u00e9e permet de conna\u00eetre le sens de la pente et la valeur de la d\u00e9riv\u00e9e permet de conna\u00eetre l'intensit\u00e9 de la pente. On peut v\u00e9rifier nos r\u00e9sultats \u00e0 l'aide des d\u00e9riv\u00e9es usuelles  : Si $f(x)=2x\u00b2-3x+4$ alors $f'(x)=4x-3$ On retrouve $f'(-1)=4 \\times -1-3=-7 \\approx -6.999800000002665$ et $f'(2)=4 \\times 2-3=5 \\approx 5.000200000004895$ Les r\u00e9sultats ne sont pas exacts car h n'est pas infiniment petit dans notre calcul num\u00e9rique.</p> In\u00a0[5]: Copied! <pre># On d\u00e9finit deriv_f = f'(x)\ndef deriv_f(x):\n  return 4*x-3\n</pre> # On d\u00e9finit deriv_f = f'(x) def deriv_f(x):   return 4*x-3 <p>Le but de l'optimisation va \u00eatre de minimiser ou maximiser une fonction objectif qui correspond \u00e0 l'objectif que l'on s'est fix\u00e9. Si notre but est de trouver le minimum de la fonction, il y a plusieurs fa\u00e7ons de faire. Une premi\u00e8re fa\u00e7on est de r\u00e9soudre l'\u00e9quation $f'(x)=0$ : $4x-3=0$ $4x=3$ $x=\\frac{3}{4}$ Ce calcul nous permet de trouver le minimum de $f(x)$ dans notre cas. Cependant, dans un cas g\u00e9n\u00e9ral, r\u00e9soudre $f'(x)=0$ permet de trouver les optimums (maximums et minimums) et pas forc\u00e9ment le minimum global.</p> <p>Une seconde fa\u00e7on est d'utiliser une m\u00e9thode d'optimisation que l'on appelle la descente du gradient. Pour appliquer cette m\u00e9thode, on commence par se placer en un point quelconque, par exemple $x=2$. On calcule la d\u00e9riv\u00e9e $f'(2)=5$. La pente est positive c'est-\u00e0-dire que si $x$ augmente alors $f(x)$ augmente et si $x$ diminue alors $f(x)$ diminue. Notre but est de trouver le minimum de $f(x)$, on va donc changer notre x en fonction de la pente et d'un facteur $\\alpha$ = 0,1 (learning rate). On se retrouve avec $x_{new}=x - pente \\times \\alpha= 2-0.5=1.5$ et on peut recalculer la d\u00e9riv\u00e9e \u00e0 notre nouveau point $f'(1.5)=4 \\times 1.5-3=3$, la d\u00e9riv\u00e9e est encore positive, il faut donc \u00e0 nouveau diminuer la valeur de $x$. La m\u00e9thode d'optimisation de la descente du gradient consiste \u00e0 cr\u00e9er une boucle qui va modifier la valeur de $x$ jusqu'\u00e0 atteindre un minimum en se basant sur un facteur $\\alpha$ et le signe de la d\u00e9riv\u00e9e. A noter que l'on tient compte de l'intensit\u00e9 de la pente dans notre calcul. On peut coder la descente du gradient de cette mani\u00e8re.</p> In\u00a0[6]: Copied! <pre># Descente du gradient\nx=2.0 # valeur al\u00e9atoire de x\nalpha=0.01 # pas\niterations=250 # nombre d'it\u00e9rations\n\nfor i in range(iterations):\n  grad=deriv_f(x)\n  if (grad&gt;0):\n    x=x-alpha\n  elif(grad&lt;0):\n    x=x+alpha\n  else:\n    print(\"minimum found YAY, x = \",x)\nprint(\"approximate minimum found YAY, x = \",x)\n</pre> # Descente du gradient x=2.0 # valeur al\u00e9atoire de x alpha=0.01 # pas iterations=250 # nombre d'it\u00e9rations  for i in range(iterations):   grad=deriv_f(x)   if (grad&gt;0):     x=x-alpha   elif(grad&lt;0):     x=x+alpha   else:     print(\"minimum found YAY, x = \",x) print(\"approximate minimum found YAY, x = \",x) <pre>approximate minimum found YAY, x =  0.7599999999999989\n</pre> <p>On retrouve $x \\approx \\frac{3}{4}$. Avec un pas plus faible ($\\alpha$) et plus d'it\u00e9rations, on peut retrouver une valeur encore plus pr\u00e9cise.</p> <p>Avant de passer aux choses s\u00e9rieuses, il est n\u00e9cessaire de faire un dernier rappel math\u00e9matique d'une importance capitale pour l'apprentissage profond. C'est cette r\u00e8gle qui permet l'entra\u00eenement des param\u00e8tres des couches \"cach\u00e9es\" du r\u00e9seau. Il s'agit du th\u00e9or\u00e8me de d\u00e9rivation des fonctions compos\u00e9es (plus souvent appel\u00e9 r\u00e8gle de la cha\u00eene ou chain-rule en anglais) qui \u00e9nonce le principe suivant : Si une variable y d\u00e9pend d'une seconde variable u, qui d\u00e9pend \u00e0 son tour d'une variable x, alors : $\\frac{dy}{dx}=\\frac{dy}{du}\\cdot\\frac{du}{dx}$</p> <p>Reprenons un exemple de descente du gradient sur des fonctions qui d\u00e9pendent les unes des autres. $u=2x\u00b2-x-2$ $y=3u+1$ $\\frac{dy}{dx}=\\frac{dy}{du}\\cdot\\frac{du}{dx}$ avec $\\frac{dy}{du}=3$ et $\\frac{du}{dx}=2x-1$ $\\frac{dy}{dx}=3(2x-1)$ $\\frac{dy}{dx}=6x-3$ On sait maintenant comment la variation de $x$ impacte $y$ et on peut appliquer notre algorithme de descente du gradient.</p> In\u00a0[7]: Copied! <pre>x=2.0\ndef deriv_y_x(x):\n  return 6*x-3\nfor i in range(iterations):\n  grad=deriv_y_x(x)\n  if (grad&gt;0):\n    x=x-alpha\n  elif(grad&lt;0):\n    x=x+alpha\n  else:\n    print(\"minimum found YAY, x = \",x)\nprint(\"approximate minimum found YAY, x = \",x)\n</pre> x=2.0 def deriv_y_x(x):   return 6*x-3 for i in range(iterations):   grad=deriv_y_x(x)   if (grad&gt;0):     x=x-alpha   elif(grad&lt;0):     x=x+alpha   else:     print(\"minimum found YAY, x = \",x) print(\"approximate minimum found YAY, x = \",x) <pre>approximate minimum found YAY, x =  0.49999999999999867\n</pre> <p>Jusqu'\u00e0 pr\u00e9sent, on s'est content\u00e9 de trouver le minimum d'une fonction contenant une seule variable $x$. Un avantage des m\u00e9thodes d'optimisation est que l'on peut optimiser plusieurs variables simultan\u00e9ment avec la descente du gradient. Pour cela, il faut calculer la d\u00e9riv\u00e9e par rapport \u00e0 chacune des variables.</p> <p>Pour notre exemple, prenons 3 variables a, b et c dans le mod\u00e8le suivant : $u=3a\u00b2-2a+b\u00b2+1$ $y=2u+c$ Pour pouvoir appliquer la descente du gradient, on doit calculer $\\frac{dy}{da}$, $\\frac{dy}{db}$ et $\\frac{dy}{dc}$ Pour calculer les deux premiers, on utilise la r\u00e8gle de la cha\u00eene ce qui nous donne :</p> <ul> <li><p>Pour la variable a : $\\frac{dy}{da} = \\frac{dy}{du}\\cdot\\frac{du}{da}$ $\\frac{dy}{da} = 2(6a-2) = 12a-4$</p> </li> <li><p>Pour la variable b : $\\frac{dy}{da} = \\frac{dy}{du}\\cdot\\frac{du}{db}$ $\\frac{dy}{da} = 2(2b) = 4b$</p> </li> <li><p>Pour la varible c : $\\frac{dy}{dc}=1$ \u00c0 partir de ces \u00e9quations, on peut appliquer la descente du gradient.</p> </li> </ul> In\u00a0[8]: Copied! <pre>def deriv_y_a(a):\n  return 12*a-4\ndef deriv_y_b(b):\n  return 4*b\ndef deriv_y_c(c):\n  return 1\n\na=1.0\nb=1.0\nc=1.0\nalpha=0.05\ndef deriv_y_x(x):\n  return 6*x-3\nfor i in range(iterations):\n  grad_a=deriv_y_a(a)\n  grad_b=deriv_y_b(b)\n  grad_c=deriv_y_c(b)\n  if (grad_a&gt;0):\n    a=a-alpha\n  else:\n    a=a+alpha\n  if (grad_b&gt;0):\n    b=b-alpha\n  else:\n    b=b+alpha\n  if (grad_c&gt;0):\n    c=c-alpha\n  else:\n    c=c+alpha\nprint(\"approximate minimum found YAY, a = \"+str(a)+\" b = \"+str(b)+\" c = \"+str(c))\n</pre> def deriv_y_a(a):   return 12*a-4 def deriv_y_b(b):   return 4*b def deriv_y_c(c):   return 1  a=1.0 b=1.0 c=1.0 alpha=0.05 def deriv_y_x(x):   return 6*x-3 for i in range(iterations):   grad_a=deriv_y_a(a)   grad_b=deriv_y_b(b)   grad_c=deriv_y_c(b)   if (grad_a&gt;0):     a=a-alpha   else:     a=a+alpha   if (grad_b&gt;0):     b=b-alpha   else:     b=b+alpha   if (grad_c&gt;0):     c=c-alpha   else:     c=c+alpha print(\"approximate minimum found YAY, a = \"+str(a)+\" b = \"+str(b)+\" c = \"+str(c)) <pre>approximate minimum found YAY, a = 0.29999999999999966 b = -3.191891195797325e-16 c = -11.50000000000003\n</pre> <p>On a pu trouver des valeurs qui minimisent la valeur de $y$. La param\u00e8tre c va tendre vers moins l'infini avec beaucoup d'it\u00e9rations tandis que le param\u00e8tre b va valoir 0 et le param\u00e8tre a va valoir 0.3.</p>"},{"location":"01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html#derivee-et-descente-du-gradient","title":"D\u00e9riv\u00e9e et descente du gradient\u00b6","text":""},{"location":"01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html#comprehension-intuitive-de-la-derivee","title":"Compr\u00e9hension intuitive de la d\u00e9riv\u00e9e\u00b6","text":""},{"location":"01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html#les-bases-de-loptimisation-par-descente-de-gradient","title":"Les bases de l'optimisation par descente de gradient\u00b6","text":""},{"location":"01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html#la-regle-de-la-chaine","title":"La r\u00e8gle de la cha\u00eene\u00b6","text":""},{"location":"01_Fondations/01_D%C3%A9riv%C3%A9esEtDescenteDuGradient.html#optimisation-de-plusieurs-variables","title":"Optimisation de plusieurs variables\u00b6","text":""},{"location":"01_Fondations/02_R%C3%A9gressionLogistique.html","title":"R\u00e9gression logistique","text":"<p>Il est temps de passer aux choses s\u00e9rieuses et de pr\u00e9senter le neurone articifiel.</p> <p>La figure ci-dessous montre le fonctionnement d'un neurone artificiel :</p> <p></p> <p>Le neurone artificiel prend en entr\u00e9e un vecteur $\\mathbf{x}=(x_1,x_2,...,x_n)$, chaque \u00e9l\u00e9ment $x_i$ du vecteur $\\mathbf{x}$ est ensuite multipli\u00e9 par un poids $w_i$ puis on somme l'ensemble et on ajoute un biais $b$. Cette somme est ensuite pass\u00e9 dans une fonction qu'on appelle fonction d'activation $\\phi$. $Sortie = \\phi(\\sum_{i=0}^{n} w_i x_i  + b)$ On appelle ce proc\u00e9d\u00e9 neurone artificiel par analogie avec le fonctionnement d'un neurone biologique.</p> <p>Fonction de Heaviside : A l'origine, le premier neurone articiel (le perceptron) utilisait une fonction de seuillage comme fonction d'activation. Cela permet de prendre une d\u00e9cision (0 ou 1) par rapport \u00e0 la somme pond\u00e9r\u00e9e et un seuil d\u00e9fini. $heaviside(x) = \\left\\{     \\begin{array}{ll}         1 &amp; \\text{si } x &gt; 0 \\\\         0 &amp; \\text{sinon}     \\end{array} \\right. \\text{avec } x=\\sum_{i=0}^{n} w_i x_i  + b$ Cette fonction d'activation est efficace pour obtenir une classification binaire mais elle ne fonctionne pas pour plusieurs classes. De plus, la fonction n'est pas d\u00e9rivable, il est donc compliqu\u00e9 d'utiliser l'algorithme de la descente du gradient pour optimiser les poids $w_i$ du neurone.</p> <p>Les fonctions d'activation r\u00e9cente sont beaucoup plus int\u00e9ressante pour l'entra\u00eenement de r\u00e9seaux de neurones par descente du gradient. Premi\u00e8rement, on choisit des fonctions d\u00e9rivables ce qui permet d'appliquer notre algorithme de descente du gradient. Deuxi\u00e8mement, on choisit des fonctions non-lin\u00e9aire ce qui permet aux r\u00e9seaux d'apprendre des repr\u00e9sentations complexes. Il y a \u00e9galement d'autres avantages sp\u00e9cifiques \u00e0 chaque fonction d'activation.</p> <p>Une des fonctions d'activation \"r\u00e9cente\" est la fonction sigmo\u00efde que nous d\u00e9taillons ici :</p> <p>Fonction sigmo\u00efde : Une autre fonction d'activation particuli\u00e8rement int\u00e9ressante par son analogie avec une probabilit\u00e9 est la fonction sigmo\u00efde. Cette fois-ci, la fonction va permettre d'obtenir une valeur entre 0 et 1 par la formule : $sigmoid(x) = \\frac{1}{1 + e^{-x}} \\text{ avec } x=\\sum_{i=0}^{n} w_i x_i  + b$</p> In\u00a0[1]: Copied! <pre># Trac\u00e9 de la fonction sigmo\u00efde\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\nx = np.linspace(-10, 10, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.title('Fonction sigmo\u00efde')\nplt.xlabel('x')\nplt.ylabel('sigmoid(x)')\nplt.grid(True)\nplt.show()\n</pre> # Trac\u00e9 de la fonction sigmo\u00efde import numpy as np import matplotlib.pyplot as plt  def sigmoid(x):     return 1 / (1 + np.exp(-x)) x = np.linspace(-10, 10, 100) y = sigmoid(x)  plt.plot(x, y) plt.title('Fonction sigmo\u00efde') plt.xlabel('x') plt.ylabel('sigmoid(x)') plt.grid(True) plt.show() <p>Cette fonction est d\u00e9rivable et sa d\u00e9riv\u00e9e vaut : $sigmoid'(x) = sigmoid(x) \\cdot (1 - f(x))$ On peut donc appliquer la descente du gradient sur notre neurone artificiel lorsqu'on utilise cette fonction d'activation.</p> <p>Il existe de nombreuses autres fonctions d'activations qui ont chacune leurs utilit\u00e9s que nous verrons dans les cours suivants. ($Tanh$,$ReLU$,$Softmax$)</p> <p>Pour comprendre la r\u00e9gression logistique, le mieux est de passer par un exemple d'application. Dans cet exemple, nous allons d\u00e9terminer si un \u00e9tudiant va \u00eatre admis dans l'universit\u00e9 de ses r\u00eaves \u00e0 partir de trois informations : son score \u00e0 l'examen d'entr\u00e9e, ses notes moyennes de l'ann\u00e9e pr\u00e9c\u00e9dente et la qualit\u00e9 de sa lettre de motivation. Nous n'avons pas acc\u00e8s \u00e0 la m\u00e9thode de calcul pour l'admission ou non d'un \u00e9tudiant mais nous disposons de donn\u00e9es et de la d\u00e9cision correspondantes. Les informations d'entr\u00e9e sont entre 0 et 1, avec 1 indiquant le meilleur score. Admis = 1 correspond \u00e0 une admission tandis qu'admis = 0 correspond \u00e0 un refus. Nous disposons des informations suivantes :</p> In\u00a0[2]: Copied! <pre>from tabulate import tabulate\n\n# D\u00e9finition des donn\u00e9es d'entra\u00eenement\nvalues_train = [[0.7, 0.8, 0.1], [0.4, 0.9, 0.5], [0.2, 0.3, 0.9], [0.9, 0.9, 0.6]]\nlabels_train = [1, 0, 0, 1]\n\n# Ajout des noms de colonnes\ndata = [['Examen', 'Moyenne', 'Motivation', 'Admis']]\ndata.extend([[values_train[i][0], values_train[i][1], values_train[i][2], labels_train[i]] for i in range(len(values_train))])\n\n# Affichage du tableau\nprint(tabulate(data, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n</pre> from tabulate import tabulate  # D\u00e9finition des donn\u00e9es d'entra\u00eenement values_train = [[0.7, 0.8, 0.1], [0.4, 0.9, 0.5], [0.2, 0.3, 0.9], [0.9, 0.9, 0.6]] labels_train = [1, 0, 0, 1]  # Ajout des noms de colonnes data = [['Examen', 'Moyenne', 'Motivation', 'Admis']] data.extend([[values_train[i][0], values_train[i][1], values_train[i][2], labels_train[i]] for i in range(len(values_train))])  # Affichage du tableau print(tabulate(data, headers=\"firstrow\", tablefmt=\"fancy_grid\")) <pre>\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502   Examen \u2502   Moyenne \u2502   Motivation \u2502   Admis \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502      0.7 \u2502       0.8 \u2502          0.1 \u2502       1 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      0.4 \u2502       0.9 \u2502          0.5 \u2502       0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      0.2 \u2502       0.3 \u2502          0.9 \u2502       0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      0.9 \u2502       0.9 \u2502          0.6 \u2502       1 \u2502\n\u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b\n</pre> <p>Notre but dans ce probl\u00e8me est de d\u00e9terminer si les \u00e9tudiants ayant eu les scores $[Examen=0.8, Moyenne=0.7, Motivation=0.2]$ et $[Examen=0.4, Moyenne=0.5, Motivation=0.9]$ ont \u00e9t\u00e9 admis.</p> <p>Vous l'aurez compris, les donn\u00e9es $Examen$, $Moyenne$ et $Motivation$ correspondent \u00e0 nos $x_i$. Notre but avec la r\u00e9gression logistique est de trouver une valeur optimale des $w_i$ en concordance avec nos donn\u00e9es d'entrainement. Par simplicit\u00e9, posons $x_0=Examen$, $x_1=Moyenne$ et $x_2=Motivation$ et $y_{true}=Admis$.</p> <p>Dans notre exemple de descente du gradient, notre but \u00e9tait de trouver le minimum d'une fonction. C'est dans ces sc\u00e9narios que la descente du gradient excelle. Pour notre nouveau probl\u00e8me, il nous faut trouver une fonction qui, lorsqu'on la minimise, am\u00e9liore les pr\u00e9dictions. Dans notre classification binaire, $y_{true}$ vaut 1 si l'\u00e9l\u00e8ve est admis et 0 sinon. Notre but est de pr\u00e9dire si l'\u00e9l\u00e8ve est admis ou non en pr\u00e9disant la sortie $pred$. Lors de l'entra\u00eenement, on veut entra\u00eener notre mod\u00e8le de r\u00e9gression logistique \u00e0 pr\u00e9dire $ pred \\approx y_{true}$. Pour cela, on utilise la fonction de vraisemblance n\u00e9gative qui s'exprime de la mani\u00e8re suivante : $\\text{loss} = - \\left( y_{\\text{true}} \\cdot \\log(\\text{pred}) + (1 - y_{\\text{true}}) \\cdot \\log(1 - \\text{pred}) \\right)$</p> <p>Pour plus de d\u00e9tails sur la r\u00e9gression logistique et la perte de vraisemblance n\u00e9gative, vous pouvez consulter ce lien.</p> <p>L'important est de comprendre comment varie cette fonction en fonction de notre pr\u00e9diction $pred$ et du label $y_{true}$. Pour cela, prenons le cas o\u00f9 le label est $y_{true}=1$. Analysons deux cas : Si $pred=0.9$, c'est-\u00e0-dire que notre mod\u00e8le pr\u00e9dit que l'\u00e9l\u00e8ve sera admis \u00e0 90% de chance ce qui est une bonne pr\u00e9diction alors : $\\text{loss} = - \\left( 1.0 \\cdot \\log(0.9) + (1 - 1.0) \\cdot \\log(1 - 0.9) \\right)$ $\\text{loss} = - \\left( 1.0 \\cdot \\log(0.9) + 0 \\cdot \\log(1 - 0.9) \\right)$ $\\text{loss} = - \\left( 1.0 \\cdot \\log(0.9)\\right)$ $\\text{loss} = - \\left( 1.0 \\cdot \\log(0.9)\\right)$ $\\text{loss} = 0.046$ Le loss est faible, c'est une bonne chose car la pr\u00e9diction est bonne.</p> <p>Si $pred=0.2$, c'est-\u00e0-dire que notre mod\u00e8le pr\u00e9dit que l'\u00e9l\u00e8ve sera admis \u00e0 20% de chance ce qui est une mauvaise pr\u00e9diction alors : $\\text{loss} = - \\left( 1.0 \\cdot \\log(0.2) + (1 - 1.0) \\cdot \\log(1 - 0.2) \\right)$ $\\text{loss} = - \\left( 1.0 \\cdot \\log(0.2) + 0 \\cdot \\log(1 - 0.2) \\right)$ $\\text{loss} = - \\left( 1.0 \\cdot \\log(0.2)\\right)$ $\\text{loss} = - \\left( 1.0 \\cdot \\log(0.2)\\right)$ $\\text{loss} = 0.70$ Le loss est important, c'est une bonne chose car la pr\u00e9diction est mauvaise.</p> <p>Pour un cas o\u00f9 $y_{true}=0$, on retrouve un loss faible quand la $pred$ est proche 0 et un loss important quand $pred$ est proche de 1 (faire le calcul pour s'exercer si besoin).</p> <p>Maintenant que l'on dispose d'une fonction \u00e0 minimiser, il est n\u00e9cessaire de calculer la d\u00e9riv\u00e9e de cette fonction en fonction de chacun des poids $w_0$, $w_1$, $w_2$ et $b$. On doit donc calculer $\\frac{\\partial loss}{\\partial w_0}$, $\\frac{\\partial loss}{\\partial w_1}$, $\\frac{\\partial loss}{\\partial w_2}$ et $\\frac{\\partial loss}{\\partial b}$. Pour les poids $w_0$, $w_1$ et $w_2$, la d\u00e9riv\u00e9e s'effectue de la m\u00eame mani\u00e8re. Avec la r\u00e8gle de la cha\u00eene, pour $w_0$, on a : $\\frac{\\partial loss}{\\partial w_0} = \\frac{\\partial loss}{\\partial pred} \\cdot \\frac{\\partial pred}{\\partial w_0}$</p> <p>Pour rappel, notre pr\u00e9diction $pred$ correspond \u00e0 la sortie de notre r\u00e9gression logistique avec la fonction d'activation $sigmoid$.</p> <p>Pour le premier terme, la d\u00e9riv\u00e9e du loss en fonction de pred nous donne : $\\frac{\\partial loss}{\\partial pred} = -(\\frac{y_{true}}{pred} - \\frac{1-y_{true}}{1-pred}) $ Le calcul ne sera pas d\u00e9taill\u00e9 ici mais vous pouvez le faire vous-m\u00eame pour vous en assurer.</p> <p>Pour le second terme, la d\u00e9riv\u00e9e de pred en fonction de $w_0$ nous donne : $\\frac{\\partial pred}{\\partial w_0} = pred \\cdot (1-pred) \\cdot x_0$</p> <p>En combinant les deux termes, on obtient : $\\frac{\\partial loss}{\\partial w_0} =-(\\frac{y_{true}}{pred} - \\frac{1-y_{true}}{1-pred}) \\cdot pred \\cdot (1-pred) \\cdot x_0$</p> <p>Et apr\u00e8s simplification (magique), $\\frac{\\partial loss}{\\partial w_0} = (pred-y_{true}) \\cdot x_0$</p> <p>Sans d\u00e9tailler le calcul, on obtient \u00e9galement : $\\frac{\\partial loss}{\\partial b} = pred-y_{true}$</p> <p>Maintenant que l'on a tous les \u00e9l\u00e9ments, d\u00e9finissons notre fonction de regression logistique en python :</p> In\u00a0[3]: Copied! <pre># Notre classe de regression logistique\nclass logistic_regression():\n  def __init__(self) -&gt; None:\n    self.w0=np.random.randn()\n    self.w1=np.random.randn()\n    self.w2=np.random.randn()\n    self.b=0\n  def __call__(self,x0,x1,x2):\n    # Somme pond\u00e9r\u00e9e et ajout du biais\n    pond=self.w0*x0+self.w1*x1+self.w2*x2+self.b\n    # Application de la sigmo\u00efde\n    pred=sigmoid(pond)\n    return pred\n    \ndef loss(y_true, y_pred):\n  # Calcul du loss (log vraisemblance n\u00e9gative)\n  loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n  return loss\n    \n\ndef update_weights(model,pred, x0, x1, x2, y_true, learning_rate):\n       \n  # On calcule les d\u00e9riv\u00e9es en fonction des poids et du biais \n  dL_dw0 = (pred - y_true) * x0\n  dL_dw1 = (pred - y_true) * x1\n  dL_dw2 = (pred - y_true) * x2\n  dL_db = pred - y_true\n      \n  # On modifie les param\u00e8tres pour r\u00e9duire le loss \n  # La modification des poids d\u00e9pend du learning rate, du signe de la d\u00e9riv\u00e9e et de la valeur de la d\u00e9riv\u00e9e\n  model.w0 -= learning_rate * dL_dw0\n  model.w1 -= learning_rate * dL_dw1\n  model.w2 -= learning_rate * dL_dw2\n  model.b -= learning_rate * dL_db\n</pre> # Notre classe de regression logistique class logistic_regression():   def __init__(self) -&gt; None:     self.w0=np.random.randn()     self.w1=np.random.randn()     self.w2=np.random.randn()     self.b=0   def __call__(self,x0,x1,x2):     # Somme pond\u00e9r\u00e9e et ajout du biais     pond=self.w0*x0+self.w1*x1+self.w2*x2+self.b     # Application de la sigmo\u00efde     pred=sigmoid(pond)     return pred      def loss(y_true, y_pred):   # Calcul du loss (log vraisemblance n\u00e9gative)   loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))   return loss       def update_weights(model,pred, x0, x1, x2, y_true, learning_rate):           # On calcule les d\u00e9riv\u00e9es en fonction des poids et du biais    dL_dw0 = (pred - y_true) * x0   dL_dw1 = (pred - y_true) * x1   dL_dw2 = (pred - y_true) * x2   dL_db = pred - y_true          # On modifie les param\u00e8tres pour r\u00e9duire le loss    # La modification des poids d\u00e9pend du learning rate, du signe de la d\u00e9riv\u00e9e et de la valeur de la d\u00e9riv\u00e9e   model.w0 -= learning_rate * dL_dw0   model.w1 -= learning_rate * dL_dw1   model.w2 -= learning_rate * dL_dw2   model.b -= learning_rate * dL_db In\u00a0[4]: Copied! <pre># Initialisation du mod\u00e8le et des hyperparam\u00e8tres\nlearning_rate = 0.01\nepochs = 1000 # le nombre d'it\u00e9rations d'entrainement\nmodel = logistic_regression()\n</pre>  # Initialisation du mod\u00e8le et des hyperparam\u00e8tres learning_rate = 0.01 epochs = 1000 # le nombre d'it\u00e9rations d'entrainement model = logistic_regression() <p>Avant d'entra\u00eener le mod\u00e8le, testons nos pr\u00e9dictions sur les deux \u00e9l\u00e8ves dont on d\u00e9sire conna\u00eetre le r\u00e9sultat de l'admission.</p> In\u00a0[5]: Copied! <pre>values_test=[[0.8,0.7,0.7],[0.4,0.5,0.9]]\nfor value in values_test:\n  x0,x1,x2=value\n  pred = model(x0, x1, x2)\n  print(\"L'\u00e9l\u00e8ve avec Examen = \"+str(x0)+ \", Moyenne = \"+str(x1)+\" et Motivation = \"+str(x2)+ \" a \"+str(round(pred*100)) + \"% de chance d'\u00eatre admis\")\n</pre> values_test=[[0.8,0.7,0.7],[0.4,0.5,0.9]] for value in values_test:   x0,x1,x2=value   pred = model(x0, x1, x2)   print(\"L'\u00e9l\u00e8ve avec Examen = \"+str(x0)+ \", Moyenne = \"+str(x1)+\" et Motivation = \"+str(x2)+ \" a \"+str(round(pred*100)) + \"% de chance d'\u00eatre admis\") <pre>L'\u00e9l\u00e8ve avec Examen = 0.8, Moyenne = 0.7 et Motivation = 0.7 a 60% de chance d'\u00eatre admis\nL'\u00e9l\u00e8ve avec Examen = 0.4, Moyenne = 0.5 et Motivation = 0.9 a 59% de chance d'\u00eatre admis\n</pre> <p>On voit que le mod\u00e8le est tr\u00e8s incertain et qu'il donne des probabilit\u00e9s au hasard ce qui est logique car ses poids sont initialis\u00e9s al\u00e9atoirement. Maintenant, entrainons le mod\u00e8le sur nos donn\u00e9es d'entrainement.</p> In\u00a0[9]: Copied! <pre># Entra\u00eenement\nfor epoch in range(epochs):\n  # Mise \u00e0 jour des poids pour chaque exemple\n  total_loss = 0\n  for (x0, x1, x2), y_true in zip(values_train, labels_train):\n    pred = model(x0, x1, x2)\n    update_weights(model,pred, x0, x1, x2, y_true, learning_rate)\n    total_loss += loss(y_true, pred)\n\n  avg_loss = total_loss / len(labels_train)\n  \n  # Affichage de la perte pour suivre la progression de l'entra\u00eenement\n  if ((epoch + 1) % 5000 == 0) or (epoch==0):\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")\n</pre> # Entra\u00eenement for epoch in range(epochs):   # Mise \u00e0 jour des poids pour chaque exemple   total_loss = 0   for (x0, x1, x2), y_true in zip(values_train, labels_train):     pred = model(x0, x1, x2)     update_weights(model,pred, x0, x1, x2, y_true, learning_rate)     total_loss += loss(y_true, pred)    avg_loss = total_loss / len(labels_train)      # Affichage de la perte pour suivre la progression de l'entra\u00eenement   if ((epoch + 1) % 5000 == 0) or (epoch==0):     print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\") <pre>Epoch 1/40000, Loss: 0.01468091027998586\nEpoch 5000/40000, Loss: 0.013032955086147595\nEpoch 10000/40000, Loss: 0.011715352279809266\nEpoch 15000/40000, Loss: 0.010638348324912276\nEpoch 20000/40000, Loss: 0.009741762611763436\nEpoch 25000/40000, Loss: 0.008983896958517028\nEpoch 30000/40000, Loss: 0.008334957514714105\nEpoch 35000/40000, Loss: 0.007773096000082178\nEpoch 40000/40000, Loss: 0.007281930357182074\n</pre> In\u00a0[10]: Copied! <pre>print(model.w0, model.w1, model.w2, model.b)\n\nfor value in values_test:\n  x0,x1,x2=value\n  pred = model(x0, x1, x2)\n  print(\"L'\u00e9l\u00e8ve avec Examen = \"+str(x0)+ \", Moyenne = \"+str(x1)+\" et Motivation = \"+str(x2)+ \" a \"+str(round(pred*100)) + \"% de chance d'\u00eatre admis\")\n</pre> print(model.w0, model.w1, model.w2, model.b)  for value in values_test:   x0,x1,x2=value   pred = model(x0, x1, x2)   print(\"L'\u00e9l\u00e8ve avec Examen = \"+str(x0)+ \", Moyenne = \"+str(x1)+\" et Motivation = \"+str(x2)+ \" a \"+str(round(pred*100)) + \"% de chance d'\u00eatre admis\") <pre>19.464301071981186 -3.27230109363944 -8.244865180820856 -4.903197398150705\nL'\u00e9l\u00e8ve avec Examen = 0.8, Moyenne = 0.7 et Motivation = 0.7 a 93% de chance d'\u00eatre admis\nL'\u00e9l\u00e8ve avec Examen = 0.4, Moyenne = 0.5 et Motivation = 0.9 a 0% de chance d'\u00eatre admis\n</pre> <p>Comme on peut le voir, notre mod\u00e8le est maintenant beaucoup plus confiant des ses pr\u00e9dictions et il nous donne des pr\u00e9dictions coh\u00e9rentes par rapport aux donn\u00e9es d'entrainement.</p>"},{"location":"01_Fondations/02_R%C3%A9gressionLogistique.html#regression-logistique","title":"R\u00e9gression logistique\u00b6","text":""},{"location":"01_Fondations/02_R%C3%A9gressionLogistique.html#le-neurone-artificiel","title":"Le neurone artificiel\u00b6","text":""},{"location":"01_Fondations/02_R%C3%A9gressionLogistique.html#les-fonctions-dactivation","title":"Les fonctions d'activation\u00b6","text":""},{"location":"01_Fondations/02_R%C3%A9gressionLogistique.html#application","title":"Application\u00b6","text":""},{"location":"01_Fondations/02_R%C3%A9gressionLogistique.html#fonction-de-cout","title":"Fonction de co\u00fbt\u00b6","text":""},{"location":"01_Fondations/02_R%C3%A9gressionLogistique.html#calcul-des-derivees","title":"Calcul des d\u00e9riv\u00e9es\u00b6","text":""},{"location":"01_Fondations/02_R%C3%A9gressionLogistique.html#regression-logistique","title":"R\u00e9gression logistique\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/index.html","title":"\ud83e\udde0 R\u00e9seau Fully Connected \ud83e\udde0","text":"<p>Ce cours introduit le fonctionnement d'un r\u00e9seau de neurones avec d'abord un exemple d'un r\u00e9seau cod\u00e9 avec micrograd pour permettre d'explorer cette library pour bien comprendre le fonctionnement. Une version fran\u00e7aise MicrogradFR est disponible dans ce repository.  Ensuite, pour introduire la library pytorch, le m\u00eame exemple est reconstruit mais en utilisant pytorch au lieu de micrograd. Le dernier notebook de cette partie introduit des techniques avanc\u00e9es d'entra\u00eenement de r\u00e9seau de neurones qu'il est utile de conna\u00eetre pour am\u00e9liorer les performances de nos r\u00e9seaux. </p>"},{"location":"02_R%C3%A9seauFullyConnected/index.html#notebook-1-mon-premier-reseau","title":"Notebook 1\ufe0f\u20e3 : Mon premier r\u00e9seau","text":"<p>Ce notebook propose une premi\u00e8re implementation d'un r\u00e9seau de neurones avec la library micrograd. Un dataset al\u00e9atoire est cr\u00e9e et on utilise la descente du gradient stochastique pour entra\u00eener le mod\u00e8le.</p>"},{"location":"02_R%C3%A9seauFullyConnected/index.html#notebook-2-pytorch-introduction","title":"Notebook 2\ufe0f\u20e3 : Pytorch introduction","text":"<p>Ce notebook r\u00e9sout exactement le m\u00eame probl\u00e8me que le notebook pr\u00e9c\u00e9dent mais en utilisant la library pytorch. La library pytorch est l'une des library les plus utilis\u00e9es en Deep Learning et il est important de savoir s'en servir.</p>"},{"location":"02_R%C3%A9seauFullyConnected/index.html#notebook-3-techniques-avancees","title":"Notebook 3\ufe0f\u20e3 : Techniques avanc\u00e9es","text":"<p>Ce notebook s'attaque au probl\u00e8me plus complexe de classification de chiffres sur la dataset MNIST. Des techniques de r\u00e9gularisation ainsi que la BatchNorm sont \u00e9galement introduites.</p>"},{"location":"02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html","title":"Mon premier r\u00e9seau","text":"<p>Dans cette ce cours, nous allons construire notre premier r\u00e9seau de neurones \u00e0 partir de la library micrograd. Micrograd est une library simple et compr\u00e9hensible permettant le calcul automatique des gradients. Il est conseill\u00e9 de comprendre les \u00e9l\u00e9ments de la library \u00e0 partir de la traduction propos\u00e9e dans le dossier MicrogradFR ou de la vid\u00e9o d'introduction d'Andrej Karpathy (en anglais). Ce notebook s'inspire \u00e9galement du notebook pr\u00e9sent dans le repository de micrograd.</p> In\u00a0[1]: Copied! <pre>#!pip install micrograd # uncomment to install micrograd\nimport random\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom micrograd.engine import Value\nfrom micrograd.nn import Neuron, Layer, MLP\n# Pour la reproducibilit\u00e9\nnp.random.seed(1337)\nrandom.seed(1337)\n</pre> #!pip install micrograd # uncomment to install micrograd import random import numpy as np  import matplotlib.pyplot as plt from micrograd.engine import Value from micrograd.nn import Neuron, Layer, MLP # Pour la reproducibilit\u00e9 np.random.seed(1337) random.seed(1337) <p>Pour construire un r\u00e9seau de neurones, nous avons d'abord besoin d'un probl\u00e8me \u00e0 r\u00e9soudre. Pour cela, nous utilisons la fonction make_moons de scikit-learn qui permet de cr\u00e9er un dataset. Notons qu'il est n\u00e9cessaire de faire un changement de variable pour faciliter le calcul du loss des parties suivantes. Au lieu d'avoir la classe 0 et la classe 1, nous aurons la classe -1 et la classe 1.</p> In\u00a0[2]: Copied! <pre>from sklearn.datasets import make_moons, make_blobs\nX, y = make_moons(n_samples=100, noise=0.1) # 100 \u00e9l\u00e9ments et un bruit Gaussien d'\u00e9cart type 0.1 ajout\u00e9 sur les donn\u00e9es \n\nprint(\"Les donn\u00e9es d'entr\u00e9e sont de la forme : \",X[1])\n\ny = y*2 - 1 # Pour avoir y=-1 ou y=1 (au lieu de 0 et 1)\n\n# Visualisation des donn\u00e9es en 2D\nplt.figure(figsize=(5,5))\nplt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n</pre> from sklearn.datasets import make_moons, make_blobs X, y = make_moons(n_samples=100, noise=0.1) # 100 \u00e9l\u00e9ments et un bruit Gaussien d'\u00e9cart type 0.1 ajout\u00e9 sur les donn\u00e9es   print(\"Les donn\u00e9es d'entr\u00e9e sont de la forme : \",X[1])  y = y*2 - 1 # Pour avoir y=-1 ou y=1 (au lieu de 0 et 1)  # Visualisation des donn\u00e9es en 2D plt.figure(figsize=(5,5)) plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet') <pre>Les donn\u00e9es d'entr\u00e9e sont de la forme :  [-0.81882941  0.05879006]\n</pre> Out[2]: <pre>&lt;matplotlib.collections.PathCollection at 0x177253e50&gt;</pre> <p>Maintenant, on va initialiser notre r\u00e9seau de neurones. Notre r\u00e9seau prend 2 valeurs en entr\u00e9e et doit nous ressortir un label -1 ou 1. Le r\u00e9seau de neurones que l'on va construire contient 2 couches cach\u00e9es de 16 neurones chacune. Chaque neurone est une r\u00e9gression logistique, il s'agit donc d'un assemblage non lin\u00e9aire de plusieurs r\u00e9gressions logistiques.</p> <p>Voici un aper\u00e7u de l'architecture de ce r\u00e9seau :</p> <p></p> In\u00a0[3]: Copied! <pre># Initialisation du mod\u00e8le \nmodel = MLP(2, [16, 16, 1]) # Couches d'entr\u00e9e de taille 2, deux couches cach\u00e9es de 16 neurones et un neurone de sortie\nprint(model)\nprint(\"Nombre de param\u00e8tres\", len(model.parameters()))\n</pre> # Initialisation du mod\u00e8le  model = MLP(2, [16, 16, 1]) # Couches d'entr\u00e9e de taille 2, deux couches cach\u00e9es de 16 neurones et un neurone de sortie print(model) print(\"Nombre de param\u00e8tres\", len(model.parameters())) <pre>MLP of [Layer of [ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2)], Layer of [ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16)], Layer of [LinearNeuron(16)]]\nNombre de param\u00e8tres 337\n</pre> <p>Avant de passer \u00e0 la suite de notre code, il est n\u00e9cessaire de faire un point sur la descente de gradient stochastique. Pour appliquer l'algorithme de descente de gradient sur un dataset de taille $N$, il faudrait en th\u00e9orie calculer la perte et le gradient sur chacun des \u00e9l\u00e9ments avant de mettre \u00e0 jour les poids. Cette m\u00e9thode a l'avantage de garantir une diminution de la perte \u00e0 chaque it\u00e9ration de l'entra\u00eenement mais est tr\u00e8s couteuse pour des datasets o\u00f9 $N$ est grand. En pratique, on a souvent $N&gt;10\u2076$. De plus, il faudrait garder les valeurs des gradients de l'ensemble des $N$ \u00e9l\u00e9ments en m\u00e9moire ce qui est impossible pour des ensembles de donn\u00e9es trop important.</p> <p>Pour palier \u00e0 ce probl\u00e8me, nous utilisons des mini-batch qui correspondent \u00e0 des groupes d'\u00e9chantillons du dataset. L'optimisation se fait de la m\u00eame mani\u00e8re que sur la descente de gradient sauf que la mise \u00e0 jour des poids se fait \u00e0 chaque mini-batch (donc beaucoup plus fr\u00e9quemment). Le processus d'optimisation est alors beaucoup plus rapide et permet de traiter des grandes quantit\u00e9s de donn\u00e9es. A titre indicatif, le param\u00e8tre d\u00e9finissant la taille d'un mini-batch est appel\u00e9 batch size et sa valeur est souvent 16, 32 ou 64. Pour en apprendre plus sur la descente du gradient stochastique : wikipedia ou blogpost</p> <p>D\u00e9finissons, en python, une fonction pour r\u00e9cuperer batch_size \u00e9l\u00e9ments al\u00e9atoires de notre dataset.</p> In\u00a0[4]: Copied! <pre>def get_batch(batch_size=64):\n  ri = np.random.permutation(X.shape[0])[:batch_size]\n  Xb, labels = X[ri], y[ri]\n  #inputs = [list(map(Value, xrow)) for xrow in Xb] # OLD\n  # Conversion des inputs en Value pour pouvoir utiliser micrograd\n  inputs = [list([Value(xrow[0]),Value(xrow[1])]) for xrow in Xb]\n  return inputs,labels\n</pre> def get_batch(batch_size=64):   ri = np.random.permutation(X.shape[0])[:batch_size]   Xb, labels = X[ri], y[ri]   #inputs = [list(map(Value, xrow)) for xrow in Xb] # OLD   # Conversion des inputs en Value pour pouvoir utiliser micrograd   inputs = [list([Value(xrow[0]),Value(xrow[1])]) for xrow in Xb]   return inputs,labels <p>Pour entra\u00eener notre r\u00e9seau de neurones, il est n\u00e9cessaire de d\u00e9finir une fonction de perte (loss). Dans notre cas, nous avons deux classes et nous souhaitons maximiser la marge entre les exemples appartenant \u00e0 des classes diff\u00e9rentes. Contrairement \u00e0 la perte de log-vraisemblance n\u00e9gative utilis\u00e9e pr\u00e9c\u00e9demment, nous allons chercher \u00e0 maximiser cet \u00e9cart, ce qui rendra notre m\u00e9thode plus robuste aux nouveaux \u00e9l\u00e9ments.</p> <p>On utilise le loss max-margin (marge maximale) qui est d\u00e9fini comme : $\\text{loss} = \\max(0, 1 - y_i \\cdot \\text{score}_i)$</p> In\u00a0[5]: Copied! <pre>def loss_function(scores,labels):\n  # La fonction .relu() prend le maximum entre 0 et la valeur de 1 - yi*scorei\n  losses = [(1 - yi*scorei).relu() for yi, scorei in zip(labels, scores)]\n  # On divise le loss par le nombre d'\u00e9l\u00e9ments du mini-batch\n  data_loss = sum(losses) * (1.0 / len(losses))\n  return data_loss\n</pre> def loss_function(scores,labels):   # La fonction .relu() prend le maximum entre 0 et la valeur de 1 - yi*scorei   losses = [(1 - yi*scorei).relu() for yi, scorei in zip(labels, scores)]   # On divise le loss par le nombre d'\u00e9l\u00e9ments du mini-batch   data_loss = sum(losses) * (1.0 / len(losses))   return data_loss <p>Maintenant que l'on poss\u00e9de les \u00e9l\u00e9ments cl\u00e9s de l'entrainement, il est temps de d\u00e9finir notre boucle d'entra\u00eenement</p> In\u00a0[6]: Copied! <pre># D\u00e9finissons nos hyper-param\u00e8tres d'entra\u00eenement \nbatch_size=128\niteration=50\n</pre> # D\u00e9finissons nos hyper-param\u00e8tres d'entra\u00eenement  batch_size=128 iteration=50 <p>On peut maintenant lancer l'entra\u00eenement de notre mod\u00e8le :</p> In\u00a0[7]: Copied! <pre>for k in range(iteration):\n    \n  # On r\u00e9cup\u00e8re notre mini-batch random\n  inputs,labels=get_batch(batch_size=batch_size)\n\n  # On fait appel au mod\u00e8le pour calculer les scores Y\n  scores = list(map(model, inputs))\n  \n  # On calcule le loss\n  loss=loss_function(scores,labels)\n\n\n  accuracy = [(label &gt; 0) == (scorei.data &gt; 0) for label, scorei in zip(labels, scores)]\n  accuracy=sum(accuracy) / len(accuracy)\n  \n  # Remise \u00e0 z\u00e9ro de valeurs de gradients avant de les calculer\n  model.zero_grad() \n  # Calcul des gradients gr\u00e2ce \u00e0 l'autograd de micrograd\n  loss.backward() \n  \n  # Mise \u00e0 jour des poids avec les gradients calcul\u00e9s (SGD)\n  learning_rate = 1.0 - 0.9*k/100 # On diminue le learning rate au fur et \u00e0 mesure de l'entra\u00eenement\n  for p in model.parameters():\n      p.data -= learning_rate * p.grad\n  \n  if k % 1 == 0:\n      print(f\"step {k} loss {loss.data}, accuracy {accuracy*100}%\")\n</pre> for k in range(iteration):        # On r\u00e9cup\u00e8re notre mini-batch random   inputs,labels=get_batch(batch_size=batch_size)    # On fait appel au mod\u00e8le pour calculer les scores Y   scores = list(map(model, inputs))      # On calcule le loss   loss=loss_function(scores,labels)     accuracy = [(label &gt; 0) == (scorei.data &gt; 0) for label, scorei in zip(labels, scores)]   accuracy=sum(accuracy) / len(accuracy)      # Remise \u00e0 z\u00e9ro de valeurs de gradients avant de les calculer   model.zero_grad()    # Calcul des gradients gr\u00e2ce \u00e0 l'autograd de micrograd   loss.backward()       # Mise \u00e0 jour des poids avec les gradients calcul\u00e9s (SGD)   learning_rate = 1.0 - 0.9*k/100 # On diminue le learning rate au fur et \u00e0 mesure de l'entra\u00eenement   for p in model.parameters():       p.data -= learning_rate * p.grad      if k % 1 == 0:       print(f\"step {k} loss {loss.data}, accuracy {accuracy*100}%\") <pre>step 0 loss 0.8862514464368221, accuracy 50.0%\nstep 1 loss 1.7136790633950052, accuracy 81.0%\nstep 2 loss 0.733396126728699, accuracy 77.0%\nstep 3 loss 0.7615247055858604, accuracy 82.0%\nstep 4 loss 0.35978083334534205, accuracy 84.0%\nstep 5 loss 0.3039360355411295, accuracy 86.0%\nstep 6 loss 0.2716587340549048, accuracy 89.0%\nstep 7 loss 0.25896576803013205, accuracy 91.0%\nstep 8 loss 0.2468445503533517, accuracy 91.0%\nstep 9 loss 0.26038987927745966, accuracy 91.0%\nstep 10 loss 0.23569710047306525, accuracy 91.0%\nstep 11 loss 0.2403768930229477, accuracy 92.0%\nstep 12 loss 0.20603128479123115, accuracy 91.0%\nstep 13 loss 0.22061157796029193, accuracy 93.0%\nstep 14 loss 0.19010711228374735, accuracy 92.0%\nstep 15 loss 0.21687609382796402, accuracy 93.0%\nstep 16 loss 0.18642445342175254, accuracy 92.0%\nstep 17 loss 0.2064478196088666, accuracy 92.0%\nstep 18 loss 0.15299793102189654, accuracy 94.0%\nstep 19 loss 0.18164592701596197, accuracy 93.0%\nstep 20 loss 0.15209012673698674, accuracy 92.0%\nstep 21 loss 0.17985784886850317, accuracy 93.0%\nstep 22 loss 0.13186703020683818, accuracy 95.0%\nstep 23 loss 0.13971668862318976, accuracy 95.0%\nstep 24 loss 0.1052987732627048, accuracy 96.0%\nstep 25 loss 0.11575214222251122, accuracy 95.0%\nstep 26 loss 0.10138692709798285, accuracy 97.0%\nstep 27 loss 0.13983249990522056, accuracy 95.0%\nstep 28 loss 0.13439433831552547, accuracy 92.0%\nstep 29 loss 0.14946491199753797, accuracy 95.0%\nstep 30 loss 0.08058651087395295, accuracy 97.0%\nstep 31 loss 0.08894402478347964, accuracy 97.0%\nstep 32 loss 0.14846171628770516, accuracy 95.0%\nstep 33 loss 0.08534324122933533, accuracy 97.0%\nstep 34 loss 0.06930425973662234, accuracy 97.0%\nstep 35 loss 0.10224309625633382, accuracy 95.0%\nstep 36 loss 0.05690038135075194, accuracy 97.0%\nstep 37 loss 0.04361190103707672, accuracy 97.0%\nstep 38 loss 0.04085954349889362, accuracy 98.0%\nstep 39 loss 0.06115860793647568, accuracy 97.0%\nstep 40 loss 0.06712605274368717, accuracy 99.0%\nstep 41 loss 0.1007776535938798, accuracy 95.0%\nstep 42 loss 0.047219615684323896, accuracy 97.0%\nstep 43 loss 0.03229533112572595, accuracy 99.0%\nstep 44 loss 0.028346610758982437, accuracy 99.0%\nstep 45 loss 0.05011775341000794, accuracy 99.0%\nstep 46 loss 0.09321634150737651, accuracy 96.0%\nstep 47 loss 0.055859575569553566, accuracy 98.0%\nstep 48 loss 0.02709965160774589, accuracy 99.0%\nstep 49 loss 0.054945465674614925, accuracy 98.0%\n</pre> <p>Comme vous pouvez le constater, le loss ne diminue pas syst\u00e9matiquement \u00e0 chaque \u00e9tape d'entra\u00eenement. Cela s'explique par la descente de gradient stochastique : le fait de ne pas prendre en compte l'ensemble des donn\u00e9es \u00e0 chaque it\u00e9ration introduit une part d'al\u00e9atoire. Cependant, le loss diminue au cours de l'entra\u00eenement (en moyenne) et on obtient un mod\u00e8le robuste beaucoup plus rapidement.</p> In\u00a0[8]: Copied! <pre># Visualisation de la fronti\u00e8re de d\u00e9cision \nh = 0.25\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nXmesh = np.c_[xx.ravel(), yy.ravel()]\ninputs = [list(map(Value, xrow)) for xrow in Xmesh]\nscores = list(map(model, inputs))\nZ = np.array([s.data &gt; 0 for s in scores])\nZ = Z.reshape(xx.shape)\n\nfig = plt.figure()\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n</pre> # Visualisation de la fronti\u00e8re de d\u00e9cision  h = 0.25 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                      np.arange(y_min, y_max, h)) Xmesh = np.c_[xx.ravel(), yy.ravel()] inputs = [list(map(Value, xrow)) for xrow in Xmesh] scores = list(map(model, inputs)) Z = np.array([s.data &gt; 0 for s in scores]) Z = Z.reshape(xx.shape)  fig = plt.figure() plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) Out[8]: <pre>(-1.548639298268643, 1.951360701731357)</pre>"},{"location":"02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html#mon-premier-reseau","title":"Mon premier r\u00e9seau\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html#construction-dun-reseau-avec-micrograd","title":"Construction d'un r\u00e9seau avec micrograd\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html#initialisation-du-dataset","title":"Initialisation du dataset\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html#creation-du-reseau-de-neurones","title":"Cr\u00e9ation du r\u00e9seau de neurones\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html#descente-de-gradient-stochastique-sgd","title":"Descente de gradient stochastique (SGD)\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html#fonction-de-perte","title":"Fonction de perte\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/01_MonPremierR%C3%A9seau.html#entrainement-du-modele","title":"Entra\u00eenement du mod\u00e8le\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html","title":"Introduction \u00e0 pytorch","text":"<p>Ce notebook commence par une introduction \u00e0 pytorch qui est la library la plus utilis\u00e9e actuellement dans le domaine du deep learning. Tout d'abord, reprenons notre r\u00e9seau de neurones du notebook pr\u00e9c\u00e9dent mais impl\u00e9mentons le en pytorch.</p> In\u00a0[2]: Copied! <pre>import random\nimport numpy as np \nimport matplotlib.pyplot as plt\n\n# Import classiques des utilisateurs de pytorch \nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n</pre> import random import numpy as np  import matplotlib.pyplot as plt  # Import classiques des utilisateurs de pytorch  import torch  import torch.nn as nn import torch.nn.functional as F <p>On va reconstruire un dataset de mani\u00e8re similaire au notebook pr\u00e9c\u00e9dent. Il faut aussi convertir les X et y en tensor (\u00e9quivalent de Value de micrograd).</p> In\u00a0[5]: Copied! <pre># Initialisation du dataset \nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=100, noise=0.1)\nX=torch.tensor(X).float() # Conversion en tensor pytorch (\u00e9quivalent de Value)\ny=torch.tensor(y).float() # Conversion en tensor pytorch\ny = y*2 - 1 \n\nplt.figure(figsize=(5,5))\nplt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n</pre> # Initialisation du dataset  from sklearn.datasets import make_moons  X, y = make_moons(n_samples=100, noise=0.1) X=torch.tensor(X).float() # Conversion en tensor pytorch (\u00e9quivalent de Value) y=torch.tensor(y).float() # Conversion en tensor pytorch y = y*2 - 1   plt.figure(figsize=(5,5)) plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet') Out[5]: <pre>&lt;matplotlib.collections.PathCollection at 0x7c6677d37350&gt;</pre> <p>Maintenant, profitons des fonctionnalit\u00e9s de pytorch pour cr\u00e9er notre dataset et notre dataloader. Le dataset regroupe simplement les entr\u00e9es et les labels alors que le dataloader permet de simplifier l'utilisation de la descente de gradient stochastique et obtenir directement un mini-batch de donn\u00e9es al\u00e9atoire provenant du dataset (le dataloader est un it\u00e9rateur).</p> In\u00a0[6]: Copied! <pre>from torch.utils.data import TensorDataset, DataLoader\n# Cr\u00e9ation d'un dataset qui stocke les couples de valeurs X,y\ndataset = TensorDataset(X, y)\n\n# Cr\u00e9ation d'un dataloader qui permet de g\u00e9rer automatiquement les mini-batchs pour la descente de gradient stochastique. \ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n</pre> from torch.utils.data import TensorDataset, DataLoader # Cr\u00e9ation d'un dataset qui stocke les couples de valeurs X,y dataset = TensorDataset(X, y)  # Cr\u00e9ation d'un dataloader qui permet de g\u00e9rer automatiquement les mini-batchs pour la descente de gradient stochastique.  dataloader = DataLoader(dataset, batch_size=32, shuffle=True) <p>Pour en apprendre plus sur le dataset et le dataloader, vous pouvez consulter la documentation de pytorch.</p> <p>Maintenant, construisons notre mod\u00e8le de 2 couches cach\u00e9es.</p> <p>Pour cela, nous allons construire une classe mlp qui h\u00e9rite de la classe nn.Module. Ensuite, nous construisons nos couches du r\u00e9seau gr\u00e2ce \u00e0 la fonction nn.Linear(in_features,out_features) qui construit une couche enti\u00e8rement connect\u00e9e prenant en entr\u00e9e in_features et renvoyant en sortie out_features. C'est une couche de out_features neurones connect\u00e9e \u00e0 in_features entr\u00e9es.</p> In\u00a0[7]: Copied! <pre>class mlp(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.fc1=nn.Linear(2,16) # premi\u00e8re couche cach\u00e9e \n    self.fc2=nn.Linear(16,16) # seconde couche cach\u00e9e \n    self.fc3=nn.Linear(16,1) # couche de sortie\n  \n  # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)\n  def forward(self,x):\n    x=F.relu(self.fc1(x)) # le F.relu permet d'appliquer la fonction d'activation ReLU sur la sortie de notre couche \n    x=F.relu(self.fc2(x))\n    output=self.fc3(x)\n    return output\n</pre> class mlp(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)     self.fc1=nn.Linear(2,16) # premi\u00e8re couche cach\u00e9e      self.fc2=nn.Linear(16,16) # seconde couche cach\u00e9e      self.fc3=nn.Linear(16,1) # couche de sortie      # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)   def forward(self,x):     x=F.relu(self.fc1(x)) # le F.relu permet d'appliquer la fonction d'activation ReLU sur la sortie de notre couche      x=F.relu(self.fc2(x))     output=self.fc3(x)     return output In\u00a0[8]: Copied! <pre>model = mlp() # Couches d'entr\u00e9e de taille 2, deux couches cach\u00e9es de 16 neurones et un neurone de sortie\nprint(model)\nprint(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters()))\n</pre> model = mlp() # Couches d'entr\u00e9e de taille 2, deux couches cach\u00e9es de 16 neurones et un neurone de sortie print(model) print(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters())) <pre>mlp(\n  (fc1): Linear(in_features=2, out_features=16, bias=True)\n  (fc2): Linear(in_features=16, out_features=16, bias=True)\n  (fc3): Linear(in_features=16, out_features=1, bias=True)\n)\nNombre de param\u00e8tres 337\n</pre> <p>On a bien le m\u00eame nombre de param\u00e8tres que dans le notebook pr\u00e9c\u00e9dent o\u00f9 on avait utilis\u00e9 micrograd.</p> <p>En pytorch, certaines fonctions de perte sont d\u00e9j\u00e0 implement\u00e9es dans la library. Avant d'impl\u00e9menter votre propre fonction, je vous invite \u00e0 v\u00e9rifier si celle-ci existe d\u00e9j\u00e0.</p> <p>Ici, nous allons utiliser la fonction torch.nn.MSELoss de pytorch. Il s'agit de la perte suivante : $\\text{MSE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ o\u00f9 $y_i$ est la valeur r\u00e9elle (label), $\\hat{y}_i$ est la valeur pr\u00e9dite et $n$ est nombre total d'exemples du mini-batch.</p> <p>En pytorch, cette fonction de perte s'impl\u00e9mente facilement :</p> In\u00a0[10]: Copied! <pre>criterion=torch.nn.MSELoss()\n</pre> criterion=torch.nn.MSELoss() <p>Pour l'entra\u00eenement, d\u00e9finissons les hyper-param\u00e8tres et l'optimizer. L'optimizer est une classe pytorch qui sp\u00e9cifie la m\u00e9thode de mise \u00e0 jour des poids lors de l'entra\u00eenement (mise \u00e0 jour des poids du mod\u00e8le et learning rate). Il existe plusieurs d'optimizer : SGD, Adam, Adagrad, RMSProp, etc... Nous utilisons SGD (Stochastic Gradient Descent) pour reproduire les conditions d'entra\u00eenement du notebook pr\u00e9c\u00e9dent mais, en pratique, on pr\u00e9f\u00e8re souvent utiliser Adam (Adaptive Moment Estimation). Pour en savoir plus sur les optimizers et leurs diff\u00e9rences, vous pouvez consulter le cours bonus sur les optimizers, la documentation pytorch ou le blogpost.</p> In\u00a0[11]: Copied! <pre># Hyper-param\u00e8tres\nepochs = 100 # Nombre de fois o\u00f9 l'on parcoure l'ensemble des donn\u00e9es d'entra\u00eenement\nlearning_rate=0.1\noptimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n</pre> # Hyper-param\u00e8tres epochs = 100 # Nombre de fois o\u00f9 l'on parcoure l'ensemble des donn\u00e9es d'entra\u00eenement learning_rate=0.1 optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate) <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>On peut \u00e9galement utiliser un scheduler qui a pour fonction de modifier la valeur du learning rate au cours de l'entra\u00eenement. Cela peut permettre d'acc\u00e9lerer la convergence en commen\u00e7ant avec un learning rate important et en le r\u00e9duisant au cours de l'entra\u00eenement. Pour avoir un learning rate qui diminue au cours de l'entra\u00eenement comme dans l'exemple micrograd, on peut utiliser LinearLR. Pour en savoir plus sur les diff\u00e9rents types de scheduler, vous pouvez regarder la documentation pytorch</p> In\u00a0[12]: Copied! <pre>scheduler=torch.optim.lr_scheduler.LinearLR(optimizer=optimizer,start_factor=1,end_factor=0.05)\n</pre> scheduler=torch.optim.lr_scheduler.LinearLR(optimizer=optimizer,start_factor=1,end_factor=0.05) In\u00a0[14]: Copied! <pre>for i in range(epochs):\n  loss_epoch=0\n  accuracy=[]\n  for batch_X, batch_y in dataloader:\n    scores=model(batch_X)        \n    loss=criterion(scores,batch_y.unsqueeze(1))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    #scheduler.step() # Pour activer le scheduler  \n    loss_epoch+=loss\n    \n    accuracy += [(label &gt; 0) == (scorei.data &gt; 0) for label, scorei in zip(batch_y, scores)]\n  accuracy=sum(accuracy) / len(accuracy) \n  \n  if i % 10 == 0:\n    print(f\"step {i} loss {loss_epoch}, pr\u00e9cision {accuracy*100}%\")\n</pre> for i in range(epochs):   loss_epoch=0   accuracy=[]   for batch_X, batch_y in dataloader:     scores=model(batch_X)             loss=criterion(scores,batch_y.unsqueeze(1))     optimizer.zero_grad()     loss.backward()     optimizer.step()     #scheduler.step() # Pour activer le scheduler       loss_epoch+=loss          accuracy += [(label &gt; 0) == (scorei.data &gt; 0) for label, scorei in zip(batch_y, scores)]   accuracy=sum(accuracy) / len(accuracy)       if i % 10 == 0:     print(f\"step {i} loss {loss_epoch}, pr\u00e9cision {accuracy*100}%\") <pre>step 0 loss 0.12318143993616104, pr\u00e9cision tensor([100.])%\nstep 10 loss 0.1347985863685608, pr\u00e9cision tensor([100.])%\nstep 20 loss 0.10458286106586456, pr\u00e9cision tensor([100.])%\nstep 30 loss 0.14222581684589386, pr\u00e9cision tensor([100.])%\nstep 40 loss 0.1081438660621643, pr\u00e9cision tensor([100.])%\nstep 50 loss 0.10838648676872253, pr\u00e9cision tensor([100.])%\nstep 60 loss 0.09485019743442535, pr\u00e9cision tensor([100.])%\nstep 70 loss 0.07788567245006561, pr\u00e9cision tensor([100.])%\nstep 80 loss 0.10796503722667694, pr\u00e9cision tensor([100.])%\nstep 90 loss 0.07869727909564972, pr\u00e9cision tensor([100.])%\n</pre> <p>L'entra\u00eenement est beaucoup plus rapide !</p> <p>On peut visualiser les r\u00e9sultats sur nos donn\u00e9es :</p> In\u00a0[15]: Copied! <pre>h = 0.25\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nXmesh = np.c_[xx.ravel(), yy.ravel()]\n\ninputs=torch.tensor(Xmesh).float()\nscores=model(inputs)\n\nZ = np.array([s.data &gt; 0 for s in scores])\nZ = Z.reshape(xx.shape)\n\nfig = plt.figure()\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n</pre> h = 0.25 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                      np.arange(y_min, y_max, h)) Xmesh = np.c_[xx.ravel(), yy.ravel()]  inputs=torch.tensor(Xmesh).float() scores=model(inputs)  Z = np.array([s.data &gt; 0 for s in scores]) Z = Z.reshape(xx.shape)  fig = plt.figure() plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) Out[15]: <pre>(-1.6418534517288208, 2.108146548271179)</pre> <p>Comme vous pouvez le voir, l'entra\u00eenement s'est bien pass\u00e9 et nous avons une bonne s\u00e9paration des donn\u00e9es.</p>"},{"location":"02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html#introduction-a-pytorch","title":"Introduction \u00e0 pytorch\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html#dataset-et-dataloader","title":"Dataset et Dataloader\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html#construction-du-modele","title":"Construction du mod\u00e8le\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html#fonction-de-perte","title":"Fonction de perte\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/02_PytorchIntroduction.html#entrainement","title":"Entra\u00eenement\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html","title":"Techniques avanc\u00e9es","text":"<p>Dans ce cours, nous allons introduire des techniques permettant de fiabiliser et de faciliter l'entra\u00eenement des r\u00e9seaux de neurones. Pour d\u00e9montrer l'int\u00earet de ces techniques, nous allons prendre le dataset MNIST qui regroupe des images de chiffres trac\u00e9s \u00e0 la main allant de 1 \u00e0 9. Le but de notre r\u00e9seau va \u00eatre de prendre une image en entr\u00e9e et d'identifier le chiffre.</p> In\u00a0[3]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n</pre> import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as T from torchvision import datasets from torch.utils.data import DataLoader import matplotlib.pyplot as plt <p>Pour commencer, nous allons t\u00e9l\u00e9charger le dataset MNIST. La library torchvision permet la gestion des images sur pytorch et poss\u00e8de un outil pour charger les datasets communs.</p> In\u00a0[4]: Copied! <pre>transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement\ndataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform)\ntest_dataset = datasets.MNIST(root='./../data', train=False,transform=transform)\n</pre> transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement dataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform) test_dataset = datasets.MNIST(root='./../data', train=False,transform=transform) In\u00a0[5]: Copied! <pre># On peut visualiser les \u00e9l\u00e9ments du dataset\nplt.imshow(dataset[0][0].permute(1,2,0).numpy(), cmap='gray')\nplt.show()\nprint(\"Le chiffre sur l'image est un \"+str(dataset[0][1]))\n</pre> # On peut visualiser les \u00e9l\u00e9ments du dataset plt.imshow(dataset[0][0].permute(1,2,0).numpy(), cmap='gray') plt.show() print(\"Le chiffre sur l'image est un \"+str(dataset[0][1])) <pre>Le chiffre sur l'image est un 5\n</pre> <p>Comme vous l'aurez remarqu\u00e9, lors du chargement du dataset, on a charg\u00e9 un train_dataset et un test_dataset. C'est une pratique que nous n'avons pas encore abord\u00e9e mais qui est d'une importance capitale lors d'un entra\u00eenement de r\u00e9seau de neurones. En effet, lorsque l'on entra\u00eene un r\u00e9seau sur des donn\u00e9es, il est logique que celui-ci soit bon sur les donn\u00e9es sur lesquelles il a \u00e9t\u00e9 entra\u00een\u00e9. Il est donc n\u00e9cessaire de cr\u00e9er un dataset de test qui va servir \u00e0 \u00e9valuer le mod\u00e8le sur des donn\u00e9es non vues pendant l'entra\u00eenement.</p> <p>En pratique, on utilise 3 sous-dataset, le training split que l'on utilise pour l'entra\u00eenement du mod\u00e8le, le validation split qui est utilis\u00e9 pour \u00e9valuer le mod\u00e8le au cours de l'entra\u00eenement et le test split qui permet d'\u00e9valuer le mod\u00e8le \u00e0 la fin de l'entra\u00eenement et qui est le r\u00e9sultat qui nous importe le plus.</p> <p>Une pratique courante est d'utiliser le split 60-20-20, c'est-\u00e0-dire 60% des donn\u00e9es pour l'entra\u00eenement, 20% pour la validation et 20% pour le test. Cependant, cette recommandation n'est pas pertinente pour toutes les tailles de dataset. En effet, si le dataset contient \u00e9normement d'images, on peut prendre moins de donn\u00e9es pour la validation et le test. A titre informatif, sur des datasets de plusieurs milliards d'images, on utilise souvent des splits 98-1-1 ou m\u00eame 99.8-0.1-0.1.</p> In\u00a0[6]: Copied! <pre>#Le train et test sont d\u00e9j\u00e0 s\u00e9par\u00e9, on va donc s\u00e9parer le train_dataset en train et validation\ntrain_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\n\n# Cr\u00e9ation des dataloaders pour s\u00e9parer en mini-batch automatiquement\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n</pre> #Le train et test sont d\u00e9j\u00e0 s\u00e9par\u00e9, on va donc s\u00e9parer le train_dataset en train et validation train_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])  # Cr\u00e9ation des dataloaders pour s\u00e9parer en mini-batch automatiquement train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) <p>Comme dans le notebook pr\u00e9c\u00e9dent, nous allons cr\u00e9er un mod\u00e8le enti\u00e8rement connect\u00e9 pour l'entra\u00eenement. Comme les donn\u00e9es d'entr\u00e9e sont des images de taille $28 \\times 28$, il est n\u00e9cessaire de les convertir en vecteur 1D de taille $28 \\times 28=784$ pour les faire passer dans le r\u00e9seau.</p> In\u00a0[8]: Copied! <pre>class mlp(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.fc1=nn.Linear(784,256) # premi\u00e8re couche cach\u00e9e \n    self.fc2=nn.Linear(256,256) # seconde couche cach\u00e9e \n    self.fc3=nn.Linear(256,10) # couche de sortie\n    \n  # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)\n  def forward(self,x):\n    x=x.view(-1,28*28) # Pour convertir l'image de taille 28x28 en tensor de taille 784\n    x=F.relu(self.fc1(x)) # le F.relu permet d'appliquer la fonction d'activation ReLU sur la sortie de notre couche \n    x=F.relu(self.fc2(x))\n    output=self.fc3(x)\n    return output\n</pre> class mlp(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)     self.fc1=nn.Linear(784,256) # premi\u00e8re couche cach\u00e9e      self.fc2=nn.Linear(256,256) # seconde couche cach\u00e9e      self.fc3=nn.Linear(256,10) # couche de sortie        # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)   def forward(self,x):     x=x.view(-1,28*28) # Pour convertir l'image de taille 28x28 en tensor de taille 784     x=F.relu(self.fc1(x)) # le F.relu permet d'appliquer la fonction d'activation ReLU sur la sortie de notre couche      x=F.relu(self.fc2(x))     output=self.fc3(x)     return output In\u00a0[10]: Copied! <pre>model = mlp()\nprint(model)\nprint(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters()))\n</pre> model = mlp() print(model) print(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters())) <pre>mlp(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\n)\nNombre de param\u00e8tres 269322\n</pre> <p>Pour la fonction de perte, on utilise la cross entropy loss de pytorch qui correspond \u00e0 la fonction de perte de la r\u00e9gression logistique mais pour un nombre de classes sup\u00e9rieur \u00e0 2. La fonction de perte s'\u00e9crit comme \u00e7a : $\\text{Cross Entropy Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{ic} \\log(p_{ic})$ o\u00f9 $N$ est le nombre d'exemple dans le mini-batch, $C$ est le nombre de classes, $y_{ic}$ est la valeur cible ($1$ si l'exemple appartient \u00e0 la classe $c$ et $0$ sinon) et $p_{ic}$ est la pr\u00e9diction de la probabilit\u00e9 d'appartenance \u00e0 la classe $c$.</p> In\u00a0[11]: Copied! <pre># En pytorch\ncriterion = nn.CrossEntropyLoss()\n</pre> # En pytorch criterion = nn.CrossEntropyLoss() In\u00a0[12]: Copied! <pre>epochs=5\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n</pre> epochs=5 learning_rate=0.001 optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate) <p>Entra\u00eenement du mod\u00e8le (peut durer quelques minutes en fonction de la puissance de votre ordinateur).</p> In\u00a0[13]: Copied! <pre>for i in range(epochs):\n  loss_train=0\n  for images, labels in train_loader:\n    preds=model(images)\n    loss=criterion(preds,labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, labels in val_loader:\n    with torch.no_grad(): # permet de ne pas calculer les gradients\n      preds=model(images)\n      loss=criterion(preds,labels)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> for i in range(epochs):   loss_train=0   for images, labels in train_loader:     preds=model(images)     loss=criterion(preds,labels)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, labels in val_loader:     with torch.no_grad(): # permet de ne pas calculer les gradients       preds=model(images)       loss=criterion(preds,labels)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 0.29076647758483887\nstep 0 val loss 0.15385286509990692\nstep 1 train loss 0.10695428401231766\nstep 1 val loss 0.10097559541463852\nstep 2 train loss 0.07086848467588425\nstep 2 val loss 0.09286081790924072\nstep 3 train loss 0.05028771981596947\nstep 3 val loss 0.08867377787828445\nstep 4 train loss 0.04254501312971115\nstep 4 val loss 0.0835222601890564\n</pre> <p>Maintenant que le mod\u00e8le est entra\u00een\u00e9, on peut regarder ses performances sur notre \"testing split\".</p> In\u00a0[14]: Copied! <pre>correct = 0\ntotal = 0\nfor images,labels in test_loader: \n  with torch.no_grad():\n    preds=model(images)\n    \n    _, predicted = torch.max(preds.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()     \ntest_acc = 100 * correct / total\nprint(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc)\n</pre> correct = 0 total = 0 for images,labels in test_loader:    with torch.no_grad():     preds=model(images)          _, predicted = torch.max(preds.data, 1)     total += labels.size(0)     correct += (predicted == labels).sum().item()      test_acc = 100 * correct / total print(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc) <pre>Pr\u00e9cision du mod\u00e8le en phase de test :  97.69\n</pre> <p>Notre mod\u00e8le obtient une tr\u00e8s bonne pr\u00e9cision en phase de test ce qui est bon signe. Cependant, on peut remarquer que, lors de l'entra\u00eenement, le loss de training est plus bas que le loss de validation. C'est un point important \u00e0 consid\u00e9rer. Cela signifie que le mod\u00e8le est en l\u00e9ger \"overfitting\".</p> <p>Un \u00e9l\u00e9ment cl\u00e9 de l'apprentissage profond est la capacit\u00e9 du mod\u00e8le \u00e0 ne pas overfit les donn\u00e9es d'entra\u00eenement. L'overfitting correspond \u00e0 un mod\u00e8le qui aurait trop bien appris sur les donn\u00e9es d'entra\u00eenement mais qui ne serait pas capable de g\u00e9n\u00e9raliser \u00e0 de nouveaux \u00e9l\u00e8ments issus de la m\u00eame distribution. Pour comprendre le principe, voici une figure faisant la diff\u00e9rence entre underfitting (mod\u00e8le trop simple incapable d'apprendre la complexit\u00e9 des donn\u00e9es), mod\u00e8le bien entrain\u00e9 et overfitting.</p> <p></p> <p>Dans le cas le plus critique de l'overfitting, le mod\u00e8le a une pr\u00e9cision presque parfaite sur les donn\u00e9es d'entra\u00eenement mais est mauvais sur les donn\u00e9es de validation et de test. Dans ce cours, nous allons introduire 2 m\u00e9thodes permettant d'\u00e9viter ce probl\u00e8me d'overfitting.</p> <p>La r\u00e9gularisation L2 est une m\u00e9thode qui consiste \u00e0 ajouter une p\u00e9nalit\u00e9 \u00e0 la perte bas\u00e9e sur la valeur des poids du mod\u00e8le. Cette p\u00e9nalit\u00e9 est proportionnelle au carr\u00e9 des valeurs des poids du mod\u00e8le (\u00e0 noter qu'il existe aussi la r\u00e9gularisation L1 qui est lin\u00e9airement proportionnelle au valeurs des poids du mod\u00e8le). Cette p\u00e9nalit\u00e9 encourage les poids du mod\u00e8le \u00e0 rester petits et moins sensibles au bruit des donn\u00e9es d'entra\u00eenement. On peut formuler la r\u00e9gularisation L2 de cette mani\u00e8re : $L(w) = L_0(w) + \\lambda \\sum_{i=1}^{n} w_i^2$ o\u00f9 $L(w)$ est la perte r\u00e9gularis\u00e9e, $L_0(w)$ est la fonction de perte classique, $\\lambda$ est le coefficient de r\u00e9gularisation et $w_i$ est un poids du mod\u00e8le. Pour en apprendre plus sur la r\u00e9gularisation L2, vous pouvez consulter le cours bonus sur la r\u00e9gularisation ou ce blogpost.</p> <p>Faisons \u00e0 nouveau notre entra\u00eenement mais en ajoutant la r\u00e9gularisation. En pytorch, la r\u00e9gularisation s'utilise en ajoutant le param\u00e8tre weight_decay \u00e0 notre optimizer. La valeur de weight_decay correspond au $\\lambda$ de l'\u00e9quation pr\u00e9c\u00e9dente.</p> In\u00a0[15]: Copied! <pre>model_with_reg=mlp()\nepochs=5\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model_with_reg.parameters(),lr=learning_rate,weight_decay=1e-5)\n\nfor i in range(epochs):\n  loss_train=0\n  for images, labels in train_loader:\n    preds=model_with_reg(images)\n    loss=criterion(preds,labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, labels in val_loader:\n    with torch.no_grad(): # permet de ne pas calculer les gradients\n      preds=model_with_reg(images)\n      loss=criterion(preds,labels)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> model_with_reg=mlp() epochs=5 learning_rate=0.001 optimizer=torch.optim.Adam(model_with_reg.parameters(),lr=learning_rate,weight_decay=1e-5)  for i in range(epochs):   loss_train=0   for images, labels in train_loader:     preds=model_with_reg(images)     loss=criterion(preds,labels)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, labels in val_loader:     with torch.no_grad(): # permet de ne pas calculer les gradients       preds=model_with_reg(images)       loss=criterion(preds,labels)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 0.2986273467540741\nstep 0 val loss 0.1439662128686905\nstep 1 train loss 0.11165566742420197\nstep 1 val loss 0.10781095176935196\nstep 2 train loss 0.07492929697036743\nstep 2 val loss 0.09555892646312714\nstep 3 train loss 0.05378309637308121\nstep 3 val loss 0.08672302216291428\nstep 4 train loss 0.041800014674663544\nstep 4 val loss 0.0883878618478775\n</pre> In\u00a0[16]: Copied! <pre>correct = 0\ntotal = 0\nfor images,labels in test_loader: \n  with torch.no_grad():\n    preds=model_with_reg(images)\n    \n    _, predicted = torch.max(preds.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()     \ntest_acc = 100 * correct / total\nprint(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc)\n</pre> correct = 0 total = 0 for images,labels in test_loader:    with torch.no_grad():     preds=model_with_reg(images)          _, predicted = torch.max(preds.data, 1)     total += labels.size(0)     correct += (predicted == labels).sum().item()      test_acc = 100 * correct / total print(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc) <pre>Pr\u00e9cision du mod\u00e8le en phase de test :  97.73\n</pre> <p>La diff\u00e9rence n'est pas flagrante mais on constaste une diminution de la diff\u00e9rence entre la perte de validation et la perte d'entra\u00eenement.</p> <p>Intuition : La r\u00e9gularisation L2 fonctionne parce qu'en p\u00e9nalisant les grands coefficients, elle favorise des solutions o\u00f9 les poids sont r\u00e9partis de mani\u00e8re plus uniforme, ce qui r\u00e9duit la sensibilit\u00e9 du mod\u00e8le aux variations sp\u00e9cifiques des donn\u00e9es d'entra\u00eenement et am\u00e9liore ainsi la robustesse et la g\u00e9n\u00e9ralisation du mod\u00e8le.</p> <p>Une autre m\u00e9thode de r\u00e9gularisation est le dropout. Cette m\u00e9thode consiste \u00e0 d\u00e9sactiver al\u00e9atoirement un pourcentage de neurones dans le r\u00e9seau \u00e0 chaque \u00e9tape de l'entra\u00eenement (les poids d\u00e9sactiv\u00e9s changent pendant l'entra\u00eenement). Chaque neurone d'une couche a une probabilit\u00e9 $p$ d'\u00eatre desactiv\u00e9.</p> <p>Cette technique force le r\u00e9seau \u00e0 ne pas se reposer sur certains neurones mais plut\u00f4t \u00e0 apprendre des representations plus robuste et qui g\u00e9n\u00e9ralisent mieux. On peut voir le dropout comme une sorte d'ensemble de mod\u00e8les o\u00f9 chaque mod\u00e8le est diff\u00e9rent (car certains neurones sont d\u00e9sactiv\u00e9s) et lors de la phase de test, on prend la \"moyenne\" de ces diff\u00e9rents mod\u00e8les. Lors de la phase de test, le dropout est d\u00e9sactiv\u00e9.</p> <p>Pour appliquer le dropout, il est n\u00e9cessaire de l'ajoute directement dans l'architecture du r\u00e9seau.</p> In\u00a0[17]: Copied! <pre>class mlp_dropout(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.fc1=nn.Linear(784,256) \n    self.dropout1 = nn.Dropout(0.2) # on d\u00e9sactive 20% des neurones al\u00e9atoirement\n    self.fc2=nn.Linear(256,256) \n    self.dropout2 = nn.Dropout(0.2) # on d\u00e9sactive 20% des neurones al\u00e9atoirement\n    self.fc3=nn.Linear(256,10) \n  \n  def forward(self,x):\n    x=x.view(-1,28*28)\n    x=F.relu(self.dropout1(self.fc1(x)))\n    x=F.relu(self.dropout2(self.fc2(x)))\n    output=self.fc3(x)\n    return output\n</pre> class mlp_dropout(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)     self.fc1=nn.Linear(784,256)      self.dropout1 = nn.Dropout(0.2) # on d\u00e9sactive 20% des neurones al\u00e9atoirement     self.fc2=nn.Linear(256,256)      self.dropout2 = nn.Dropout(0.2) # on d\u00e9sactive 20% des neurones al\u00e9atoirement     self.fc3=nn.Linear(256,10)       def forward(self,x):     x=x.view(-1,28*28)     x=F.relu(self.dropout1(self.fc1(x)))     x=F.relu(self.dropout2(self.fc2(x)))     output=self.fc3(x)     return output In\u00a0[18]: Copied! <pre>model_with_dropout=mlp_dropout()\nepochs=5\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model_with_dropout.parameters(),lr=learning_rate)\n\nfor i in range(epochs):\n  loss_train=0\n  for images, labels in train_loader:\n    preds=model_with_dropout(images)\n    loss=criterion(preds,labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, labels in val_loader:\n    with torch.no_grad(): # permet de ne pas calculer les gradients\n      preds=model_with_dropout(images)\n      loss=criterion(preds,labels)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> model_with_dropout=mlp_dropout() epochs=5 learning_rate=0.001 optimizer=torch.optim.Adam(model_with_dropout.parameters(),lr=learning_rate)  for i in range(epochs):   loss_train=0   for images, labels in train_loader:     preds=model_with_dropout(images)     loss=criterion(preds,labels)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, labels in val_loader:     with torch.no_grad(): # permet de ne pas calculer les gradients       preds=model_with_dropout(images)       loss=criterion(preds,labels)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 0.3267715573310852\nstep 0 val loss 0.19353896379470825\nstep 1 train loss 0.13504144549369812\nstep 1 val loss 0.14174170792102814\nstep 2 train loss 0.10012412816286087\nstep 2 val loss 0.13484247028827667\nstep 3 train loss 0.07837768644094467\nstep 3 val loss 0.10895466059446335\nstep 4 train loss 0.0631122887134552\nstep 4 val loss 0.10599609464406967\n</pre> In\u00a0[25]: Copied! <pre>correct = 0\ntotal = 0\nfor images,labels in test_loader: \n  with torch.no_grad():\n    preds=model_with_dropout(images)\n    \n    _, predicted = torch.max(preds.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()     \ntest_acc = 100 * correct / total\nprint(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc)\n</pre> correct = 0 total = 0 for images,labels in test_loader:    with torch.no_grad():     preds=model_with_dropout(images)          _, predicted = torch.max(preds.data, 1)     total += labels.size(0)     correct += (predicted == labels).sum().item()      test_acc = 100 * correct / total print(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc) <pre>Pr\u00e9cision du mod\u00e8le en phase de test :  96.96\n</pre> <p>On constate \u00e0 nouveau une l\u00e9g\u00e8re am\u00e9lioration du r\u00e9sultat de l'entra\u00eenement.</p> <p>Intuition : Le dropout am\u00e9liore la g\u00e9n\u00e9ralisation en d\u00e9sactivant al\u00e9atoirement des neurones pendant l'entra\u00eenement, ce qui emp\u00eache le mod\u00e8le de trop se fier \u00e0 certains neurones et force une distribution plus robuste et diversifi\u00e9e des caract\u00e9ristiques apprises.</p> <p>Une autre technique d'am\u00e9lioration de l'entra\u00eenement d'un r\u00e9seau de neurones est la Batch Normalization (BatchNorm). Le principe est de normaliser les entr\u00e9es de chaque couche d'une r\u00e9seau avec une distribution avec une moyenne nulle et une variance de 1. La normalisation s'effectue sur le batch complet de la mani\u00e8re suivante :</p> <p>Pour un mini-batch $B$ avec activations $x$ :</p> <p>$\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i$ la moyenne des activations $x_i$ des m \u00e9l\u00e9ments</p> <p>$\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2$ la variance des activations $x_i$ des m \u00e9l\u00e9ments</p> <p>$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$ la valeur de $x_i$ normalis\u00e9e</p> <p>$y_i = \\gamma \\hat{x}_i + \\beta$ l'ajout des param\u00e8tres $\\gamma $ et $\\beta$ permettre au r\u00e9seau d'apprendre les distributions d'activation optimales.</p> <p>o\u00f9 $m$ est la taille du mini-batch $B$, $\\epsilon$ est une petite constante ajout\u00e9e pour \u00e9viter la division par z\u00e9ro, et $\\gamma $ et $\\beta$ sont des param\u00e8tres apprenables.</p> <p>En pratique, on constate 4 principaux avantages lors de l'utilisation de la BatchNorm.</p> <ul> <li>Acc\u00e9leration de l'entra\u00eenement : La normalisation des entr\u00e9es de chaque couche permet d'utiliser un learning rate plus \u00e9l\u00e9v\u00e9 et donc d'acc\u00e9l\u00e8rer la convergence de l'entra\u00eenement.</li> <li>R\u00e9duction de la sensibilit\u00e9 \u00e0 l'initialisation des poids : La BatchNorm permet de stabiliser la distribution des activations ce qui rend le r\u00e9seau moins sensible \u00e0 l'initialisation des poids.</li> <li>Am\u00e9lioration de la g\u00e9n\u00e9ralisation : Comme le dropout et la r\u00e9gularisation l2, la BatchNorm agit comme une forme de r\u00e9gularisation. Cela est d\u00fb au bruit induit par le fait de normaliser sur le batch.</li> <li>R\u00e9duction du \"Internal Covariate Shift\" : La stabilisation des activations tout au long du r\u00e9seau permet de r\u00e9duire le changement des distributions des couches internes ce qui facilite l'apprentissage.</li> </ul> <p>Ce qu'il faut retenir c'est que la BatchNorm offre de nombreux avantages et il est donc conseill\u00e9 de l'utiliser syst\u00e9matiquement.</p> <p>Il existe \u00e9galement d'autres techniques de normalisation comme la LayerNorm, la InstanceNorm, la GroupNorm et d'autres ... Pour en apprendre plus sur la batch normalization, vous pouvez faire le cours bonus sur la batch norm, lire le papier ou le blogpost. Pour avoir des informations suppl\u00e9mentaires sur l'int\u00earet de la normalisation pour l'entra\u00eenement des r\u00e9seaux de neurones, vous pouvez consulter le blogpost.</p> <p>Pour impl\u00e9menter la BatchNorm en pytorch, il faut l'ajouter directement dans la construction de notre mod\u00e8le. A noter qu'on applique souvent la BatchNorm avant la fonction d'activation mais que les deux approches sont possibles (avant ou apr\u00e8s).</p> In\u00a0[26]: Copied! <pre>class mlp_bn(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.fc1=nn.Linear(784,256) \n    self.bn1=nn.BatchNorm1d(256) # Batch Normalization\n    self.fc2=nn.Linear(256,256) \n    self.bn2=nn.BatchNorm1d(256) # Batch Normalization\n    self.fc3=nn.Linear(256,10) \n  \n  def forward(self,x):\n    x=x.view(-1,28*28)\n    x=F.relu(self.bn1(self.fc1(x)))\n    x=F.relu(self.bn1(self.fc2(x)))\n    output=self.fc3(x)\n    return output\n</pre> class mlp_bn(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)     self.fc1=nn.Linear(784,256)      self.bn1=nn.BatchNorm1d(256) # Batch Normalization     self.fc2=nn.Linear(256,256)      self.bn2=nn.BatchNorm1d(256) # Batch Normalization     self.fc3=nn.Linear(256,10)       def forward(self,x):     x=x.view(-1,28*28)     x=F.relu(self.bn1(self.fc1(x)))     x=F.relu(self.bn1(self.fc2(x)))     output=self.fc3(x)     return output In\u00a0[27]: Copied! <pre>model_with_bn=mlp_bn()\nepochs=5\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model_with_bn.parameters(),lr=learning_rate)\n\nfor i in range(epochs):\n  loss_train=0\n  for images, labels in train_loader:\n    preds=model_with_bn(images)\n    loss=criterion(preds,labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, labels in val_loader:\n    with torch.no_grad(): # permet de ne pas calculer les gradients\n      preds=model_with_bn(images)\n      loss=criterion(preds,labels)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> model_with_bn=mlp_bn() epochs=5 learning_rate=0.001 optimizer=torch.optim.Adam(model_with_bn.parameters(),lr=learning_rate)  for i in range(epochs):   loss_train=0   for images, labels in train_loader:     preds=model_with_bn(images)     loss=criterion(preds,labels)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, labels in val_loader:     with torch.no_grad(): # permet de ne pas calculer les gradients       preds=model_with_bn(images)       loss=criterion(preds,labels)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 0.20796926319599152\nstep 0 val loss 0.1327729970216751\nstep 1 train loss 0.09048832952976227\nstep 1 val loss 0.10177803039550781\nstep 2 train loss 0.0635765939950943\nstep 2 val loss 0.09861738979816437\nstep 3 train loss 0.045849185436964035\nstep 3 val loss 0.09643400460481644\nstep 4 train loss 0.0397462323307991\nstep 4 val loss 0.08524414896965027\n</pre> In\u00a0[28]: Copied! <pre>correct = 0\ntotal = 0\nfor images,labels in test_loader: \n  with torch.no_grad():\n    preds=model_with_bn(images)\n    \n    _, predicted = torch.max(preds.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()     \ntest_acc = 100 * correct / total\nprint(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc)\n</pre> correct = 0 total = 0 for images,labels in test_loader:    with torch.no_grad():     preds=model_with_bn(images)          _, predicted = torch.max(preds.data, 1)     total += labels.size(0)     correct += (predicted == labels).sum().item()      test_acc = 100 * correct / total print(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc) <pre>Pr\u00e9cision du mod\u00e8le en phase de test :  97.19\n</pre> <p>Comme vous pouvez le voir, la BatchNorm permet d'obtenir un meilleur score sur nos donn\u00e9es dans les m\u00eames conditions d'entra\u00eenement.</p>"},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#techniques-avancees","title":"Techniques avanc\u00e9es\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#creation-du-dataset","title":"Cr\u00e9ation du dataset\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#separation-trainvalidationtest","title":"S\u00e9paration train/validation/test\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#creation-et-entrainement-dun-premier-modele","title":"Cr\u00e9ation et entra\u00eenement d'un premier mod\u00e8le\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#fonction-de-perte","title":"Fonction de perte\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#hyperparametres-et-entrainement","title":"Hyperparam\u00e8tres et entra\u00eenement\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#verification-du-modele-sur-les-donnees-de-test","title":"V\u00e9rification du mod\u00e8le sur les donn\u00e9es de test\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#overfitting-et-underfitting","title":"Overfitting et underfitting\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#regularisation-l2","title":"R\u00e9gularisation L2\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#dropout","title":"Dropout\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/03_TechniquesAvanc%C3%A9es.html#batch-normalization","title":"Batch Normalization\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/index.html","title":"micrograd en fran\u00e7ais","text":"<p>Cette library reprend l'impl\u00e9mentation de micrograd de Andrej Karpathy avec un ajout de commentaire en version fran\u00e7aise.  Pour comprendre micrograd dans son entieret\u00e9, veuillez vous r\u00e9ferer \u00e0 la vid\u00e9o youtube d'Andrej Karpathy qui montre la cr\u00e9ation de la librairy et qui est une excellente introduction aux r\u00e9seaux de neurones.  Le but \u00e9tant d'utiliser une library compr\u00e9hensible par un \u00e9tudiant plut\u00f4t que pytorch directement</p> <p> </p> <p>Cette library contient petit Autograd engine qui permet d'effectuer la retropropagation du gradient sur un graphe construit dynamiquement et contient \u00e9galement une library pour construire un petit r\u00e9seau de neurones. L'API est similaire \u00e0 celle de Pytorch. Les deux fichiers sont petits (50 et 100 lignes environ). Le graphe n'op\u00e9re que sur des scalaires et il est donc n\u00e9cessaire de d\u00e9couper chaque neurone en petites additions et multiplications. C'est cependant suffisant pour construie des r\u00e9seaux de neurones de base pouvant \u00eatre utilis\u00e9 \u00e0 des fins \u00e9ducatives. </p>"},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/index.html#installation","title":"Installation","text":"<p>Il est possible d'installer micrograd directement avec pip.</p> <pre><code>pip install micrograd\n</code></pre>"},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/index.html#exemple-dutilisation","title":"Exemple d'utilisation","text":"<p>En dessous, voici un petit exemple montrant les op\u00e9rations possibles pour la classe Value()</p> <pre><code>from micrograd.engine import Value\n\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a * b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d * 2 + (b + a).relu()\nd += 3 * d + (b - a).relu()\ne = c - d\nf = e**2\ng = f / 2.0\ng += 10.0 / f\nprint(f'{g.data:.4f}') # prints 24.7041, La sortie\ng.backward() # Calcule la r\u00e9tropropagation du gradient\nprint(f'{a.grad:.4f}') # prints 138.8338, i.e. la valeur num\u00e9rique dg/da\nprint(f'{b.grad:.4f}') # prints 645.5773, i.e. la valeur num\u00e9rique of dg/db\n</code></pre>"},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/index.html#entrainement-dun-reseau-de-neurones","title":"Entrainement d'un r\u00e9seau de neurones","text":"<p>Le notebook 'demo.ipynb' offre une d\u00e9monstration compl\u00e8te de l'entrainement d'un r\u00e9seau de neurones (MLP) de deux couches pour la classification binaire. C'est r\u00e9alis\u00e9 en initialisant un r\u00e9seau de neurones depuis le module micrograd.nn, en impl\u00e9mentant un svm \"max-margin\" loss de classification et utilisant la descente de gradient stochastique pour l'optimisation (SGD). Comme montr\u00e9 dans le notebook, un r\u00e9seau de neurones de 2 couches de 16 neurones chacune permet d'obtenir la classification suivante sur le dataset moon : </p> <p></p>"},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/index.html#visualisation","title":"Visualisation","text":"<p>Pour plus de simplicit\u00e9, le notebook <code>trace_graph.ipynb</code> montre comment produite une visualisation graphviz. La visualisation ci-dessous est un simple neurone 2D, que l'on a trac\u00e9 en appelant la fonction <code>draw_dot</code> du code ci-dessous. Cette visualisation montre \u00e0 la fois les donn\u00e9es (nombre de gauche de chaque noeud) et le gradient (nombre de droite de chaque noeud).</p> <pre><code>from micrograd import nn\nn = nn.Neuron(2)\nx = [Value(1.0), Value(-2.0)]\ny = n(x)\ndot = draw_dot(y)\n</code></pre> <p></p>"},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/index.html#test-unitaires","title":"Test unitaires","text":"<p>Pour lancer les tests unitaires, vous devez installer PyTorch, que les tests utilisent comme une r\u00e9ference pour v\u00e9rifier la valeur des gradients calcul\u00e9s. Ensuite il suffit de faire :</p> <pre><code>python -m pytest\n</code></pre>"},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/index.html#license","title":"License","text":"<p>MIT</p>"},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/demo.html","title":"Demo","text":"In\u00a0[2]: Copied! <pre>#!pip install micrograd\n#!pip install scikit-learn\n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> #!pip install micrograd #!pip install scikit-learn  import random import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[3]: Copied! <pre>from micrograd.engine import Value\nfrom micrograd.nn import Neuron, Layer, MLP\n</pre> from micrograd.engine import Value from micrograd.nn import Neuron, Layer, MLP In\u00a0[4]: Copied! <pre># Seed pour pouvoir reproduire les r\u00e9sultats\nnp.random.seed(1337)\nrandom.seed(1337)\n</pre> # Seed pour pouvoir reproduire les r\u00e9sultats np.random.seed(1337) random.seed(1337) In\u00a0[5]: Copied! <pre># cr\u00e9ation du dataset (dataset moon)\n\nfrom sklearn.datasets import make_moons, make_blobs\nX, y = make_moons(n_samples=100, noise=0.1)\n\ny = y*2 - 1 # make y be -1 or 1\n# Visualisation du dataset en 2D\nplt.figure(figsize=(5,5))\nplt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n</pre> # cr\u00e9ation du dataset (dataset moon)  from sklearn.datasets import make_moons, make_blobs X, y = make_moons(n_samples=100, noise=0.1)  y = y*2 - 1 # make y be -1 or 1 # Visualisation du dataset en 2D plt.figure(figsize=(5,5)) plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet') Out[5]: <pre>&lt;matplotlib.collections.PathCollection at 0x166671d50&gt;</pre> In\u00a0[6]: Copied! <pre># Creation d'un mod\u00e8le de 2 couches  \nmodel = MLP(2, [16, 16, 1]) # 2 couches cach\u00e9es de 16 neurones\nprint(model)\nprint(\"number of parameters\", len(model.parameters()))\n</pre> # Creation d'un mod\u00e8le de 2 couches   model = MLP(2, [16, 16, 1]) # 2 couches cach\u00e9es de 16 neurones print(model) print(\"number of parameters\", len(model.parameters())) <pre>MLP of [Layer of [ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2)], Layer of [ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16)], Layer of [LinearNeuron(16)]]\nnumber of parameters 337\n</pre> In\u00a0[7]: Copied! <pre># Fonction de perte (loss)\ndef loss(batch_size=None):\n    \n    # Permet de traiter en batch ou non, avec permutation al\u00e9atoire\n    if batch_size is None:\n        Xb, yb = X, y\n    else:\n        ri = np.random.permutation(X.shape[0])[:batch_size]\n        Xb, yb = X[ri], y[ri]\n\n    # map permet d'appliquer Value \u00e0 chaque \u00e9l\u00e9ment de la liste\n    inputs = [list(map(Value, xrow)) for xrow in Xb]\n    \n    # on applique le mod\u00e8le \u00e0 chaque entr\u00e9es\n    scores = list(map(model, inputs))\n    \n    # on calcule la perte (svm max-margin loss)\n    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n    data_loss = sum(losses) * (1.0 / len(losses))\n    # On applique une r\u00e9gularisation L2\n    alpha = 1e-4\n    reg_loss = alpha * sum((p*p for p in model.parameters()))\n    total_loss = data_loss + reg_loss\n    \n    # On r\u00e9cup\u00e8re la pr\u00e9cision du mod\u00e8le en plus du loss\n    accuracy = [(yi &gt; 0) == (scorei.data &gt; 0) for yi, scorei in zip(yb, scores)]\n    return total_loss, sum(accuracy) / len(accuracy)\n\ntotal_loss, acc = loss()\nprint(total_loss, acc)\n</pre> # Fonction de perte (loss) def loss(batch_size=None):          # Permet de traiter en batch ou non, avec permutation al\u00e9atoire     if batch_size is None:         Xb, yb = X, y     else:         ri = np.random.permutation(X.shape[0])[:batch_size]         Xb, yb = X[ri], y[ri]      # map permet d'appliquer Value \u00e0 chaque \u00e9l\u00e9ment de la liste     inputs = [list(map(Value, xrow)) for xrow in Xb]          # on applique le mod\u00e8le \u00e0 chaque entr\u00e9es     scores = list(map(model, inputs))          # on calcule la perte (svm max-margin loss)     losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]     data_loss = sum(losses) * (1.0 / len(losses))     # On applique une r\u00e9gularisation L2     alpha = 1e-4     reg_loss = alpha * sum((p*p for p in model.parameters()))     total_loss = data_loss + reg_loss          # On r\u00e9cup\u00e8re la pr\u00e9cision du mod\u00e8le en plus du loss     accuracy = [(yi &gt; 0) == (scorei.data &gt; 0) for yi, scorei in zip(yb, scores)]     return total_loss, sum(accuracy) / len(accuracy)  total_loss, acc = loss() print(total_loss, acc) <pre>Value(data=0.8958441028683222, grad=0) 0.5\n</pre> In\u00a0[15]: Copied! <pre># On optimise le mod\u00e8le avec la descente de gradient stochastique\nfor k in range(100):\n    \n    # forward\n    total_loss, acc = loss()\n    \n    # backward\n    model.zero_grad()\n    total_loss.backward()\n    \n    # update (sgd)\n    learning_rate = 1.0 - 0.9*k/100\n    for p in model.parameters():\n        p.data -= learning_rate * p.grad\n    \n    if k % 1 == 0:\n        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n</pre> # On optimise le mod\u00e8le avec la descente de gradient stochastique for k in range(100):          # forward     total_loss, acc = loss()          # backward     model.zero_grad()     total_loss.backward()          # update (sgd)     learning_rate = 1.0 - 0.9*k/100     for p in model.parameters():         p.data -= learning_rate * p.grad          if k % 1 == 0:         print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")  <pre>step 0 loss 1.4685917829173556, accuracy 49.0%\nstep 1 loss 0.8928663245908091, accuracy 50.0%\nstep 2 loss 0.81867496281009, accuracy 70.0%\nstep 3 loss 0.7042295404971071, accuracy 67.0%\nstep 4 loss 0.539262007434332, accuracy 78.0%\nstep 5 loss 0.3873343752864074, accuracy 84.0%\nstep 6 loss 0.3002090298074754, accuracy 85.0%\n</pre> In\u00a0[14]: Copied! <pre># Visualisation de la fronti\u00e8re de d\u00e9cision\n\nh = 0.25\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nXmesh = np.c_[xx.ravel(), yy.ravel()]\ninputs = [list(map(Value, xrow)) for xrow in Xmesh]\nscores = list(map(model, inputs))\nZ = np.array([s.data &gt; 0 for s in scores])\nZ = Z.reshape(xx.shape)\n\nfig = plt.figure()\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n</pre> # Visualisation de la fronti\u00e8re de d\u00e9cision  h = 0.25 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                      np.arange(y_min, y_max, h)) Xmesh = np.c_[xx.ravel(), yy.ravel()] inputs = [list(map(Value, xrow)) for xrow in Xmesh] scores = list(map(model, inputs)) Z = np.array([s.data &gt; 0 for s in scores]) Z = Z.reshape(xx.shape)  fig = plt.figure() plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max())  Out[14]: <pre>(-1.548639298268643, 1.951360701731357)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/demo.html#micrograd-demo","title":"MicroGrad demo\u00b6","text":""},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/engine.html","title":"Engine","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>class Value:\n    \"\"\" Classe permettant de stocker un scalaire et son gradient, permet aussi de faire des op\u00e9rations sur cet objet\"\"\"\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0 # le gradient est initialis\u00e9 \u00e0 0\n        \n        self._backward = lambda: None # initilisation avec la fonction None \n        self._prev = set(_children) # Initialise la valeur en amont dans le graphe\n        self._op = _op # L'op\u00e9ration qui a produit ce noeud pour visualisation et debugging\n\n    # M\u00e9thode d'ajout, la m\u00e9thode __add__ sur une classe python correspond \u00e0 l'op\u00e9rateur '+'\n    def __add__(self, other): #self+other\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        # Le backward permet de d\u00e9finir l'op\u00e9ration \u00e0 effectuer lors de la descente de gradient\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    # M\u00e9thode de multiplication, la m\u00e9thode __mul__ sur une classe python correspond \u00e0 l'op\u00e9rateur '*'\n    def __mul__(self, other): #self*other\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    # M\u00e9thode de puissance, la m\u00e9thode __pow__ sur une classe python correspond \u00e0 l'op\u00e9rateur '**'\n    def __pow__(self, other): ##self**other\n        assert isinstance(other, (int, float)), \"ne supporte que les int et float pour le moment\"\n        out = Value(self.data**other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += (other * self.data**(other-1)) * out.grad\n        out._backward = _backward\n\n        return out\n\n    # La fonction d'activation ReLU, l'output vaut 0 si l'input est inf\u00e9rieur \u00e0 0, sinon il est \u00e9gal \u00e0 l'input\n    def relu(self):\n        out = Value(0 if self.data &lt; 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data &gt; 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    # Fonction backward qui permet de calculer les gradients sur l'ensemble du graphe relativement \u00e0 ce noeud\n    def backward(self):\n\n        # ordre topologique de tous les noeuds enfant du graphe \n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # Calcule le gradient de chaque varible une par une et applique la r\u00e8gle de la cha\u00eene pour calculer son gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    # M\u00e9thode de n\u00e9gation, la m\u00e9thode __neg__ sur une classe python correspond \u00e0 l'op\u00e9rateur pour passer de x \u00e0 -x\n    def __neg__(self): # -self\n        return self * -1\n    \n    # M\u00e9thode de n\u00e9gation, la m\u00e9thode __neg__ sur une classe python correspond \u00e0 l'op\u00e9rateur pour passer de x \u00e0 -x\n    def __radd__(self, other): # other + self\n        return self + other\n\n    # M\u00e9thode de soustraction, la m\u00e9thode __neg__ sur une classe python correspond \u00e0 l'op\u00e9rateur '-'\n    def __sub__(self, other): # self - other\n        return self + (-other)\n\n    # M\u00e9thode de soustraction, permet de pouvoir r\u00e9aliser l'op\u00e9ration other - self m\u00eame si other n'est pas une Value\n    def __rsub__(self, other): # other - self\n        return other + (-self)\n\n    # M\u00e9thode de multiplication, permet de pouvoir r\u00e9aliser l'op\u00e9ration other * self m\u00eame si other n'est pas une Value\n    def __rmul__(self, other): # other * self\n        return self * other\n\n    # M\u00e9thode de division, la m\u00e9thode __truediv__ sur une classe python correspond \u00e0 l'op\u00e9rateur '/'\n    def __truediv__(self, other): # self / other\n        return self * other**-1\n    \n    # M\u00e9thode de division, permet de pouvoir r\u00e9aliser l'op\u00e9ration other / self m\u00eame si other n'est pas une Value\n    def __rtruediv__(self, other): # other / self\n        return other * self**-1\n\n    # M\u00e9thode de representation d'un objet, donne les informations importantes d'un objet, c'est ce qui va \n    # s'afficher lorsqu'on fait print()\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> class Value:     \"\"\" Classe permettant de stocker un scalaire et son gradient, permet aussi de faire des op\u00e9rations sur cet objet\"\"\"      def __init__(self, data, _children=(), _op=''):         self.data = data         self.grad = 0 # le gradient est initialis\u00e9 \u00e0 0                  self._backward = lambda: None # initilisation avec la fonction None          self._prev = set(_children) # Initialise la valeur en amont dans le graphe         self._op = _op # L'op\u00e9ration qui a produit ce noeud pour visualisation et debugging      # M\u00e9thode d'ajout, la m\u00e9thode __add__ sur une classe python correspond \u00e0 l'op\u00e9rateur '+'     def __add__(self, other): #self+other         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data + other.data, (self, other), '+')          # Le backward permet de d\u00e9finir l'op\u00e9ration \u00e0 effectuer lors de la descente de gradient         def _backward():             self.grad += out.grad             other.grad += out.grad         out._backward = _backward          return out      # M\u00e9thode de multiplication, la m\u00e9thode __mul__ sur une classe python correspond \u00e0 l'op\u00e9rateur '*'     def __mul__(self, other): #self*other         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data * other.data, (self, other), '*')          def _backward():             self.grad += other.data * out.grad             other.grad += self.data * out.grad         out._backward = _backward          return out      # M\u00e9thode de puissance, la m\u00e9thode __pow__ sur une classe python correspond \u00e0 l'op\u00e9rateur '**'     def __pow__(self, other): ##self**other         assert isinstance(other, (int, float)), \"ne supporte que les int et float pour le moment\"         out = Value(self.data**other, (self,), f'**{other}')          def _backward():             self.grad += (other * self.data**(other-1)) * out.grad         out._backward = _backward          return out      # La fonction d'activation ReLU, l'output vaut 0 si l'input est inf\u00e9rieur \u00e0 0, sinon il est \u00e9gal \u00e0 l'input     def relu(self):         out = Value(0 if self.data &lt; 0 else self.data, (self,), 'ReLU')          def _backward():             self.grad += (out.data &gt; 0) * out.grad         out._backward = _backward          return out      # Fonction backward qui permet de calculer les gradients sur l'ensemble du graphe relativement \u00e0 ce noeud     def backward(self):          # ordre topologique de tous les noeuds enfant du graphe          topo = []         visited = set()         def build_topo(v):             if v not in visited:                 visited.add(v)                 for child in v._prev:                     build_topo(child)                 topo.append(v)         build_topo(self)          # Calcule le gradient de chaque varible une par une et applique la r\u00e8gle de la cha\u00eene pour calculer son gradient         self.grad = 1         for v in reversed(topo):             v._backward()      # M\u00e9thode de n\u00e9gation, la m\u00e9thode __neg__ sur une classe python correspond \u00e0 l'op\u00e9rateur pour passer de x \u00e0 -x     def __neg__(self): # -self         return self * -1          # M\u00e9thode de n\u00e9gation, la m\u00e9thode __neg__ sur une classe python correspond \u00e0 l'op\u00e9rateur pour passer de x \u00e0 -x     def __radd__(self, other): # other + self         return self + other      # M\u00e9thode de soustraction, la m\u00e9thode __neg__ sur une classe python correspond \u00e0 l'op\u00e9rateur '-'     def __sub__(self, other): # self - other         return self + (-other)      # M\u00e9thode de soustraction, permet de pouvoir r\u00e9aliser l'op\u00e9ration other - self m\u00eame si other n'est pas une Value     def __rsub__(self, other): # other - self         return other + (-self)      # M\u00e9thode de multiplication, permet de pouvoir r\u00e9aliser l'op\u00e9ration other * self m\u00eame si other n'est pas une Value     def __rmul__(self, other): # other * self         return self * other      # M\u00e9thode de division, la m\u00e9thode __truediv__ sur une classe python correspond \u00e0 l'op\u00e9rateur '/'     def __truediv__(self, other): # self / other         return self * other**-1          # M\u00e9thode de division, permet de pouvoir r\u00e9aliser l'op\u00e9ration other / self m\u00eame si other n'est pas une Value     def __rtruediv__(self, other): # other / self         return other * self**-1      # M\u00e9thode de representation d'un objet, donne les informations importantes d'un objet, c'est ce qui va      # s'afficher lorsqu'on fait print()     def __repr__(self):         return f\"Value(data={self.data}, grad={self.grad})\""},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/nn.html","title":"Nn","text":"In\u00a0[\u00a0]: Copied! <pre>import random\nfrom engine import Value\n</pre> import random from engine import Value In\u00a0[\u00a0]: Copied! <pre># Definnition de la classe Module\nclass Module:\n    # Mise \u00e0 z\u00e9ro des gradients \n    def zero_grad(self):\n        for p in self.parameters():\n            p.grad = 0\n\n    def parameters(self):\n        return []\n</pre> # Definnition de la classe Module class Module:     # Mise \u00e0 z\u00e9ro des gradients      def zero_grad(self):         for p in self.parameters():             p.grad = 0      def parameters(self):         return [] In\u00a0[\u00a0]: Copied! <pre># D\u00e9finition de la classe Neuron\nclass Neuron(Module):\n\n    def __init__(self, nin, nonlin=True):\n        # Chaque neurone a un vecteur de poids w et un biais bs\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n        self.b = Value(0)\n        # Utilisation ou non de la fonction d'activation ReLU\n        self.nonlin = nonlin\n\n\n    def __call__(self, x):\n        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n        return act.relu() if self.nonlin else act\n\n    def parameters(self):\n        return self.w + [self.b]\n\n    def __repr__(self):\n        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n</pre> # D\u00e9finition de la classe Neuron class Neuron(Module):      def __init__(self, nin, nonlin=True):         # Chaque neurone a un vecteur de poids w et un biais bs         self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]         self.b = Value(0)         # Utilisation ou non de la fonction d'activation ReLU         self.nonlin = nonlin       def __call__(self, x):         act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)         return act.relu() if self.nonlin else act      def parameters(self):         return self.w + [self.b]      def __repr__(self):         return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\" In\u00a0[\u00a0]: Copied! <pre># D\u00e9finition de la classe Layer\nclass Layer(Module):\n\n    def __init__(self, nin, nout, **kwargs):\n        # Permet de cr\u00e9er nout neurones prenant nin entr\u00e9es\n        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n\n    def __call__(self, x):\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n    def __repr__(self):\n        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n</pre> # D\u00e9finition de la classe Layer class Layer(Module):      def __init__(self, nin, nout, **kwargs):         # Permet de cr\u00e9er nout neurones prenant nin entr\u00e9es         self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]      def __call__(self, x):         out = [n(x) for n in self.neurons]         return out[0] if len(out) == 1 else out      def parameters(self):         return [p for n in self.neurons for p in n.parameters()]      def __repr__(self):         return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\" In\u00a0[\u00a0]: Copied! <pre># D\u00e9finition de la classe MLP\nclass MLP(Module):\n\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        # Permet de cr\u00e9er nin input et avec des couches de taille nouts[i]\n        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n    def __repr__(self):\n        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n</pre> # D\u00e9finition de la classe MLP class MLP(Module):      def __init__(self, nin, nouts):         sz = [nin] + nouts         # Permet de cr\u00e9er nin input et avec des couches de taille nouts[i]         self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]      def __call__(self, x):         for layer in self.layers:             x = layer(x)         return x      def parameters(self):         return [p for layer in self.layers for p in layer.parameters()]      def __repr__(self):         return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""},{"location":"02_R%C3%A9seauFullyConnected/MicrogradFR/trace_graph.html","title":"Trace graph","text":"In\u00a0[1]: Copied! <pre># brew install graphviz\n# !pip install graphviz\nfrom graphviz import Digraph\n</pre> # brew install graphviz # !pip install graphviz from graphviz import Digraph <pre>Collecting graphviz\n  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\nDownloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 47.1/47.1 kB 1.6 MB/s eta 0:00:00\nInstalling collected packages: graphviz\nSuccessfully installed graphviz-0.20.3\n</pre> In\u00a0[2]: Copied! <pre>from micrograd.engine import Value\n</pre> from micrograd.engine import Value In\u00a0[\u00a0]: Copied! <pre>def trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n    \n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n    \n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n    \n    return dot\n</pre> def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})          for n in nodes:         dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))          for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)          return dot In\u00a0[\u00a0]: Copied! <pre># a very simple example\nx = Value(1.0)\ny = (x * 2 + 1).relu()\ny.backward()\ndraw_dot(y)\n</pre> # a very simple example x = Value(1.0) y = (x * 2 + 1).relu() y.backward() draw_dot(y) In\u00a0[\u00a0]: Copied! <pre># a simple 2D neuron\nimport random\nfrom micrograd import nn\n\nrandom.seed(1337)\nn = nn.Neuron(2)\nx = [Value(1.0), Value(-2.0)]\ny = n(x)\ny.backward()\n\ndot = draw_dot(y)\ndot\n</pre> # a simple 2D neuron import random from micrograd import nn  random.seed(1337) n = nn.Neuron(2) x = [Value(1.0), Value(-2.0)] y = n(x) y.backward()  dot = draw_dot(y) dot In\u00a0[\u00a0]: Copied! <pre>dot.render('gout')\n</pre> dot.render('gout')"},{"location":"03_R%C3%A9seauConvolutifs/index.html","title":"\ud83d\uddbc\ufe0f R\u00e9seaux convolutifs \ud83d\uddbc\ufe0f","text":"<p>Ce cours aborde tout d'abord le principe de fonctionnement des couches de convolution puis montre comment on les utilise au sein d'un r\u00e9seau de neurones. Plusieurs exemples sont ensuite abord\u00e9s pour montrer les capacit\u00e9s d'un r\u00e9seau convolutif : classification sur MNIST, classification sur CIFAR-10 et segmentation sur \"Oxford-IIIT Pet Dataset\".</p>"},{"location":"03_R%C3%A9seauConvolutifs/index.html#notebook-1-couches-de-convolutions","title":"Notebook 1\ufe0f\u20e3 : Couches de convolutions","text":"<p>Ce notebook introduit les couches de convolutions en expliquant la motivation principale derri\u00e8re cette architecture puis en introduisant les diff\u00e9rents param\u00e8tres pour les utiliser. Les couches de pooling sont \u00e9galement introduites.</p>"},{"location":"03_R%C3%A9seauConvolutifs/index.html#notebook-2-reseau-convolutif","title":"Notebook 2\ufe0f\u20e3 : R\u00e9seau Convolutif","text":"<p>Ce notebook introduit l'architecture du r\u00e9seau de neurones convolutifs avec une explication de la logique derri\u00e8re cette architecture ainsi que la description du concept de champ r\u00e9ceptif.</p>"},{"location":"03_R%C3%A9seauConvolutifs/index.html#notebook-3-conv-implementation","title":"Notebook 3\ufe0f\u20e3 : Conv Implementation","text":"<p>Ce notebook propose une impl\u00e9mentation d'une couche de convolution 1D \u00e0 partir de z\u00e9ro et son utilisation pour la classification sur le dataset MNIST. Une impl\u00e9mentation de la convolution 2D est ensuite propos\u00e9e pour voir si l'on obtient de meilleurs r\u00e9sultats.</p>"},{"location":"03_R%C3%A9seauConvolutifs/index.html#notebook-4-reseau-convolutif-pytorch","title":"Notebook 4\ufe0f\u20e3 : R\u00e9seau Convolutif Pytorch","text":"<p>Ce notebook propose une impl\u00e9mentation d'un premier r\u00e9seau convolutif sur le dataset MNIST.</p>"},{"location":"03_R%C3%A9seauConvolutifs/index.html#notebook-5-application-classification","title":"Notebook 5\ufe0f\u20e3 : Application Classification","text":"<p>Ce notebook montre l'int\u00earet des r\u00e9seaux convolutifs en proposant une impl\u00e9mentation d'un mod\u00e8le de classification sur le dataset CIFAR-10.</p>"},{"location":"03_R%C3%A9seauConvolutifs/index.html#notebook-6-application-segmentation","title":"Notebook 6\ufe0f\u20e3 : Application Segmentation","text":"<p>Dans ce notebook, le probl\u00e8me de la segmentation d'objets est introduit et une impl\u00e9mentation est propos\u00e9e pour segmenter les chiens et les chats sur le dataset Oxford-IIIT Pet.</p>"},{"location":"03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html","title":"Couches de convolutions","text":"<p>Dans les cours pr\u00e9c\u00e9dents, nous avons utilis\u00e9 des r\u00e9seaux de neurones enti\u00e8rement connect\u00e9s (ou Fully Connected) sur des images. Pour rappel, nous avions utilis\u00e9 le dataset MNIST compos\u00e9 d'images en nuance de gris de taille $28 \\times 28$ pour introduire la r\u00e9gularisation, la normalisation et montrer le potentiel d'un r\u00e9seau de neurones pour le traitement d'images. Cependant, l'entr\u00e9e de notre r\u00e9seau \u00e9tait de taille $28 \\times 28 = 784$ ce qui est une taille d'entr\u00e9e assez cons\u00e9quente pour un r\u00e9seau de neurones malgr\u00e9 la petite taille de l'image d'entr\u00e9e.  Pour une image de taille $224 \\times 224 \\times 3$ (petite image pour les standards actuels), on aurai une taille d'entr\u00e9e de $150528$ ce qui est inenvisageable pour un traitement rapide.</p> <p>Pour r\u00e9soudre ce probl\u00e8me, on peut envisager de redimensionner l'image d'entr\u00e9e mais, en faisant cela, la perte d'informations est trop importante et on ne reconna\u00eet m\u00eame plus l'objet sur l'image.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom PIL import Image\n\nimg=Image.open(\"images/SampleImageNet.JPEG\").resize((224,224))\nimgResized_64=img.resize((64,64))\nimgResized_28=img.resize((28,28))\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].imshow(img)\naxes[0].set_title(\"Original\")\naxes[0].axis('off')  \naxes[1].imshow(imgResized_64)\naxes[1].set_title(\"64x64\")\naxes[1].axis('off')  \naxes[2].imshow(imgResized_28)\naxes[2].set_title(\"28x28\")\naxes[2].axis('off')\nplt.show()\n</pre> import matplotlib.pyplot as plt from PIL import Image  img=Image.open(\"images/SampleImageNet.JPEG\").resize((224,224)) imgResized_64=img.resize((64,64)) imgResized_28=img.resize((28,28)) fig, axes = plt.subplots(1, 3, figsize=(15, 5))  axes[0].imshow(img) axes[0].set_title(\"Original\") axes[0].axis('off')   axes[1].imshow(imgResized_64) axes[1].set_title(\"64x64\") axes[1].axis('off')   axes[2].imshow(imgResized_28) axes[2].set_title(\"28x28\") axes[2].axis('off') plt.show() <p>Comment on peut le voir, le redimensionnement n'est pas la bonne solution surtout si l'on a besoin d'identifier des d\u00e9tails de l'image pour une classification ou d\u00e9tection efficace.</p> <p>Une entr\u00e9e de taille $150528$ est d'autant plus probl\u00e9matique lorsque l'on a une premi\u00e8re couche cach\u00e9e qui contient \u00e9galement un nombre important de neurones. En effet, les poids correspondent \u00e0 chaque connexion entre deux neurones de couches successives. Si on suppose que la premi\u00e8re couche cach\u00e9e a $1024$ neurones, on aurait $150528*1024 = 154 \\text{ millions}$ de poids ce qui est colossal pour uniquement la liaison entre les deux premi\u00e8res couches. On pourrait envisager d'avoir une premi\u00e8re couche cach\u00e9e avec seulement quelques dizaines de neurones pour limiter le nombre de poids. Cependant, r\u00e9duire drastiquement la dimension entra\u00eene une perte consid\u00e9rable d'informations, ce qui emp\u00eache d'obtenir des r\u00e9sultats satisfaisants avec cette approche.</p> <p>Intuition : Une particularit\u00e9 des images est la pr\u00e9sence de motifs r\u00e9currents. A partir de cette id\u00e9e, on peut envisager un partage de poids entre les diff\u00e9rents pixels de l'image.</p> <p>Principe de base : Cela nous conduit \u00e0 l'utilisation des couches de convolutions qui utilisent filtres de convolutions parcourant chaque pixel de l'image. Les valeurs de ces filtres sont entra\u00eenables et correspondent aux poids de notre r\u00e9seau de neurones.</p> <p>Voici une illustration du principe de convolution :</p> <p></p> <p>Pourquoi \u00e7a marche ? :</p> <ul> <li>L'utilisation des couches de convolutions permet de ne pas augmenter le nombre de poids en fonction de la taille de l'image, cela permet donc de traiter des images de r\u00e9solution importante.</li> <li>La couche de convolution permet une invariance \u00e0 la translation ce qui est tr\u00e8s int\u00e9ressant pour une image (Une image de chien decal\u00e9e de 4 pixels reste une image de chien).</li> <li>M\u00eame si le traitement de l'information est locale sur une couche de convolution, la succession de couches de convolutions permet d'avoir un traitement global de l'image (voir partie sur le champ r\u00e9ceptif dans le notebook suivant).</li> </ul> <p>Informations suppl\u00e9mentaires :</p> <ul> <li>Sur la figure d'explication, l'image d'entr\u00e9e est de taille $4 \\times 4$. En pratique, une image couleur contient $3$ canaux. Pour tra\u00eeter une image de taille $4 \\times 4 \\times 3$, on utiliserait des filtres de taille $n \\times n \\times 3$ avec $n$ la taille du filtre ($3$ sur la figure d'explication). Il faut retenir que la profondeur du filtre de convolution d\u00e9pend du nombre de canaux de l'entr\u00e9e de la couche.</li> <li>On peut voir la convolution comme une boucle for appliqu\u00e9e sur une couche fully connected, o\u00f9 un petit filtre (avec des poids partag\u00e9s plut\u00f4t que des poids uniques par connexion) glisse sur l'image pour d\u00e9tecter les motifs locaux. On gagne \u00e9normement en efficacit\u00e9 et on permet la partage des poids ce qui limite grandement les n\u00e9cessit\u00e9s en terme de m\u00e9moire.</li> </ul> <p>Nous allons maintenant d\u00e9tailler les diff\u00e9rents param\u00e8tres d'une couche de convolution en se basant sur les param\u00e8tres d'entr\u00e9e de la fonction torch.nn.Conv2d de pytorch.</p> <p>Pour plus de clart\u00e9, la sortie d'une couche de convolutions est appel\u00e9e FeatureMap.</p> <p>in_channels : Le nombre de canaux en entr\u00e9e de la couche (permet de d\u00e9finir la profondeur des filtres de convolutions utilis\u00e9s)</p> <p>out_channels : Le nombre de filtres de convolutions utilis\u00e9s dans la couche (correspond \u00e9galement au nombre de canaux de la FeatureMap de sortie).</p> <p>kernel_size : La taille d'une filtre de convolution, si  ce param\u00e8tre vaut $3$ alors le filtre de convolution est de taille $3 \\times 3$. Tous les filtres de convolution d'une m\u00eame couche ont forc\u00e9ment la m\u00eame taille.</p> <p>stride : Le stride correspond au pas de l'application du filtre de convolution. Par exemple, un stride de $1$ signifie qu'il faut appliquer le filtre sur tous les pixels tandis qu'un stride de $2$ signifie qu'il faut l'appliquer sur un pixel sur deux. Un stride important a pour effet de r\u00e9duire la dimension de l'image (Avec un stride de 2, la FeatureMap de sortie sera 2 fois plus petit qu'avec un stride de 1).</p> <p>padding : Ce param\u00e8tre ajoute une bordure de taille padding autour de l'image pour permettre aux filtres d'\u00eatre appliqu\u00e9s uniform\u00e9ment sur tous les pixels, y compris ceux des bords. Sans padding, les pixels en bordure n'auraient pas de voisins et le filtre ne pourrait pas \u00eatre appliqu\u00e9s, ce qui r\u00e9duirait la taille de l'image lors de la convolution. Le padding rem\u00e9die \u00e0 cela en ajoutant des valeurs (comme des z\u00e9ros ou des valeurs r\u00e9fl\u00e9chies) autour de l'image avant d'appliquer les filtres (voir zero-padding sur la figure d'explication).</p> <p>padding_mode : Permet de s\u00e9l\u00e9ctionner le mode de padding (\"zeros\" pour zero_padding par exemple).</p> <p>dilation : Le param\u00e8tre dilation dans une couche de convolution d\u00e9finit l'espacement entre les \u00e9l\u00e9ments du filtre, permettant \u00e0 ce dernier de couvrir une zone plus large sans augmenter la taille du filtre, ce qui capture davantage de contexte spatial dans l'image. La figure suivante illustre la dilation :</p> <p></p> <p>Figure extraite de Du, Jinglong &amp; Wang, Lulu &amp; Liu, Yulu &amp; Zhou, Zexun &amp; He, Zhongshi &amp; Jia, Yuanyuan. (2020). Brain MRI Super-Resolution Using 3D Dilated Convolutional Encoder\u2013Decoder Network. IEEE Access. PP. 1-1. 10.1109/ACCESS.2020.2968395.</p> <p>Une autre couche fondamentale des r\u00e9seaux de neurones convolutifs est la couche de pooling. C'est une couche qui ne contient aucun param\u00e8tre apprenable et qui se contente de redimensionner la feature map \u00e0 l'aide d'une technique de regroupement de pixels adjacents. Dans le notebook suivant, nous verrons comment cette couche s'utilise dans un r\u00e9seau de neurones convolutif. Les couches de pooling ont aussi pour effet de r\u00e9duire la sensibilit\u00e9 aux d\u00e9calages et aux distorsions de motifs pour une plus grande g\u00e9n\u00e9ralisation.</p> <p>Il existe 2 types principaux de pooling : MaxPooling :  Le MaxPooling est la m\u00e9thode de pooling qui consiste \u00e0 prendre la valeur maximum d'un groupe de pixels adjacents et de la d\u00e9finir comme nouvelle valeur de l'image r\u00e9duite. AveragePooling : L'AveragePooling prend la valeur moyenne du groupe de pixels adjacent et la d\u00e9finit comme nouvelle valeur de l'image r\u00e9duite.</p> <p>La figure suivante montre les diff\u00e9rences entre le MaxPooling et l'AveragePooling :</p> <p></p>"},{"location":"03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html#couches-de-convolutions","title":"Couches de convolutions\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html#motivation","title":"Motivation\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html#solutions","title":"Solutions\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html#redimensionnement","title":"Redimensionnement\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html#premiere-couche-cachee-tres-limitee","title":"Premi\u00e8re couche cach\u00e9e tr\u00e8s limit\u00e9e\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html#convolutions","title":"Convolutions\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html#couche-de-convolution-en-detail","title":"Couche de convolution en d\u00e9tail\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/01_CouchesDeConvolutions.html#couche-de-pooling","title":"Couche de pooling\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/02_R%C3%A9seauConvolutif.html","title":"R\u00e9seau convolutif","text":"<p>A l'image des r\u00e9seaux fully connected, les r\u00e9seaux convolutifs sont \u00e9galement constitu\u00e9s d'une succession de couches. L'id\u00e9e principale est d'augmenter les filtres (canaux) avec la profondeur tout en r\u00e9duisant la r\u00e9solution spatiale des FeatureMap. En faisant cela, on augmente l'abstraction au fur et \u00e0 mesure du r\u00e9seau avec les premi\u00e8res couches qui d\u00e9tecteront plus des informations de contours alors que les couches profondes contiendront des informations plus contextuelles.</p> <p>Pour un probl\u00e8me de classification (MNIST par exemple), les derni\u00e8res couches du r\u00e9seau seront souvent des couches enti\u00e8rement connect\u00e9es pour adapter la dimension de sortie des couches de convolutions avec le nombre de classes.</p> <p>De mani\u00e8re g\u00e9n\u00e9rale, un r\u00e9seau convolutif est constitu\u00e9 d'une succession de couches de convolution, de couche d'activation (ReLU, sigmoid, tanH etc ) et de couche de pooling. La couche de convolution va permettre de changer le nombre de filtres et d'ajouter des param\u00e8tres entra\u00eenables, la couche d'activation permet de rendre le r\u00e9seau non lin\u00e9aire et la couche de pooling permet de diminuer la r\u00e9solution spatiale de l'image.</p> <p>L'architecture classique d'un r\u00e9seau de neurones convolutif est la suivante :</p> <p></p> <p>Comme indiqu\u00e9 dans le notebook pr\u00e9c\u00e9dent, une unique couche de convolution permet uniquement une interaction locale entre les pixels (pour un filtre de taille $3 \\times 3$, chaque pixel n'est influenc\u00e9 que par ses pixels adjacents). Cela est tr\u00e8s probl\u00e9matique lorsqu'il faut d\u00e9tecter un \u00e9l\u00e9ment qui occupe l'integralit\u00e9 de l'image.</p> <p>Cependant, on peut comprendre assez facilement qu'un empilement de couches de convolutions peut conduire \u00e0 une augmentation de la zone d'influence d'un pixel. L'image suivante illustre le principe :</p> <p>Figure extraire de blogpost.</p> <p>Plus formellement, on d\u00e9finit le champ r\u00e9ceptif d'un pixel par la formule suivante : $R_{Eff}=R_{Init} + (k-1)*S$ Avec $R_{Eff}$ le champ r\u00e9ceptif de la couche de sortie, $R_{Init}$ le champ r\u00e9ceptif initial, $k$ la taille du noyau de convolution et $S$ le stride.</p> <p>Lorsqu'on impl\u00e9mente des r\u00e9seaux convolutifs, il faut donc faire attention \u00e0 ce param\u00e8tre pour \u00eatre s\u00fbr que notre r\u00e9seau \u00e9tudie bien l'interaction de l'ensemble des pixels les uns avec les autres. Plus l'image d'entr\u00e9e est grande, plus il faut un champ r\u00e9ceptif important.</p> <p>Pr\u00e9cision : Les outils vu dans les cours pr\u00e9c\u00e9dents permettant d'am\u00e9liorer le mod\u00e8le sont utilisables \u00e9galement avec les r\u00e9seaux convolutifs (BatchNorm, Dropout, etc ...)</p> <p>Pour appr\u00e9hender le fonctionnement d'un r\u00e9seau de neurones convolutifs et le r\u00f4le de chaque couche, on peut visualiser les activations des FeatureMaps en fonction de la profondeur.</p> <p>Voici une visualisation en fonction de la profondeur du r\u00e9seau :</p> <p>Comment on peut le voir, les couches peu profondes vont plus regarder les informations locales comme les contours et les formes basiques tandis que les couches profondes contiendront plut\u00f4t des informations de contexte. Bien s\u00fbr, il s'agit d'un continuum et les couches interm\u00e9diaires contiendront des informations d'une partie importante de l'image.</p>"},{"location":"03_R%C3%A9seauConvolutifs/02_R%C3%A9seauConvolutif.html#reseau-convolutif","title":"R\u00e9seau convolutif\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/02_R%C3%A9seauConvolutif.html#intuition","title":"Intuition\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/02_R%C3%A9seauConvolutif.html#champ-receptif","title":"Champ r\u00e9ceptif\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/02_R%C3%A9seauConvolutif.html#visualisation-de-ce-que-le-reseau-apprend","title":"Visualisation de ce que le r\u00e9seau apprend\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/03_ConvImplementation.html","title":"Implementation de la couche de convolution","text":"<p>Dans ce cours, nous allons impl\u00e9menter une couche de convolution 1D. Dans le cours expliquant les couches de convolutions, nous avons parl\u00e9 exclusivement de la convolution 2D car c'est cette couche qui est la plus utilis\u00e9e (de loin). Cependant, pour bien comprendre le code et l'ing\u00e9niosit\u00e9 des couches de convolution, il est essentiel de commencer par la convolution 1D.</p> <p>De mani\u00e8re assez \u00e9vidente, la convolution 1D est similaire \u00e0 la convolution 2D mais sur une seule dimension. C'est lors de l'application de la couche de convolution 1D que l'on voit tr\u00e8s rapidement pourquoi une couche de convolution est en fait une boucle for.</p> <p></p> <p>Figure extraite du forum.</p> <p>Les param\u00e8tres classiques de la convolution 2D s'applique \u00e9galement :</p> <ul> <li>Le padding va ajouter des valeurs au bord du vecteur 1D (d\u00e9but et fin)</li> <li>Le stride d\u00e9finit le pas</li> <li>La kernel_size d\u00e9finit la taille du filtre</li> <li>Les in_channels et out_channels qui correspondent aux nombres de channels d'entr\u00e9e et de sortie.</li> <li>etc ...</li> </ul> <p>Passons maintenant \u00e0 l'impl\u00e9mentation de la couche de convolution 1D.</p> <p>Note : Sur pytorch, les couches de convolutions ne sont pas impl\u00e9ment\u00e9es en python mais en C++ pour une plus grande rapidit\u00e9 de calcul.</p> In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n</pre> import torch import torch.nn as nn import torch.nn.functional as F <p>Dans une couche de convolution, on a une dimension de filtre kernel_size et un nombre de filtre out_channels. L'id\u00e9e va \u00eatre de boucler sur l'ensemble de notre vecteur et de calculer les valeurs de sortie en appliquant chaque filtre \u00e0 chaque position possible du vecteur d'entr\u00e9e. A chaque position, on va appliquer une couche fully connected prenant en entr\u00e9e les \u00e9l\u00e9ments contenus dans le filtre de taille $(KernelSize \\times InChannels)$ et renvoyant $OutChannels$ \u00e9l\u00e9ments. C'est comme si on appliquait une boucle for sur chaque position de notre s\u00e9quence d'entr\u00e9e.</p> <p>Note : Sur la figure explicative, il n'y a qu'une seule dimension de channel mais en r\u00e9alit\u00e9 il y en a plusieurs la plupart du temps.</p> In\u00a0[14]: Copied! <pre>in_channels = 3\nout_channels = 16\nkernel_size = 3\nkernel=nn.Linear(in_channels*kernel_size, out_channels)\n</pre> in_channels = 3 out_channels = 16 kernel_size = 3 kernel=nn.Linear(in_channels*kernel_size, out_channels) <p>Maintenant, il faut appliquer cette couche de convolution sur l'ensemble des \u00e9l\u00e9ments de notre s\u00e9quence avec un pas stride. On peut \u00e9galement ajouter du padding si on veut que la s\u00e9quence d'entr\u00e9e soit de la m\u00eame taille que la s\u00e9quence de sortie.</p> In\u00a0[21]: Copied! <pre># Imaginons une s\u00e9quence de 100 \u00e9l\u00e9ments, avec 3 canaux et un batch de 8\ndummy_input = torch.randn(8, in_channels, 100)\nprint(\"Dimension de l'entr\u00e9e: \",dummy_input.shape)\nstride=1\npadding=1\nouts=[]\n\n# On pad les deux c\u00f4t\u00e9s de l'entr\u00e9e pour \u00e9viter les probl\u00e8mes de dimensions\ndummy_input=F.pad(dummy_input, (padding, padding))\n\nfor i in range(kernel_size,dummy_input.shape[2]+1,stride):\n  chunk=dummy_input[:,:,i-kernel_size:i]\n  # On redimensionne pour la couche fully connected\n  chunk=chunk.reshape(dummy_input.shape[0],-1)\n  # On applique la couche fully connected\n  out=kernel(chunk)\n  # On ajoute \u00e0 la liste des sorties\n  outs.append(out)\n# On convertit la liste en un tenseur\nouts=torch.stack(outs, dim=2)\nprint(\"Dimension de la sortie: \",outs.shape)\n</pre> # Imaginons une s\u00e9quence de 100 \u00e9l\u00e9ments, avec 3 canaux et un batch de 8 dummy_input = torch.randn(8, in_channels, 100) print(\"Dimension de l'entr\u00e9e: \",dummy_input.shape) stride=1 padding=1 outs=[]  # On pad les deux c\u00f4t\u00e9s de l'entr\u00e9e pour \u00e9viter les probl\u00e8mes de dimensions dummy_input=F.pad(dummy_input, (padding, padding))  for i in range(kernel_size,dummy_input.shape[2]+1,stride):   chunk=dummy_input[:,:,i-kernel_size:i]   # On redimensionne pour la couche fully connected   chunk=chunk.reshape(dummy_input.shape[0],-1)   # On applique la couche fully connected   out=kernel(chunk)   # On ajoute \u00e0 la liste des sorties   outs.append(out) # On convertit la liste en un tenseur outs=torch.stack(outs, dim=2) print(\"Dimension de la sortie: \",outs.shape) <pre>Dimension de l'entr\u00e9e:  torch.Size([8, 3, 100])\nDimension de la sortie:  torch.Size([8, 16, 100])\n</pre> <p>Comme pour les convolutions 2D, on peut choisir de diminuer la taille de la s\u00e9quence (ou des feature map si on parle d'une conv 2D). Pour cela, on peut utiliser un stride sup\u00e9rieur \u00e0 1 ou bien une couche de pooling.</p> <p>En pratique, l'utilisation du stride est souvent pr\u00e9f\u00e9r\u00e9e mais nous allons impl\u00e9menter le max pooling pour bien comprendre son fonctionnement.</p> In\u00a0[27]: Copied! <pre>pooling=2\nouts2=[]\nfor i in range(pooling,outs.shape[2]+1,pooling):\n  # On prend les \u00e9l\u00e9ments entre i-pooling et i\n  chunk=outs[:,:,i-pooling:i]\n  # On prend le max sur la dimension 2, pour le average pooling on aurait utilis\u00e9 torch.mean\n  out2=torch.max(chunk, dim=2)[0]\n  outs2.append(out2)\n# On convertit la liste en un tenseur \nouts2=torch.stack(outs2, dim=2)\nprint(\"Dimension de la sortie apr\u00e8s pooling: \",outs2.shape)\n</pre> pooling=2 outs2=[] for i in range(pooling,outs.shape[2]+1,pooling):   # On prend les \u00e9l\u00e9ments entre i-pooling et i   chunk=outs[:,:,i-pooling:i]   # On prend le max sur la dimension 2, pour le average pooling on aurait utilis\u00e9 torch.mean   out2=torch.max(chunk, dim=2)[0]   outs2.append(out2) # On convertit la liste en un tenseur  outs2=torch.stack(outs2, dim=2) print(\"Dimension de la sortie apr\u00e8s pooling: \",outs2.shape) <pre>Dimension de la sortie apr\u00e8s pooling:  torch.Size([8, 16, 50])\n</pre> <p>Maintenant que nous avons compris comment la convolution 1D et le maxpooling fonctionnent, on va cr\u00e9er des classes pour faciliter leurs utilisations.</p> In\u00a0[29]: Copied! <pre>class Conv1D(nn.Module):\n  def __init__(self, in_channels, out_channels, stride, kernel_size, padding):\n    super(Conv1D, self).__init__()\n\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.stride = stride\n    self.kernel_width = kernel_size\n    self.kernel = nn.Linear(kernel_size * in_channels, out_channels)\n    self.padding=padding\n\n  def forward(self, x):\n    x=F.pad(x, (self.padding, self.padding))\n    # Boucle en une seule ligne pour un code plus concis\n    l = [self.kernel(x[:, :, i - self.kernel_width: i].reshape(x.shape[0], self.in_channels * self.kernel_width)) for i in range(self.kernel_width, x.shape[2]+1, self.stride)]\n    return torch.stack(l, dim=2)\n\n\nclass MaxPool1D(nn.Module):\n  def __init__(self, pooling):\n    super(MaxPool1D, self).__init__()\n    self.pooling = pooling\n    \n  def forward(self, x):\n    # Boucle en une seule ligne pour un code plus concis\n    l = [torch.max(x[:, :, i - self.pooling: i], dim=2)[0] for i in range(self.pooling, x.shape[2]+1, self.pooling)]\n    return torch.stack(l, dim=2)\n</pre> class Conv1D(nn.Module):   def __init__(self, in_channels, out_channels, stride, kernel_size, padding):     super(Conv1D, self).__init__()      self.in_channels = in_channels     self.out_channels = out_channels     self.stride = stride     self.kernel_width = kernel_size     self.kernel = nn.Linear(kernel_size * in_channels, out_channels)     self.padding=padding    def forward(self, x):     x=F.pad(x, (self.padding, self.padding))     # Boucle en une seule ligne pour un code plus concis     l = [self.kernel(x[:, :, i - self.kernel_width: i].reshape(x.shape[0], self.in_channels * self.kernel_width)) for i in range(self.kernel_width, x.shape[2]+1, self.stride)]     return torch.stack(l, dim=2)   class MaxPool1D(nn.Module):   def __init__(self, pooling):     super(MaxPool1D, self).__init__()     self.pooling = pooling        def forward(self, x):     # Boucle en une seule ligne pour un code plus concis     l = [torch.max(x[:, :, i - self.pooling: i], dim=2)[0] for i in range(self.pooling, x.shape[2]+1, self.pooling)]     return torch.stack(l, dim=2) <p>Maintenant que l'on a impl\u00e9ment\u00e9 nos couches de convolutions et de pooling, nous allons les tester sur MNIST. Sur MNIST, on traite des images, il est donc plus logique d'utiliser des convolutions 2D en pratique (cf cours suivant). Ici, nous allons juste v\u00e9rifier que notre impl\u00e9mentation des convolutions fonctionne :</p> In\u00a0[38]: Copied! <pre>import torchvision.transforms as T\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n</pre> import torchvision.transforms as T from torchvision import datasets from torch.utils.data import DataLoader import matplotlib.pyplot as plt In\u00a0[39]: Copied! <pre>transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement\ndataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform)\ntest_dataset = datasets.MNIST(root='./../data', train=False,transform=transform)\n</pre> transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement dataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform) test_dataset = datasets.MNIST(root='./../data', train=False,transform=transform) In\u00a0[40]: Copied! <pre>plt.imshow(dataset[0][0].permute(1,2,0).numpy(), cmap='gray')\nplt.show()\nprint(\"Le chiffre sur l'image est un \"+str(dataset[1][1]))\n</pre> plt.imshow(dataset[0][0].permute(1,2,0).numpy(), cmap='gray') plt.show() print(\"Le chiffre sur l'image est un \"+str(dataset[1][1])) <pre>Le chiffre sur l'image est un 0\n</pre> In\u00a0[41]: Copied! <pre>train_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n</pre> train_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2]) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) <p>On peut maintenant cr\u00e9er notre mod\u00e8le. Notons que l'on utilise un stride de 2 et pas de maxpooling pour gagner du temps sur le traitement.</p> In\u00a0[49]: Copied! <pre>class cnn1d(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.conv1=Conv1D(1,8,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 8 filtres\n    self.conv2=Conv1D(8,16,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 16 filtres\n    self.conv3=Conv1D(16,32,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 32 filtres\n    self.fc=nn.Linear(3136,10)\n  \n  # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)\n  def forward(self,x):\n    x=F.relu(self.conv1(x))\n    x=F.relu(self.conv2(x))\n    x=F.relu(self.conv3(x))\n    x=x.view(-1,x.shape[1]*x.shape[2]) # Pour convertir la feature map de taille CxL en vecteur 1D (avec une dimension batch)\n    output=self.fc(x)\n    return output\ndummy_input=torch.randn(8,1,784)\nmodel=cnn1d()\noutput=model(dummy_input)\nprint(output.shape)\n\nprint(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters()))\n</pre> class cnn1d(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)     self.conv1=Conv1D(1,8,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 8 filtres     self.conv2=Conv1D(8,16,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 16 filtres     self.conv3=Conv1D(16,32,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 32 filtres     self.fc=nn.Linear(3136,10)      # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)   def forward(self,x):     x=F.relu(self.conv1(x))     x=F.relu(self.conv2(x))     x=F.relu(self.conv3(x))     x=x.view(-1,x.shape[1]*x.shape[2]) # Pour convertir la feature map de taille CxL en vecteur 1D (avec une dimension batch)     output=self.fc(x)     return output dummy_input=torch.randn(8,1,784) model=cnn1d() output=model(dummy_input) print(output.shape)  print(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters())) <pre>torch.Size([8, 10])\nNombre de param\u00e8tres 33370\n</pre> <p>Le mod\u00e8le a presque 10 fois moins de param\u00e8tres que notre mod\u00e8le fully connected du cours pr\u00e9c\u00e9dent !</p> In\u00a0[50]: Copied! <pre>criterion = nn.CrossEntropyLoss()\nepochs=5\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n</pre> criterion = nn.CrossEntropyLoss() epochs=5 learning_rate=0.001 optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate) In\u00a0[51]: Copied! <pre>for i in range(epochs):\n  loss_train=0\n  for images, labels in train_loader:\n    images=images.view(images.shape[0],1,784)\n    preds=model(images)\n    loss=criterion(preds,labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, labels in val_loader:\n    with torch.no_grad(): # permet de ne pas calculer les gradients\n      images=images.view(images.shape[0],1,784)\n      preds=model(images)\n      loss=criterion(preds,labels)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> for i in range(epochs):   loss_train=0   for images, labels in train_loader:     images=images.view(images.shape[0],1,784)     preds=model(images)     loss=criterion(preds,labels)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, labels in val_loader:     with torch.no_grad(): # permet de ne pas calculer les gradients       images=images.view(images.shape[0],1,784)       preds=model(images)       loss=criterion(preds,labels)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 0.4011246860027313\nstep 0 val loss 0.2103319615125656\nstep 1 train loss 0.17427290976047516\nstep 1 val loss 0.1769915670156479\nstep 2 train loss 0.14464063942432404\nstep 2 val loss 0.14992524683475494\nstep 3 train loss 0.12802869081497192\nstep 3 val loss 0.13225941359996796\nstep 4 train loss 0.11609579622745514\nstep 4 val loss 0.12663421034812927\n</pre> In\u00a0[53]: Copied! <pre>correct = 0\ntotal = 0\nfor images,labels in test_loader:\n  images=images.view(images.shape[0],1,784) \n  with torch.no_grad():\n    preds=model(images)\n    \n    _, predicted = torch.max(preds.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()     \ntest_acc = 100 * correct / total\nprint(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc)\n</pre> correct = 0 total = 0 for images,labels in test_loader:   images=images.view(images.shape[0],1,784)    with torch.no_grad():     preds=model(images)          _, predicted = torch.max(preds.data, 1)     total += labels.size(0)     correct += (predicted == labels).sum().item()      test_acc = 100 * correct / total print(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc) <pre>Pr\u00e9cision du mod\u00e8le en phase de test :  96.35\n</pre> <p>On obtient une tr\u00e8s bonne pr\u00e9cision bien que l\u00e9g\u00e9rement inf\u00e9rieure \u00e0 celle obtenue avec le r\u00e9seau fully connected du cours pr\u00e9c\u00e9dent.</p> <p>Note : L'entra\u00eenement \u00e9tait assez lent, c'est \u00e0 cause de notre impl\u00e9mentation qui n'est pas tr\u00e8s efficace, l'impl\u00e9mentation pytorch en C++ est beaucoup plus performante.</p> <p>Note 2 : Nous avons utilis\u00e9 des convolutions 1D pour tra\u00eeter des images ce qui n'est pas optimal. L'id\u00e9al est d'utiliser des convolutions 2D.</p> <p>On peut impl\u00e9menter la convolution 2D en suivant le m\u00eame principe mais en deux dimensions :</p> In\u00a0[15]: Copied! <pre>in_channels = 3\nout_channels = 16\nkernel_size = 3\n# On a un kernel de taille 3x3 car on est en 2D\nkernel=nn.Linear(in_channels*kernel_size**2, out_channels)\n</pre> in_channels = 3 out_channels = 16 kernel_size = 3 # On a un kernel de taille 3x3 car on est en 2D kernel=nn.Linear(in_channels*kernel_size**2, out_channels) In\u00a0[16]: Copied! <pre># Pour une image de taille 10x10 avec 3 canaux et un batch de 8\ndummy_input = torch.randn(8, in_channels, 10,10)\nb, c, h, w = dummy_input.shape\nprint(\"Dimension de l'entr\u00e9e: \",dummy_input.shape)\nstride=1\npadding=1\nouts=[]\n\n# Le padding change pour une image 2D, on doit pad en hauteur et en largeur\ndummy_input=F.pad(dummy_input, (padding, padding,padding,padding))\nprint(\"Dimension de l'entr\u00e9e apr\u00e8s padding: \",dummy_input.shape)\n\n# On boucle sur les dimensions de l'image : W x H \nfor i in range(kernel_size,dummy_input.shape[2]+1,stride):\n  for j in range(kernel_size,dummy_input.shape[3]+1,stride):\n    chunk=dummy_input[:,:,i-kernel_size:i,j-kernel_size:j]\n    # On redimensionne pour la couche fully connected\n    chunk=chunk.reshape(dummy_input.shape[0],-1)\n    # On applique la couche fully connected\n    out=kernel(chunk)\n    # On ajoute \u00e0 la liste des sorties\n    outs.append(out)\n# On convertit la liste en un tenseur\nouts=torch.stack(outs, dim=2)\nouts=outs.reshape(b,out_channels,h, w)\nprint(\"Dimension de la sortie: \",outs.shape)\n</pre> # Pour une image de taille 10x10 avec 3 canaux et un batch de 8 dummy_input = torch.randn(8, in_channels, 10,10) b, c, h, w = dummy_input.shape print(\"Dimension de l'entr\u00e9e: \",dummy_input.shape) stride=1 padding=1 outs=[]  # Le padding change pour une image 2D, on doit pad en hauteur et en largeur dummy_input=F.pad(dummy_input, (padding, padding,padding,padding)) print(\"Dimension de l'entr\u00e9e apr\u00e8s padding: \",dummy_input.shape)  # On boucle sur les dimensions de l'image : W x H  for i in range(kernel_size,dummy_input.shape[2]+1,stride):   for j in range(kernel_size,dummy_input.shape[3]+1,stride):     chunk=dummy_input[:,:,i-kernel_size:i,j-kernel_size:j]     # On redimensionne pour la couche fully connected     chunk=chunk.reshape(dummy_input.shape[0],-1)     # On applique la couche fully connected     out=kernel(chunk)     # On ajoute \u00e0 la liste des sorties     outs.append(out) # On convertit la liste en un tenseur outs=torch.stack(outs, dim=2) outs=outs.reshape(b,out_channels,h, w) print(\"Dimension de la sortie: \",outs.shape) <pre>Dimension de l'entr\u00e9e:  torch.Size([8, 3, 10, 10])\nDimension de l'entr\u00e9e apr\u00e8s padding:  torch.Size([8, 3, 12, 12])\nDimension de la sortie:  torch.Size([8, 16, 10, 10])\n</pre> <p>On peut le faire en classe maintenant comme pour la conv1D :</p> In\u00a0[34]: Copied! <pre>class Conv2D(nn.Module):\n  def __init__(self, in_channels, out_channels, stride, kernel_size, padding):\n    super(Conv2D, self).__init__()\n\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.stride = stride\n    self.kernel_width = kernel_size\n    self.kernel = nn.Linear(in_channels*kernel_size**2 , out_channels)\n    self.padding=padding\n\n  def forward(self, x):\n    b, c, h, w = x.shape\n    x=F.pad(x, (self.padding, self.padding,self.padding,self.padding))\n    # Sur une seule ligne, c'est absolument illisible, on garde la boucle\n    l=[]\n    for i in range(self.kernel_width, x.shape[2]+1, self.stride):\n      for j in range(self.kernel_width, x.shape[3]+1, self.stride):\n        chunk=self.kernel(x[:,:,i-self.kernel_width:i,j-self.kernel_width:j].reshape(x.shape[0],-1))\n        l.append(chunk)\n    # La version en une ligne, pour les curieux\n    #l = [self.kernel(x[:, :, i - self.kernel_width: i, j - self.kernel_width: j].reshape(x.shape[0], ,-1)) for i in range(self.kernel_width, x.shape[2]+1, self.stride) for j in range(self.kernel_width, x.shape[3]+1, self.stride)]\n    outs=torch.stack(l, dim=2)\n    return outs.reshape(b,self.out_channels,h//self.stride, w//self.stride)\n\ndummy_input=torch.randn(8,3,32,32)\nmodel=Conv2D(3,16,stride=2,kernel_size=3,padding=1)\noutput=model(dummy_input)\nprint(output.shape)\n</pre> class Conv2D(nn.Module):   def __init__(self, in_channels, out_channels, stride, kernel_size, padding):     super(Conv2D, self).__init__()      self.in_channels = in_channels     self.out_channels = out_channels     self.stride = stride     self.kernel_width = kernel_size     self.kernel = nn.Linear(in_channels*kernel_size**2 , out_channels)     self.padding=padding    def forward(self, x):     b, c, h, w = x.shape     x=F.pad(x, (self.padding, self.padding,self.padding,self.padding))     # Sur une seule ligne, c'est absolument illisible, on garde la boucle     l=[]     for i in range(self.kernel_width, x.shape[2]+1, self.stride):       for j in range(self.kernel_width, x.shape[3]+1, self.stride):         chunk=self.kernel(x[:,:,i-self.kernel_width:i,j-self.kernel_width:j].reshape(x.shape[0],-1))         l.append(chunk)     # La version en une ligne, pour les curieux     #l = [self.kernel(x[:, :, i - self.kernel_width: i, j - self.kernel_width: j].reshape(x.shape[0], ,-1)) for i in range(self.kernel_width, x.shape[2]+1, self.stride) for j in range(self.kernel_width, x.shape[3]+1, self.stride)]     outs=torch.stack(l, dim=2)     return outs.reshape(b,self.out_channels,h//self.stride, w//self.stride)  dummy_input=torch.randn(8,3,32,32) model=Conv2D(3,16,stride=2,kernel_size=3,padding=1) output=model(dummy_input) print(output.shape) <pre>torch.Size([8, 16, 16, 16])\n</pre> <p>On peut maintenant cr\u00e9er notre mod\u00e8le :</p> In\u00a0[50]: Copied! <pre>class cnn2d(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.conv1=Conv2D(1,8,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 8 filtres\n    self.conv2=Conv2D(8,16,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 16 filtres\n    self.conv3=Conv2D(16,32,kernel_size=3,stride=1,padding=1) # Couche de convolution 1D de 32 filtres\n    self.fc=nn.Linear(1568,10)\n  \n  # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)\n  def forward(self,x):\n    x=F.relu(self.conv1(x))\n    x=F.relu(self.conv2(x))\n    x=F.relu(self.conv3(x))\n    x=x.view(-1,x.shape[1]*x.shape[2]*x.shape[3]) # Pour convertir la feature map de taille CxL en vecteur 1D (avec une dimension batch)\n    output=self.fc(x)\n    return output\ndummy_input=torch.randn(8,1,28,28)\nmodel=cnn2d()\noutput=model(dummy_input)\nprint(output.shape)\n\nprint(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters()))\n</pre> class cnn2d(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)     self.conv1=Conv2D(1,8,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 8 filtres     self.conv2=Conv2D(8,16,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 16 filtres     self.conv3=Conv2D(16,32,kernel_size=3,stride=1,padding=1) # Couche de convolution 1D de 32 filtres     self.fc=nn.Linear(1568,10)      # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)   def forward(self,x):     x=F.relu(self.conv1(x))     x=F.relu(self.conv2(x))     x=F.relu(self.conv3(x))     x=x.view(-1,x.shape[1]*x.shape[2]*x.shape[3]) # Pour convertir la feature map de taille CxL en vecteur 1D (avec une dimension batch)     output=self.fc(x)     return output dummy_input=torch.randn(8,1,28,28) model=cnn2d() output=model(dummy_input) print(output.shape)  print(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters())) <pre>torch.Size([8, 10])\nNombre de param\u00e8tres 21578\n</pre> <p>On peut maintenant entrainer notre mod\u00e8le sur MNIST et regarder si on a des meilleurs r\u00e9sultats qu'avec les convolutions 1D.</p> In\u00a0[51]: Copied! <pre>criterion = nn.CrossEntropyLoss()\nepochs=5\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n</pre> criterion = nn.CrossEntropyLoss() epochs=5 learning_rate=0.001 optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate) In\u00a0[52]: Copied! <pre>for i in range(epochs):\n  loss_train=0\n  for images, labels in train_loader:\n    preds=model(images)\n    loss=criterion(preds,labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, labels in val_loader:\n    with torch.no_grad(): # permet de ne pas calculer les gradients\n      preds=model(images)\n      loss=criterion(preds,labels)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> for i in range(epochs):   loss_train=0   for images, labels in train_loader:     preds=model(images)     loss=criterion(preds,labels)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, labels in val_loader:     with torch.no_grad(): # permet de ne pas calculer les gradients       preds=model(images)       loss=criterion(preds,labels)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 0.36240848898887634\nstep 0 val loss 0.14743468165397644\nstep 1 train loss 0.1063414067029953\nstep 1 val loss 0.1019362062215805\nstep 2 train loss 0.07034476101398468\nstep 2 val loss 0.08669546991586685\nstep 3 train loss 0.05517915263772011\nstep 3 val loss 0.07208992540836334\nstep 4 train loss 0.04452721029520035\nstep 4 val loss 0.0664198026061058\n</pre> <p>En terme de loss, on est descendu plus bas que pour notre mod\u00e8le avec les convolutions 1D.</p> In\u00a0[53]: Copied! <pre>correct = 0\ntotal = 0\nfor images,labels in test_loader:\n  with torch.no_grad():\n    preds=model(images)\n    _, predicted = torch.max(preds.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()     \ntest_acc = 100 * correct / total\nprint(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc)\n</pre> correct = 0 total = 0 for images,labels in test_loader:   with torch.no_grad():     preds=model(images)     _, predicted = torch.max(preds.data, 1)     total += labels.size(0)     correct += (predicted == labels).sum().item()      test_acc = 100 * correct / total print(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc) <pre>Pr\u00e9cision du mod\u00e8le en phase de test :  98.23\n</pre> <p>La pr\u00e9cision est tr\u00e8s bonne ! C'est mieux que ce que nous avions avec les r\u00e9seaux fully connected alors que l'on a 10 fois moins de param\u00e8tres.</p> <p>Note : De la m\u00eame mani\u00e8re, on peut impl\u00e9menter des convolutions 3D qui peuvent \u00eatre utilis\u00e9es pour le traitement des vid\u00e9os (l'axe temporel est rajout\u00e9).</p>"},{"location":"03_R%C3%A9seauConvolutifs/03_ConvImplementation.html#implementation-de-la-couche-de-convolution","title":"Implementation de la couche de convolution\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/03_ConvImplementation.html#convolution-1d-comment-ca-marche","title":"Convolution 1D, comment \u00e7a marche ?\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/03_ConvImplementation.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/03_ConvImplementation.html#cas-pratique-mnist","title":"Cas pratique : MNIST\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/03_ConvImplementation.html#dataset","title":"Dataset\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/03_ConvImplementation.html#bonus-conv2d","title":"Bonus : Conv2D\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html","title":"R\u00e9seau Convolutif sur Pytorch","text":"In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n</pre> import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as T from torchvision import datasets from torch.utils.data import DataLoader import matplotlib.pyplot as plt <p>Pour d\u00e9montrer l'efficacit\u00e9 des r\u00e9seaux de neurones convolutifs 2D, nous allons commencer par reprendre l'exemple du dataset MNIST de classification de chiffres allant de 0 \u00e0 9. Pour cela, nous reprenons le code du notebook pr\u00e9c\u00e9dent et du notebook TechniquesAvanc\u00e9es du cours sur les r\u00e9seau fully connected.</p> In\u00a0[2]: Copied! <pre>transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement\ndataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform)\ntest_dataset = datasets.MNIST(root='./../data', train=False,transform=transform)\n</pre> transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement dataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform) test_dataset = datasets.MNIST(root='./../data', train=False,transform=transform) In\u00a0[3]: Copied! <pre>plt.imshow(dataset[1][0].permute(1,2,0).numpy(), cmap='gray')\nplt.show()\nprint(\"Le chiffre sur l'image est un \"+str(dataset[1][1]))\n</pre> plt.imshow(dataset[1][0].permute(1,2,0).numpy(), cmap='gray') plt.show() print(\"Le chiffre sur l'image est un \"+str(dataset[1][1])) <pre>Le chiffre sur l'image est un 0\n</pre> In\u00a0[4]: Copied! <pre>train_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n</pre> train_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2]) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) In\u00a0[5]: Copied! <pre>class cnn(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.conv1=nn.Conv2d(1,8,kernel_size=3,padding=1) # Couche de convolution de 8 filtres\n    self.conv2=nn.Conv2d(8,16,kernel_size=3,padding=1) # Couche de convolution de 16 filtres\n    self.conv3=nn.Conv2d(16,32,kernel_size=3,padding=1) # Couche de convolution de 32 filtres\n    self.pool1=nn.MaxPool2d((2,2)) # Couche de max pooling \n    self.pool2=nn.MaxPool2d((2,2))\n    self.fc=nn.Linear(32*7*7,10)\n  \n  # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)\n  def forward(self,x):\n    x=F.relu(self.conv1(x))\n    x=self.pool1(x)\n    x=F.relu(self.conv2(x))\n    x=self.pool2(x)\n    x=F.relu(self.conv3(x))\n    x=x.view(-1,32*7*7) # Pour convertir la feature map de taille 32x7x7 en taille vecteur de taille 1568\n    output=self.fc(x)\n    return output\n</pre> class cnn(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)     self.conv1=nn.Conv2d(1,8,kernel_size=3,padding=1) # Couche de convolution de 8 filtres     self.conv2=nn.Conv2d(8,16,kernel_size=3,padding=1) # Couche de convolution de 16 filtres     self.conv3=nn.Conv2d(16,32,kernel_size=3,padding=1) # Couche de convolution de 32 filtres     self.pool1=nn.MaxPool2d((2,2)) # Couche de max pooling      self.pool2=nn.MaxPool2d((2,2))     self.fc=nn.Linear(32*7*7,10)      # La fonction forward est la fonction appel\u00e9e lorsqu'on fait model(x)   def forward(self,x):     x=F.relu(self.conv1(x))     x=self.pool1(x)     x=F.relu(self.conv2(x))     x=self.pool2(x)     x=F.relu(self.conv3(x))     x=x.view(-1,32*7*7) # Pour convertir la feature map de taille 32x7x7 en taille vecteur de taille 1568     output=self.fc(x)     return output In\u00a0[6]: Copied! <pre>model = cnn()\nprint(model)\nprint(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters()))\n</pre> model = cnn() print(model) print(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters())) <pre>cnn(\n  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (fc): Linear(in_features=1568, out_features=10, bias=True)\n)\nNombre de param\u00e8tres 21578\n</pre> <p>Comme vous pouvez le constater, le nombre de param\u00e8tres du mod\u00e8le est tr\u00e8s faible en comparaison de notre r\u00e9seau fully connected du cours pr\u00e9c\u00e9dent.</p> <p>D\u00e9finissions la fonction de loss et les hyperparam\u00e8tres d'entra\u00eenement :</p> In\u00a0[7]: Copied! <pre>criterion = nn.CrossEntropyLoss()\nepochs=5\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n</pre> criterion = nn.CrossEntropyLoss() epochs=5 learning_rate=0.001 optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate) <p>Et maintenant, entra\u00eenons le mod\u00e8le :</p> In\u00a0[8]: Copied! <pre>for i in range(epochs):\n  loss_train=0\n  for images, labels in train_loader:\n    preds=model(images)\n    loss=criterion(preds,labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, labels in val_loader:\n    with torch.no_grad(): # permet de ne pas calculer les gradients\n      preds=model(images)\n      loss=criterion(preds,labels)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> for i in range(epochs):   loss_train=0   for images, labels in train_loader:     preds=model(images)     loss=criterion(preds,labels)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, labels in val_loader:     with torch.no_grad(): # permet de ne pas calculer les gradients       preds=model(images)       loss=criterion(preds,labels)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 0.318773478269577\nstep 0 val loss 0.08984239399433136\nstep 1 train loss 0.08383051306009293\nstep 1 val loss 0.0710655003786087\nstep 2 train loss 0.05604167655110359\nstep 2 val loss 0.0528845489025116\nstep 3 train loss 0.04518255963921547\nstep 3 val loss 0.051780227571725845\nstep 4 train loss 0.03614392504096031\nstep 4 val loss 0.0416230633854866\n</pre> <p>Et calculons la pr\u00e9cision sur le jeu de donn\u00e9es de test :</p> In\u00a0[9]: Copied! <pre>correct = 0\ntotal = 0\nfor images,labels in test_loader: \n  with torch.no_grad():\n    preds=model(images)\n    \n    _, predicted = torch.max(preds.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()     \ntest_acc = 100 * correct / total\nprint(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc)\n</pre> correct = 0 total = 0 for images,labels in test_loader:    with torch.no_grad():     preds=model(images)          _, predicted = torch.max(preds.data, 1)     total += labels.size(0)     correct += (predicted == labels).sum().item()      test_acc = 100 * correct / total print(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc) <pre>Pr\u00e9cision du mod\u00e8le en phase de test :  98.64\n</pre> <p>Comme vous pouvez le voir, on obtient une pr\u00e9cision de presque 99% contre les 97.5% qui nous avions obtenu avec un r\u00e9seau fully connected contenant 10 fois plus de param\u00e8tres.</p> <p>Si l'on ex\u00e9cute le mod\u00e8le sur un \u00e9l\u00e9ment, on obtiendra un vecteur de ce type :</p> In\u00a0[10]: Copied! <pre>images,labels=next(iter(test_loader))\n\n#Isolons un \u00e9l\u00e9ment \nimage,label=images[0].unsqueeze(0),labels[0].unsqueeze(0) # Le unsqueeze permet de garder la dimension batch\npred=model(image)\nprint(pred)\n</pre> images,labels=next(iter(test_loader))  #Isolons un \u00e9l\u00e9ment  image,label=images[0].unsqueeze(0),labels[0].unsqueeze(0) # Le unsqueeze permet de garder la dimension batch pred=model(image) print(pred) <pre>tensor([[ -9.6179,  -5.1802,  -2.6094,   0.9121, -16.0603,  -8.6510, -30.0099,\n          14.4031,  -9.0074,  -2.0431]], grad_fn=&lt;AddmmBackward0&gt;)\n</pre> <p>En regardant attentivement les r\u00e9sultats, on constate que la valeur du 7\u00e8me \u00e9l\u00e9ment est la plus importante donc on peut d\u00e9duire que le mod\u00e8le a pr\u00e9dit un 3. V\u00e9rifions avec notre label :</p> In\u00a0[11]: Copied! <pre>print(label)\n</pre> print(label) <pre>tensor([7])\n</pre> <p>Il s'agit bien d'un 7. Cependant, on prefererait avoir une probabilit\u00e9 d'appartenance \u00e0 la classe. C'est beaucoup plus lisible et \u00e7a nous donne un degr\u00e9 de confiance facilement interpr\u00e9table.</p> <p>Pour cela, nous utilisons la fonction d'activation softmax.</p> <p>Cette fonction est d\u00e9finie comme ceci : $\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$ o\u00f9 $K$ est le nombre de classe du mod\u00e8le.</p> <p>Cette fonction est appel\u00e9e softmax car c'est une op\u00e9ration qui amplifie la valeur du maximum tout en r\u00e9duisant les autres valeurs. On l'utilise pour obtenir une distribution de probabilit\u00e9 car la somme des valeurs pour les $K$ classes est \u00e9gale \u00e0 1.</p> In\u00a0[12]: Copied! <pre>print(F.softmax(pred))\n</pre> print(F.softmax(pred)) <pre>tensor([[3.6964e-11, 3.1266e-09, 4.0884e-08, 1.3833e-06, 5.8867e-14, 9.7215e-11,\n         5.1481e-20, 1.0000e+00, 6.8068e-11, 7.2027e-08]],\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n</pre> <pre>/tmp/ipykernel_232253/3904022211.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  print(F.softmax(pred))\n</pre> <p>Les valeurs sont maintenant des probabilit\u00e9s et on voit que 7 est pr\u00e9dit avec une probabilit\u00e9 de 99.9%. Le mod\u00e8le est donc plut\u00f4t confiant sur sa pr\u00e9diction.</p> <p>Les r\u00e9seaux convolutifs se sont demarqu\u00e9s comme \u00e9tant particuli\u00e8rement int\u00e9ressant dans le domaine du traitement d'images. On les utilise \u00e9galement dans le domaine de l'audio et du traitement vid\u00e9o par exemple.</p> <p>Les notebooks suivant de ce cours montrent des applications des r\u00e9seaux convolutifs sur des cas plus int\u00e9ressant que MNIST. Il est conseill\u00e9 d'avoir un GPU ou d'utiliser les notebooks sur google colab pour avoir des temps d'entra\u00eenement raisonnables (vous pouvez quand m\u00eame suivre le cours sans faire l'entra\u00eenement du mod\u00e8le).</p>"},{"location":"03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html#reseau-convolutif-sur-pytorch","title":"R\u00e9seau Convolutif sur Pytorch\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html#import-des-library-necessaires","title":"Import des library n\u00e9cessaires\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html#classification-sur-mnist","title":"Classification sur MNIST\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html#recuperation-du-dataset-et-creation-des-dataloaders-pour-lentrainement","title":"R\u00e9cuperation du dataset et cr\u00e9ation des dataloaders pour l'entra\u00eenement\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html#creation-dun-reseau-de-neurones-convolutif-en-pytorch","title":"Cr\u00e9ation d'un r\u00e9seau de neurones convolutif en pytorch\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html#entrainement-du-modele","title":"Entra\u00eenement du mod\u00e8le\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html#softmax-et-interpretation-des-resultats","title":"Softmax et interpr\u00e9tation des r\u00e9sultats\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/04_R%C3%A9seauConvolutifPytorch.html#autres-applications","title":"Autres applications\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html","title":"Application sur un dataset d'images couleur","text":"In\u00a0[2]: Copied! <pre>import torchvision\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> import torchvision import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as transforms import matplotlib.pyplot as plt import numpy as np <p>Dans cette application, nous allons utiliser le dataset CIFAR-10 qui regroupe 60 000 images couleurs de taille $3 \\times 32 \\times 32$ appartenant \u00e0 10 classes. Les classes sont les suivantes : airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. Nous allons utiliser torchvision pour charger le dataset :</p> In\u00a0[3]: Copied! <pre>classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n# Transformation des donn\u00e9es, normalisation et transformation en tensor pytorch\ntransform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# T\u00e9l\u00e9chargement et chargement du dataset\ndataset = torchvision.datasets.CIFAR10(root='./../data', train=True,download=True, transform=transform)\ntestdataset = torchvision.datasets.CIFAR10(root='./../data', train=False,download=True, transform=transform)\nprint(\"taille d'une image : \",dataset[0][0].shape)\n\n#Cr\u00e9ation des dataloaders pour le train, validation et test\ntrain_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2)\nval_loader= torch.utils.data.DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2)\n</pre> classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck') # Transformation des donn\u00e9es, normalisation et transformation en tensor pytorch transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # T\u00e9l\u00e9chargement et chargement du dataset dataset = torchvision.datasets.CIFAR10(root='./../data', train=True,download=True, transform=transform) testdataset = torchvision.datasets.CIFAR10(root='./../data', train=False,download=True, transform=transform) print(\"taille d'une image : \",dataset[0][0].shape)  #Cr\u00e9ation des dataloaders pour le train, validation et test train_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2]) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2) val_loader= torch.utils.data.DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2) test_loader = torch.utils.data.DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2) <pre>Files already downloaded and verified\nFiles already downloaded and verified\ntaille d'une image :  torch.Size([3, 32, 32])\n</pre> <p>On peut visualiser les images et les classes :</p> In\u00a0[3]: Copied! <pre>def imshow_with_labels(img, labels):\n  img = img / 2 + 0.5  # D\u00e9normaliser\n  npimg = img.numpy()\n  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n  plt.xticks([])  # Supprimer les graduations sur l'axe des x\n  plt.yticks([])  # Supprimer les graduations sur l'axe des y\n  for i in range(len(labels)):\n    plt.text(i * 35, -2, classes[labels[i]], color='black', fontsize=8, ha='left')\n\n# R\u00e9cup\u00e9ration d'un batch d'images\nimages, labels = next(iter(train_loader))\n\n# Affichage des images avec leurs classes\nimshow_with_labels(torchvision.utils.make_grid(images[:8]), labels[:8])\nplt.show()\n</pre> def imshow_with_labels(img, labels):   img = img / 2 + 0.5  # D\u00e9normaliser   npimg = img.numpy()   plt.imshow(np.transpose(npimg, (1, 2, 0)))   plt.xticks([])  # Supprimer les graduations sur l'axe des x   plt.yticks([])  # Supprimer les graduations sur l'axe des y   for i in range(len(labels)):     plt.text(i * 35, -2, classes[labels[i]], color='black', fontsize=8, ha='left')  # R\u00e9cup\u00e9ration d'un batch d'images images, labels = next(iter(train_loader))  # Affichage des images avec leurs classes imshow_with_labels(torchvision.utils.make_grid(images[:8]), labels[:8]) plt.show() <p>Dans le cours sur la couche de convolution, nous avons introduit le param\u00e8tre stride qui correspond au pas de la convolution. Avec un pas de 2, l'image aura une r\u00e9solution divis\u00e9 par deux \u00e0 la fin de l'op\u00e9ration de convolution (sous r\u00e9serve que le padding compense la taille du filtre). On voit donc que l'op\u00e9ration de pooling peut \u00eatre \"remplac\u00e9\" par l'augmentation du stride. La sortie d'une couche de convolution de stride 1 suivie d'une couche de pooling sera de m\u00eame dimension que la sortie d'une couche de convolution de stride 2 sans couche de pooling.</p> <p>Dans cette application, nous allons donc remplacer nos couches de pooling par une augmentation du stride de nos convolutions. Pour en savoir plus, vous pouvez consulter le blogpost ou l'article.</p> In\u00a0[4]: Copied! <pre>class cnn(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.conv1=nn.Conv2d(3,8,kernel_size=3,stride=2,padding=1)\n    self.conv2=nn.Conv2d(8,16,kernel_size=3,stride=2,padding=1)\n    self.conv3=nn.Conv2d(16,32,kernel_size=3,stride=2,padding=1)\n    self.fc=nn.Linear(4*4*32,10)\n  \n  def forward(self,x):\n    x=F.relu(self.conv1(x))\n    x=F.relu(self.conv2(x))\n    x=F.relu(self.conv3(x))\n    x=x.view(-1,4*4*32)\n    output=self.fc(x)\n    return output\n</pre> class cnn(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)     self.conv1=nn.Conv2d(3,8,kernel_size=3,stride=2,padding=1)     self.conv2=nn.Conv2d(8,16,kernel_size=3,stride=2,padding=1)     self.conv3=nn.Conv2d(16,32,kernel_size=3,stride=2,padding=1)     self.fc=nn.Linear(4*4*32,10)      def forward(self,x):     x=F.relu(self.conv1(x))     x=F.relu(self.conv2(x))     x=F.relu(self.conv3(x))     x=x.view(-1,4*4*32)     output=self.fc(x)     return output In\u00a0[5]: Copied! <pre>model = cnn() # Couches d'entr\u00e9e de taille 2, deux couches cach\u00e9es de 16 neurones et un neurone de sortie\nprint(model)\nprint(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters()))\n</pre> model = cnn() # Couches d'entr\u00e9e de taille 2, deux couches cach\u00e9es de 16 neurones et un neurone de sortie print(model) print(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters())) <pre>cnn(\n  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)\nNombre de param\u00e8tres 11162\n</pre> In\u00a0[6]: Copied! <pre>criterion = nn.CrossEntropyLoss()\nepochs=5\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n</pre> criterion = nn.CrossEntropyLoss() epochs=5 learning_rate=0.001 optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate) In\u00a0[7]: Copied! <pre>for i in range(epochs):\n  loss_train=0\n  for images, labels in train_loader:\n    preds=model(images)\n    loss=criterion(preds,labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, labels in val_loader:\n    with torch.no_grad(): # permet de ne pas calculer les gradients\n      preds=model(images)\n      loss=criterion(preds,labels)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> for i in range(epochs):   loss_train=0   for images, labels in train_loader:     preds=model(images)     loss=criterion(preds,labels)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, labels in val_loader:     with torch.no_grad(): # permet de ne pas calculer les gradients       preds=model(images)       loss=criterion(preds,labels)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 1.5988761186599731\nstep 0 val loss 1.4532517194747925\nstep 1 train loss 1.3778905868530273\nstep 1 val loss 1.3579093217849731\nstep 2 train loss 1.2898519039154053\nstep 2 val loss 1.2919617891311646\nstep 3 train loss 1.2295998334884644\nstep 3 val loss 1.256637692451477\nstep 4 train loss 1.186734914779663\nstep 4 val loss 1.240902304649353\n</pre> <p>On peut maintenant regarder la pr\u00e9cision sur les donn\u00e9es de test :</p> In\u00a0[8]: Copied! <pre>correct = 0\ntotal = 0\nfor images,labels in test_loader: \n  with torch.no_grad():\n    preds=model(images)\n    \n    _, predicted = torch.max(preds.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()     \ntest_acc = 100 * correct / total\nprint(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc)\n</pre> correct = 0 total = 0 for images,labels in test_loader:    with torch.no_grad():     preds=model(images)          _, predicted = torch.max(preds.data, 1)     total += labels.size(0)     correct += (predicted == labels).sum().item()      test_acc = 100 * correct / total print(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc) <pre>Pr\u00e9cision du mod\u00e8le en phase de test :  55.92\n</pre> <p>Si vous disposez d'un GPU sur votre ordinateur ou que vous avez acc\u00e8s \u00e0 un GPU par un service CLOUD, vous pouvez acc\u00e9lerer l'entra\u00eenement de votre mod\u00e8le ainsi que l'inf\u00e9rence. Voici comment faire en pytorch :</p> In\u00a0[9]: Copied! <pre>model = cnn().to('cuda') # Ajouter le .to('cuda') pour charger le mod\u00e8le sur GPU\n</pre> model = cnn().to('cuda') # Ajouter le .to('cuda') pour charger le mod\u00e8le sur GPU In\u00a0[10]: Copied! <pre>criterion = nn.CrossEntropyLoss()\nepochs=5\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n</pre> criterion = nn.CrossEntropyLoss() epochs=5 learning_rate=0.001 optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate) In\u00a0[11]: Copied! <pre>for i in range(epochs):\n  loss_train=0\n  for images, labels in train_loader:\n    images=images.to('cuda') # Ajouter le .to('cuda') pour charger les images sur GPU\n    labels=labels.to('cuda') # Ajouter le .to('cuda') pour charger les labels sur GPU\n    preds=model(images)\n    loss=criterion(preds,labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, labels in val_loader:\n    with torch.no_grad():\n      images=images.to('cuda')\n      labels=labels.to('cuda')\n      preds=model(images)\n      loss=criterion(preds,labels)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> for i in range(epochs):   loss_train=0   for images, labels in train_loader:     images=images.to('cuda') # Ajouter le .to('cuda') pour charger les images sur GPU     labels=labels.to('cuda') # Ajouter le .to('cuda') pour charger les labels sur GPU     preds=model(images)     loss=criterion(preds,labels)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, labels in val_loader:     with torch.no_grad():       images=images.to('cuda')       labels=labels.to('cuda')       preds=model(images)       loss=criterion(preds,labels)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n</pre> <pre>step 0 train loss 1.6380540132522583\nstep 0 val loss 1.4465171098709106\nstep 1 train loss 1.369913101196289\nstep 1 val loss 1.3735681772232056\nstep 2 train loss 1.2817057371139526\nstep 2 val loss 1.2942956686019897\nstep 3 train loss 1.2238909006118774\nstep 3 val loss 1.2601954936981201\nstep 4 train loss 1.186323881149292\nstep 4 val loss 1.2583365440368652\n</pre> In\u00a0[12]: Copied! <pre>correct = 0\ntotal = 0\nfor images,labels in test_loader: \n  images=images.to('cuda')\n  labels=labels.to('cuda')\n  with torch.no_grad():\n    preds=model(images)\n    \n    _, predicted = torch.max(preds.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()     \ntest_acc = 100 * correct / total\nprint(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc)\n</pre> correct = 0 total = 0 for images,labels in test_loader:    images=images.to('cuda')   labels=labels.to('cuda')   with torch.no_grad():     preds=model(images)          _, predicted = torch.max(preds.data, 1)     total += labels.size(0)     correct += (predicted == labels).sum().item()      test_acc = 100 * correct / total print(\"Pr\u00e9cision du mod\u00e8le en phase de test : \",test_acc) <pre>Pr\u00e9cision du mod\u00e8le en phase de test :  55.76\n</pre> <p>En fonction de votre GPU, vous avez sans doute constat\u00e9 une augmentation de la vitesse d'entra\u00eenement et d'inf\u00e9rence du mod\u00e8le. Cette diff\u00e9rence de vitesse devient tr\u00e8s importante lorsqu'on utilise des GPU tr\u00e8s puissants sur des mod\u00e8les tr\u00e8s profonds et parall\u00e9lisables.</p> <p>Pour vous exercer, vous pouvez essayer d'am\u00e9liorer les performances du mod\u00e8le sur le dataset CIFAR-10. Vous pouvez augmenter le nombre de couches, changer le nombre de filtres des couches, ajouter du dropout, utiliser la batchnorm, augmenter le nombre d'epochs d'entra\u00eenement, changer le learning rate etc ... Essayez d'atteindre au moins 70% de pr\u00e9cision.</p>"},{"location":"03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html#application-sur-un-dataset-dimages-couleur","title":"Application sur un dataset d'images couleur\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html#cifar-10","title":"CIFAR-10\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html#creation-dun-reseau-convolutif-pour-ce-probleme","title":"Cr\u00e9ation d'un r\u00e9seau convolutif pour ce probl\u00e8me\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html#entrainement-du-modele","title":"Entra\u00eenement du mod\u00e8le\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html#gpu","title":"GPU\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/05_ApplicationClassification.html#exercice-a-faire","title":"Exercice \u00e0 faire\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html","title":"Application segmentation","text":"<p>Dans les notebooks pr\u00e9c\u00e9dents, nous avons vu uniquement des probl\u00e8mes de classification c'est-\u00e0-dire pr\u00e9dire une classe \u00e0 partir d'une image (c'est une image de 5, c'est une image de chat etc...). Cependant, il existe deux autres cat\u00e9gories d'analyse d'images : la d\u00e9tection et la segmentation.</p> <p>La d\u00e9tection d'objet dans une image consiste \u00e0 encadrer un objet dans une image. Par exemple, si je cherche \u00e0 d\u00e9tecter les chats et que j'ai un chien et un chat sur l'image, je vais chercher \u00e0 entra\u00eener un mod\u00e8le qui va dessiner une bo\u00eete autour du chat sur l'image.</p> <p>Voici une image pour visualiser l'id\u00e9e de ce qu'est la d\u00e9tection d'objet :</p> <p></p> <p>Image provenant du blogpost.</p> <p>Comme vous pouvez l'imaginer, c'est une t\u00e2che tr\u00e8s int\u00e9ressante pour le domaine du traitement d'images mais ce n'est pas ce que nous allons aborder dans ce notebook.</p> <p>La segmentation a pour but de trouver la classe d'appartenance de chaque pixel de l'image (le pixel en position (120,300) appartient \u00e0 un chien par exemple). L'id\u00e9e \u00e9tant d'obtenir des informations tr\u00e8s pr\u00e9cises sur le contenu de l'image \u00e0 l'\u00e9chelle du pixel.</p> <p></p> <p>Image provenant du blogpost.</p> <p>Il existe plusieurs types de segmentation :</p> <ul> <li>La segmentation s\u00e9mantique : Segmentation qui consiste \u00e0 classifier chaque pixel de l'image ind\u00e9pendamment de l'instance (Toutes les voitures seront colori\u00e9es en rouge par exemple). C'est le type de segmentation que vous voyez sur l'image au dessus.</li> <li>La segmentation d'instance : Cette fois-ci, on va segmenter chaque instance d'une m\u00eame classe avec une couleur diff\u00e9rente (La premi\u00e8re voiture en rouge, la seconde en orange etc ...).</li> <li>Il existe \u00e9galement d'autres types de segmentation. Pour plus d'informations, vous pouvez consulter le blogpost.</li> </ul> <p>Dans ce notebook, nous allons faire un exemple de segmentation s\u00e9mantique.</p> <p>Nous utilisons le dataset Oxford-IIIT Pet Dataset qui contient un total de 7349 images dont 4978 images de chien et 2371 images de chat. Pour chaque image, il y a les annotations de segmentation ainsi que la bo\u00eete entourant la t\u00eate de l'animal.</p> <p></p> <p>Nous utiliserons uniquement l'annotation correspondant \u00e0 la segmentation de l'animal.</p> <p>Pour r\u00e9cuperer et utiliser le dataset, il faut le t\u00e9l\u00e9charger directement sur le site car la version propos\u00e9e par torchvision n'inclue pas les masques de segmentation.</p> <p>Lorsque le dataset que vous voulez utiliser n'est pas directement disponible sur torchvision, il est n\u00e9cessaire de cr\u00e9er une classe h\u00e9ritant de la classe Dataset pour sp\u00e9cifier la gestion des donn\u00e9es de ce dataset.</p> In\u00a0[7]: Copied! <pre># Import n\u00e9cessaire pour le projet\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch\nimport numpy as np\n</pre> # Import n\u00e9cessaire pour le projet from PIL import Image import os import matplotlib.pyplot as plt from torch.utils.data import Dataset import torchvision.transforms as T import torch.nn as nn from torch.utils.data import DataLoader import torch import numpy as np In\u00a0[\u00a0]: Copied! <pre>class datasetSeg(Dataset):\n  def __init__(self, path):\n    self.path = path\n    self.imagesPath,self.masksPath=self.loadDataset()\n    \n    self.resize=T.Resize((100, 100)) # Valeur de resize pour les images\n    self.toTensor=T.ToTensor()\n  \n  def __len__(self):\n    return len(self.imagesPath)\n  \n  def loadDataset(self): # R\u00e9cuperation des liens vers les images et les annotations\n    images=os.listdir(self.path+\"images/\")\n    images=sorted(images)\n    masks=os.listdir(self.path+\"annotations/trimaps/\")\n    masks=sorted(masks)\n    masks=[ mask for mask in masks if \"._\" not in mask ]\n    images=[image for image in images if \".mat\" not in image]\n    return images, masks\n  \n  def __getitem__(self, index):\n    image=Image.open(self.path+\"images/\"+self.imagesPath[index]).convert('RGB')\n    # Resize des images et r\u00e9cuperation du masque de segmentation\n    trimap=self.resize(Image.open(self.path+\"annotations/trimaps/\"+self.masksPath[index]).convert('L'))\n    trimap=np.array(trimap)\n    # Cr\u00e9ation des masques pour les classes\n    class1 = (trimap == 1).astype(np.uint8)  \n    class2 = (trimap == 2).astype(np.uint8)  \n    class3 = (trimap == 3).astype(np.uint8)\n    mask = np.stack([class1, class2, class3], axis=0)\n    return self.toTensor(self.resize(image)), torch.tensor(mask)\n        \n</pre> class datasetSeg(Dataset):   def __init__(self, path):     self.path = path     self.imagesPath,self.masksPath=self.loadDataset()          self.resize=T.Resize((100, 100)) # Valeur de resize pour les images     self.toTensor=T.ToTensor()      def __len__(self):     return len(self.imagesPath)      def loadDataset(self): # R\u00e9cuperation des liens vers les images et les annotations     images=os.listdir(self.path+\"images/\")     images=sorted(images)     masks=os.listdir(self.path+\"annotations/trimaps/\")     masks=sorted(masks)     masks=[ mask for mask in masks if \"._\" not in mask ]     images=[image for image in images if \".mat\" not in image]     return images, masks      def __getitem__(self, index):     image=Image.open(self.path+\"images/\"+self.imagesPath[index]).convert('RGB')     # Resize des images et r\u00e9cuperation du masque de segmentation     trimap=self.resize(Image.open(self.path+\"annotations/trimaps/\"+self.masksPath[index]).convert('L'))     trimap=np.array(trimap)     # Cr\u00e9ation des masques pour les classes     class1 = (trimap == 1).astype(np.uint8)       class2 = (trimap == 2).astype(np.uint8)       class3 = (trimap == 3).astype(np.uint8)     mask = np.stack([class1, class2, class3], axis=0)     return self.toTensor(self.resize(image)), torch.tensor(mask)          <p>Maintenant qu'on a cr\u00e9e notre classe dataset, on peut charger notre dataset et examiner son contenu.</p> In\u00a0[9]: Copied! <pre>dataset=datasetSeg(path=\"./../data/OxfordPets/\")\nimg=dataset[0][0].permute(1,2,0).numpy()\nmsk = dataset[0][1].permute(1, 2, 0).numpy() * 1.0\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\naxs[0].imshow(img)\naxs[0].set_title('Image')\naxs[1].imshow(msk)\naxs[1].set_title('Masque')\nplt.show()\n</pre> dataset=datasetSeg(path=\"./../data/OxfordPets/\") img=dataset[0][0].permute(1,2,0).numpy() msk = dataset[0][1].permute(1, 2, 0).numpy() * 1.0  fig, axs = plt.subplots(1, 2, figsize=(10, 5))  axs[0].imshow(img) axs[0].set_title('Image') axs[1].imshow(msk) axs[1].set_title('Masque') plt.show() <p>Nous avons bien l'image et sa segmentation. Maitenant, il est temps de split notre dataset et de d\u00e9finir nos dataloaders :</p> In\u00a0[10]: Copied! <pre>train_set, val_set, test_set = torch.utils.data.random_split(dataset,[0.7, 0.2, 0.1])\ntrain_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_set, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_set, batch_size=1, shuffle=True)\n</pre> train_set, val_set, test_set = torch.utils.data.random_split(dataset,[0.7, 0.2, 0.1]) train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True) val_dataloader = DataLoader(val_set, batch_size=32, shuffle=True) test_dataloader = DataLoader(test_set, batch_size=1, shuffle=True) <p>Contrairement \u00e0 un probl\u00e8me de classification d'images, la sortie de notre r\u00e9seau de neurones doit \u00eatre de la m\u00eame dimension que l'entr\u00e9e en r\u00e9solution et avoir un channel par classe que l'on d\u00e9sire segmenter (Si l'on veut segmenter dix classes sur des images d'entr\u00e9e de taille $224 \\times 224 \\times \\ 3$ on aura une sortie de taille $224 \\times 224 \\times \\ 10$). Une architecture classique de r\u00e9seau convolutifs o\u00f9 la r\u00e9solution diminue au fur et \u00e0 mesure des couches jusqu'\u00e0 une couche fully connected de classification n'est donc pas la bonne m\u00e9thode. A la place, on peut utiliser l'architecture U-Net.</p> <p>Voici \u00e0 quoi ressemble l'architecture U-Net :</p> <p></p> <p>Figure extraite de ce blogpost</p> <p>Comme vous pouvez le constaster, l'architecture est plut\u00f4t atypique. Elle consiste en un chemin de contraction (appel\u00e9 encodeur) qui r\u00e9duit la taille de l'image comme un CNN classique et un chemin d'expansion (appel\u00e9 d\u00e9codeur) qui augmenter la taille de l'image jusqu'\u00e0 une r\u00e9solution identique \u00e0 l'entr\u00e9e. Les connexions indiqu\u00e9es par les fl\u00e8ches noires permettent de conserver une information locale pr\u00e9cise tandis que la partie encodeur/d\u00e9codeur permet de d\u00e9tecter des relations plus abstraites entre diff\u00e9rents pixels. Cette architecture a \u00e9t\u00e9 introduite en premier pour la segmentation d'images m\u00e9dicales dans cet article. Elle est maintenant utilis\u00e9e dans beaucoup de domaines (segmentation, denoising, diffusion etc ...).</p> <p>Nous avons vu en d\u00e9tail les couches de convolutions qui permettent d'effectuer des op\u00e9rations sur les images avec des param\u00e8tres entra\u00eenables. On a vu que, gr\u00e2ce au param\u00e8tre stride, les couches de convolution nous permettent de conserver la r\u00e9solution initiale ou bien de diminuer cette r\u00e9solution. Cependant, on a parfois besoin d'augmenter la r\u00e9solution de l'image (pour la partie d\u00e9codeur du U-Net d\u00e9crit au dessus par exemple).</p> <p>Pour cela, on peut envisager plusieurs options :</p> <ul> <li>Interpolation suivi d'une convolution : A l'image du pooling mais dans l'\"autre sens\", cette id\u00e9e consiste \u00e0 utiliser une fonction d'interpolation non entra\u00eenable pour agrandir artificiellement l'image.</li> <li>Convolution tranpos\u00e9e : Cette op\u00e9ration joue le r\u00f4le de l'inverse d'une convolution et permet d'agrandir la taille de l'image. Voici un exemple de convolution transpos\u00e9e avec un kernel de taille $2 \\times 2$. C'est une op\u00e9ration qui contient des param\u00e8tres entra\u00eenables.</li> </ul> <p></p> <p>Figure extraite de blogpost.</p> <p>Pour ceux qui veulent entrer dans le d\u00e9tail et savoir les diff\u00e9rences principales entre les deux techniques, je vous conseille la lecture de cet article.</p> <p>Dans notre impl\u00e9mentation, nous allons utiliser la convolution transpos\u00e9e.</p> <p>Il est temps d'impl\u00e9menter l'architecture U-Net en pytorch :</p> In\u00a0[11]: Copied! <pre># Fonction pour combiner couche de convolution, activation ReLU et BatchNorm pour \u00e9viter les copier coller \ndef conv_relu_bn(input_channels, output_channels, kernel_size, stride, padding):\n  return nn.Sequential(\n    nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding),\n    nn.ReLU(),\n    nn.BatchNorm2d(output_channels,momentum=0.01)\n  )\n    \n# Idem mais avec la convolution transpos\u00e9e\ndef convT_relu_bn(input_channels, output_channels, kernel_size, stride, padding):\n  return nn.Sequential(\n    nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n    nn.ReLU(),\n    nn.BatchNorm2d(output_channels,momentum=0.01)\n  )\n\nclass UNet(nn.Module):\n  def __init__(self, num_classes=3):\n    super().__init__()\n    # encoder\n    self.conv1=conv_relu_bn(3, 64, 3, 1, 1)\n    self.conv2=conv_relu_bn(64, 64, 3, 1, 1)\n    self.maxPool1=nn.MaxPool2d(2, 2)\n    \n    self.conv3=conv_relu_bn(64, 128, 3, 1, 1)\n    self.conv4=conv_relu_bn(128, 128, 3, 1, 1)\n    self.maxPool2=nn.MaxPool2d(2, 2)\n    \n    #Module central\n    self.conv5=conv_relu_bn(128, 256, 3, 1, 1)\n    self.conv6=conv_relu_bn(256, 256, 3, 1, 1)\n    \n    # D\u00e9codeur\n    self.convT1=convT_relu_bn(256, 128, 4, 2, 1)\n    self.conv7=conv_relu_bn(256, 128, 3, 1, 1)\n    \n    self.convT2=convT_relu_bn(128, 64, 4, 2, 1)\n    self.conv8=conv_relu_bn(128, 64, 3, 1, 1)\n    \n    # On va pr\u00e9dire un channel par classe\n    self.conv9=conv_relu_bn(64, num_classes, 3, 1, 1)     \n    self.sigmoid=nn.Sigmoid()\n  \n  def forward(self,x):\n    # Encodeur\n    x=self.conv1(x)\n    x1=self.conv2(x)\n    \n    x=self.maxPool1(x1)\n    \n    x=self.conv3(x)\n    x2=self.conv4(x)\n    x=self.maxPool2(x2)\n    \n    # Module central\n    x=self.conv5(x)\n    x=self.conv6(x)\n\n    #D\u00e9codeur\n    x=self.convT1(x)\n    x=torch.cat((x,x2),dim=1)\n    x=self.conv7(x)\n    \n    x=self.convT2(x)\n    x=torch.cat((x,x1),dim=1)\n    x=self.conv8(x)\n    \n    x=self.conv9(x)\n    x=self.sigmoid(x)\n    return x\n</pre> # Fonction pour combiner couche de convolution, activation ReLU et BatchNorm pour \u00e9viter les copier coller  def conv_relu_bn(input_channels, output_channels, kernel_size, stride, padding):   return nn.Sequential(     nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding),     nn.ReLU(),     nn.BatchNorm2d(output_channels,momentum=0.01)   )      # Idem mais avec la convolution transpos\u00e9e def convT_relu_bn(input_channels, output_channels, kernel_size, stride, padding):   return nn.Sequential(     nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),     nn.ReLU(),     nn.BatchNorm2d(output_channels,momentum=0.01)   )  class UNet(nn.Module):   def __init__(self, num_classes=3):     super().__init__()     # encoder     self.conv1=conv_relu_bn(3, 64, 3, 1, 1)     self.conv2=conv_relu_bn(64, 64, 3, 1, 1)     self.maxPool1=nn.MaxPool2d(2, 2)          self.conv3=conv_relu_bn(64, 128, 3, 1, 1)     self.conv4=conv_relu_bn(128, 128, 3, 1, 1)     self.maxPool2=nn.MaxPool2d(2, 2)          #Module central     self.conv5=conv_relu_bn(128, 256, 3, 1, 1)     self.conv6=conv_relu_bn(256, 256, 3, 1, 1)          # D\u00e9codeur     self.convT1=convT_relu_bn(256, 128, 4, 2, 1)     self.conv7=conv_relu_bn(256, 128, 3, 1, 1)          self.convT2=convT_relu_bn(128, 64, 4, 2, 1)     self.conv8=conv_relu_bn(128, 64, 3, 1, 1)          # On va pr\u00e9dire un channel par classe     self.conv9=conv_relu_bn(64, num_classes, 3, 1, 1)          self.sigmoid=nn.Sigmoid()      def forward(self,x):     # Encodeur     x=self.conv1(x)     x1=self.conv2(x)          x=self.maxPool1(x1)          x=self.conv3(x)     x2=self.conv4(x)     x=self.maxPool2(x2)          # Module central     x=self.conv5(x)     x=self.conv6(x)      #D\u00e9codeur     x=self.convT1(x)     x=torch.cat((x,x2),dim=1)     x=self.conv7(x)          x=self.convT2(x)     x=torch.cat((x,x1),dim=1)     x=self.conv8(x)          x=self.conv9(x)     x=self.sigmoid(x)     return x <p>Pour entra\u00eener le mod\u00e8le, nous ne distinguerons pas entre les chiens et les chats, mais nous chercherons simplement \u00e0 identifier les pixels appartenant \u00e0 l'animal, les pixels de bordure et les pixels du fond.</p> In\u00a0[12]: Copied! <pre># Nous d\u00e9fissons\nmodel=UNet(num_classes=3).to('cuda')\ncriterion=nn.CrossEntropyLoss()\nlr=0.001\noptimizer=torch.optim.Adam(model.parameters(), lr=lr)\nepochs=5\n</pre> # Nous d\u00e9fissons model=UNet(num_classes=3).to('cuda') criterion=nn.CrossEntropyLoss() lr=0.001 optimizer=torch.optim.Adam(model.parameters(), lr=lr) epochs=5 <p>La base de donn\u00e9es est assez cons\u00e9quente et le mod\u00e8le est assez profond. Il est possible que l'entra\u00eenement dure plusieurs dizaines de minutes si votre GPU n'est pas tr\u00e8s puissant (Si vous n'avez pas de GPU, je ne vous conseille pas d'essayer d'entra\u00eener le mod\u00e8le).</p> In\u00a0[13]: Copied! <pre>for epoch in range(epochs):\n  train_loss=0\n  for images,masks in train_dataloader:\n    images=images.to('cuda')\n    masks=masks.to('cuda').float()\n    optimizer.zero_grad()\n    seg=model(images)\n    loss=criterion(seg,masks)\n    loss.backward()\n    optimizer.step()\n    train_loss += loss.item()\n  print(f\"step {epoch} train loss {train_loss/len(train_dataloader)}\")  \n  val_loss=0\n  for images,masks in val_dataloader:\n    images=images.to('cuda')\n    masks=masks.to('cuda').float()\n    with torch.no_grad():\n      seg=model(images)\n      loss=criterion(seg,masks)\n    val_loss += loss.item() \n  print(f\"step {epoch} train loss {val_loss/len(val_dataloader)}\")  \n</pre> for epoch in range(epochs):   train_loss=0   for images,masks in train_dataloader:     images=images.to('cuda')     masks=masks.to('cuda').float()     optimizer.zero_grad()     seg=model(images)     loss=criterion(seg,masks)     loss.backward()     optimizer.step()     train_loss += loss.item()   print(f\"step {epoch} train loss {train_loss/len(train_dataloader)}\")     val_loss=0   for images,masks in val_dataloader:     images=images.to('cuda')     masks=masks.to('cuda').float()     with torch.no_grad():       seg=model(images)       loss=criterion(seg,masks)     val_loss += loss.item()    print(f\"step {epoch} train loss {val_loss/len(val_dataloader)}\")   <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n  return F.conv2d(input, weight, bias, self.stride,\n</pre> <pre>step 0 train loss 0.9442852522120063\nstep 0 train loss 0.9016501573806114\nstep 1 train loss 0.8738441905121744\nstep 1 train loss 0.8578698165873264\nstep 2 train loss 0.8354099944785789\nstep 2 train loss 0.8258832287281117\nstep 3 train loss 0.8052123431806211\nstep 3 train loss 0.7977393028583932\nstep 4 train loss 0.7819347337440208\nstep 4 train loss 0.7781971261856404\n</pre> <p>On peut maintenant calculer la pr\u00e9cision sur nos donn\u00e9es de test.</p> In\u00a0[14]: Copied! <pre>def calculate_class_accuracy(preds, masks, class_idx):\n  #On convertit les pr\u00e9dictions en valeur entre 0 et 1 pour chaque classe\n  preds = torch.argmax(preds, dim=1)\n  \n  # On ne r\u00e9cup\u00e8re que les pixels de la classe d'int\u00earet\n  preds_class = (preds == class_idx).float()\n  masks_class = (masks == class_idx).float()\n  \n  # Calculer la pr\u00e9cision pour la classe choisie\n  correct = (preds_class == masks_class).float()\n  accuracy = correct.sum() / correct.numel()\n  return accuracy\n\nmodel.eval()\ntest_accuracy = 0.0\nnum_batches = 0\n\n\nwith torch.no_grad(): # D\u00e9sactivation du calcul du gradient\n  for images, masks in test_dataloader:\n    images = images.to('cuda')\n    masks = masks.to('cuda').long()\n    \n    seg = model(images)\n    \n    # On calcule la pr\u00e9cision pour la classe d'int\u00earet (0 correspond \u00e0 la segmentation de l'animal)\n    class_idx = 0\n    batch_accuracy = calculate_class_accuracy(seg, masks, class_idx)\n    \n    test_accuracy += batch_accuracy.item()\n    num_batches += 1\n\n# On calcule la pr\u00e9cision moyenne sur l'ensemble du dataset de test\ntest_accuracy /= num_batches\nprint(f'Pr\u00e9cision pour la segmentation de l animal : {test_accuracy*100:.1f}%')\n</pre> def calculate_class_accuracy(preds, masks, class_idx):   #On convertit les pr\u00e9dictions en valeur entre 0 et 1 pour chaque classe   preds = torch.argmax(preds, dim=1)      # On ne r\u00e9cup\u00e8re que les pixels de la classe d'int\u00earet   preds_class = (preds == class_idx).float()   masks_class = (masks == class_idx).float()      # Calculer la pr\u00e9cision pour la classe choisie   correct = (preds_class == masks_class).float()   accuracy = correct.sum() / correct.numel()   return accuracy  model.eval() test_accuracy = 0.0 num_batches = 0   with torch.no_grad(): # D\u00e9sactivation du calcul du gradient   for images, masks in test_dataloader:     images = images.to('cuda')     masks = masks.to('cuda').long()          seg = model(images)          # On calcule la pr\u00e9cision pour la classe d'int\u00earet (0 correspond \u00e0 la segmentation de l'animal)     class_idx = 0     batch_accuracy = calculate_class_accuracy(seg, masks, class_idx)          test_accuracy += batch_accuracy.item()     num_batches += 1  # On calcule la pr\u00e9cision moyenne sur l'ensemble du dataset de test test_accuracy /= num_batches print(f'Pr\u00e9cision pour la segmentation de l animal : {test_accuracy*100:.1f}%') <pre>Pr\u00e9cision pour la segmentation de l animal : 43.4%\n</pre> <p>Nous pouvons visualiser les r\u00e9sultats de notre entra\u00eenement sur une image tir\u00e9e du dataset de test. Pour une t\u00e2che de segmentation, c'est int\u00e9ressant de regarder ce que le r\u00e9sultat donne sur un ou plusieurs exemples. Si vous lancez plusieurs fois le block de code suivant, vous pourrez voir la g\u00e9n\u00e9ration sur des images diff\u00e9rentes.</p> In\u00a0[32]: Copied! <pre>images,labels=next(iter(test_dataloader))\n\n#Isolons un \u00e9l\u00e9ment \nimage=images[0].unsqueeze(0).to('cuda') # Le unsqueeze permet de garder la dimension batch\nwith torch.no_grad():\n  seg=model(image)\n    \n# Affichons la segmentation pr\u00e9dite par le mod\u00e8le pour cet \u00e9l\u00e9ment\nfig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\naxs[0].imshow(images[0].permute(1, 2, 0).cpu().numpy())\naxs[0].set_title('Image de base')\naxs[0].axis('off')\n\naxs[1].imshow(seg[0][0].cpu().numpy(), cmap='gray')\naxs[1].set_title('Animal')\naxs[1].axis('off')\n\naxs[2].imshow(seg[0][1].cpu().numpy(), cmap='gray')\naxs[2].set_title('Fond')\naxs[2].axis('off')\n\naxs[3].imshow(seg[0][2].cpu().numpy(), cmap='gray')\naxs[3].set_title('Contours')\naxs[3].axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> images,labels=next(iter(test_dataloader))  #Isolons un \u00e9l\u00e9ment  image=images[0].unsqueeze(0).to('cuda') # Le unsqueeze permet de garder la dimension batch with torch.no_grad():   seg=model(image)      # Affichons la segmentation pr\u00e9dite par le mod\u00e8le pour cet \u00e9l\u00e9ment fig, axs = plt.subplots(1, 4, figsize=(20, 5))  axs[0].imshow(images[0].permute(1, 2, 0).cpu().numpy()) axs[0].set_title('Image de base') axs[0].axis('off')  axs[1].imshow(seg[0][0].cpu().numpy(), cmap='gray') axs[1].set_title('Animal') axs[1].axis('off')  axs[2].imshow(seg[0][1].cpu().numpy(), cmap='gray') axs[2].set_title('Fond') axs[2].axis('off')  axs[3].imshow(seg[0][2].cpu().numpy(), cmap='gray') axs[3].set_title('Contours') axs[3].axis('off')  plt.tight_layout() plt.show() <p>Malgr\u00e9 le score assez faible de 43% de pr\u00e9cision, on remarque que la segmentation est correcte sur la majorit\u00e9 des images.</p> <p>Pour vous exercer, vous pouvez essayer d'am\u00e9liorer les performances du mod\u00e8le. Vous pouvez augmenter le nombre de couches, changer le nombre de filtres des couches, ajouter du dropout, utiliser la batchnorm, augmenter le nombre d'epochs d'entra\u00eenement, changer le learning rate etc ... Essayer de maximiser les performances de votre mod\u00e8le de segmentation !</p>"},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#application-segmentation","title":"Application segmentation\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#quest-ce-que-la-segmentation","title":"Qu'est ce que la segmentation ?\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#detection-dobjets-dans-une-image","title":"D\u00e9tection d'objets dans une image\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#segmentation-dobjets-dans-une-image","title":"Segmentation d'objets dans une image\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#dataset-utilise","title":"Dataset utilis\u00e9\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#architecture-du-modele-u-net","title":"Architecture du mod\u00e8le : U-Net\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#point-sur-la-convolution-transposee","title":"Point sur la convolution transpos\u00e9e\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#implementation-pytorch","title":"Impl\u00e9mentation pytorch\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#entrainement-du-modele","title":"Entra\u00eenement du mod\u00e8le\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#visualisation","title":"Visualisation\u00b6","text":""},{"location":"03_R%C3%A9seauConvolutifs/06_ApplicationSegmentation.html#exercice-a-faire","title":"Exercice \u00e0 faire\u00b6","text":""},{"location":"04_Autoencodeurs/index.html","title":"\ud83d\udd04 Autoencodeurs \ud83d\udd04","text":"<p>Ce cours aborde la notion d'entra\u00eenement non supervis\u00e9 en pr\u00e9sentant les diff\u00e9rences entre supervis\u00e9 et non supervis\u00e9. L'exemple de l'autoencodeur est ensuite abord\u00e9 ainsi que son application pour la d\u00e9tection d'anomalies non supervis\u00e9e. Pour finir, un notebook montre le potentiel de l'autoencodeur pour le probl\u00e8me du \"denoising\". </p>"},{"location":"04_Autoencodeurs/index.html#notebook-1-intuition-et-premier-ae","title":"Notebook 1\ufe0f\u20e3 : Intuition et premier AE","text":"<p>Ce notebook introduit les concepts d'entrainement supervis\u00e9 et non supervis\u00e9 puis pr\u00e9sente l'achitecture autoencodeur. Enfin, une impl\u00e9mentation d'un autoencodeur est propos\u00e9e pour la d\u00e9tection d'anomalies.</p>"},{"location":"04_Autoencodeurs/index.html#notebook-2-denoising-ae","title":"Notebook 2\ufe0f\u20e3 : Denoising AE","text":"<p>Ce notebook pr\u00e9sente le probl\u00e8me de denoising et propose une impl\u00e9mentation d'un denoising autoencodeur.</p>"},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html","title":"Introduction aux autoencodeurs","text":"<p>Dans les cours pr\u00e9c\u00e9dents, nous avons uniquement abord\u00e9 des cas d'apprentissage supervis\u00e9. Pour faire simple, ce sont les cas o\u00f9, dans les donn\u00e9es d'entra\u00eenement, on poss\u00e8de un \u00e9l\u00e8ment d'entr\u00e9e x et une sortie y. Le but du mod\u00e8le sera alors de prendre x en entr\u00e9e et de pr\u00e9dire la valeur de y. Pour MNIST, nous avions une image x et un label y correspondant \u00e0 un chiffre en 0 et 9. Pour la segmentation, on avait une image x et un masque y en sortie.</p> <p>Dans le cas de l'apprentissage non supervis\u00e9, nous avons des donn\u00e9es non labelis\u00e9es c'est \u00e0 dire qu'on a le x mais pas le y. Dans ce cas, il ne sera pas possible de pr\u00e9dire une valeur bien d\u00e9finie par exemple mais on peut imaginer entra\u00eener un mod\u00e8le \u00e0 regrouper des \u00e9l\u00e9ments semblables (c'est ce qu'on appelle le clustering). Dans ce cours, nous allons plut\u00f4t nous int\u00e9resser \u00e0 la d\u00e9tection d'anomalies non supervis\u00e9es. Cela va consister \u00e0 entra\u00eener un mod\u00e8le sur un certain type de donn\u00e9es et utiliser ce mod\u00e8le pour d\u00e9tecter des \u00e9l\u00e8ments qui seraient distincts du jeu de donn\u00e9es d'entra\u00eenement.</p> <p>Le mod\u00e8le de base utilis\u00e9 pour ce type de t\u00e2ches se nomme \"autoencodeur\". Il a une architecture proche du mod\u00e8le U-Net que l'on a vu dans le cours pr\u00e9c\u00e9dent. Voici l'architecture classique d'un autoencodeur :</p> <p></p> <p>Comme vous pouvez le voir, il a une forme de \"sablier\". L'id\u00e9e de l'autoencodeur est de cr\u00e9er une representation compress\u00e9e de la donn\u00e9es d'entr\u00e9e et de la reconstruire \u00e0 partir de cette representation compress\u00e9e. On peut d'ailleurs utiliser ce mod\u00e8le dans un objectif de compression de donn\u00e9es.</p> <p>Pour la d\u00e9tection d'anomalies non supervis\u00e9e, prenons un exemple de ce qu'il va se passer. On entra\u00eene notre autoencodeur \u00e0 reconstruire des images du chiffre 5 pendant l'entra\u00eenement. Une fois que le mod\u00e8le est entra\u00een\u00e9, il pourra reconstruire une image de 5 \u00e0 la perfection. Si notre objectif est de d\u00e9tecter si une image est une image de 5 ou une image d'un autre chiffre, il suffit de donner l'image \u00e0 notre autoencodeur et en fonction de la qualit\u00e9 de la reconstruction ($image_{base} - image_{recons}$), on pourra savoir si il s'agit d'un 5 ou non. L'image suivante illustre cette description :</p> <p></p> <p>Pour illustrer la description pr\u00e9c\u00e9dente, nous allons entra\u00eener le mod\u00e8le autoencodeur de reconstruction des 5 avec Pytorch.</p> In\u00a0[14]: Copied! <pre>import numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# Pour la reproducibilit\u00e9\nnp.random.seed(1337)\nrandom.seed(1337)\n</pre> import numpy as np import random import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as T from torchvision import datasets from torch.utils.data import DataLoader import matplotlib.pyplot as plt  # Pour la reproducibilit\u00e9 np.random.seed(1337) random.seed(1337) In\u00a0[15]: Copied! <pre>transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement\ndataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform)\ntest_dataset = datasets.MNIST(root='./../data', train=False,transform=transform)\n</pre> transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement dataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform) test_dataset = datasets.MNIST(root='./../data', train=False,transform=transform) <p>Nous avons r\u00e9cuper\u00e9 nos dataset de train/val et de test. On cherche \u00e0 ne garder que les 5 dans notre dataset d'entra\u00eenement. Pour cela, supprimons les \u00e9l\u00e9ments du dataset qui contiennent un chiffre diff\u00e9rent de 5.</p> In\u00a0[16]: Copied! <pre># On r\u00e9cupere les indices des images de 5\nindices = [i for i, label in enumerate(dataset.targets) if label == 5]\n# On cr\u00e9er un nouveau dataset avec uniquement les 5\nfiltered_dataset = torch.utils.data.Subset(dataset, indices)\n</pre> # On r\u00e9cupere les indices des images de 5 indices = [i for i, label in enumerate(dataset.targets) if label == 5] # On cr\u00e9er un nouveau dataset avec uniquement les 5 filtered_dataset = torch.utils.data.Subset(dataset, indices) <p>On peut visualiser quelques images pour s'assurer que l'on a bien que des images de 5.</p> In\u00a0[17]: Copied! <pre>fig, axes = plt.subplots(1, 5, figsize=(15, 3))\nfor i in range(5):\n  image, label = filtered_dataset[i]\n  image = image.squeeze().numpy() \n  axes[i].imshow(image, cmap='gray')\n  axes[i].set_title(f'Label: {label}')\n  axes[i].axis('off')\nplt.show()\n</pre> fig, axes = plt.subplots(1, 5, figsize=(15, 3)) for i in range(5):   image, label = filtered_dataset[i]   image = image.squeeze().numpy()    axes[i].imshow(image, cmap='gray')   axes[i].set_title(f'Label: {label}')   axes[i].axis('off') plt.show() <p>Divisons maintenant le dataset en train et validation puis cr\u00e9ons nos dataloaders.</p> In\u00a0[18]: Copied! <pre>train_dataset, validation_dataset=torch.utils.data.random_split(filtered_dataset, [0.8,0.2])\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n</pre> train_dataset, validation_dataset=torch.utils.data.random_split(filtered_dataset, [0.8,0.2]) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) <p>Pour la dataset MNIST, il n'est pas n\u00e9cessaire de faire une architecture tr\u00e8s profonde pour avoir des r\u00e9sultats satisfaisants.</p> In\u00a0[19]: Copied! <pre>class ae(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n\n    self.encoder = nn.Sequential( # Sequential permet de groupe une s\u00e9rie de transformation\n      nn.Linear(28 * 28, 512), \n      nn.ReLU(),\n      nn.Linear(512, 256),\n      nn.ReLU(),\n      nn.Linear(256, 128),\n      nn.ReLU(),\n    )\n    self.decoder = nn.Sequential(\n      nn.Linear(128, 256),\n      nn.ReLU(),\n      nn.Linear(256, 512),\n      nn.ReLU(),\n      nn.Linear(512, 28 * 28),\n      nn.Sigmoid()\n    )\n  \n  def forward(self,x):\n    x=x.view(-1,28*28) \n    x = self.encoder(x)\n    x = self.decoder(x)\n    recons=x.view(-1,28,28)\n    return recons\n</pre> class ae(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)      self.encoder = nn.Sequential( # Sequential permet de groupe une s\u00e9rie de transformation       nn.Linear(28 * 28, 512),        nn.ReLU(),       nn.Linear(512, 256),       nn.ReLU(),       nn.Linear(256, 128),       nn.ReLU(),     )     self.decoder = nn.Sequential(       nn.Linear(128, 256),       nn.ReLU(),       nn.Linear(256, 512),       nn.ReLU(),       nn.Linear(512, 28 * 28),       nn.Sigmoid()     )      def forward(self,x):     x=x.view(-1,28*28)      x = self.encoder(x)     x = self.decoder(x)     recons=x.view(-1,28,28)     return recons In\u00a0[20]: Copied! <pre>model = ae()\nprint(model)\nprint(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters()))\n</pre> model = ae() print(model) print(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters())) <pre>ae(\n  (encoder): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=256, out_features=128, bias=True)\n    (5): ReLU()\n  )\n  (decoder): Sequential(\n    (0): Linear(in_features=128, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=784, bias=True)\n    (5): Sigmoid()\n  )\n)\nNombre de param\u00e8tres 1132944\n</pre> <p>Pour la fonction de loss, nous utilisons la fonction MSELoss qui est la perte quadratique moyenne d\u00e9finie comme ceci : $\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$ o\u00f9 $N$ est le nombre total de pixels dans l'image, $y_i$ est la valeur du pixel $i$ dans l'image originale, et $\\hat{y}_i$ est la valeur du pixel $i$ dans l'image reconstruite. C'est une fonction classique pour quantifier la qualit\u00e9 d'une reconstruction.</p> In\u00a0[21]: Copied! <pre>criterion = nn.MSELoss()\nepochs=10\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n</pre> criterion = nn.MSELoss() epochs=10 learning_rate=0.001 optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate) In\u00a0[22]: Copied! <pre>for i in range(epochs):\n  loss_train=0\n  for images, _ in train_loader:\n    recons=model(images)\n    loss=criterion(recons,images)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, _ in val_loader:\n    with torch.no_grad():\n      recons=model(images)\n      loss=criterion(recons,images)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> for i in range(epochs):   loss_train=0   for images, _ in train_loader:     recons=model(images)     loss=criterion(recons,images)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, _ in val_loader:     with torch.no_grad():       recons=model(images)       loss=criterion(recons,images)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 0.08228749781847\nstep 0 val loss 0.06261523813009262\nstep 1 train loss 0.06122465804219246\nstep 1 val loss 0.06214689463376999\nstep 2 train loss 0.06105153635144234\nstep 2 val loss 0.06189680099487305\nstep 3 train loss 0.06086035445332527\nstep 3 val loss 0.06180128455162048\nstep 4 train loss 0.0608210563659668\nstep 4 val loss 0.06169722229242325\nstep 5 train loss 0.06080913543701172\nstep 5 val loss 0.061976321041584015\nstep 6 train loss 0.060783520340919495\nstep 6 val loss 0.06190618872642517\nstep 7 train loss 0.06072703003883362\nstep 7 val loss 0.06161761283874512\nstep 8 train loss 0.06068740040063858\nstep 8 val loss 0.061624933034181595\nstep 9 train loss 0.060728199779987335\nstep 9 val loss 0.061608292162418365\n</pre> <p>Regardons maintenant la reconstruction sur les images de notre dataset de test.</p> In\u00a0[23]: Copied! <pre>images,_=next(iter(test_loader))\n\n#Isolons un \u00e9l\u00e9ment \nimage=images[0].unsqueeze(0)\nwith torch.no_grad():\n  recons=model(image)\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Image d'origine\naxs[0].imshow(image[0].squeeze().cpu().numpy(), cmap='gray')\naxs[0].set_title('Image d\\'origine')\naxs[0].axis('off')\n\n# Image reconstruite\naxs[1].imshow(recons[0].squeeze().cpu().numpy(), cmap='gray')\naxs[1].set_title('Image reconstruite')\naxs[1].axis('off')\nplt.show()\nprint(\"difference : \", criterion(image,recons).item())\n</pre> images,_=next(iter(test_loader))  #Isolons un \u00e9l\u00e9ment  image=images[0].unsqueeze(0) with torch.no_grad():   recons=model(image) fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Image d'origine axs[0].imshow(image[0].squeeze().cpu().numpy(), cmap='gray') axs[0].set_title('Image d\\'origine') axs[0].axis('off')  # Image reconstruite axs[1].imshow(recons[0].squeeze().cpu().numpy(), cmap='gray') axs[1].set_title('Image reconstruite') axs[1].axis('off') plt.show() print(\"difference : \", criterion(image,recons).item()) <pre>difference :  0.0687035545706749\n</pre> <p>On constate que la reconstruction du 7 est tr\u00e8s mauvaise, on peut donc d\u00e9duire qu'il s'agit d'une anomalie.</p>"},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#introduction-aux-autoencodeurs","title":"Introduction aux autoencodeurs\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#apprentissage-supervise-et-non-supervise","title":"Apprentissage supervis\u00e9 et non supervis\u00e9\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#apprentissage-supervise","title":"Apprentissage supervis\u00e9\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#apprentissage-non-supervise","title":"Apprentissage non supervis\u00e9\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#autoencodeur","title":"Autoencodeur\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#architecture","title":"Architecture\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#utilisation-pour-la-detection-danomalies-non-supervisee","title":"Utilisation pour la d\u00e9tection d'anomalies non supervis\u00e9e\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#application-pratique-sur-mnist","title":"Application pratique sur MNIST\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#creation-des-datasets-dentrainement-et-de-test","title":"Cr\u00e9ation des datasets d'entra\u00eenement et de test\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#creation-du-modele-autoencodeur","title":"Cr\u00e9ation du mod\u00e8le autoencodeur\u00b6","text":""},{"location":"04_Autoencodeurs/01_IntuitionEtPremierAE.html#entrainement-du-modele","title":"Entra\u00eenement du mod\u00e8le\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html","title":"Denoising Autoencodeur","text":"<p>La t\u00e2che de denoising consiste \u00e0 enlever le bruit ind\u00e9sirable d'une image. C'est une t\u00e2che tr\u00e8s importante dans le domaine du traitement d'images. Pour nous, l'id\u00e9e serait d'avoir en entr\u00e9e du r\u00e9seau une image bruit\u00e9e et en sortie une image nette.</p> <p></p> <p>Figure extraite de article.</p> <p>Pour utiliser l'architecture de l'autoencodeur pour ce probl\u00e8me, il suffit de donner en entr\u00e9e du d\u00e9codeur une image bruit\u00e9e (par nos soins), lui faire reconstruire l'image et comparer l'image reconstruite \u00e0 l'image non bruit\u00e9e.</p> <p></p> <p>En utilisant ce type d'architecture on cherche \u00e0 cr\u00e9er un mod\u00e8le de d\u00e9bruitage robuste qui sera capable de d\u00e9bruiter toutes les images. Bien entendu, pour son entra\u00eenement, il faudra une grosse base d'image et il faut aussi veiller \u00e0 ce que le bruit g\u00e9n\u00e9r\u00e9 soit du m\u00eame \"type\" que le bruit que l'on peut rencontrer sur des images r\u00e9elles.</p> <p>Encore une fois, nous allons utiliser le dataset MNIST. On va g\u00e9n\u00e9rer du bruit artificiel sur les images et entra\u00eener notre autoencodeur \u00e0 enlever ce bruit pour nous produire une image nette.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as T from torchvision import datasets from torch.utils.data import DataLoader import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre>transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement\ndataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform)\ntest_dataset = datasets.MNIST(root='./../data', train=False,transform=transform)\ntrain_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n</pre> transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement dataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform) test_dataset = datasets.MNIST(root='./../data', train=False,transform=transform) train_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2]) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) <p>Faisons un exemple d'application de bruit pour visualiser la d\u00e9gradation de l'image en fonction du niveau de bruit.</p> In\u00a0[3]: Copied! <pre>image,_=dataset[0]\n# Le param\u00e8tre dans le np.sqrt correspond \u00e0 la variance d\u00e9sir\u00e9e donc np.sqrt(...) est l'\u00e9cart type\n# torch.randn g\u00e9n\u00e9re des valeurs al\u00e9atoire extraites d'une distribution gaussienne de mean 0 et variance 1\nimageNoisy1 = image + np.sqrt(0.001)*torch.randn(1, 28, 28)\nimageNoisy2 = image + np.sqrt(0.01)*torch.randn(1, 28, 28)\nimageNoisy3 = image + np.sqrt(0.1)*torch.randn(1, 28, 28)\n\nplt.subplot(2, 2, 1)\nplt.imshow(image.squeeze().numpy(), cmap='gray')\nplt.title(\"Image originale\")\n\nplt.subplot(2, 2, 2)\nplt.imshow(imageNoisy1.squeeze().numpy(), cmap='gray')\nplt.title(\"Faible bruit\")\n\nplt.subplot(2, 2, 3)\nplt.imshow(imageNoisy2.squeeze().numpy(), cmap='gray')\nplt.title(\"Bruit moyen\")\n\nplt.subplot(2, 2, 4)\nplt.imshow(imageNoisy3.squeeze().numpy(), cmap='gray')\nplt.title(\"Fort bruit\")\n\nplt.tight_layout()\nplt.show()\n</pre> image,_=dataset[0] # Le param\u00e8tre dans le np.sqrt correspond \u00e0 la variance d\u00e9sir\u00e9e donc np.sqrt(...) est l'\u00e9cart type # torch.randn g\u00e9n\u00e9re des valeurs al\u00e9atoire extraites d'une distribution gaussienne de mean 0 et variance 1 imageNoisy1 = image + np.sqrt(0.001)*torch.randn(1, 28, 28) imageNoisy2 = image + np.sqrt(0.01)*torch.randn(1, 28, 28) imageNoisy3 = image + np.sqrt(0.1)*torch.randn(1, 28, 28)  plt.subplot(2, 2, 1) plt.imshow(image.squeeze().numpy(), cmap='gray') plt.title(\"Image originale\")  plt.subplot(2, 2, 2) plt.imshow(imageNoisy1.squeeze().numpy(), cmap='gray') plt.title(\"Faible bruit\")  plt.subplot(2, 2, 3) plt.imshow(imageNoisy2.squeeze().numpy(), cmap='gray') plt.title(\"Bruit moyen\")  plt.subplot(2, 2, 4) plt.imshow(imageNoisy3.squeeze().numpy(), cmap='gray') plt.title(\"Fort bruit\")  plt.tight_layout() plt.show() <p>Nous allons utiliser le niveau de bruit moyen lors de l'entra\u00eenement, on pourra ensuite voir comme notre denoising autoencodeur se comporte sur d'autres niveaux de bruit.</p> <p>Pour cette t\u00e2che complexe, nous allons utiliser un autoencodeur convolutif.</p> In\u00a0[4]: Copied! <pre># Nous r\u00e9utilisons les fonctions introduites dans l'exemple de segmentation du cours 3 \ndef conv_relu_bn(input_channels, output_channels, kernel_size, stride, padding):\n  return nn.Sequential(\n    nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding),\n    nn.ReLU(),\n    nn.BatchNorm2d(output_channels,momentum=0.01)\n  )\n    \ndef convT_relu_bn(input_channels, output_channels, kernel_size, stride, padding):\n  return nn.Sequential(\n    nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n    nn.ReLU(),\n    nn.BatchNorm2d(output_channels,momentum=0.01)\n  )\n    \n    \nclass ae_conv(nn.Module):\n  def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n\n    self.encoder = nn.Sequential( # Sequential permet de groupe une s\u00e9rie de transformation\n      conv_relu_bn(1,8,kernel_size=3,stride=2,padding=1),\n      conv_relu_bn(8,16,kernel_size=3,stride=2,padding=1),\n      conv_relu_bn(16,32,kernel_size=3,stride=1,padding=1),\n    )\n    self.decoder = nn.Sequential(\n      convT_relu_bn(32,16,kernel_size=4,stride=2,padding=1),\n      convT_relu_bn(16,8,kernel_size=4,stride=2,padding=1),\n      nn.Conv2d(8,1,kernel_size=3,stride=1,padding=1),\n      nn.Sigmoid()\n    )\n  \n  def forward(self,x): \n    x = self.encoder(x)\n    denoise = self.decoder(x)\n    return denoise\n</pre> # Nous r\u00e9utilisons les fonctions introduites dans l'exemple de segmentation du cours 3  def conv_relu_bn(input_channels, output_channels, kernel_size, stride, padding):   return nn.Sequential(     nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding),     nn.ReLU(),     nn.BatchNorm2d(output_channels,momentum=0.01)   )      def convT_relu_bn(input_channels, output_channels, kernel_size, stride, padding):   return nn.Sequential(     nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),     nn.ReLU(),     nn.BatchNorm2d(output_channels,momentum=0.01)   )           class ae_conv(nn.Module):   def __init__(self, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)      self.encoder = nn.Sequential( # Sequential permet de groupe une s\u00e9rie de transformation       conv_relu_bn(1,8,kernel_size=3,stride=2,padding=1),       conv_relu_bn(8,16,kernel_size=3,stride=2,padding=1),       conv_relu_bn(16,32,kernel_size=3,stride=1,padding=1),     )     self.decoder = nn.Sequential(       convT_relu_bn(32,16,kernel_size=4,stride=2,padding=1),       convT_relu_bn(16,8,kernel_size=4,stride=2,padding=1),       nn.Conv2d(8,1,kernel_size=3,stride=1,padding=1),       nn.Sigmoid()     )      def forward(self,x):      x = self.encoder(x)     denoise = self.decoder(x)     return denoise In\u00a0[5]: Copied! <pre>model = ae_conv() # Couches d'entr\u00e9e de taille 2, deux couches cach\u00e9es de 16 neurones et un neurone de sortie\nprint(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters()))\n</pre> model = ae_conv() # Couches d'entr\u00e9e de taille 2, deux couches cach\u00e9es de 16 neurones et un neurone de sortie print(\"Nombre de param\u00e8tres\", sum(p.numel() for p in model.parameters())) <pre>Nombre de param\u00e8tres 16385\n</pre> In\u00a0[6]: Copied! <pre>criterion = nn.MSELoss()\nepochs=10\nlearning_rate=0.001\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n</pre> criterion = nn.MSELoss() epochs=10 learning_rate=0.001 optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate) In\u00a0[7]: Copied! <pre>for i in range(epochs):\n  loss_train=0\n  for images, _ in train_loader:\n    images=images+np.sqrt(0.01)*torch.randn(images.shape)\n    recons=model(images)\n    loss=criterion(recons,images)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_train+=loss   \n  if i % 1 == 0:\n    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n  loss_val=0    \n  for images, _ in val_loader:\n    with torch.no_grad():\n      images=images+np.sqrt(0.01)*torch.randn(images.shape)\n      recons=model(images)\n      loss=criterion(recons,images)\n      loss_val+=loss \n  if i % 1 == 0:\n    print(f\"step {i} val loss {loss_val/len(val_loader)}\")\n</pre> for i in range(epochs):   loss_train=0   for images, _ in train_loader:     images=images+np.sqrt(0.01)*torch.randn(images.shape)     recons=model(images)     loss=criterion(recons,images)     optimizer.zero_grad()     loss.backward()     optimizer.step()     loss_train+=loss      if i % 1 == 0:     print(f\"step {i} train loss {loss_train/len(train_loader)}\")   loss_val=0       for images, _ in val_loader:     with torch.no_grad():       images=images+np.sqrt(0.01)*torch.randn(images.shape)       recons=model(images)       loss=criterion(recons,images)       loss_val+=loss    if i % 1 == 0:     print(f\"step {i} val loss {loss_val/len(val_loader)}\") <pre>step 0 train loss 0.0253756046295166\nstep 0 val loss 0.010878251865506172\nstep 1 train loss 0.00976449716836214\nstep 1 val loss 0.008979358710348606\nstep 2 train loss 0.00827114749699831\nstep 2 val loss 0.007526080124080181\nstep 3 train loss 0.00706455297768116\nstep 3 val loss 0.0066648973152041435\nstep 4 train loss 0.006312613375484943\nstep 4 val loss 0.005955129396170378\nstep 5 train loss 0.00576859712600708\nstep 5 val loss 0.005603262223303318\nstep 6 train loss 0.0055686105042696\nstep 6 val loss 0.005487256217747927\nstep 7 train loss 0.0054872902110219\nstep 7 val loss 0.005444051697850227\nstep 8 train loss 0.0054359594359993935\nstep 8 val loss 0.005416598636657\nstep 9 train loss 0.005397486500442028\nstep 9 val loss 0.005359680857509375\n</pre> In\u00a0[8]: Copied! <pre>images,_=next(iter(test_loader))\nvariance=0.01\n#Isolons un \u00e9l\u00e9ment\nfig, axs = plt.subplots(2, 3, figsize=(10, 6)) \nfor i in range(2):\n  image=images[i].unsqueeze(0)\n  noisy_image=image+np.sqrt(variance)*torch.randn(image.shape)\n\n  with torch.no_grad():\n      recons=model(noisy_image)\n\n  # Image d'origine\n  axs[i][0].imshow(noisy_image[0].squeeze().cpu().numpy(), cmap='gray')\n  axs[i][0].set_title('Image d\\'origine')\n  axs[i][0].axis('off')\n\n  # Image reconstruite\n  axs[i][1].imshow(recons[0].squeeze().cpu().numpy(), cmap='gray')\n  axs[i][1].set_title('Image reconstruite')\n  axs[i][1].axis('off')\n\n  axs[i][2].imshow(image[0].squeeze().cpu().numpy(), cmap='gray')\n  axs[i][2].set_title('Image de base')\n  axs[i][2].axis('off')\nplt.show()\n</pre> images,_=next(iter(test_loader)) variance=0.01 #Isolons un \u00e9l\u00e9ment fig, axs = plt.subplots(2, 3, figsize=(10, 6))  for i in range(2):   image=images[i].unsqueeze(0)   noisy_image=image+np.sqrt(variance)*torch.randn(image.shape)    with torch.no_grad():       recons=model(noisy_image)    # Image d'origine   axs[i][0].imshow(noisy_image[0].squeeze().cpu().numpy(), cmap='gray')   axs[i][0].set_title('Image d\\'origine')   axs[i][0].axis('off')    # Image reconstruite   axs[i][1].imshow(recons[0].squeeze().cpu().numpy(), cmap='gray')   axs[i][1].set_title('Image reconstruite')   axs[i][1].axis('off')    axs[i][2].imshow(image[0].squeeze().cpu().numpy(), cmap='gray')   axs[i][2].set_title('Image de base')   axs[i][2].axis('off') plt.show() <p>Les r\u00e9sultats de notre denoising sont assez corrects bien qu'il reste encore quelques artefacts. En faisant varier le param\u00e8tre variance, vous pouvez visualiser ce que le denoising autoencodeur est capable de faire sur d'autres niveaux de bruit.</p> <p>Vous pouvez essayer d'entra\u00eener le mod\u00e8le sur des images avec un niveau de bruit al\u00e9atoire (compris entre certaines valeurs de variance) et voir si le mod\u00e8le est capable de g\u00e9n\u00e9raliser sur n'importe quel type de bruit gaussien compris dans l'intervalle.  Vous pourrez avoir besoin de complexifier le mod\u00e8le et d'ajouter des epochs lors de l'entra\u00eenement.</p> <p>U-Net : Vous pouvez \u00e9galement essayer de tester l'achitecture U-Net (voir cours 3 sur la segmentation) pour la t\u00e2che de denoising et comparer les r\u00e9sultats avec l'autoencodeur.</p>"},{"location":"04_Autoencodeurs/02_DenoisingAE.html#denoising-autoencodeur","title":"Denoising Autoencodeur\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html#intuition","title":"Intuition\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html#denoising-quest-ce-que-cest","title":"Denoising, qu'est ce que c'est ?\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html#utilisation-dun-autoencodeur-pour-la-tache-de-denoising","title":"Utilisation d'un autoencodeur pour la t\u00e2che de denoising\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html#denoising-autoencodeur-en-pytorch","title":"Denoising autoencodeur en Pytorch\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html#dataset-et-dataloader","title":"Dataset et dataloader\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html#generation-de-bruit","title":"G\u00e9neration de bruit\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html#creation-de-notre-modele","title":"Cr\u00e9ation de notre mod\u00e8le\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html#entrainement-du-modele","title":"Entra\u00eenement du mod\u00e8le\u00b6","text":""},{"location":"04_Autoencodeurs/02_DenoisingAE.html#exercice","title":"Exercice\u00b6","text":""},{"location":"05_NLP/index.html","title":"\ud83d\udde8\ufe0f NLP \ud83d\udde8\ufe0f","text":"<p>Ce cours est grandement inspir\u00e9 de la s\u00e9rie de vid\u00e9o de Andrej Karpathy \"Building makemore\" qui tra\u00eete les NLP avec une approche de pr\u00e9diction du prochain token. Le cours aborde d'abord des mod\u00e8les tr\u00e8s simples pour avoir une intuition sur le tra\u00eetement de donn\u00e9es discr\u00e8tes avec un r\u00e9seau neurones puis les mod\u00e8les se complexifient petit \u00e0 petit. </p>"},{"location":"05_NLP/index.html#notebook-1-introduction","title":"Notebook 1\ufe0f\u20e3 : Introduction","text":"<p>Ce notebook introduit le concept de natural language processing (NLP), d\u00e9crit le cours \u00e0 venir et construit un dataset que l'on utilisera dans les notebooks suivants.</p>"},{"location":"05_NLP/index.html#notebook-2-bigramme","title":"Notebook 2\ufe0f\u20e3 : Bigramme","text":"<p>Ce notebook introduit l'architecture bigramme pour la pr\u00e9diction du prochain caract\u00e8re dans un objectif de g\u00e9n\u00e9ration de pr\u00e9noms.</p>"},{"location":"05_NLP/index.html#notebook-3-reseau-fully-connected","title":"Notebook 3\ufe0f\u20e3 : R\u00e9seau Fully Connected","text":"<p>Ce notebook impl\u00e9mente l'architecture d'un article scientifique utilisant un r\u00e9seau fully connected pour la pr\u00e9dication du prochain caract\u00e8re.</p>"},{"location":"05_NLP/index.html#notebook-4-wavenet","title":"Notebook 4\ufe0f\u20e3 : WaveNet","text":"<p>Ce notebook impl\u00e9mente l'architecture d'un article scientifique utilisant un wavenet pour la pr\u00e9diction du prochain caract\u00e8re.</p>"},{"location":"05_NLP/index.html#notebook-5-rnn","title":"Notebook 5\ufe0f\u20e3 : RNN","text":"<p>Ce notebook pr\u00e9sente le concept de r\u00e9seau de neurones r\u00e9currents et propose une impl\u00e9mentation d'un r\u00e9seau de neurones r\u00e9currents pour la pr\u00e9diction du prochain caract\u00e8re \u00e0 partir des pi\u00e8ces de Moli\u00e8re.</p>"},{"location":"05_NLP/index.html#notebook-6-lstm","title":"Notebook 6\ufe0f\u20e3 : LSTM","text":"<p>Ce notebook introduit l'architecture long short-term memory (LSTM) et l'impl\u00e9mente en pytorch pour la pr\u00e9diction du prochain caract\u00e8re.</p>"},{"location":"05_NLP/01_Introduction.html","title":"Introduction NLP","text":"<p>Le NLP ou natural language processing (traitement du langage naturel) est un probl\u00e8me important dans le domaine du machine learning. Ce domaine regroupe plusieurs t\u00e2ches li\u00e9es au texte comme la traduction, la compr\u00e9hension de texte, les questions/r\u00e9ponses et bien d'autres.</p> <p></p> <p>Figure extraite du blogpost.</p> <p>C'est une t\u00e2che un peu \u00e0 part dans le domaine du deep learning car il s'agit de traiter des donn\u00e9es discr\u00e8tes qui se lisent en g\u00e9n\u00e9ral de gauche \u00e0 droite.</p> <p>Ce cours va aborder principalement l'aspect \"pr\u00e9diction du prochain token\" en prenant l'exemple de la pr\u00e9diction du prochain caract\u00e8re dans un premier temps pour plus de simplicit\u00e9. Ce probl\u00e8me est le fondement des mod\u00e8les de langages (type GPT, Llama, gemini etc ...). L'id\u00e9e est de pr\u00e9dire le mot qui va suivre en fonction des mots pr\u00e9c\u00e9dents (avec plus ou moins de contexte en fonction de la m\u00e9thode et de la puissance du mod\u00e8le). Le contexte est d\u00e9fini par le nombre de token (ou mots) qui vont \u00eatre utilis\u00e9s pour pr\u00e9dire le prochain mot.</p> <p>Un token, qu'est ce que c'est ? : Un token correspond \u00e0 un \u00e9l\u00e9ment d'entr\u00e9e du mod\u00e8le, il peut s'agir d'un caract\u00e8re, d'un groupe de caract\u00e8res ou d'un mot qui est converti en vecteur avant d'\u00eatre donn\u00e9 en entr\u00e9e au mod\u00e8le.</p> <p>Ce cours s'inspire fortement de la s\u00e9rie de vid\u00e9os de Andrej Karpathy (lien du repository github) et en particulier des cours \"building makemore\". Ce cours va consister \u00e0 apporter une version \u00e9crite et en fran\u00e7ais des enseignements de cette s\u00e9rie de vid\u00e9os. Je vous invite \u00e0 regarder cette s\u00e9rie de vid\u00e9os en plus, il s'agit d'un des meilleurs cours sur les mod\u00e8les de langages \u00e0 ce jour (et c'est gratuit).</p> <p>Nous subdiviserons ce cours en plusieurs notebooks avec des mod\u00e8les de difficult\u00e9 croissante. L'int\u00earet est de voir les limitations de chaque mod\u00e8le avant de passer \u00e0 un mod\u00e8le plus complexe. Voici un plan du cours :</p> <ul> <li>Cours 1 : Bigramme : M\u00e9thode classique et r\u00e9seau de neurones</li> <li>Cours 2 : Pr\u00e9diction du prochain mot avec R\u00e9seau Fully Connected</li> <li>Cours 3 : WaveNet : architecture hierarchique</li> <li>Cours 4 : RNN : r\u00e9seau de neurones r\u00e9currents avec architecture s\u00e9quentielle</li> <li>Cours 5 : LSTM : r\u00e9seau r\u00e9current \"am\u00e9lior\u00e9\"</li> </ul> <p>Notes : Le cours 7 sur les transformers s'attaque au m\u00eame probl\u00e8me de g\u00e9n\u00e9ration du prochain caract\u00e8re mais avec une architecture transformer et sur un dataset plus complexe.</p> <p>Dans ce cours, nous travaillons sur un dataset de pr\u00e9noms qui contient environ 30 000 pr\u00e9noms les plus courant en France depuis 1900 (donn\u00e9es de l'INSEE).  Le fichier prenoms.txt existe d\u00e9j\u00e0 dans le dossier, il n'est pas donc pas n\u00e9cessaire d'executer le code ci-dessous. Si vous souhaitez le faire il faut d'abord t\u00e9l\u00e9charger le fichier 'nat2022.csv' sur le site de l'INSEE.</p> In\u00a0[13]: Copied! <pre>import pandas as pd\n\n# Chargement du fichier CSV\ndf = pd.read_csv('nat2022.csv', sep=';')\n\n# On enl\u00e8ve la cat\u00e9gorie '_PRENOMS_RARES' qui regroupe les pr\u00e9noms peu fr\u00e9quents\ndf_filtered = df[df['preusuel'] != '_PRENOMS_RARES']\n\n# Pour compter, on fait la somme des nombres de naissances pour chaque pr\u00e9nom\ndf_grouped = df_filtered.groupby('preusuel', as_index=False)['nombre'].sum()\n\n# On va trier les pr\u00e9noms par popularit\u00e9\ndf_sorted = df_grouped.sort_values(by='nombre', ascending=False)\n\n# On extrait les 30 000 pr\u00e9noms les plus populaires\ntop_prenoms = df_sorted['preusuel'].head(30000).values\n\nwith open('prenoms.txt', 'w', encoding='utf-8') as file:\n  for prenom in top_prenoms:\n    file.write(f\"{prenom}\\n\")\n</pre> import pandas as pd  # Chargement du fichier CSV df = pd.read_csv('nat2022.csv', sep=';')  # On enl\u00e8ve la cat\u00e9gorie '_PRENOMS_RARES' qui regroupe les pr\u00e9noms peu fr\u00e9quents df_filtered = df[df['preusuel'] != '_PRENOMS_RARES']  # Pour compter, on fait la somme des nombres de naissances pour chaque pr\u00e9nom df_grouped = df_filtered.groupby('preusuel', as_index=False)['nombre'].sum()  # On va trier les pr\u00e9noms par popularit\u00e9 df_sorted = df_grouped.sort_values(by='nombre', ascending=False)  # On extrait les 30 000 pr\u00e9noms les plus populaires top_prenoms = df_sorted['preusuel'].head(30000).values  with open('prenoms.txt', 'w', encoding='utf-8') as file:   for prenom in top_prenoms:     file.write(f\"{prenom}\\n\")  <p>Dans le notebook suivant, nous commencerons par analyser le dataset (caract\u00e8res distincts etc ...).</p>"},{"location":"05_NLP/01_Introduction.html#introduction-nlp","title":"Introduction NLP\u00b6","text":""},{"location":"05_NLP/01_Introduction.html#nlp-quest-ce-que-cest","title":"NLP, Qu'est ce que c'est ?\u00b6","text":""},{"location":"05_NLP/01_Introduction.html#quest-ce-que-ce-cours-aborde","title":"Qu'est ce que ce cours aborde ?\u00b6","text":""},{"location":"05_NLP/01_Introduction.html#inspiration-pour-le-cours","title":"Inspiration pour le cours\u00b6","text":""},{"location":"05_NLP/01_Introduction.html#recuperation-du-dataset-prenomtxt","title":"R\u00e9cuperation du dataset prenom.txt\u00b6","text":""},{"location":"05_NLP/02_bigramme.html","title":"Bigramme","text":"In\u00a0[1]: Copied! <pre>words = open('prenoms.txt', 'r').read().splitlines()\nprint('Les 5 pr\u00e9noms les plus populaires : ',words[:5])\nprint('Les 5 pr\u00e9noms les moins populaires : ',words[-5:])\nprint('Le pr\u00e9nom le plus long : ',max(words, key=len))\nprint('Le pr\u00e9nom le plus court : ',min(words, key=len))\n</pre> words = open('prenoms.txt', 'r').read().splitlines() print('Les 5 pr\u00e9noms les plus populaires : ',words[:5]) print('Les 5 pr\u00e9noms les moins populaires : ',words[-5:]) print('Le pr\u00e9nom le plus long : ',max(words, key=len)) print('Le pr\u00e9nom le plus court : ',min(words, key=len))  <pre>Les 5 pr\u00e9noms les plus populaires :  ['MARIE', 'JEAN', 'PIERRE', 'MICHEL', 'ANDR\u00c9']\nLes 5 pr\u00e9noms les moins populaires :  ['\u00c9LOUEN', 'CHEYNA', 'BLONDIE', 'IMANN', 'GHILAIN']\nLe pr\u00e9nom le plus long :  GUILLAUME-ALEXANDRE\nLe pr\u00e9nom le plus court :  GUY\n</pre> In\u00a0[2]: Copied! <pre>unique_characters = set()\nfor word in words:\n  # Ajouter chaque caract\u00e8re de la ligne \u00e0 l'ensemble des caract\u00e8res uniques\n  for char in word.strip():\n    unique_characters.add(char)\nprint('Nombre de caract\u00e8res uniques : ',len(unique_characters))\nprint('Caract\u00e8res uniques : ',unique_characters)\n</pre> unique_characters = set() for word in words:   # Ajouter chaque caract\u00e8re de la ligne \u00e0 l'ensemble des caract\u00e8res uniques   for char in word.strip():     unique_characters.add(char) print('Nombre de caract\u00e8res uniques : ',len(unique_characters)) print('Caract\u00e8res uniques : ',unique_characters) <pre>Nombre de caract\u00e8res uniques :  45\nCaract\u00e8res uniques :  {'\u00cf', '\u00dc', '\u0178', 'U', '\u00d4', 'S', '\u00c6', '\u00c0', '\u00c8', '-', 'W', 'H', '\u00ca', '\u00c9', 'R', 'M', 'E', '\u00cb', 'N', '\u00ce', 'X', '\u00c4', 'F', '\u00c2', 'K', 'D', '\u00d6', 'I', 'J', 'Y', 'A', 'C', 'O', '\u00db', '\u00d9', 'B', 'Z', 'P', 'T', \"'\", 'Q', '\u00c7', 'G', 'L', 'V'}\n</pre> <p>Je rappelle que l'id\u00e9e du projet est de pr\u00e9dire le prochain caract\u00e8re \u00e0 partir des caract\u00e8res pr\u00e9c\u00e9dents. Dans le mod\u00e8le bigramme, on se base uniquement sur un seul caract\u00e8re pr\u00e9c\u00e9dent pour pr\u00e9dire le caract\u00e8re actuel. C'est la version la plus basique de ce type de mod\u00e8le.</p> <p>Bien entendu, on veut pr\u00e9dire un pr\u00e9nom \u00e0 partir de rien et pour pr\u00e9dire la premi\u00e8re lettre, on a besoin de savoir la probabilit\u00e9 que cette lettre soit la premi\u00e8re (et de m\u00eame pour la derni\u00e8re lettre). On va donc ajouter un caract\u00e8re sp\u00e9cial '.' \u00e0 au d\u00e9but et \u00e0 la fin de chaque mot avant de construire nos bigrammes.</p> <p>Dans chaque pr\u00e9nom, on a en fait plusieurs exemples de bigramme (chacun est ind\u00e9pendant). Si on consid\u00e8re le premier pr\u00e9nom, regardons le nombre de bigramme que l'on a :</p> In\u00a0[3]: Copied! <pre>chs = ['.'] + list(words[0]) + ['.']\nfor ch1, ch2 in zip(chs, chs[1:]):\n  bigram = (ch1, ch2)\n  print(bigram)\n</pre> chs = ['.'] + list(words[0]) + ['.'] for ch1, ch2 in zip(chs, chs[1:]):   bigram = (ch1, ch2)   print(bigram) <pre>('.', 'M')\n('M', 'A')\n('A', 'R')\n('R', 'I')\n('I', 'E')\n('E', '.')\n</pre> <p>Le premier pr\u00e9nom Marie contient 6 bigrammes.</p> <p>Construisons maintenant un dictionnaire python qui regroupe tous les bigrammes du dataset en comptant leurs occurences.</p> In\u00a0[4]: Copied! <pre>b = {}\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    bigram = (ch1, ch2)\n    b[bigram] = b.get(bigram, 0) + 1\nsorted(b.items(), key = lambda kv: -kv[1])\nprint('Les 5 bigrammes les plus fr\u00e9quents : ',sorted(b.items(), key = lambda kv: -kv[1])[:5])\n</pre> b = {} for w in words:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     bigram = (ch1, ch2)     b[bigram] = b.get(bigram, 0) + 1 sorted(b.items(), key = lambda kv: -kv[1]) print('Les 5 bigrammes les plus fr\u00e9quents : ',sorted(b.items(), key = lambda kv: -kv[1])[:5]) <pre>Les 5 bigrammes les plus fr\u00e9quents :  [(('A', '.'), 7537), (('E', '.'), 6840), (('A', 'N'), 6292), (('N', '.'), 3741), (('N', 'E'), 3741)]\n</pre> <p>On a donc notre dictionnaire de fr\u00e9quence des bigrammes dans l'int\u00e9gralit\u00e9 de notre dataset. Comme on peut le voir, il est fr\u00e9quent que des pr\u00e9noms se terminent par A,E ou N et que les lettres A et N se suivent ainsi que les lettres N et E.</p> <p>Il est beaucoup plus simple de visualiser et de traiter les donn\u00e9es sous forme matricielle. On va constuire une matrice de taille 46x46 (45 caract\u00e8res compt\u00e9s + le caract\u00e8re sp\u00e9cial '.') avec la colonne correspondants \u00e0 la ligne correspondant \u00e0 la premi\u00e8re lettre et la colonne correspondant \u00e0 la seconde.</p> In\u00a0[5]: Copied! <pre>import torch\nN = torch.zeros((46, 46), dtype=torch.int32)\n</pre> import torch N = torch.zeros((46, 46), dtype=torch.int32) <p>On va trier nos caract\u00e8res et cr\u00e9er des table de recherche (look-up table) avec l'objet dictionnaire de python.  On veut pouvoir passer de caract\u00e8re \u00e0 entier (pour indexer dans la matrice) et inversement (pour reconstruire le pr\u00e9noms \u00e0 partir d'entiers).</p> In\u00a0[6]: Copied! <pre>chars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\n</pre> chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} <p>On va maintenant remplir notre matrice :</p> In\u00a0[7]: Copied! <pre>for w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    N[ix1, ix2] += 1\n</pre> for w in words:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     N[ix1, ix2] += 1 <p>Et on peut maintenant afficher la matrice (look-up table).</p> In\u00a0[10]: Copied! <pre>#Code pour dessiner une jolie matrice\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(32,32))\nplt.imshow(N, cmap='Blues')\nfor i in range(46):\n  for j in range(46):\n    chstr = itos[i] + itos[j]\n    plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n    plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\nplt.axis('off');\n</pre> #Code pour dessiner une jolie matrice import matplotlib.pyplot as plt %matplotlib inline  plt.figure(figsize=(32,32)) plt.imshow(N, cmap='Blues') for i in range(46):   for j in range(46):     chstr = itos[i] + itos[j]     plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')     plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray') plt.axis('off'); <p>Pour conna\u00eetre la probabilit\u00e9 qu'un pr\u00e9nom commence par une certaine lettre, il faut regarder la ligne du caract\u00e8re '.', c'est \u00e0 dire la ligne 0 et normaliser chaque valeur par la somme des valeurs de cette ligne (pour obtenir des valeurs entre 0 et 1 qui ont une somme \u00e9gale \u00e0 1).</p> In\u00a0[15]: Copied! <pre>p = N[0].float()\np = p / p.sum()\nprint(\"Compte de la premi\u00e8re ligne : \",N[0])\nprint(\"Probabilit\u00e9s : \",p)\n</pre> p = N[0].float() p = p / p.sum() print(\"Compte de la premi\u00e8re ligne : \",N[0]) print(\"Probabilit\u00e9s : \",p) <pre>Compte de la premi\u00e8re ligne :  tensor([   0,    0,    0, 3399,  825, 1483, 1208, 1400,  864,  907, 1039,  788,\n        1352, 1503, 2108, 3606, 1501,  546,  620,   32, 1142, 2539, 1185,   72,\n         329,  294,   29,  661,  393,    0,    2,    0,    0,    1,    2,  161,\n           0,    0,    2,    2,    0,    5,    0,    0,    0,    0],\n       dtype=torch.int32)\nProbabilit\u00e9s :  tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1330e-01, 2.7500e-02, 4.9433e-02,\n        4.0267e-02, 4.6667e-02, 2.8800e-02, 3.0233e-02, 3.4633e-02, 2.6267e-02,\n        4.5067e-02, 5.0100e-02, 7.0267e-02, 1.2020e-01, 5.0033e-02, 1.8200e-02,\n        2.0667e-02, 1.0667e-03, 3.8067e-02, 8.4633e-02, 3.9500e-02, 2.4000e-03,\n        1.0967e-02, 9.8000e-03, 9.6667e-04, 2.2033e-02, 1.3100e-02, 0.0000e+00,\n        6.6667e-05, 0.0000e+00, 0.0000e+00, 3.3333e-05, 6.6667e-05, 5.3667e-03,\n        0.0000e+00, 0.0000e+00, 6.6667e-05, 6.6667e-05, 0.0000e+00, 1.6667e-04,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n</pre> <p>Pour g\u00e9n\u00e9rer des pr\u00e9noms de mani\u00e8re al\u00e9atoires, on ne veut pas prendre la lettre avec le plus de probabilit\u00e9 d'\u00eatre en premier (car on ne g\u00e9n\u00e9rerait que le m\u00eame pr\u00e9nom \u00e0 chaque fois). Ce qu'on voudrait c'est choisir une lettre en fonction de sa probabilit\u00e9. Si la lettre n a une probabilit\u00e9 de 0.1, on voudrait la choisir 10% du temps. Pour faire cela \u00e0 partir de notre vecteur de probabilit\u00e9, on va utiliser la fonction torch.multinomial de pytorch.</p> In\u00a0[16]: Copied! <pre>ix = torch.multinomial(p, num_samples=1, replacement=True).item()\nitos[ix]\n</pre> ix = torch.multinomial(p, num_samples=1, replacement=True).item() itos[ix] Out[16]: <pre>'Z'</pre> <p>A chaque fois qu'on l'appelle, on va obtenir une lettre diff\u00e9rente en fonction de sa probabilit\u00e9 d'apparition dans notre dataset de test.</p> <p>Avec tous ces \u00e9l\u00e9ments, on est maintenant pr\u00eat \u00e0 g\u00e9n\u00e9rer des pr\u00e9noms \u00e0 partir de notre matrice N. L'id\u00e9al pour \u00e9viter de renormalizer \u00e0 chaque fois serait de cr\u00e9er une matrice avec directement les probabilit\u00e9s.</p> In\u00a0[17]: Copied! <pre># On copie N et on la convertit en float\nP = N.float()\n# On normalise chaque ligne\n# On somme sur la premi\u00e8re dimension (les colonnes)\nprint(\"Somme des lignes : \",P.sum(1, keepdims=True).shape)\nP /= P.sum(1, keepdims=True) # /= est un raccourci pour P = P / P.sum(1, keepdims=True)\nprint(\"Matrice normalis\u00e9e P est de taille : \",P.shape)\n# On v\u00e9rifie que la somme d'une ligne est \u00e9gale \u00e0 1\nprint(\"Somme de la premi\u00e8re ligne de P : \",P.sum(1)[0].item())\n</pre> # On copie N et on la convertit en float P = N.float() # On normalise chaque ligne # On somme sur la premi\u00e8re dimension (les colonnes) print(\"Somme des lignes : \",P.sum(1, keepdims=True).shape) P /= P.sum(1, keepdims=True) # /= est un raccourci pour P = P / P.sum(1, keepdims=True) print(\"Matrice normalis\u00e9e P est de taille : \",P.shape) # On v\u00e9rifie que la somme d'une ligne est \u00e9gale \u00e0 1 print(\"Somme de la premi\u00e8re ligne de P : \",P.sum(1)[0].item()) <pre>Somme des lignes :  torch.Size([46, 1])\nMatrice normalis\u00e9e P est de taille :  torch.Size([46, 46])\nSomme de la premi\u00e8re ligne de P :  1.0\n</pre> <p>Point sur la division de matrice de taille diff\u00e9rente : Comme vous l'avez remarqu\u00e9, on divise la matrice de taille (46,46) par une matrice de taille (46,1) ce qui semble \u00eatre une op\u00e9ration impossible. Avec pytorch, il y a ce qu'on appelle des broadcasting rules. Je vous invite tr\u00e8s fortement \u00e0 vous familiariser \u00e0 ce qu'il y \u00e9crit sur ce lien, c'est une source d'erreur fr\u00e9quente. Pour comprendre en d\u00e9tails les broadcasting rules, vous pouvez regarder le cours bonus. En pratique, diviser la matrice de taille (46,46) par la matrice de taille (46,1) va \"broadcaster\" la matrice (46,1) en (46,46) en copiant 46 fois la matrice de base. Cela va permettre de r\u00e9aliser l'op\u00e9ration comme on le souhaite.</p> <p>Il est enfin temps de g\u00e9n\u00e9rer des pr\u00e9noms avec notre m\u00e9thode bigramme !!! Nous allons d\u00e9finir une fonction de g\u00e9n\u00e9ration de pr\u00e9noms :</p> In\u00a0[18]: Copied! <pre>def genName():\n  out = []\n  ix = 0 # On commence par '.'\n  while True: # Tant qu'on n'a pas g\u00e9n\u00e9r\u00e9 le caract\u00e8re '.'\n    p = P[ix] # On r\u00e9cup\u00e8re la distribution de probabilit\u00e9 de la ligne correspondant au caract\u00e8re actuel\n    ix = torch.multinomial(p, num_samples=1, replacement=True).item() # On tire un \u00e9chantillon\n    out.append(itos[ix]) # On ajoute le caract\u00e8re \u00e0 notre pr\u00e9nom\n    if ix == 0:\n      break\n  return ''.join(out)\ngenName()\n</pre> def genName():   out = []   ix = 0 # On commence par '.'   while True: # Tant qu'on n'a pas g\u00e9n\u00e9r\u00e9 le caract\u00e8re '.'     p = P[ix] # On r\u00e9cup\u00e8re la distribution de probabilit\u00e9 de la ligne correspondant au caract\u00e8re actuel     ix = torch.multinomial(p, num_samples=1, replacement=True).item() # On tire un \u00e9chantillon     out.append(itos[ix]) # On ajoute le caract\u00e8re \u00e0 notre pr\u00e9nom     if ix == 0:       break   return ''.join(out) genName() Out[18]: <pre>'MARAUSUR.'</pre> <p>On peut par exemple g\u00e9n\u00e9rer 10 pr\u00e9noms al\u00e9atoires :</p> In\u00a0[19]: Copied! <pre>for i in range(10):\n  print(genName())\n</pre> for i in range(10):   print(genName()) <pre>DA.\nTYEYSE-SSCL.\nDE.\nANINEDANDVI.\nSOKE.\nRENNA.\nFUXA.\nEROA.\nFA.\nKALEN.\n</pre> <p>Comme vous pouvez le constater, la g\u00e9n\u00e9ration est assez catastrophique... Comment \u00e7a se fait ? Et bien, c'est simplement parce que le bigramme est une m\u00e9thode tr\u00e8s limit\u00e9e. Le fait de se baser uniquement sur le derniere caract\u00e8re ne permet pas d'avoir une connaissance assez pouss\u00e9e pour permettre la g\u00e9n\u00e9ration de pr\u00e9noms corrects.</p> <p>On voudrait maintenant \u00e9valuer notre mod\u00e8le sur l'ensemble d'entra\u00eenement. Pour \u00e7a on va utiliser le maximum de vraisemblance comme dans le second notebook du cours 1. Le maximum de vraisemblance ou likelihood est une mesure correspondant au produit des probabilit\u00e9s des \u00e9venements. Pour avoir un bon mod\u00e8le, on cherche \u00e0 maximiser le likelihood.</p> In\u00a0[20]: Copied! <pre>productOfProbs = 1\nfor w in words[:2]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    prob = P[ix1, ix2]\n    productOfProbs *= prob\n    print(f\"La probabilit\u00e9 de {ch1}-&gt;{ch2} est {prob.item():.3f}\")\nprint(\"Le produit des probabilit\u00e9s est : \",productOfProbs.item())\n</pre> productOfProbs = 1 for w in words[:2]:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     prob = P[ix1, ix2]     productOfProbs *= prob     print(f\"La probabilit\u00e9 de {ch1}-&gt;{ch2} est {prob.item():.3f}\") print(\"Le produit des probabilit\u00e9s est : \",productOfProbs.item()) <pre>La probabilit\u00e9 de .-&gt;M est 0.120\nLa probabilit\u00e9 de M-&gt;A est 0.431\nLa probabilit\u00e9 de A-&gt;R est 0.084\nLa probabilit\u00e9 de R-&gt;I est 0.256\nLa probabilit\u00e9 de I-&gt;E est 0.119\nLa probabilit\u00e9 de E-&gt;. est 0.321\nLa probabilit\u00e9 de .-&gt;J est 0.045\nLa probabilit\u00e9 de J-&gt;E est 0.232\nLa probabilit\u00e9 de E-&gt;A est 0.024\nLa probabilit\u00e9 de A-&gt;N est 0.201\nLa probabilit\u00e9 de N-&gt;. est 0.212\nLe produit des probabilit\u00e9s est :  4.520583629652464e-10\n</pre> <p>On voit rapidement que multiplier les probabilit\u00e9s va \u00eatre un probl\u00e8me, ici on les multiplie sur 2 des 30 000 \u00e9l\u00e9ments du dataset et on obtient une valeur tr\u00e8s faible. Si on les multiplie sur l'ensemble du dataset, \u00e7a sera une valeur non representable par un ordinateur.</p> <p>Pour r\u00e9soudre ce probl\u00e8me de pr\u00e9cision, on va utiliser le logarithme et ce pour plusieurs raisons:</p> <ul> <li><p>La fonction log est monotone, c'est-\u00e0-dire que : Si $a&gt;b$ alors $log(a)&gt;log(b)$, le fait de maximiser le log likelihood est \u00e9quivalent \u00e0 maximiser le likelihood dans un contexte d'optimisation.</p> </li> <li><p>Une propri\u00e9t\u00e9 int\u00e9ressante des logs (qui font que c'est fonction est tr\u00e8s souvent utilis\u00e9e en optimisation et en probabilit\u00e9) est la r\u00e8gle suivante : $log(a \\times b) = log(a) + log(b)$, cela nous permet d'\u00e9viter de multiplier des petites valeurs qui pourraient nous faire sortir de la pr\u00e9cision d'un ordinateur.</p> </li> </ul> <p>On peut donc maximiser le log-likelihood plut\u00f4t que de maximiser le likelihood. Reprenons la boucle pr\u00e9c\u00e9dente et regardons ce que \u00e7a donne :</p> In\u00a0[21]: Copied! <pre>sumOfLogs = 0\nfor w in words[:2]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    prob = P[ix1, ix2]\n    sumOfLogs += torch.log(prob)\nprint(\"La somme des log est : \",sumOfLogs.item())\n</pre> sumOfLogs = 0 for w in words[:2]:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     prob = P[ix1, ix2]     sumOfLogs += torch.log(prob) print(\"La somme des log est : \",sumOfLogs.item()) <pre>La somme des log est :  -21.517210006713867\n</pre> <p>On obtient une valeur beaucoup plus raisonnable. Pour les probl\u00e8mes d'optimisation, on aime souvent avoir une fonction \u00e0 minimiser. Dans le cas d'un mod\u00e8le parfait, chaque probabilit\u00e9 vaut 1 donc chaque log vaut 0 et la somme des logs va donc valoir 0 dans le meilleur cas. Sinon \u00e7a sera forc\u00e9ment des valeurs n\u00e9gatives car une probabilit\u00e9 est forc\u00e9ment inf\u00e9rieur \u00e0 1 et $log(a)&lt;0 \\text{ si } a&lt;1$. Pour avoir un probl\u00e8me de minimisation, on va donc utiliser le negative log-likelihood qui correspond simplement \u00e0 l'oppos\u00e9 du log-likelihood.</p> <p>Souvent, on va \u00e9galement prendre la moyenne plut\u00f4t que la somme car c'est plus lisible et \u00e9quivalent en terme d'optimisation. Et nous allons le calculer sur l'ensemble des pr\u00e9noms du dataset.</p> In\u00a0[22]: Copied! <pre>sumOfLogs = 0\nn=0\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    prob = P[ix1, ix2]\n    sumOfLogs += - torch.log(prob)\n    n+=1\nprint(\"La somme des negative log est : \",sumOfLogs.item())\nprint(\"La moyenne des negative log est : \",sumOfLogs.item()/n)\n</pre> sumOfLogs = 0 n=0 for w in words:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     prob = P[ix1, ix2]     sumOfLogs += - torch.log(prob)     n+=1 print(\"La somme des negative log est : \",sumOfLogs.item()) print(\"La moyenne des negative log est : \",sumOfLogs.item()/n) <pre>La somme des negative log est :  564925.125\nLa moyenne des negative log est :  2.4960792002651053\n</pre> <p>Le negative log likelihood du dataset est donc de 2.49.</p> <p>Vous pouvez \u00e9galement voir si votre pr\u00e9nom est commun ou peu commun par rapport \u00e0 la moyenne du dataset. Pour cela, il suffit de remplacer mon pr\u00e9nom \"SIMON\" par votre pr\u00e9nom (en majuscule).</p> In\u00a0[23]: Copied! <pre>sumOfLogs = 0\nn=0\nfor w in \"SIMON\":\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    prob = P[ix1, ix2]\n    sumOfLogs += - torch.log(prob)\n    n+=1\nprint(\"La moyenne des negative log est : \",sumOfLogs.item()/n)\n</pre> sumOfLogs = 0 n=0 for w in \"SIMON\":   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     prob = P[ix1, ix2]     sumOfLogs += - torch.log(prob)     n+=1 print(\"La moyenne des negative log est : \",sumOfLogs.item()/n) <pre>La moyenne des negative log est :  2.598056602478027\n</pre> <p>Si la valeur negative log likelihood correspondant \u00e0 votre pr\u00e9nom est inf\u00e9rieure \u00e0 celui du dataset, votre pr\u00e9nom est assez commun sinon il est plut\u00f4t peu commun.</p> <p>Nous allons maintenant essayer de r\u00e9soudre le m\u00eame probl\u00e8me d'une mani\u00e8re diff\u00e9rente. Nous avons r\u00e9solu ce probl\u00e8me en comptant simplement les occurences des bigrammes et en calculant la probabilit\u00e9 par rapport \u00e0 \u00e7a. C'est une m\u00e9thode qui fonctionne pour des bigrammes mais qui ne fonctionnera pas pour des choses plus complexes comme des N-grammes.</p> <p>En effet, notre table de recherche est de taille 46x46 pour deux caract\u00e8res. Si on consid\u00e8re N caract\u00e8res (donc N-1 caract\u00e8res pour pr\u00e9dire le Ni\u00e8me), on a tout de suite beaucoup plus de possibilit\u00e9s. On peut calculer simplement que la table sera de taille $46^N$. Pour N=4 \u00e7a ferait une table de taille 4477456. Autant dire que pour des valeurs de contexte importantes (les mod\u00e8les d'aujourd'hui ont un contexte de dizaines de milliers de tokens et il y a plus de 46 possibilit\u00e9s \u00e0 chaque fois), c'est une approche qui ne fonctionnera pas du tout.</p> <p>C'est pour cela que l'approche par r\u00e9seau de neurones est tr\u00e8s int\u00e9ressante. Dans la suite de cours, nous allons montrer comment r\u00e9soudre ce m\u00eame probl\u00e8me \u00e0 l'aide d'un r\u00e9seau de neurones ce qui vous donnera une intuition sur les capacit\u00e9s du r\u00e9seau lorsque le contexte augmente.</p> <p>Notre r\u00e9seau de neurones va recevoir un caract\u00e8re en entr\u00e9e et va devoir pr\u00e9dire le caract\u00e8re suivant. Comme fonction de loss, on pourra utiliser la fonction negative log likelihood pour essayer de se rapprocher de la valeur du bigramme par \"comptage\".</p> <p>Commen\u00e7ons par cr\u00e9er notre dataset d'entra\u00eenement. On reprend la boucle de parcours des bigrammes de la partie pr\u00e9c\u00e9dente et cette fois on indexe deux listes xs pour les entr\u00e9es et ys pour les labels.</p> In\u00a0[18]: Copied! <pre># create the training set of bigrams (x,y)\nxs, ys = [], []\n\nfor w in words[:1]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    print(ch1, ch2)\n    xs.append(ix1)\n    ys.append(ix2)\n    \nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n</pre> # create the training set of bigrams (x,y) xs, ys = [], []  for w in words[:1]:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     print(ch1, ch2)     xs.append(ix1)     ys.append(ix2)      xs = torch.tensor(xs) ys = torch.tensor(ys) <pre>. M\nM A\nA R\nR I\nI E\nE .\n</pre> In\u00a0[19]: Copied! <pre>print(\"valeurs d'entr\u00e9e : \",xs)\nprint(\"valeurs de sortie : \",ys)\n</pre> print(\"valeurs d'entr\u00e9e : \",xs) print(\"valeurs de sortie : \",ys) <pre>valeurs d'entr\u00e9e :  tensor([ 0, 15,  3, 20, 11,  7])\nvaleurs de sortie :  tensor([15,  3, 20, 11,  7,  0])\n</pre> <p>Pour la valeur d'entr\u00e9e 0 qui correspond \u00e0 '.', on veut pr\u00e9dire un label 15 qui correspond \u00e0 'M'.</p> <p>Le probl\u00e8me de ces listes, c'est que ce sont des entiers et il n'est pas possible de donner un entier en entr\u00e9e d'un r\u00e9seau de neurones. Dans le domaine du NLP, on utilise souvent le one hot encoding qui consiste \u00e0 convertir un index en un vecteur de 0 avec un 1 \u00e0 la position de l'index. La taille du vecteur correspond au nombre de classe possibles donc ici 46.</p> In\u00a0[20]: Copied! <pre>import torch.nn.functional as F\n# one-hot encoding\nxenc = F.one_hot(xs, num_classes=46).float() # conversion en float pour le NN\nprint(\"Encodage one-hot des deux premiers caract\u00e8res: \",xenc[:2])\n</pre> import torch.nn.functional as F # one-hot encoding xenc = F.one_hot(xs, num_classes=46).float() # conversion en float pour le NN print(\"Encodage one-hot des deux premiers caract\u00e8res: \",xenc[:2]) <pre>Encodage one-hot des deux premiers caract\u00e8res:  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n</pre> <p>Comme vous le voyez, on a un 1 \u00e0 la position 0 du premier vecteur et un 1 \u00e0 la position 15 du second. Ce sont ces vecteurs qui serviront d'entr\u00e9e \u00e0 notre r\u00e9seau de neurones. On peut visualiser \u00e0 quoi ressemble ces vecteurs pour avoir une intuition plus pouss\u00e9e de ce que le one hot encoding fait.</p> In\u00a0[21]: Copied! <pre># Les 5 premiers vecteurs one-hot\nplt.imshow(xenc)\n</pre> # Les 5 premiers vecteurs one-hot plt.imshow(xenc) Out[21]: <pre>&lt;matplotlib.image.AxesImage at 0x784579d81f10&gt;</pre> <p>Nous allons maintenant cr\u00e9er notre r\u00e9seau de neurones. Il va s'agir d'un r\u00e9seau de neurones extremement simple contenant une seule couche. Pour la taille de la couche, nous prenons en entr\u00e9e un vecteur de taille $n \\times 46$ il faudra donc une premi\u00e8re dimension de taille 46 et en sortie on veut une distribution de probabilit\u00e9 sur l'ensemble des caract\u00e8res. Notre couche de r\u00e9seau sera donc de taille $46 \\times 46$</p> <p>Commen\u00e7ons par initialiser notre couche avec des valeurs al\u00e9atoires :</p> In\u00a0[22]: Copied! <pre># On met le param\u00e8tre requires_grad \u00e0 True pour pouvoir optimiser la matrice par descente de gradient\nW = torch.randn((46, 46), requires_grad=True) \n</pre> # On met le param\u00e8tre requires_grad \u00e0 True pour pouvoir optimiser la matrice par descente de gradient W = torch.randn((46, 46), requires_grad=True)  <p>Le forward de notre r\u00e9seau de neurones va simplement consister en une multiplication matricielle entre l'entr\u00e9e et la couche. On va ensuite appliquer la fonction softmax (voir cours sur les CNN) pour obtenir une distribution de probabilit\u00e9s.</p> In\u00a0[23]: Copied! <pre># One hot encoding sur les entr\u00e9es\nxenc = F.one_hot(xs, num_classes=46).float() \n# Multiplication matricielle (forward pass)\nlogits = xenc @ W  # @ est la multiplication matricielle\n#Softmax pour obtenir des probabilit\u00e9s\ncounts = logits.exp() \nprobs = counts / counts.sum(1, keepdims=True) \nprint(probs.shape)\n</pre> # One hot encoding sur les entr\u00e9es xenc = F.one_hot(xs, num_classes=46).float()  # Multiplication matricielle (forward pass) logits = xenc @ W  # @ est la multiplication matricielle #Softmax pour obtenir des probabilit\u00e9s counts = logits.exp()  probs = counts / counts.sum(1, keepdims=True)  print(probs.shape)  <pre>torch.Size([6, 46])\n</pre> <p>On obtient une distribution de probabilit\u00e9s pour chacun de nos 6 caract\u00e8res. On va visualiser les sorties de notre r\u00e9seau de neurones non-entrain\u00e9 et calculer le negative log likelihood pour voir o\u00f9 l'on se situe par rapport \u00e0 notre mod\u00e8le obtenu par \"comptage\".</p> In\u00a0[24]: Copied! <pre>nlls = torch.zeros(6)\nfor i in range(6):\n  x = xs[i].item() # index de l'entr\u00e9e\n  y = ys[i].item() # index du label\n  print('--------')\n  print(f'bigramme actuel {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n  print('entr\u00e9e du r\u00e9seau de neurones :', x)\n  print('sortie du r\u00e9seau (probabilit\u00e9) :', probs[i])\n  print('vrai label :', y)\n  p = probs[i, y]\n  print('probabilit\u00e9 donn\u00e9 par le r\u00e9seau sur le caract\u00e8re r\u00e9el :', p.item())\n  logp = torch.log(p)\n  nll = -logp\n  print('negative log likelihood:', nll.item())\n  nlls[i] = nll\n\nprint('=========')\nprint('negative log likelihood moyen, i.e. loss =', nlls.mean().item())\n</pre> nlls = torch.zeros(6) for i in range(6):   x = xs[i].item() # index de l'entr\u00e9e   y = ys[i].item() # index du label   print('--------')   print(f'bigramme actuel {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')   print('entr\u00e9e du r\u00e9seau de neurones :', x)   print('sortie du r\u00e9seau (probabilit\u00e9) :', probs[i])   print('vrai label :', y)   p = probs[i, y]   print('probabilit\u00e9 donn\u00e9 par le r\u00e9seau sur le caract\u00e8re r\u00e9el :', p.item())   logp = torch.log(p)   nll = -logp   print('negative log likelihood:', nll.item())   nlls[i] = nll  print('=========') print('negative log likelihood moyen, i.e. loss =', nlls.mean().item()) <pre>--------\nbigramme actuel 1: .M (indexes 0,15)\nentr\u00e9e du r\u00e9seau de neurones : 0\nsortie du r\u00e9seau (probabilit\u00e9) : tensor([0.0146, 0.0210, 0.0823, 0.0077, 0.0160, 0.0483, 0.0943, 0.0204, 0.0079,\n        0.0112, 0.0085, 0.0179, 0.0188, 0.0292, 0.0022, 0.0092, 0.0200, 0.0094,\n        0.0097, 0.0191, 0.1091, 0.0122, 0.0092, 0.0287, 0.0120, 0.0088, 0.0053,\n        0.0217, 0.0177, 0.0050, 0.0038, 0.0483, 0.0320, 0.0441, 0.0105, 0.0126,\n        0.0266, 0.0092, 0.0262, 0.0081, 0.0430, 0.0012, 0.0102, 0.0025, 0.0126,\n        0.0116], grad_fn=&lt;SelectBackward0&gt;)\nvrai label : 15\nprobabilit\u00e9 donn\u00e9 par le r\u00e9seau sur le caract\u00e8re r\u00e9el : 0.009214116260409355\nnegative log likelihood: 4.687018394470215\n--------\nbigramme actuel 2: MA (indexes 15,3)\nentr\u00e9e du r\u00e9seau de neurones : 15\nsortie du r\u00e9seau (probabilit\u00e9) : tensor([0.0574, 0.1353, 0.0227, 0.0032, 0.1142, 0.0148, 0.1007, 0.0162, 0.0242,\n        0.0089, 0.0040, 0.0459, 0.0023, 0.0081, 0.0064, 0.0124, 0.0083, 0.0112,\n        0.0172, 0.0062, 0.0033, 0.0045, 0.0131, 0.0144, 0.0218, 0.0080, 0.0225,\n        0.0097, 0.0164, 0.0074, 0.0165, 0.0091, 0.0412, 0.0087, 0.0100, 0.0039,\n        0.0080, 0.0036, 0.0377, 0.0150, 0.0345, 0.0048, 0.0253, 0.0036, 0.0164,\n        0.0210], grad_fn=&lt;SelectBackward0&gt;)\nvrai label : 3\nprobabilit\u00e9 donn\u00e9 par le r\u00e9seau sur le caract\u00e8re r\u00e9el : 0.0031920599285513163\nnegative log likelihood: 5.74708890914917\n--------\nbigramme actuel 3: AR (indexes 3,20)\nentr\u00e9e du r\u00e9seau de neurones : 3\nsortie du r\u00e9seau (probabilit\u00e9) : tensor([0.0199, 0.0169, 0.0239, 0.0122, 0.0174, 0.0203, 0.0043, 0.0822, 0.0517,\n        0.0228, 0.0118, 0.0121, 0.0210, 0.0088, 0.0063, 0.0128, 0.1041, 0.0100,\n        0.0338, 0.0772, 0.0056, 0.0565, 0.0134, 0.0032, 0.0253, 0.0120, 0.0337,\n        0.0080, 0.0083, 0.0060, 0.0068, 0.0020, 0.0405, 0.0120, 0.0366, 0.0080,\n        0.0111, 0.0135, 0.0164, 0.0038, 0.0133, 0.0029, 0.0094, 0.0047, 0.0504,\n        0.0271], grad_fn=&lt;SelectBackward0&gt;)\nvrai label : 20\nprobabilit\u00e9 donn\u00e9 par le r\u00e9seau sur le caract\u00e8re r\u00e9el : 0.005596297327429056\nnegative log likelihood: 5.185649871826172\n--------\nbigramme actuel 4: RI (indexes 20,11)\nentr\u00e9e du r\u00e9seau de neurones : 20\nsortie du r\u00e9seau (probabilit\u00e9) : tensor([0.0030, 0.0300, 0.0056, 0.0311, 0.0361, 0.0294, 0.0462, 0.0163, 0.0369,\n        0.0178, 0.0251, 0.0125, 0.0162, 0.0019, 0.0828, 0.0173, 0.0068, 0.0113,\n        0.0204, 0.0124, 0.0653, 0.0059, 0.0038, 0.0075, 0.0165, 0.0332, 0.0065,\n        0.0354, 0.0169, 0.0062, 0.0683, 0.0203, 0.0189, 0.0179, 0.0113, 0.0119,\n        0.0549, 0.0035, 0.0051, 0.0061, 0.0569, 0.0268, 0.0164, 0.0021, 0.0146,\n        0.0088], grad_fn=&lt;SelectBackward0&gt;)\nvrai label : 11\nprobabilit\u00e9 donn\u00e9 par le r\u00e9seau sur le caract\u00e8re r\u00e9el : 0.012452212162315845\nnegative log likelihood: 4.385857105255127\n--------\nbigramme actuel 5: IE (indexes 11,7)\nentr\u00e9e du r\u00e9seau de neurones : 11\nsortie du r\u00e9seau (probabilit\u00e9) : tensor([0.0265, 0.0211, 0.0312, 0.0235, 0.0020, 0.0151, 0.0145, 0.0083, 0.0141,\n        0.0062, 0.0168, 0.0183, 0.0600, 0.0047, 0.0969, 0.0438, 0.0083, 0.0584,\n        0.0572, 0.0061, 0.0159, 0.0475, 0.0079, 0.0116, 0.0331, 0.0043, 0.0049,\n        0.0134, 0.0057, 0.0077, 0.0350, 0.0276, 0.0174, 0.0050, 0.0176, 0.0022,\n        0.0169, 0.0029, 0.0281, 0.0115, 0.0291, 0.0250, 0.0071, 0.0126, 0.0277,\n        0.0491], grad_fn=&lt;SelectBackward0&gt;)\nvrai label : 7\nprobabilit\u00e9 donn\u00e9 par le r\u00e9seau sur le caract\u00e8re r\u00e9el : 0.008320074528455734\nnegative log likelihood: 4.789083957672119\n--------\nbigramme actuel 6: E. (indexes 7,0)\nentr\u00e9e du r\u00e9seau de neurones : 7\nsortie du r\u00e9seau (probabilit\u00e9) : tensor([0.0397, 0.0266, 0.0185, 0.0024, 0.0054, 0.0061, 0.0143, 0.0269, 0.0398,\n        0.0084, 0.0134, 0.0247, 0.1220, 0.0039, 0.0062, 0.0829, 0.0452, 0.0086,\n        0.0062, 0.0130, 0.0106, 0.0137, 0.0073, 0.1132, 0.0146, 0.0252, 0.0112,\n        0.0955, 0.0133, 0.0196, 0.0091, 0.0122, 0.0160, 0.0092, 0.0128, 0.0337,\n        0.0058, 0.0112, 0.0070, 0.0029, 0.0033, 0.0073, 0.0052, 0.0049, 0.0125,\n        0.0087], grad_fn=&lt;SelectBackward0&gt;)\nvrai label : 0\nprobabilit\u00e9 donn\u00e9 par le r\u00e9seau sur le caract\u00e8re r\u00e9el : 0.0397193469107151\nnegative log likelihood: 3.225916862487793\n=========\nnegative log likelihood moyen, i.e. loss = 4.670102596282959\n</pre> <p>Pour le calcul du loss, on va calculer le negative log likelihood de la sortie de notre r\u00e9seau par rapport au label de la mani\u00e8re suivante :</p> In\u00a0[25]: Copied! <pre># Calcul de la loss\nloss = -probs[torch.arange(6), ys].log().mean()\nprint(loss.item())\n# On remet les gradients \u00e0 z\u00e9ro (None est plus efficace)\nW.grad = None \n# Calcul des gradients automatique de pytorch\nloss.backward()\nprint(W.grad)\n</pre> # Calcul de la loss loss = -probs[torch.arange(6), ys].log().mean() print(loss.item()) # On remet les gradients \u00e0 z\u00e9ro (None est plus efficace) W.grad = None  # Calcul des gradients automatique de pytorch loss.backward() print(W.grad) <pre>4.670102596282959\ntensor([[0.0024, 0.0035, 0.0137,  ..., 0.0004, 0.0021, 0.0019],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n</pre> <p>Comme vous le voyez, on a calcul\u00e9 les gradients de notre matrice W par rapport au loss. De la m\u00eame mani\u00e8re que dans les cours pr\u00e9c\u00e9dents, on peut mettre \u00e0 jour les poids du mod\u00e8le dans le sens du gradient avec un pas (le learning_rate).</p> In\u00a0[26]: Copied! <pre># avec un learning_rate de 0.1\nW.data += -0.1 * W.grad\n</pre> # avec un learning_rate de 0.1 W.data += -0.1 * W.grad <p>A partir de tout ce que nous venons de voir, nous pouvons maintenant rassembler les morceaux et optimiser notre mod\u00e8le.</p> <p>Cr\u00e9ation du dataset complet On va commencer par cr\u00e9er notre dataset complet en reprenant la boucle pr\u00e9c\u00e9dente mais en parcourant l'ensemble des pr\u00e9noms.</p> In\u00a0[27]: Copied! <pre>xs, ys = [], []\nfor w in words:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    xs.append(ix1)\n    ys.append(ix2)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint('number of examples: ', num)\n</pre> xs, ys = [], [] for w in words:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     xs.append(ix1)     ys.append(ix2) xs = torch.tensor(xs) ys = torch.tensor(ys) num = xs.nelement() print('number of examples: ', num)  <pre>number of examples:  226325\n</pre> <p>Initialisation du mod\u00e8le On peut maintenant initialiser notre mod\u00e8le comme pr\u00e9c\u00e9dement, choisir le learning_rate le nombre d'it\u00e9rations.</p> In\u00a0[28]: Copied! <pre>W = torch.randn((46, 46), requires_grad=True)\nlr=50 # en pratique, dans ce petit probl\u00e8me, un learning rate de 50 fonctionne bien ce qui peut sembler \u00e9tonnant\niterations=100\n</pre> W = torch.randn((46, 46), requires_grad=True) lr=50 # en pratique, dans ce petit probl\u00e8me, un learning rate de 50 fonctionne bien ce qui peut sembler \u00e9tonnant iterations=100 <p>Descente du gradient Appliquons maintenant l'algorithme de descente du gradient sur notre mod\u00e8le.</p> In\u00a0[29]: Copied! <pre># Descente du gradient\nfor k in range(iterations):\n  \n  # forward pass\n  xenc = F.one_hot(xs, num_classes=46).float() # transformation one hot sur les entr\u00e9es\n  logits = xenc @ W\n  probs=F.softmax(logits,dim=1) # On applique le softmax\n  loss = -probs[torch.arange(num), ys].log().mean() # Calcul du negative log likelihood (loss)\n  if k%10==0:\n    print('loss iteration '+str(k)+' : ',loss.item())\n  \n  # retropropagation\n  W.grad = None # Remettre la gradient \u00e0 z\u00e9ro \u00e0 chaque it\u00e9ration (\u00e0 ne pas oublier !!!!)\n  loss.backward()\n  \n  # Mise \u00e0 jour des poids\n  W.data += -50 * W.grad\n</pre> # Descente du gradient for k in range(iterations):      # forward pass   xenc = F.one_hot(xs, num_classes=46).float() # transformation one hot sur les entr\u00e9es   logits = xenc @ W   probs=F.softmax(logits,dim=1) # On applique le softmax   loss = -probs[torch.arange(num), ys].log().mean() # Calcul du negative log likelihood (loss)   if k%10==0:     print('loss iteration '+str(k)+' : ',loss.item())      # retropropagation   W.grad = None # Remettre la gradient \u00e0 z\u00e9ro \u00e0 chaque it\u00e9ration (\u00e0 ne pas oublier !!!!)   loss.backward()      # Mise \u00e0 jour des poids   W.data += -50 * W.grad <pre>loss iteration 0 :  4.346113204956055\nloss iteration 10 :  2.94492769241333\nloss iteration 20 :  2.7590363025665283\nloss iteration 30 :  2.6798315048217773\nloss iteration 40 :  2.637108087539673\nloss iteration 50 :  2.610524892807007\nloss iteration 60 :  2.5923469066619873\nloss iteration 70 :  2.5791807174682617\nloss iteration 80 :  2.569261074066162\nloss iteration 90 :  2.561541795730591\n</pre> <p>Apr\u00e8s 100 it\u00e9rations, on obtient un negative log likelihood proche de celui du mod\u00e8le par \"comptage\". C'est en fait la capacit\u00e9 maximum du mod\u00e8le bigramme sur les donn\u00e9es d'entra\u00eenement.</p> <p>Generation de pr\u00e9noms avec notre mod\u00e8le On peut maintenant g\u00e9n\u00e9rer des pr\u00e9noms avec notre mod\u00e8le.</p> In\u00a0[30]: Copied! <pre>for i in range(5):\n  \n  out = []\n  ix = 0\n  while True:\n    xenc = F.one_hot(torch.tensor([ix]), num_classes=46).float()\n    logits = xenc @ W \n    # Pr\u00e9diction des probabilit\u00e9s de la lettre suivante\n    p=F.softmax(logits,dim=1)\n    \n    # On fait un tirage al\u00e9atoire de la prochaine lettre en suivante la distribution p \n    ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n    # Conversion en lettre\n    out.append(itos[ix])\n    if ix == 0:\n      break\n  print(''.join(out))\n</pre> for i in range(5):      out = []   ix = 0   while True:     xenc = F.one_hot(torch.tensor([ix]), num_classes=46).float()     logits = xenc @ W      # Pr\u00e9diction des probabilit\u00e9s de la lettre suivante     p=F.softmax(logits,dim=1)          # On fait un tirage al\u00e9atoire de la prochaine lettre en suivante la distribution p      ix = torch.multinomial(p, num_samples=1, replacement=True).item()     # Conversion en lettre     out.append(itos[ix])     if ix == 0:       break   print(''.join(out)) <pre>JE.\nS.\nADJULA.\nM.\nLVERTY\u00dcCI.\n</pre> <p>La matrice de poids $W$ a la m\u00eame taille que la matrice $N$ utilis\u00e9e dans la m\u00e9thode par comptage. Ce que nous venons de r\u00e9aliser avec l'approche par r\u00e9seau de neurones est en fait l'apprentissage de la matrice $N$. On peut confirmer l'intuition en regardant ce qu'il se passe lorsqu'on fait l'op\u00e9ration xenc @ W. Il s'agit d'une multiplication matricielle d'une matrice ligne de taille $1 \\times 46$ par une matrice carr\u00e9 de taille $46 \\times 46$, de plus la matrice ligne contient uniquement des zeros except\u00e9 un 1 \u00e0 l'index $i$ \u00e0 la lettre. Cette multiplication matricielle donne en r\u00e9sultat la ligne $i$ de la matrice $W$. Cela correspond exactement \u00e0 ce qu'on faisait dans la m\u00e9thode par comptage o\u00f9 l'on r\u00e9cuperait les probabilit\u00e9s de la ligne $i$ de $P$.</p>"},{"location":"05_NLP/02_bigramme.html#bigramme","title":"Bigramme\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#analyse-du-dataset","title":"Analyse du dataset\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#bigramme-quest-ce-que-cest","title":"Bigramme, qu'est ce que c'est ?\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#methode-par-comptage","title":"M\u00e9thode par comptage\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#matrice-doccurence","title":"Matrice d'occurence\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#probabilites","title":"Probabilit\u00e9s\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#generation","title":"G\u00e9n\u00e9ration\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#evaluation-du-modele","title":"Evaluation du mod\u00e8le\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#maximum-de-vraisemblance-ou-likelihood","title":"Maximum de vraisemblance ou likelihood\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#log-likelihood","title":"log-likelihood\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#approche-par-reseau-de-neurones","title":"Approche par r\u00e9seau de neurones\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#probleme-de-lapproche-comptage","title":"Probl\u00e8me de l'approche \"comptage\"\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#dataset-de-notre-reseau-de-neurones","title":"Dataset de notre r\u00e9seau de neurones\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#notre-reseau-de-neurones","title":"Notre r\u00e9seau de neurones\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#optimization","title":"Optimization\u00b6","text":""},{"location":"05_NLP/02_bigramme.html#notes-supplementaires","title":"Notes suppl\u00e9mentaires\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html","title":"R\u00e9seau fully connected","text":"<p>Dans les cours pr\u00e9c\u00e9dents (cours 2), nous avons construits des r\u00e9seaux de neurones fully connected pour des probl\u00e8mes de classification. Ici, nous avons \u00e0 faire \u00e0 un probl\u00e8me de pr\u00e9diction et nos donn\u00e9es sont discr\u00e8tes.</p> <p>Le r\u00e9seau construit dans ce notebook est bas\u00e9 sur l'article \"A Neural Probabilistic Language Model\".</p> <p>Voici \u00e0 quoi ressemble l'architecture de ce r\u00e9seau :</p> <p></p> <p>Figure extraite de l'article original.</p> <p>Dans l'article, le mod\u00e8le utilise des mots en entr\u00e9e (3 mots dans l'exemple illustr\u00e9) pour pr\u00e9dire le mot suivant. Dans notre cas, nous allons utiliser des caract\u00e8res, comme nous l'avons fait dans le notebook pr\u00e9c\u00e9dent.</p> <p>Matrice d'embedding $C$ : \u00c0 premi\u00e8re vue, on remarque que le r\u00e9seau comporte une matrice $C$ qui va encoder le mot (ou le caract\u00e8re) dans un espace latent. C'est une pratique presque syst\u00e9matiquement utilis\u00e9 en NLP car cela permet de rapprocher les mots \"similaires\" dans un espace latent. Par exemple, dans la plupart des phrases on pourrait interchanger les mots \"chien\" et \"chat\", ce qui signifie que ces mots vont avoir une representation proche dans l'espace latent tandis que les mots \"chien\" et \"est\" ne vont pas avoir une representation proche.</p> <p>Reste du r\u00e9seau : Le reste du r\u00e9seau est plus classique, il prend en entr\u00e9e la concatenation des embeddings des diff\u00e9rents mots (ou caract\u00e8res) et pr\u00e9dit un mot (ou caract\u00e8re en sortie).</p> <p>Le mod\u00e8le de l'article est entra\u00een\u00e9 par minimisation du negative log likelihood (comme ce que nous avons fait dans le notebook pr\u00e9c\u00e9dent avec le mod\u00e8le bigramme).</p> <p>Dans l'article, ils utilisent 3 mots pour pr\u00e9dire le 4\u00e8me mot. Nous allons nous baser sur le m\u00eame principe et pr\u00e9dire le 4\u00e8me caract\u00e8re \u00e0 partir des 3 caract\u00e8res pr\u00e9c\u00e9dents. La dimension de l'espace latent utilis\u00e9 dans l'article est 30 pour un dictionnaire contenant 17 000 mots distincts. Comme nous avons 46 caracact\u00e8res, prenons une dimension d'embedding de 10 assez arbitrairement.</p> <p>Commen\u00e7ons par reconstruire nos listes stoi et itos du notebook pr\u00e9c\u00e9dent :</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt from torch.utils.data import TensorDataset, DataLoader, random_split %matplotlib inline In\u00a0[2]: Copied! <pre>words = open('prenoms.txt', 'r').read().splitlines()\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\n</pre> words = open('prenoms.txt', 'r').read().splitlines() chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} <p>Construisons notre dataset, qui sera l\u00e9g\u00e8rement diff\u00e9rent, car les entr\u00e9es seront au nombre de trois au lieu d'une.</p> In\u00a0[3]: Copied! <pre>block_size = 3 # La longueur du contexte, combien de caract\u00e8res pour pr\u00e9dire le suivant ?\nX, Y = [], []\nfor k,w in enumerate(words):\n  context = [0] * block_size\n  for ch in w + '.':\n    ix = stoi[ch]\n    X.append(context)\n    Y.append(ix)\n    if (k&lt;2): ## On affiche ce \u00e0 quoi ressemble le dataset pour les deux premiers mots\n      print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n    context = context[1:] + [ix] # crop and append\n</pre> block_size = 3 # La longueur du contexte, combien de caract\u00e8res pour pr\u00e9dire le suivant ? X, Y = [], [] for k,w in enumerate(words):   context = [0] * block_size   for ch in w + '.':     ix = stoi[ch]     X.append(context)     Y.append(ix)     if (k&lt;2): ## On affiche ce \u00e0 quoi ressemble le dataset pour les deux premiers mots       print(''.join(itos[i] for i in context), '---&gt;', itos[ix])     context = context[1:] + [ix] # crop and append <pre>... ---&gt; M\n..M ---&gt; A\n.MA ---&gt; R\nMAR ---&gt; I\nARI ---&gt; E\nRIE ---&gt; .\n... ---&gt; J\n..J ---&gt; E\n.JE ---&gt; A\nJEA ---&gt; N\nEAN ---&gt; .\n</pre> In\u00a0[4]: Copied! <pre>X = torch.tensor(X)\nY = torch.tensor(Y)\nprint(X.shape, X.dtype, Y.shape, Y.dtype)\n</pre> X = torch.tensor(X) Y = torch.tensor(Y) print(X.shape, X.dtype, Y.shape, Y.dtype) <pre>torch.Size([226325, 3]) torch.int64 torch.Size([226325]) torch.int64\n</pre> <p>On va maintenant utiliser pytorch pour construire nos dataset de training, validation et test.</p> In\u00a0[5]: Copied! <pre>dataset=TensorDataset(X, Y)\ntrain_size = int(0.8 * len(dataset))\nval_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = random_split(TensorDataset(X, Y),[train_size, val_size, test_size])\nprint(\"Taille du dataset de training : \",len(train_dataset))\nprint(\"Taille du dataset de validation : \",len(val_dataset))\nprint(\"Taille du dataset de test : \",len(test_dataset))\n</pre> dataset=TensorDataset(X, Y) train_size = int(0.8 * len(dataset)) val_size = int(0.1 * len(dataset)) test_size = len(dataset) - train_size - val_size train_dataset, val_dataset, test_dataset = random_split(TensorDataset(X, Y),[train_size, val_size, test_size]) print(\"Taille du dataset de training : \",len(train_dataset)) print(\"Taille du dataset de validation : \",len(val_dataset)) print(\"Taille du dataset de test : \",len(test_dataset)) <pre>Taille du dataset de training :  181060\nTaille du dataset de validation :  22632\nTaille du dataset de test :  22633\n</pre> <p>Et on va cr\u00e9er nos dataloaders pour l'optimisation par mini-batch.</p> In\u00a0[6]: Copied! <pre>train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n</pre> train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False) test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False) <p>Pour comprendre en profondeur le r\u00e9seau que l'on construit ici, nous n'utiliserons pas les fonctions nn.Linear() de pytorch pour la construction des couches. On va commencer par d\u00e9finir le nombre de neurones des diff\u00e9rentes couches.</p> In\u00a0[7]: Copied! <pre>embed_dim=10 # Dimension de l'embedding de C\nhidden_dim=200 # Dimension de la couche cach\u00e9e\n</pre> embed_dim=10 # Dimension de l'embedding de C hidden_dim=200 # Dimension de la couche cach\u00e9e <p>Construisons notre matrice $C$ d'embedding (qui a des param\u00e8tres apprenables).</p> In\u00a0[8]: Copied! <pre>C = torch.randn((46, embed_dim))\nC[X].shape\n</pre> C = torch.randn((46, embed_dim)) C[X].shape Out[8]: <pre>torch.Size([226325, 3, 10])</pre> <p>En appelant C[X], gr\u00e2ce au formidable indexing de pytorch. On obtient les valeurs d'embedding de chacun des 3 caract\u00e8res de nos 226325 exemples.</p> <p>On peut maintenant cr\u00e9er nos couches cach\u00e9es $W_1$ et $W_2$ ainsi que leurs biais $b_1$ et $b_2$.</p> In\u00a0[9]: Copied! <pre>W1 = torch.randn((block_size*embed_dim, hidden_dim))\nb1 = torch.randn(hidden_dim)\nW2 = torch.randn((hidden_dim, 46))\nb2 = torch.randn(46)\nparameters = [C, W1, b1, W2, b2]\nprint(\"Nombre de param\u00e8tres du mod\u00e8le : \",sum(p.nelement() for p in parameters))\n</pre> W1 = torch.randn((block_size*embed_dim, hidden_dim)) b1 = torch.randn(hidden_dim) W2 = torch.randn((hidden_dim, 46)) b2 = torch.randn(46) parameters = [C, W1, b1, W2, b2] print(\"Nombre de param\u00e8tres du mod\u00e8le : \",sum(p.nelement() for p in parameters)) <pre>Nombre de param\u00e8tres du mod\u00e8le :  15906\n</pre> <p>Pour pouvoir entra\u00eener ces couches, il faut activer le param\u00e8tre requires_grad de pytorch. Cela permet de sp\u00e9cifier que l'on souhaite calculer les gradients sur ces \u00e9l\u00e9ments.</p> In\u00a0[10]: Copied! <pre>for p in parameters:\n  p.requires_grad = True\n</pre> for p in parameters:   p.requires_grad = True <p>Le choix du learning rate est tr\u00e8s important lors de l'entra\u00eenement d'un r\u00e9seau de neurones et il est souvent difficile de savoir quelle valeur choisir avant d'avoir fait des tests. Une bonne fa\u00e7on de choisir le learning rate est la suivante : On construit une liste de 1000 valeurs entre -3 et 0 puis on prend $10^{valeur}$ pour chaque valeur Cela va nous donner une liste de valeurs entre $10^{-3} = 0.001$ et $10^{0}=1$ qui sont un panel de valeurs potentielles pour notre learning rate. Les valeurs -3 et 0 peuvent \u00eatre diff\u00e9rentes, vous devez essayer de trouver des valeurs encadrant le learning rate optimal.</p> In\u00a0[11]: Copied! <pre>lre = torch.linspace(-3, 0, 1000)\nlrs = 10**lre\n</pre> lre = torch.linspace(-3, 0, 1000) lrs = 10**lre <p>On va ensuite tracker les valeurs de loss en fonction du learning rate sur l'ensemble de nos valeurs sur l'entra\u00eenement.</p> In\u00a0[12]: Copied! <pre>lri = []\nlossi = []\ncount=0\nwhile count&lt;999:\n  for x,y in train_loader:\n    count+=1\n    if count==999:\n        break\n    # forward pass\n    emb = C[x]\n    h = torch.tanh(emb.view(-1, block_size*embed_dim) @ W1 + b1)\n    logits = h @ W2 + b2 \n    loss = F.cross_entropy(logits, y)\n    \n    # retropropagation\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    \n    # Mise \u00e0 jour des poids du mod\u00e8le\n    lr = lrs[count]\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    lri.append(lre[count])\n    lossi.append(loss.log10().item())\n</pre> lri = [] lossi = [] count=0 while count&lt;999:   for x,y in train_loader:     count+=1     if count==999:         break     # forward pass     emb = C[x]     h = torch.tanh(emb.view(-1, block_size*embed_dim) @ W1 + b1)     logits = h @ W2 + b2      loss = F.cross_entropy(logits, y)          # retropropagation     for p in parameters:         p.grad = None     loss.backward()          # Mise \u00e0 jour des poids du mod\u00e8le     lr = lrs[count]     for p in parameters:         p.data += -lr * p.grad      lri.append(lre[count])     lossi.append(loss.log10().item()) In\u00a0[13]: Copied! <pre>plt.plot(lri, lossi)\n</pre> plt.plot(lri, lossi) Out[13]: <pre>[&lt;matplotlib.lines.Line2D at 0x72e6c044e590&gt;]</pre> <p>Avec cette courbe, on peut d\u00e9duire qu'une bonne valeur de learning rate se situe aux alentours de $10^{-1}$ et $10^{-0.5}$. On va donc choisir un learning rate de 0.2 que l'on diminuera au cours de l'entra\u00eenement (pratique courante pour une convergence rapide et un optimisation pr\u00e9cise sur la fin de l'entra\u00eenement).</p> <p>Dans notre optimisation, nous avons utilis\u00e9 la fonction tangente hyperbolique comme fonction d'activation. Elle est d\u00e9finie de la mani\u00e8re suivante : $\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ Et on peut la visualiser en python :</p> In\u00a0[14]: Copied! <pre>import numpy as np\nx = np.linspace(-10, 10, 400)\n\ny = np.tanh(x)\n\nplt.figure(figsize=(4, 3))\nplt.plot(x, y, label='tanh(x)')\nplt.title('Tangente Hyperbolique')\nplt.xlabel('x')\nplt.ylabel('tanh(x)')\nplt.grid(True)\nplt.legend()\nplt.show()\n</pre> import numpy as np x = np.linspace(-10, 10, 400)  y = np.tanh(x)  plt.figure(figsize=(4, 3)) plt.plot(x, y, label='tanh(x)') plt.title('Tangente Hyperbolique') plt.xlabel('x') plt.ylabel('tanh(x)') plt.grid(True) plt.legend() plt.show() <p>En g\u00e9n\u00e9ral, dans les couches cach\u00e9es de notre r\u00e9seau, on privilegie l'utilisaton de tanh plut\u00f4t que sigmoid et cela pour plusieurs raisons :</p> <ul> <li>La plage de sortie centr\u00e9e sur z\u00e9ro (-1 \u00e0 1) facilite l'apprentissage.</li> <li>Les gradients sont plus importants pour des valeurs entre -2 et 2 que pour la fonction sigmo\u00efde</li> <li>Les deux \u00e9lements pr\u00e9c\u00e9dents conduisent \u00e0 une diminution du probl\u00e8me de vanishing gradient et permettent une convergence plus rapide lors de l'entra\u00eenement.</li> </ul> <p>Passons maintenant \u00e0 l'optimisation de notre r\u00e9seau. D\u00e9finissons nos hyperparam\u00e8tres :</p> In\u00a0[15]: Copied! <pre>lr=0.2\nepochs=100\n\n# Reinitialisons les param\u00e8tres pour plus de simplicit\u00e9 si on a besoin de relancer l'entra\u00eenement\nC = torch.randn((46, embed_dim))\nW1 = torch.randn((block_size*embed_dim, hidden_dim))\nb1 = torch.randn(hidden_dim)\nW2 = torch.randn((hidden_dim, 46))\nb2 = torch.randn(46)\nparameters = [C, W1, b1, W2, b2]\nfor p in parameters:\n  p.requires_grad = True\n</pre> lr=0.2 epochs=100  # Reinitialisons les param\u00e8tres pour plus de simplicit\u00e9 si on a besoin de relancer l'entra\u00eenement C = torch.randn((46, embed_dim)) W1 = torch.randn((block_size*embed_dim, hidden_dim)) b1 = torch.randn(hidden_dim) W2 = torch.randn((hidden_dim, 46)) b2 = torch.randn(46) parameters = [C, W1, b1, W2, b2] for p in parameters:   p.requires_grad = True  In\u00a0[16]: Copied! <pre>lossi=[]\nlossvali=[]\nstepi = []\nfor epoch in range(epochs):\n  loss_epoch=0\n  for x,y in train_loader:\n    \n    # forward pass\n    emb = C[x]\n    h = torch.tanh(emb.view(-1, block_size*embed_dim) @ W1 + b1)\n    logits = h @ W2 + b2 \n    loss = F.cross_entropy(logits, y)\n    \n    # retropropagation\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    \n    # Mise \u00e0 jour des poids du mod\u00e8le\n    lr=lr if epoch&lt;50 else lr*0.1\n    for p in parameters:\n        p.data += -lr * p.grad\n    loss_epoch+=loss\n\n  loss_epoch=loss_epoch/len(train_loader)\n  stepi.append(epoch)\n  lossi.append(loss_epoch.item())\n  # Calcul du loss de validation (pour surveiller l'overfitting)\n  loss_val=0\n  for x,y in val_loader:\n    emb = C[x]\n    h = torch.tanh(emb.view(-1, block_size*embed_dim) @ W1 + b1)\n    logits = h @ W2 + b2 \n    loss = F.cross_entropy(logits, y)\n    loss_val+=loss\n  loss_val=loss_val/len(val_loader)\n  lossvali.append(loss_val.item())\n  if epoch%10==0:\n    print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\")\n</pre> lossi=[] lossvali=[] stepi = [] for epoch in range(epochs):   loss_epoch=0   for x,y in train_loader:          # forward pass     emb = C[x]     h = torch.tanh(emb.view(-1, block_size*embed_dim) @ W1 + b1)     logits = h @ W2 + b2      loss = F.cross_entropy(logits, y)          # retropropagation     for p in parameters:         p.grad = None     loss.backward()          # Mise \u00e0 jour des poids du mod\u00e8le     lr=lr if epoch&lt;50 else lr*0.1     for p in parameters:         p.data += -lr * p.grad     loss_epoch+=loss    loss_epoch=loss_epoch/len(train_loader)   stepi.append(epoch)   lossi.append(loss_epoch.item())   # Calcul du loss de validation (pour surveiller l'overfitting)   loss_val=0   for x,y in val_loader:     emb = C[x]     h = torch.tanh(emb.view(-1, block_size*embed_dim) @ W1 + b1)     logits = h @ W2 + b2      loss = F.cross_entropy(logits, y)     loss_val+=loss   loss_val=loss_val/len(val_loader)   lossvali.append(loss_val.item())   if epoch%10==0:     print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\") <pre>Epoch 0 - Training loss: 5.273, Validation loss: 3.519\nEpoch 10 - Training loss: 2.424, Validation loss: 2.594\nEpoch 20 - Training loss: 2.337, Validation loss: 2.421\nEpoch 30 - Training loss: 2.289, Validation loss: 2.468\nEpoch 40 - Training loss: 2.259, Validation loss: 2.424\nEpoch 50 - Training loss: 2.327, Validation loss: 2.372\nEpoch 60 - Training loss: 2.326, Validation loss: 2.372\nEpoch 70 - Training loss: 2.326, Validation loss: 2.372\nEpoch 80 - Training loss: 2.326, Validation loss: 2.372\nEpoch 90 - Training loss: 2.326, Validation loss: 2.372\n</pre> <p>Tra\u00e7ons les courbes de training et de validation.</p> In\u00a0[17]: Copied! <pre>plt.plot(stepi, lossi)\nplt.plot(stepi,lossvali)\n</pre> plt.plot(stepi, lossi) plt.plot(stepi,lossvali) Out[17]: <pre>[&lt;matplotlib.lines.Line2D at 0x72e6a443edd0&gt;]</pre> <p>Maintenant que le mod\u00e8le est entra\u00een\u00e9, on va v\u00e9rifier ses performances sur les donn\u00e9es de test. Si le loss sur les donn\u00e9es de test est \u00e0 peu pr\u00e8s similaire au loss d'entra\u00eenement alors le mod\u00e8le est bien entra\u00een\u00e9. Dans le cas contraire, il peut s'agir d'overfitting.</p> In\u00a0[18]: Copied! <pre># On annule le calcul des gradients car on n'est plus en phase d'entra\u00eenement.\nfor p in parameters:\n  p.requires_grad = False\nloss_test=0\nfor x,y in test_loader:\n      \n  # forward pass\n  emb = C[x]\n  h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n  logits = h @ W2 + b2 \n  loss = F.cross_entropy(logits, y)\n\n  loss_test+=loss\nloss_test=loss_test/len(test_loader)\nprint(loss_test)\n</pre> # On annule le calcul des gradients car on n'est plus en phase d'entra\u00eenement. for p in parameters:   p.requires_grad = False loss_test=0 for x,y in test_loader:          # forward pass   emb = C[x]   h = torch.tanh(emb.view(-1, 30) @ W1 + b1)   logits = h @ W2 + b2    loss = F.cross_entropy(logits, y)    loss_test+=loss loss_test=loss_test/len(test_loader) print(loss_test) <pre>tensor(2.3505)\n</pre> <p>On a un likelihood sur les donn\u00e9es de test relativement proche de celui des donn\u00e9es d'entra\u00eenement, on peut donc conclure que l'entra\u00eenement s'est plut\u00f4t bien pass\u00e9.</p> <p>On peut remarquer que la valeur du negative log likelihood sur notre mod\u00e8le est inf\u00e9rieure \u00e0 celle du mod\u00e8le bigramme du notebook pr\u00e9c\u00e9dent ($2.3&lt;2.5$). La qualit\u00e9 des pr\u00e9noms g\u00e9n\u00e9r\u00e9s devrait donc \u00eatre am\u00e9lior\u00e9e.</p> <p>G\u00e9n\u00e9rons une vingtaine de pr\u00e9noms pour juger nous-m\u00eame de la qualit\u00e9 de la g\u00e9n\u00e9ration.</p> In\u00a0[19]: Copied! <pre>for _ in range(20):\n  out = []\n  context = [0] * block_size \n  while True:\n    emb = C[torch.tensor([context])] \n    h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n    logits = h @ W2 + b2\n    probs = F.softmax(logits, dim=1)\n    ix = torch.multinomial(probs, num_samples=1).item()\n    context = context[1:] + [ix]\n    out.append(ix)\n    if ix == 0:\n      break\n  \n  print(''.join(itos[i] for i in out))\n</pre> for _ in range(20):   out = []   context = [0] * block_size    while True:     emb = C[torch.tensor([context])]      h = torch.tanh(emb.view(1, -1) @ W1 + b1)     logits = h @ W2 + b2     probs = F.softmax(logits, dim=1)     ix = torch.multinomial(probs, num_samples=1).item()     context = context[1:] + [ix]     out.append(ix)     if ix == 0:       break      print(''.join(itos[i] for i in out)) <pre>JA\u00cfMANT.\nSONELIUWAN.\nLYPHELS\u00cfL.\nDJELINATHEYMONDALYANE.\nERNANDRAN.\nESMALLOONIS.\nASHAMLANCHOND.\nANNAE.\nCHALLA.\nETTE.\nASSANE.\nMARIANE.\nFIHAYLAY.\nSHANA.\nALPHENELIESON.\nES\u00cfL.\nEVEY.\nYSLALLYSSIA.\nETHELDOF.\nKELLAH.\n</pre> <p>Les pr\u00e9noms g\u00e9n\u00e9r\u00e9s sont certes encore \u00e9tranges, mais ils ressemblent d\u00e9j\u00e0 beaucoup plus \u00e0 des pr\u00e9noms \"possibles\" compar\u00e9s \u00e0 ceux produits par le mod\u00e8le bigramme.</p> <p>Exercice : Vous pouvez essayer de modifier les neurones des couches ou les hyperparam\u00e8tres pour am\u00e9liorer le mod\u00e8le et constater la diff\u00e9rence dans la qualit\u00e9 de g\u00e9n\u00e9ration.</p> <p>Plus t\u00f4t dans le notebook, nous avons expliqu\u00e9 l'intuition derri\u00e8re la matrice d'embedding $C$ qui sert \u00e0 \"rapprocher\" les mots (ou caract\u00e8res) ayant un sens proche. On ne peut pas facilement visualiser la positions de chaque caract\u00e8re dans la matrice $C$. Pour avoir une visualisation, on va donc r\u00e9-entra\u00eener un mod\u00e8le mais avec une dimension d'embedding de 2 au lieu de 10. Cela nous permettra de visualiser la matrice $C$.</p> <p>Note : Pour visualiser les embeddings de dimension sup\u00e9rieure \u00e0 2 en 2D, on peut utiliser la m\u00e9thode T-SNE ou UMAP.</p> In\u00a0[23]: Copied! <pre>lr=0.2\nepochs=100\n\nC = torch.randn((46, 2)) # 2 au lieu de embed_dim\nW1 = torch.randn((block_size*2, hidden_dim))\nb1 = torch.randn(hidden_dim)\nW2 = torch.randn((hidden_dim, 46))\nb2 = torch.randn(46)\nparameters = [C, W1, b1, W2, b2]\nfor p in parameters:\n  p.requires_grad = True\n</pre> lr=0.2 epochs=100  C = torch.randn((46, 2)) # 2 au lieu de embed_dim W1 = torch.randn((block_size*2, hidden_dim)) b1 = torch.randn(hidden_dim) W2 = torch.randn((hidden_dim, 46)) b2 = torch.randn(46) parameters = [C, W1, b1, W2, b2] for p in parameters:   p.requires_grad = True In\u00a0[24]: Copied! <pre>lossi=[]\nstepi = []\nfor epoch in range(epochs):\n  loss_epoch=0\n  for x,y in train_loader:\n    # forward pass\n    emb = C[x]\n    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #6 au lieu de 30\n    logits = h @ W2 + b2 \n    loss = F.cross_entropy(logits, y)\n    # retropropagation\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    # Mise \u00e0 jour des poids du mod\u00e8le\n    lr=lr if epoch&lt;50 else lr*0.1\n    for p in parameters:\n        p.data += -lr * p.grad\n    loss_epoch+=loss\n  loss_epoch=loss_epoch/len(train_loader)\n  stepi.append(epoch)\n  lossi.append(loss_epoch.item())\n  \n  # Validation\n  loss_val=0\n  for x,y in val_loader:\n    emb = C[x]\n    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #6 au lieu de 30\n    logits = h @ W2 + b2 \n    loss = F.cross_entropy(logits, y)\n    loss_val+=loss\n  loss_val=loss_val/len(val_loader)\n  lossvali.append(loss_val.item())\n  if epoch%10==0:\n    print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\")\n</pre> lossi=[] stepi = [] for epoch in range(epochs):   loss_epoch=0   for x,y in train_loader:     # forward pass     emb = C[x]     h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #6 au lieu de 30     logits = h @ W2 + b2      loss = F.cross_entropy(logits, y)     # retropropagation     for p in parameters:         p.grad = None     loss.backward()     # Mise \u00e0 jour des poids du mod\u00e8le     lr=lr if epoch&lt;50 else lr*0.1     for p in parameters:         p.data += -lr * p.grad     loss_epoch+=loss   loss_epoch=loss_epoch/len(train_loader)   stepi.append(epoch)   lossi.append(loss_epoch.item())      # Validation   loss_val=0   for x,y in val_loader:     emb = C[x]     h = torch.tanh(emb.view(-1, 6) @ W1 + b1) #6 au lieu de 30     logits = h @ W2 + b2      loss = F.cross_entropy(logits, y)     loss_val+=loss   loss_val=loss_val/len(val_loader)   lossvali.append(loss_val.item())   if epoch%10==0:     print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\") <pre>Epoch 0 - Training loss: 3.822, Validation loss: 3.294\nEpoch 10 - Training loss: 2.490, Validation loss: 2.616\nEpoch 20 - Training loss: 2.425, Validation loss: 2.532\nEpoch 30 - Training loss: 2.388, Validation loss: 2.498\nEpoch 40 - Training loss: 2.365, Validation loss: 2.529\nEpoch 50 - Training loss: 2.386, Validation loss: 2.399\nEpoch 60 - Training loss: 2.385, Validation loss: 2.399\nEpoch 70 - Training loss: 2.386, Validation loss: 2.399\nEpoch 80 - Training loss: 2.385, Validation loss: 2.399\nEpoch 90 - Training loss: 2.385, Validation loss: 2.399\n</pre> <p>Comme vous le voyez, le loss est plus important car une dimension d'embedding de 2 est insuffisante pour correctement representer chaque caract\u00e8re. Par contre, on peut maintenant visualiser la position des mots dans l'espace latent.</p> In\u00a0[\u00a0]: Copied! <pre># visualize dimensions 0 and 1 of the embedding matrix C for all characters\nplt.figure(figsize=(8,8))\nplt.scatter(C[:,0].data, C[:,1].data, s=200)\nfor i in range(C.shape[0]):\n  plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\nplt.grid('minor')\n</pre> # visualize dimensions 0 and 1 of the embedding matrix C for all characters plt.figure(figsize=(8,8)) plt.scatter(C[:,0].data, C[:,1].data, s=200) for i in range(C.shape[0]):   plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white') plt.grid('minor') <p>On peut voir une tendance avec un regroupement des voyelles et un regroupement des consonnes (qui sont souvent interchangeables dans un pr\u00e9nom). Les caract\u00e8res tr\u00e8s rares ont des embeddings vraiment \u00e0 part ('\u00e7', '\u00f6', '\u00eb'). On peut \u00e9galement remarquer la proximit\u00e9 entre '.' et '-' ce qui est tr\u00e8s logique quand on pense \u00e0 ce qu'est un pr\u00e9nom compos\u00e9 dans la langue fran\u00e7aise. Cela montre que la matrice $C$ a appris une sorte de mapping des caract\u00e8res en fonction de leur proximit\u00e9 s\u00e9mantique.</p>"},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#reseau-fully-connected","title":"R\u00e9seau fully connected\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#architecture","title":"Architecture\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#inspiration","title":"Inspiration\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#notre-approche","title":"Notre approche\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#dataset-et-dataloader","title":"Dataset et dataloader\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#couches-du-reseau","title":"Couches du r\u00e9seau\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#comment-trouver-le-bon-learning-rate","title":"Comment trouver le bon learning rate ?\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#point-sur-la-tangente-hyperbolique","title":"Point sur la tangente hyperbolique\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#optimisation-du-reseau","title":"Optimisation du r\u00e9seau\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#test-du-modele","title":"Test du mod\u00e8le\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#generation-de-prenoms-avec-notre-modele","title":"G\u00e9neration de pr\u00e9noms avec notre mod\u00e8le\u00b6","text":""},{"location":"05_NLP/03_R%C3%A9seauFullyConnected.html#visualisation-des-embeddings","title":"Visualisation des embeddings\u00b6","text":""},{"location":"05_NLP/04_WaveNet.html","title":"Pytorch et WaveNet","text":"<p>Dans ce cours, nous allons nous nous inspire de l'architecture du mod\u00e8le WaveNet  propos\u00e9e par google deepmind pour le traitement de l'audio. Notre objectif est d'utiliser un plus grand nombre de caract\u00e8res pour le contexte de notre pr\u00e9dicteur du prochain mot.</p> <p>L'architecture d'un WaveNet est une architecture hierarchique qui accorde plus de poids aux \u00e9l\u00e9ments de contexte les plus proches.</p> <p>Voici \u00e0 quoi ressemble l'architecture  :</p> <p></p> <p>Figure extraite de l'article original.</p> <p>Le cours commence par une adaptation des concepts du cours pr\u00e9c\u00e9dent en utilisant les fonctions de PyTorch.</p> <p>Reprenons le code du notebook pr\u00e9c\u00e9dent pour la g\u00e9n\u00e9ration du dataset.</p> In\u00a0[38]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\n\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures from torch.utils.data import TensorDataset, DataLoader, random_split  %matplotlib inline In\u00a0[39]: Copied! <pre># Lecture du dataset\nwords = open('prenoms.txt', 'r').read().splitlines()\nprint(words[:8])\n</pre> # Lecture du dataset words = open('prenoms.txt', 'r').read().splitlines() print(words[:8]) <pre>['MARIE', 'JEAN', 'PIERRE', 'MICHEL', 'ANDR\u00c9', 'JEANNE', 'PHILIPPE', 'LOUIS']\n</pre> In\u00a0[40]: Copied! <pre>words = open('prenoms.txt', 'r').read().splitlines()\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\n</pre> words = open('prenoms.txt', 'r').read().splitlines() chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} <p>Pour changer, augmentons le contexte en le passant de 3 \u00e0 8. Cela nous donnera un indicateur de performance car nous utiliserons aussi 8 pour notre WaveNet.</p> In\u00a0[41]: Copied! <pre>block_size = 8 # La longueur du contexte, combien de caract\u00e8res pour pr\u00e9dire le suivant ?\nX, Y = [], []\nfor k,w in enumerate(words):\n  \n  context = [0] * block_size\n  for ch in w + '.':\n    ix = stoi[ch]\n    X.append(context)\n    Y.append(ix)\n    context = context[1:] + [ix] \nX = torch.tensor(X)\nY = torch.tensor(Y)\n</pre> block_size = 8 # La longueur du contexte, combien de caract\u00e8res pour pr\u00e9dire le suivant ? X, Y = [], [] for k,w in enumerate(words):      context = [0] * block_size   for ch in w + '.':     ix = stoi[ch]     X.append(context)     Y.append(ix)     context = context[1:] + [ix]  X = torch.tensor(X) Y = torch.tensor(Y) In\u00a0[42]: Copied! <pre>dataset=TensorDataset(X, Y)\ntrain_size = int(0.8 * len(dataset))\nval_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = random_split(TensorDataset(X, Y),[train_size, val_size, test_size])\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n</pre> dataset=TensorDataset(X, Y) train_size = int(0.8 * len(dataset)) val_size = int(0.1 * len(dataset)) test_size = len(dataset) - train_size - val_size train_dataset, val_dataset, test_dataset = random_split(TensorDataset(X, Y),[train_size, val_size, test_size]) train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False) test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False) <p>Pour commencer, nous allons r\u00e9implementer le mod\u00e8le du notebook pr\u00e9c\u00e9dent mais avec pytorch.</p> In\u00a0[43]: Copied! <pre>import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n</pre> import torch  import torch.nn as nn import torch.nn.functional as F In\u00a0[44]: Copied! <pre>class fcn(nn.Module):\n  def __init__(self,embed_dim=10,context_len=8,hidden_dim=300, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.embed_dim=embed_dim\n    self.context_len=context_len\n    \n    #La fonction nn.Embedding de pytorch est l'\u00e9quivalent de la matrice C \n    self.embedding=nn.Embedding(46,embed_dim)\n    self.layer1=nn.Linear(embed_dim*context_len,hidden_dim)\n    self.layer2=nn.Linear(hidden_dim,46)\n\n  def forward(self,x):       \n    embed=self.embedding(x) # Remplace la matrice C\n    embed=embed.view(-1,self.embed_dim*self.context_len)\n    hidden=F.tanh(self.layer1(embed))\n    logits=self.layer2(hidden)\n    return logits\n</pre> class fcn(nn.Module):   def __init__(self,embed_dim=10,context_len=8,hidden_dim=300, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)     self.embed_dim=embed_dim     self.context_len=context_len          #La fonction nn.Embedding de pytorch est l'\u00e9quivalent de la matrice C      self.embedding=nn.Embedding(46,embed_dim)     self.layer1=nn.Linear(embed_dim*context_len,hidden_dim)     self.layer2=nn.Linear(hidden_dim,46)    def forward(self,x):            embed=self.embedding(x) # Remplace la matrice C     embed=embed.view(-1,self.embed_dim*self.context_len)     hidden=F.tanh(self.layer1(embed))     logits=self.layer2(hidden)     return logits  In\u00a0[45]: Copied! <pre>model=fcn(context_len=8)\nepochs=50\nlr=0.2\noptimizer=torch.optim.SGD(model.parameters(),lr=lr)\nfor p in model.parameters():\n  p.requires_grad = True\n</pre> model=fcn(context_len=8) epochs=50 lr=0.2 optimizer=torch.optim.SGD(model.parameters(),lr=lr) for p in model.parameters():   p.requires_grad = True <p>Notes : Pour reproduire \u00e0 l'identique le code du notebook pr\u00e9c\u00e9dent, il faudra r\u00e9duire le learning rate par un facteur 10 \u00e0 la moiti\u00e9 de l'entra\u00eenement. En pytorch, on peut faire \u00e7a \u00e0 l'aide du scheduler. Il existe plusieurs types de scheduler : LambdaLR (pour changer le lr par rappport \u00e0 une fonction), StepLR (pour diminuer le lr toutes les n epochs), LinearLR (pour diminuer le lr de mani\u00e8re lin\u00e9aire), ReduceLROnPlateau (pour diminuer le lr d\u00e8s que le loss ne change plus), OneCycleLR (pour commencer avec un lr faible puis l'augmenter et le redescendre) et bien d'autres. Pour acc\u00e9lerer la convergence d'un mod\u00e8le, je vous conseillerais l'utilisation du OneCycleLR (pour en savoir plus, consultez ce blogpost) et pour obtenir un mod\u00e8le tr\u00e8s performant je vous conseillerais plut\u00f4t le ReduceLROnPlateau. Dans tous les cas, il est int\u00e9ressant d'exp\u00e9rimenter vous-m\u00eame avec les diff\u00e9rents scheduler.</p> In\u00a0[46]: Copied! <pre>lossi=[]\nlossvali=[]\nstepi = []\nfor epoch in range(epochs):\n  loss_epoch=0\n  for x,y in train_loader:\n    # forward pass\n    logits=model(x)\n    loss = F.cross_entropy(logits, y)\n    # retropropagation\n    optimizer.zero_grad()\n    loss.backward()\n    # Mise \u00e0 jour des poids du mod\u00e8le\n    optimizer.step()\n    loss_epoch+=loss\n  loss_epoch=loss_epoch/len(train_loader)\n  stepi.append(epoch)\n  lossi.append(loss_epoch.item())\n  \n  # Validation\n  loss_val=0\n  for x,y in val_loader:\n    logits=model(x)\n    loss = F.cross_entropy(logits, y)\n    loss_val+=loss\n  loss_val=loss_val/len(val_loader)\n  lossvali.append(loss_val.item())\n  if epoch%10==0:\n    print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\")\n</pre> lossi=[] lossvali=[] stepi = [] for epoch in range(epochs):   loss_epoch=0   for x,y in train_loader:     # forward pass     logits=model(x)     loss = F.cross_entropy(logits, y)     # retropropagation     optimizer.zero_grad()     loss.backward()     # Mise \u00e0 jour des poids du mod\u00e8le     optimizer.step()     loss_epoch+=loss   loss_epoch=loss_epoch/len(train_loader)   stepi.append(epoch)   lossi.append(loss_epoch.item())      # Validation   loss_val=0   for x,y in val_loader:     logits=model(x)     loss = F.cross_entropy(logits, y)     loss_val+=loss   loss_val=loss_val/len(val_loader)   lossvali.append(loss_val.item())   if epoch%10==0:     print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\") <pre>Epoch 0 - Training loss: 2.487, Validation loss: 2.427\nEpoch 10 - Training loss: 2.056, Validation loss: 2.151\nEpoch 20 - Training loss: 1.952, Validation loss: 2.114\nEpoch 30 - Training loss: 1.896, Validation loss: 2.146\nEpoch 40 - Training loss: 1.864, Validation loss: 2.092\n</pre> In\u00a0[47]: Copied! <pre>plt.plot(stepi, lossi)\nplt.plot(stepi,lossvali)\n</pre> plt.plot(stepi, lossi) plt.plot(stepi,lossvali) Out[47]: <pre>[&lt;matplotlib.lines.Line2D at 0x74c19c438d50&gt;]</pre> <p>La diff\u00e9rence entre la courbe de training et de validation indique que le mod\u00e8le overfit un peu.</p> In\u00a0[48]: Copied! <pre># On annule le calcul des gradients car on n'est plus en phase d'entra\u00eenement.\nmodel.eval()\nloss_test=0\nfor x,y in test_loader:\n    \n  # forward pass\n  logits=model(x)\n  loss = F.cross_entropy(logits, y)\n      \n  loss_test+=loss\nloss_test=loss_test/len(test_loader)\nprint(loss_test)\n</pre> # On annule le calcul des gradients car on n'est plus en phase d'entra\u00eenement. model.eval() loss_test=0 for x,y in test_loader:        # forward pass   logits=model(x)   loss = F.cross_entropy(logits, y)          loss_test+=loss loss_test=loss_test/len(test_loader) print(loss_test) <pre>tensor(2.1220, grad_fn=&lt;DivBackward0&gt;)\n</pre> <p>Le loss de test est un peu sup\u00e9rieur au loss de train donc le mod\u00e8le a un peu overfit mais \u00e7a reste l\u00e9ger et on a encore de la marge pour augmenter les capacit\u00e9s du r\u00e9seau. On peut maintenant v\u00e9rifier la qualit\u00e9 de la g\u00e9n\u00e9ration de pr\u00e9noms.</p> In\u00a0[49]: Copied! <pre>for _ in range(5):\n  out = []\n  context = [0] * block_size \n  while True:\n    logits=model(torch.tensor([context]))\n    probs = F.softmax(logits, dim=1)\n    ix = torch.multinomial(probs, num_samples=1).item()\n    context = context[1:] + [ix]\n    out.append(ix)\n    if ix == 0:\n      break\n  \n  print(''.join(itos[i] for i in out))\n</pre> for _ in range(5):   out = []   context = [0] * block_size    while True:     logits=model(torch.tensor([context]))     probs = F.softmax(logits, dim=1)     ix = torch.multinomial(probs, num_samples=1).item()     context = context[1:] + [ix]     out.append(ix)     if ix == 0:       break      print(''.join(itos[i] for i in out)) <pre>LOUIS-ANDR\u00c9.\nYOHES.\nBRES.\nTERIGAND.\nCONKHE.\n</pre> <p>Les pr\u00e9noms g\u00e9n\u00e9r\u00e9s sont correctes mais am\u00e9liorables. Voyons voir si on peut obtenir un meilleur loss sur les donn\u00e9es de test avec l'approche WaveNet.</p> <p>Le dataset est le m\u00eame que pour la partie pr\u00e9c\u00e9dente, pas besoin de changer quoi que ce soit.</p> <p>Ce qu'on veut dans notre mod\u00e8le c'est tra\u00eeter en parallele des groupes de d'embedding en regroupant les caract\u00e8res cons\u00e9cutifs. Sur pytorch, si on fait passer un tenseur de taille $B \\times L \\times C$ dans une couche lin\u00e9aire de taille $C \\times H$, on obtient un tenseur de taille $B \\times L \\times H$ et c'est exactement ce que l'on veut pour impl\u00e9menter le r\u00e9seau wavenet.</p> <p>Maintenant, il faut trouver comment modifier la taille du tenseur pour faire les op\u00e9rations du wavenet. Nos 8 embedding sont regroup\u00e9s par deux puis trait\u00e9s en parall\u00e8le. \u00c0 la couche suivante, ils sont \u00e0 nouveau regroup\u00e9s par deux. Donc \u00e0 chaque \u00e9tape, on double la taille $H$ (ou $C$) et on divise par deux $L$. Pour \u00eatre plus clair, \u00e0 la premi\u00e8re \u00e9tape nous avons un tenseur de taille $B \\times 8 \\times 10$ que l'on veut transformer en tenseur de taille $B \\times 4 \\times 20$.</p> <p>On peut impl\u00e9menter \u00e7a avec view() de pytorch.</p> In\u00a0[50]: Copied! <pre>dummy=torch.randn([256,8,10])\n# On divise par deux L et on double H/C\ndummy=dummy.view(-1,dummy.shape[1]//2,dummy.shape[2]*2)\nprint(dummy.shape)\n</pre> dummy=torch.randn([256,8,10]) # On divise par deux L et on double H/C dummy=dummy.view(-1,dummy.shape[1]//2,dummy.shape[2]*2) print(dummy.shape) <pre>torch.Size([256, 4, 20])\n</pre> <p>Essayons de formaliser \u00e7a avec une couche que l'on pourra utiliser dans notre r\u00e9seau :</p> In\u00a0[51]: Copied! <pre>class FlattenConsecutive(nn.Module):\n  # n est le facteur de regroupement (toujours 2 pour nous)\n  def __init__(self, n):\n    super(FlattenConsecutive, self).__init__()\n    self.n = n   \n  def __call__(self, x):\n    # On r\u00e9cup\u00e8re les dimensions de l'entr\u00e9e\n    B, T, C = x.shape \n    # On fait la transformation x2 et /2\n    x = x.view(B, T//self.n, C*self.n)\n    if x.shape[1] == 1: \n      x = x.squeeze(1) # Si le tensor a une dimension qui vaut 1, on la supprime\n    self.out = x\n    return self.out\n</pre> class FlattenConsecutive(nn.Module):   # n est le facteur de regroupement (toujours 2 pour nous)   def __init__(self, n):     super(FlattenConsecutive, self).__init__()     self.n = n      def __call__(self, x):     # On r\u00e9cup\u00e8re les dimensions de l'entr\u00e9e     B, T, C = x.shape      # On fait la transformation x2 et /2     x = x.view(B, T//self.n, C*self.n)     if x.shape[1] == 1:        x = x.squeeze(1) # Si le tensor a une dimension qui vaut 1, on la supprime     self.out = x     return self.out <p>Il est temps de cr\u00e9er notre mod\u00e8le, pour plus de simplicit\u00e9, on utilise nn.Sequential pour regrouper nos couches.</p> In\u00a0[52]: Copied! <pre>class wavenet(nn.Module):\n  def __init__(self,embed_dim=10,hidden_dim=128, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    \n    self.net=nn.Sequential(nn.Embedding(46,embed_dim),\n      # B*8*10\n      FlattenConsecutive(2), nn.Linear(embed_dim*2,hidden_dim),nn.Tanh(),\n      # B*4*hidden_dim\n      FlattenConsecutive(2), nn.Linear(hidden_dim*2,hidden_dim),nn.Tanh(),\n      # B*2*hidden_dim\n      FlattenConsecutive(2), nn.Linear(hidden_dim*2,hidden_dim),nn.Tanh(),\n      # B*hidden_dim\n      nn.Linear(hidden_dim,46)\n    )   \n        \n  def forward(self,x):\n    logits=self.net(x)\n    return logits\n</pre> class wavenet(nn.Module):   def __init__(self,embed_dim=10,hidden_dim=128, *args, **kwargs) -&gt; None:     super().__init__(*args, **kwargs)          self.net=nn.Sequential(nn.Embedding(46,embed_dim),       # B*8*10       FlattenConsecutive(2), nn.Linear(embed_dim*2,hidden_dim),nn.Tanh(),       # B*4*hidden_dim       FlattenConsecutive(2), nn.Linear(hidden_dim*2,hidden_dim),nn.Tanh(),       # B*2*hidden_dim       FlattenConsecutive(2), nn.Linear(hidden_dim*2,hidden_dim),nn.Tanh(),       # B*hidden_dim       nn.Linear(hidden_dim,46)     )               def forward(self,x):     logits=self.net(x)     return logits  <p>On initialise notre mod\u00e8le et les hyperparam\u00e8tres.</p> In\u00a0[53]: Copied! <pre>model=wavenet()\nepochs=40\nlr=0.2\noptimizer=torch.optim.SGD(model.parameters(),lr=lr)\nfor p in model.parameters():\n  p.requires_grad = True\n</pre> model=wavenet() epochs=40 lr=0.2 optimizer=torch.optim.SGD(model.parameters(),lr=lr) for p in model.parameters():   p.requires_grad = True <p>Et on lance l'entra\u00eenement.</p> In\u00a0[54]: Copied! <pre>lossi=[]\nlossvali=[]\nstepi = []\nfor epoch in range(epochs):\n  loss_epoch=0\n  for x,y in train_loader:\n    logits=model(x)\n    loss = F.cross_entropy(logits, y)\n    # retropropagation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    # Mise \u00e0 jour des poids du mod\u00e8le\n    loss_epoch+=loss\n  loss_epoch=loss_epoch/len(train_loader)\n  stepi.append(epoch)\n  lossi.append(loss_epoch.item())\n  \n  # Validation\n  loss_val=0\n  for x,y in val_loader:\n    logits=model(x)\n    loss = F.cross_entropy(logits, y)\n    loss_val+=loss\n  loss_val=loss_val/len(val_loader)\n  lossvali.append(loss_val.item())\n  if epoch%10==0:\n    print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\")\n  \n</pre> lossi=[] lossvali=[] stepi = [] for epoch in range(epochs):   loss_epoch=0   for x,y in train_loader:     logits=model(x)     loss = F.cross_entropy(logits, y)     # retropropagation     optimizer.zero_grad()     loss.backward()     optimizer.step()     # Mise \u00e0 jour des poids du mod\u00e8le     loss_epoch+=loss   loss_epoch=loss_epoch/len(train_loader)   stepi.append(epoch)   lossi.append(loss_epoch.item())      # Validation   loss_val=0   for x,y in val_loader:     logits=model(x)     loss = F.cross_entropy(logits, y)     loss_val+=loss   loss_val=loss_val/len(val_loader)   lossvali.append(loss_val.item())   if epoch%10==0:     print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\")    <pre>Epoch 0 - Training loss: 2.541, Validation loss: 2.459\nEpoch 10 - Training loss: 2.013, Validation loss: 2.094\nEpoch 20 - Training loss: 1.898, Validation loss: 2.096\nEpoch 30 - Training loss: 1.834, Validation loss: 2.060\n</pre> In\u00a0[55]: Copied! <pre>plt.plot(stepi, lossi)\nplt.plot(stepi,lossvali)\n</pre> plt.plot(stepi, lossi) plt.plot(stepi,lossvali) Out[55]: <pre>[&lt;matplotlib.lines.Line2D at 0x74c21819d710&gt;]</pre> In\u00a0[56]: Copied! <pre>model.eval()\nloss_test=0\nfor x,y in test_loader:\n      \n  # forward pass\n  logits=model(x)\n  loss = F.cross_entropy(logits, y)\n      \n  loss_test+=loss\nloss_test=loss_test/len(test_loader)\nprint(loss_test)\n</pre> model.eval() loss_test=0 for x,y in test_loader:          # forward pass   logits=model(x)   loss = F.cross_entropy(logits, y)          loss_test+=loss loss_test=loss_test/len(test_loader) print(loss_test) <pre>tensor(2.0132, grad_fn=&lt;DivBackward0&gt;)\n</pre> <p>On obtient un loss tr\u00e8s correct et inf\u00e9rieur \u00e0 celui du mod\u00e8le Fully Connected avec un contexte de 8.</p> <p>Notes : En regardant la courbe de validation par rapport \u00e0 la courbe de training, on se rend compte qu'il y a peut \u00eatre un probl\u00e8me (et pareil avec le mod\u00e8le Fully Connected de contexte 8). A vous de voir si vous arrivez \u00e0 corriger ce probl\u00e8me avec ce que vous avez appris dans les cours pr\u00e9c\u00e9dents (voir cours sur les r\u00e9seaux fully connected).</p> In\u00a0[68]: Copied! <pre>for _ in range(5):\n  out = []\n  context = [0] * block_size \n  while True:\n    logits=model(torch.tensor([context]))\n    probs = F.softmax(logits, dim=1)\n    ix = torch.multinomial(probs, num_samples=1).item()\n    context = context[1:] + [ix]\n    out.append(ix)\n    if ix == 0:\n      break\n  \n  print(''.join(itos[i] for i in out))\n</pre> for _ in range(5):   out = []   context = [0] * block_size    while True:     logits=model(torch.tensor([context]))     probs = F.softmax(logits, dim=1)     ix = torch.multinomial(probs, num_samples=1).item()     context = context[1:] + [ix]     out.append(ix)     if ix == 0:       break      print(''.join(itos[i] for i in out)) <pre>MARICE.\nJEXE.\nRAYEDE.\nCHAHI.\nRISHAE.\n</pre> <p>La g\u00e9n\u00e9ration de pr\u00e9noms est de mieux en mieux !!</p> <p>Exercice : Pour vous entra\u00eener, essayez de modifier les param\u00e8tres d'entra\u00eenement, l'architecture du r\u00e9seau et autres pour obtenir un loss inf\u00e9rieur \u00e0 2.0 sur les donn\u00e9es de test. Point bonus si vous r\u00e9duisez le nombre de param\u00e8tres du mod\u00e8le.</p>"},{"location":"05_NLP/04_WaveNet.html#pytorch-et-wavenet","title":"Pytorch et WaveNet\u00b6","text":""},{"location":"05_NLP/04_WaveNet.html#architecture","title":"Architecture\u00b6","text":""},{"location":"05_NLP/04_WaveNet.html#implementation-du-modele-fully-connected-avec-pytorch","title":"Impl\u00e9mentation du mod\u00e8le fully connected avec pytorch\u00b6","text":""},{"location":"05_NLP/04_WaveNet.html#dataset","title":"Dataset\u00b6","text":""},{"location":"05_NLP/04_WaveNet.html#creation-du-modele-et-entrainement","title":"Cr\u00e9ation du mod\u00e8le et entra\u00eenement\u00b6","text":""},{"location":"05_NLP/04_WaveNet.html#implementation-du-wavenet-avec-pytorch","title":"Impl\u00e9mentation du WaveNet avec pytorch\u00b6","text":""},{"location":"05_NLP/04_WaveNet.html#comment-gerer-larchitecture-hierarchique","title":"Comment g\u00e9rer l'architecture hierarchique\u00b6","text":""},{"location":"05_NLP/04_WaveNet.html#creation-du-modele","title":"Cr\u00e9ation du mod\u00e8le\u00b6","text":""},{"location":"05_NLP/05_Rnn.html","title":"R\u00e9seau de neurones r\u00e9currents","text":"<p>Dans ce cours, nous allons introduire les r\u00e9seaux de neurones r\u00e9currents (RNN) dans le cadre de la pr\u00e9diction du prochain caract\u00e8re. Pour ce faire, nous allons nous baser sur l'architecture d\u00e9crite dans le papier Recurrent neural network based language model qui pr\u00e9sente une version basique de RNN pour la pr\u00e9diction du prochain caract\u00e8re.</p> <p>La motivation derri\u00e8re l'utilisation d'un RNN pour cette t\u00e2che est de ne pas avoir \u00e0 sp\u00e9cifier une taille de contexte pour l'entra\u00eenement du mod\u00e8le contrairement aux deux mod\u00e8les bas\u00e9es sur des r\u00e9seaux fully connected que nous avons vu dans les notebooks pr\u00e9c\u00e9dents.</p> <p>Les RNN ont pour motivation de garder une information de contexte peu importe la longueur de la s\u00e9quence. C'est une id\u00e9e tr\u00e8s int\u00e9ressante sur le papier mais nous verrons, \u00e0 la fin du cours, qu'il y a de grosses limitations.</p> <p></p> <p>Figure extraite de l'article original.</p> <p>L'architecture de r\u00e9seau de neurones r\u00e9currents se base sur une approche s\u00e9quentielle. Les caract\u00e8res vont \u00eatre pass\u00e9s un par un dans le mod\u00e8le et la valeur du caract\u00e8re suivant d\u00e9pend du state gard\u00e9 en m\u00e9moire et de l'\u00e9l\u00e9ment actuel. Le state contient les informations de contexte de tous les caract\u00e8res pr\u00e9c\u00e9dents.</p> <p>Posons le probl\u00e8me math\u00e9matiquement : Un RNN est constitu\u00e9 de 3 \u00e9l\u00e9ments : l'input $x$, le state (hidden layer) $s$ et l'output $y$. On introduit \u00e9galement le temps $t$ qui rajoute la composante temporelle pour le traitement s\u00e9quentiel. $x$ au temps $t$ est alors d\u00e9fini comme : $x(t)=w(t) + s(t-1)$ o\u00f9 $w()$ est l'op\u00e9ration de one_hot encoding et $s(t-1)$ est le state au temps $t-1$. Et ensuite, on estime $s(t)$ et $y(t)$ : $s(t)=sigmoid(x(t))$ $y(t)=softmax(s(t))$</p> <p>On peut constater que ce mod\u00e8le n'a en fait qu'un seul param\u00e8tre \u00e0 ajuster : la dimension de la couche cach\u00e9e $s$.</p> <p>Pour l'initialisation $s(0)$ peut \u00eatre initialis\u00e9e en un vecteur de petite valeurs.</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\n</pre> import torch import torch.nn as nn <p>Utiliser un RNN pour g\u00e9n\u00e9rer des pr\u00e9noms n'est pas tr\u00e8s int\u00e9ressant car les pr\u00e9noms ne sont jamais tr\u00e8s longs et la taille de contexte est donc limit\u00e9e. Pour ce type de t\u00e2ches, il est int\u00e9ressant d'utiliser un dataset avec un contexte cons\u00e9quent. Pour cela, nous utilisons un fichier texte moliere.txt qui regroupe l'int\u00e9gralit\u00e9 des dialogues des pi\u00e8ces de Moli\u00e8re. Ce dataset a \u00e9t\u00e9 cr\u00e9e \u00e0 partir des oeuvres compl\u00e8tes de Moli\u00e8re disponibles sur le site Gutenberg.org. J'ai nettoy\u00e9 un peu les donn\u00e9es afin de ne garder que les dialogues.</p> In\u00a0[2]: Copied! <pre>with open('moliere.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nprint(\"Nombre de caract\u00e8res dans le dataset : \", len(text))\n</pre> with open('moliere.txt', 'r', encoding='utf-8') as f:     text = f.read() print(\"Nombre de caract\u00e8res dans le dataset : \", len(text)) <pre>Nombre de caract\u00e8res dans le dataset :  1687290\n</pre> <p>C'est un gros dataset donc pour avoir un temps de traitement raisonnables nous prenons uniquement une partie de ce dataset (par exemple les 50 000 premiers caract\u00e8res).</p> In\u00a0[3]: Copied! <pre>text=text[:50000]\nprint(\"Nombre de caract\u00e8res dans le dataset : \", len(text))\n</pre> text=text[:50000] print(\"Nombre de caract\u00e8res dans le dataset : \", len(text)) <pre>Nombre de caract\u00e8res dans le dataset :  50000\n</pre> <p>Affichons les 250 premiers caract\u00e8res.</p> In\u00a0[4]: Copied! <pre>print(text[:250])\n</pre> print(text[:250]) <pre>VAL\u00c8RE.\n\nEh bien, Sabine, quel conseil me donnes-tu?\n\nSABINE.\n\nVraiment, il y a bien des nouvelles. Mon oncle veut r\u00e9sol\u00fbment que ma\ncousine \u00e9pouse Villebrequin, et les affaires sont tellement avanc\u00e9es,\nque je crois qu'ils eussent \u00e9t\u00e9 mari\u00e9s d\u00e8s aujo\n</pre> <p>Regardons le nombre de caract\u00e8res diff\u00e9rents :</p> In\u00a0[5]: Copied! <pre>chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(\"Nombre de caract\u00e8res diff\u00e9rents : \", vocab_size)\n</pre> chars = sorted(list(set(text))) vocab_size = len(chars) print(''.join(chars)) print(\"Nombre de caract\u00e8res diff\u00e9rents : \", vocab_size) <pre>\n !'(),-.:;?ABCDEFGHIJLMNOPQRSTUVYabcdefghijlmnopqrstuvxyz\u00c7\u00c8\u00c9\u00e0\u00e2\u00e6\u00e7\u00e8\u00e9\u00ea\u00ee\u00ef\u00f4\u00f9\u00fb\nNombre de caract\u00e8res diff\u00e9rents :  73\n</pre> <p>Cr\u00e9ation d'un mapping de caract\u00e8re \u00e0 entiers et inversement</p> In\u00a0[6]: Copied! <pre>stoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n</pre> stoi = { ch:i for i,ch in enumerate(chars) } itos = { i:ch for i,ch in enumerate(chars) } encode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string <p>Encodons notre dataset en convertissant les string en int puis en le transformant en tenseur pytorch.</p> In\u00a0[7]: Copied! <pre>data = torch.tensor(encode(text), dtype=torch.long)\nprint(data[:250]) # Les 250 premiers caract\u00e8res encod\u00e9\n</pre> data = torch.tensor(encode(text), dtype=torch.long) print(data[:250]) # Les 250 premiers caract\u00e8res encod\u00e9 <pre>tensor([32, 12, 22, 59, 28, 16,  8,  0,  0, 16, 41,  1, 35, 42, 38, 46,  6,  1,\n        29, 34, 35, 42, 46, 38,  6,  1, 49, 53, 38, 44,  1, 36, 47, 46, 51, 38,\n        42, 44,  1, 45, 38,  1, 37, 47, 46, 46, 38, 51,  7, 52, 53, 11,  0,  0,\n        29, 12, 13, 20, 24, 16,  8,  0,  0, 32, 50, 34, 42, 45, 38, 46, 52,  6,\n         1, 42, 44,  1, 56,  1, 34,  1, 35, 42, 38, 46,  1, 37, 38, 51,  1, 46,\n        47, 53, 54, 38, 44, 44, 38, 51,  8,  1, 23, 47, 46,  1, 47, 46, 36, 44,\n        38,  1, 54, 38, 53, 52,  1, 50, 66, 51, 47, 44, 72, 45, 38, 46, 52,  1,\n        49, 53, 38,  1, 45, 34,  0, 36, 47, 53, 51, 42, 46, 38,  1, 66, 48, 47,\n        53, 51, 38,  1, 32, 42, 44, 44, 38, 35, 50, 38, 49, 53, 42, 46,  6,  1,\n        38, 52,  1, 44, 38, 51,  1, 34, 39, 39, 34, 42, 50, 38, 51,  1, 51, 47,\n        46, 52,  1, 52, 38, 44, 44, 38, 45, 38, 46, 52,  1, 34, 54, 34, 46, 36,\n        66, 38, 51,  6,  0, 49, 53, 38,  1, 43, 38,  1, 36, 50, 47, 42, 51,  1,\n        49, 53,  3, 42, 44, 51,  1, 38, 53, 51, 51, 38, 46, 52,  1, 66, 52, 66,\n         1, 45, 34, 50, 42, 66, 51,  1, 37, 65, 51,  1, 34, 53, 43, 47])\n</pre> <p>On s\u00e9pare training et test :</p> In\u00a0[8]: Copied! <pre>n = int(0.9*len(data)) # 90% pour le train et 10% pour le test\ntrain_data = data[:n]\ntest = data[n:]\n</pre> n = int(0.9*len(data)) # 90% pour le train et 10% pour le test train_data = data[:n] test = data[n:] <p>Note : Chaque it\u00e9ration de l'entra\u00eenement correspondra \u00e0 un passage dans l'int\u00e9gralit\u00e9 du dataset de mani\u00e8re s\u00e9quentielle.</p> <p>Il est maintenant temps de cr\u00e9er notre mod\u00e8le !</p> <p>Dans l'article, il est indiqu\u00e9 que l'entr\u00e9e du mod\u00e8le (le caract\u00e8re) est encod\u00e9 en one hot et qu'il est ensuite somm\u00e9 avec le state \u00e0 $t-1$. On va donc avoir besoin de deux couches fully connected, la premi\u00e8re pour transformer l'entr\u00e9e $x(t)$ en state au temps $t$, $s(t)$ et la seconde pour transformer $s(t)$ en $y(t)$, notre pr\u00e9diction.</p> <p></p> <p>Equation extraite de l'article original. $f$ est la fonction sigmoid et $g$ la softmax.</p> <p>Note : L'article est tr\u00e8s accessible et concis, je vous invite \u00e0 le lire.</p> In\u00a0[9]: Copied! <pre>class rnn(nn.Module): \n  def __init__(self,hidden_dim,vocab_size) -&gt; None:\n    super(rnn, self).__init__()\n    self.hidden_to_hidden=nn.Linear(hidden_dim+vocab_size, hidden_dim)\n    self.hidden_to_output=nn.Linear(hidden_dim, vocab_size)\n    self.vocab_size=vocab_size\n    self.hidden_dim=hidden_dim\n    self.sigmoid=nn.Sigmoid() \n    \n  # Le r\u00e9seau prend en entr\u00e9e le caract\u00e8re actuel et le state pr\u00e9c\u00e9dent\n  def forward(self, x,state):\n    # On one-hot encode le caract\u00e8re\n    x = torch.nn.functional.one_hot(x, self.vocab_size).float()\n    if state is None:\n      # Si on a pas de state (d\u00e9but de la s\u00e9quence), on initialise le state avec des petites valeurs al\u00e9atoires\n      state = torch.randn(self.hidden_dim) * 0.1\n    x = torch.cat((x, state), dim=-1)  # Concat\u00e9nation de x et du state\n    state = self.sigmoid(self.hidden_to_hidden(x)) # Calcul du nouveau state\n    output = self.hidden_to_output(state) # Calcul de l'output\n    # On renvoie l'output et le state pour le prochain pas de temps\n    return output, state.detach() # detach() pour \u00e9viter de propager le gradient dans le state\n</pre> class rnn(nn.Module):    def __init__(self,hidden_dim,vocab_size) -&gt; None:     super(rnn, self).__init__()     self.hidden_to_hidden=nn.Linear(hidden_dim+vocab_size, hidden_dim)     self.hidden_to_output=nn.Linear(hidden_dim, vocab_size)     self.vocab_size=vocab_size     self.hidden_dim=hidden_dim     self.sigmoid=nn.Sigmoid()         # Le r\u00e9seau prend en entr\u00e9e le caract\u00e8re actuel et le state pr\u00e9c\u00e9dent   def forward(self, x,state):     # On one-hot encode le caract\u00e8re     x = torch.nn.functional.one_hot(x, self.vocab_size).float()     if state is None:       # Si on a pas de state (d\u00e9but de la s\u00e9quence), on initialise le state avec des petites valeurs al\u00e9atoires       state = torch.randn(self.hidden_dim) * 0.1     x = torch.cat((x, state), dim=-1)  # Concat\u00e9nation de x et du state     state = self.sigmoid(self.hidden_to_hidden(x)) # Calcul du nouveau state     output = self.hidden_to_output(state) # Calcul de l'output     # On renvoie l'output et le state pour le prochain pas de temps     return output, state.detach() # detach() pour \u00e9viter de propager le gradient dans le state <p>D\u00e9finissons nos param\u00e8tres d'entra\u00eenement :</p> In\u00a0[10]: Copied! <pre>epochs = 10\nlr=0.1\nhidden_dim=128\nmodel=rnn(hidden_dim,vocab_size)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\n</pre> epochs = 10 lr=0.1 hidden_dim=128 model=rnn(hidden_dim,vocab_size) criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr) <p>Il est temps d'entra\u00eener le mod\u00e8le !!</p> In\u00a0[11]: Copied! <pre>for epoch in range(epochs):\n    state=None\n    running_loss = 0\n    n=0\n    for i in range(len(train_data)-1):\n        x = train_data[i]\n        y = train_data[i+1]\n        optimizer.zero_grad()\n        y_pred,state = model.forward(x,state)\n        loss = criterion(y_pred, y)\n        running_loss += loss.item()\n        n+=1\n        loss.backward()\n        optimizer.step()\n\n    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n))\n</pre> for epoch in range(epochs):     state=None     running_loss = 0     n=0     for i in range(len(train_data)-1):         x = train_data[i]         y = train_data[i+1]         optimizer.zero_grad()         y_pred,state = model.forward(x,state)         loss = criterion(y_pred, y)         running_loss += loss.item()         n+=1         loss.backward()         optimizer.step()      print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n)) <pre>Epoch: 0 \t Loss: 2.63949568\nEpoch: 1 \t Loss: 2.16456994\nEpoch: 2 \t Loss: 2.00850788\nEpoch: 3 \t Loss: 1.91673251\nEpoch: 4 \t Loss: 1.84440742\nEpoch: 5 \t Loss: 1.78986003\nEpoch: 6 \t Loss: 1.74923073\nEpoch: 7 \t Loss: 1.71709289\nEpoch: 8 \t Loss: 1.68791167\nEpoch: 9 \t Loss: 1.66215199\n</pre> <p>Testons maintenant le dataset sur nos donn\u00e9es de test :</p> In\u00a0[14]: Copied! <pre>state=None\nrunning_loss = 0\nn=0\nfor i in range(len(train_data)-1):\n    with torch.no_grad():\n        x = train_data[i]\n        y = train_data[i+1]\n        y_pred,state = model.forward(x,state)\n        loss = criterion(y_pred, y)\n        running_loss += loss.item()\n        n+=1\nprint(\"Loss: {0:.8f}\".format(running_loss/n))\n</pre> state=None running_loss = 0 n=0 for i in range(len(train_data)-1):     with torch.no_grad():         x = train_data[i]         y = train_data[i+1]         y_pred,state = model.forward(x,state)         loss = criterion(y_pred, y)         running_loss += loss.item()         n+=1 print(\"Loss: {0:.8f}\".format(running_loss/n)) <pre>Loss: 1.77312289\n</pre> <p>Le loss sur nos donn\u00e9es de test est l\u00e9gerement plus \u00e9lev\u00e9 que sur notre dataset d'entra\u00eenement. Le mod\u00e8le a l\u00e9g\u00e9rement overfit.</p> <p>Maintenant que le mod\u00e8le est entra\u00een\u00e9, on va pouvoir g\u00e9n\u00e9rer du Moli\u00e8re !!!</p> In\u00a0[15]: Copied! <pre>import torch.nn.functional as F \nmoliere='.'\nsequence_length=250\nstate=None\nfor i in range(sequence_length):\n    x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()\n    y_pred,state = model.forward(x,state)\n    probs=F.softmax(torch.squeeze(y_pred), dim=0)\n    sample=torch.multinomial(probs, 1)\n    moliere+=itos[sample.item()]\nprint(moliere)\n</pre> import torch.nn.functional as F  moliere='.' sequence_length=250 state=None for i in range(sequence_length):     x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()     y_pred,state = model.forward(x,state)     probs=F.softmax(torch.squeeze(y_pred), dim=0)     sample=torch.multinomial(probs, 1)     moliere+=itos[sample.item()] print(moliere) <pre>.\n\nVARDILE.\n\nVout on est nt, jes l'un ouint; sabhil.\n\nLE DOCTE.\n\nSi vous dicefalass\u00eentes\nGIRGIB.\n\nMARGRIIL\u00c9.\n\nLE DOCTE. Jort; et\n; bieu,\net je mu tu d'ais d'ai coupce!\n\nSG\u00c9LL\u00c9.\n\nIl Sgnous elli massit que\nSuis pluagil d\u00e9s.\nCais t\u00e9scompas: y totte demes\n</pre> <p>Ce n'est pas tr\u00e8s convaincant ... Mais on reconna\u00eet quand m\u00eame quelques mots et un agencement des phrases similaire au fichier \"moliere.txt\". Ce n'est finalement pas si mal pour un r\u00e9seau r\u00e9current d'une seule couche.</p> <p>Comment am\u00e9liorer nos r\u00e9sultats ? : Pour am\u00e9liorer les r\u00e9sultats, il y a plusieurs options possibles :</p> <ul> <li>On peut augmenter le nombre de couche r\u00e9currente ou augmenter la dimension de la couche cach\u00e9e.</li> <li>On peut utiliser un embedding plut\u00f4t qu'un one hot encoding.</li> <li>On peut utiliser d'autres variantes de RNN comme LSTM ou GRU.</li> <li>On peut utiliser une architecture transformer (oups spoiler).</li> </ul> <p>Pendant longtemps, les RNN \u00e9taient au centre de la recherche en NLP et \u00e9galement utilis\u00e9s dans d'autres domaines du deep learning. Cependant, il y certains probl\u00e8mes qui font que les RNN sont difficilement utilisables en pratique et pour des gros mod\u00e8les :</p> <ul> <li>Leur architecture permet d'avoir un contexte th\u00e9oriquement infini mais leur structure s\u00e9quentielle o\u00f9 chaque \u00e9tat d\u00e9pend du pr\u00e9c\u00e9dent rend difficile la propagation de l'information sur des longues s\u00e9quences.</li> <li>Le probl\u00e8me de vanishing gradient sur des s\u00e9quences longues ne rend pas la chose facile \u00e9galement. Plus la s\u00e9quence est longue, plus le gradient peut avoir tendance \u00e0 se dissiper.</li> <li>L'archicture s\u00e9quentielle rend la parall\u00e9lisation compliqu\u00e9e et peu efficace alors que les GPU sont justement bons pour les calculs en parall\u00e8les. L'entra\u00eenement est donc beaucoup plus long que pour un mod\u00e8le qu'on peut paralleliser efficacement.</li> <li>La structure s\u00e9quentielle fixe n'est pas forc\u00e9ment adapt\u00e9e pour capturer les relations complexes entre les donn\u00e9es.</li> </ul> <p>Aujourd'hui et depuis l'arriv\u00e9e des transformers, les RNN sont de moins en moins utilis\u00e9s dans l'ensemble des domaines du deep learning.</p>"},{"location":"05_NLP/05_Rnn.html#reseau-de-neurones-recurrents","title":"R\u00e9seau de neurones r\u00e9currents\u00b6","text":""},{"location":"05_NLP/05_Rnn.html#fonctionnement-de-larchitecture-rnn","title":"Fonctionnement de l'architecture RNN\u00b6","text":""},{"location":"05_NLP/05_Rnn.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"05_NLP/05_Rnn.html#dataset","title":"Dataset\u00b6","text":""},{"location":"05_NLP/05_Rnn.html#creation-du-modele","title":"Cr\u00e9ation du mod\u00e8le\u00b6","text":""},{"location":"05_NLP/05_Rnn.html#entrainement","title":"Entra\u00eenement\u00b6","text":""},{"location":"05_NLP/05_Rnn.html#generation","title":"G\u00e9n\u00e9ration\u00b6","text":""},{"location":"05_NLP/05_Rnn.html#le-probleme-des-rnn","title":"Le probl\u00e8me des RNN\u00b6","text":""},{"location":"05_NLP/06_Lstm.html","title":"Long Short-Term Memory","text":"<p>Dans le notebook pr\u00e9c\u00e9dent, nous avons pr\u00e9sent\u00e9 la couche classique d'un RNN. Depuis l'invention de cette couche classique, de nombreuses autres couches r\u00e9currentes ont \u00e9t\u00e9 invent\u00e9es.</p> <p>Dans ce notebook, nous pr\u00e9sentons la couche LSTM (long short-term memory) qui offre une alternative \u00e0 la couche classique RNN.</p> <p>La couche LSTM est consistitu\u00e9e d'une memory unit qui comporte 4 couches fully connected. 3 de ces 4 couches sont utilis\u00e9es pour la s\u00e9lection des informations pr\u00e9c\u00e9dentes pertinentes. Il s'agit de la forget gate, l'input gate et l'output gate :</p> <ul> <li>forget gate : Permet de supprimer de l'information de la m\u00e9moire</li> <li>input gate : Permet d'ins\u00e9rer de l'information dans la m\u00e9moire</li> <li>output gate : Permet d'utiliser l'information pr\u00e9sente dans la m\u00e9moire</li> </ul> <p>La derni\u00e8re couche fully connected va cr\u00e9er une \"information candidate\" pour l'insertion dans la m\u00e9moire de la couche LSTM.</p> <p></p> <p>Figure extraite du blogpost.</p> <p>Comme on le voit sur la figure, la couche LSTM re\u00e7oit 3 vecteurs en entr\u00e9e $H_{t-1}$, $C_{t-1}$ et $X_{t}$. Les deux premiers viennent du LSTM directement et le troisi\u00e8me correspond \u00e0 l'entr\u00e9e au temps $t$ (le caract\u00e8re pour nous).</p> <p>Pour expliquer le concept sans entrer dans les d\u00e9tails : $H_{t-1}$ contient la m\u00e9moire \u00e0 court terme (short term) et $C_{t-1}$ la m\u00e9moire \u00e0 long terme. Cela permet de conserver les informations importantes sur un contexte important tout en ne n\u00e9gligeant pas le contexte plus local.</p> <p>Le but de cette architecture est de pallier le probl\u00e8me de la propagation de l'information sur de longues s\u00e9quences rencontr\u00e9 dans les RNN classiques.</p> <p>Pour comprendre la couche LSTM en d\u00e9tail, vous pouvez lire l'article ou consulter le blogpost.</p> In\u00a0[21]: Copied! <pre>import torch\nimport torch.nn as nn\n</pre> import torch import torch.nn as nn <p>Pour la cr\u00e9ation du dataset, nous utilisons \u00e0 nouveau le fichier moliere.txt et nous reprenons le code du notebook pr\u00e9c\u00e9dent.</p> In\u00a0[22]: Copied! <pre>with open('moliere.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nprint(\"Nombre de caract\u00e8res dans le dataset : \", len(text))\n</pre> with open('moliere.txt', 'r', encoding='utf-8') as f:     text = f.read() print(\"Nombre de caract\u00e8res dans le dataset : \", len(text)) <pre>Nombre de caract\u00e8res dans le dataset :  1687290\n</pre> <p>On r\u00e9duit le nombre d'\u00e9l\u00e9ments pour avoir un entra\u00eenement rapide (\u00e0 commenter si vous voulez entra\u00eener sur l'ensemble des donn\u00e9es).</p> In\u00a0[23]: Copied! <pre>text=text[:100000]\nprint(\"Nombre de caract\u00e8res dans le dataset : \", len(text))\n</pre> text=text[:100000] print(\"Nombre de caract\u00e8res dans le dataset : \", len(text)) <pre>Nombre de caract\u00e8res dans le dataset :  100000\n</pre> In\u00a0[24]: Copied! <pre>chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(\"Nombre de caract\u00e8res diff\u00e9rents : \", vocab_size)\n</pre> chars = sorted(list(set(text))) vocab_size = len(chars) print(''.join(chars)) print(\"Nombre de caract\u00e8res diff\u00e9rents : \", vocab_size) <pre>\n !'(),-.:;?ABCDEFGHIJLMNOPQRSTUVYabcdefghijlmnopqrstuvxyz\u00ab\u00bb\u00c7\u00c8\u00c9\u00ca\u00e0\u00e2\u00e6\u00e7\u00e8\u00e9\u00ea\u00ee\u00ef\u00f4\u00f9\u00fb\nNombre de caract\u00e8res diff\u00e9rents :  76\n</pre> In\u00a0[25]: Copied! <pre>stoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n</pre> stoi = { ch:i for i,ch in enumerate(chars) } itos = { i:ch for i,ch in enumerate(chars) } encode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string  data = torch.tensor(encode(text), dtype=torch.long) <p>S\u00e9paration en train et test.</p> In\u00a0[26]: Copied! <pre>n = int(0.9*len(data)) # 90% pour le train et 10% pour le test\ntrain_data = data[:n]\ntest = data[n:]\n</pre> n = int(0.9*len(data)) # 90% pour le train et 10% pour le test train_data = data[:n] test = data[n:] <p>Pour cr\u00e9er notre mod\u00e8le, nous allons utiliser directement l'impl\u00e9mentation pytorch de la couche du LSTM. A l'inverse des couches lin\u00e9aires ou convolutives, la couche nn.LSTM permet de stacker plusieurs couches gr\u00e2ce au param\u00e8tre num_layers. Si on veut d\u00e9finir les couches une par une, il faut utiliser nn.LSTMCell.</p> In\u00a0[27]: Copied! <pre>class lstm(nn.Module):\n    def __init__(self, vocab_size, hidden_size,num_layers=1):\n        super(lstm, self).__init__()\n        self.hidden_size = hidden_size\n        # On utilise un embedding pour transformer les entiers(caract\u00e8res) en vecteurs\n        self.embedding = nn.Embedding(vocab_size, hidden_size)\n        # La couche LSTM peut prendre l'argument num_layers pour empiler plusieurs couches LSTM\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n        # Une derni\u00e8re couche lin\u00e9aire pour pr\u00e9dire le prochain caract\u00e8re\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x, hidden):\n        x = self.embedding(x)\n        x, hidden = self.lstm(x, hidden)\n        x = self.fc(x)\n        return x, (hidden[0].detach(), hidden[1].detach())\n\n    def init_hidden(self, batch_size):\n        return (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))\n</pre> class lstm(nn.Module):     def __init__(self, vocab_size, hidden_size,num_layers=1):         super(lstm, self).__init__()         self.hidden_size = hidden_size         # On utilise un embedding pour transformer les entiers(caract\u00e8res) en vecteurs         self.embedding = nn.Embedding(vocab_size, hidden_size)         # La couche LSTM peut prendre l'argument num_layers pour empiler plusieurs couches LSTM         self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)         # Une derni\u00e8re couche lin\u00e9aire pour pr\u00e9dire le prochain caract\u00e8re         self.fc = nn.Linear(hidden_size, vocab_size)              def forward(self, x, hidden):         x = self.embedding(x)         x, hidden = self.lstm(x, hidden)         x = self.fc(x)         return x, (hidden[0].detach(), hidden[1].detach())      def init_hidden(self, batch_size):         return (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size)) In\u00a0[57]: Copied! <pre>epochs = 20\nlr=0.001\nhidden_dim=128\nseq_len=100\nnum_layers=1\nmodel=lstm(vocab_size,hidden_dim,num_layers)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n</pre> epochs = 20 lr=0.001 hidden_dim=128 seq_len=100 num_layers=1 model=lstm(vocab_size,hidden_dim,num_layers) criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.AdamW(model.parameters(), lr=lr) <p>La couche LSTM prend en entr\u00e9e directement une s\u00e9quence et renvoie une s\u00e9quence de la m\u00eame taille. Cela permet d'acc\u00e9lerer l'entra\u00eenement car on peut tra\u00eeter plusieurs exemples en m\u00eame temps.</p> <p>Note : Il est aussi possible d'acc\u00e9lerer l'entra\u00eenement en faisant un tra\u00eetement en batch avec plusieurs s\u00e9quences en parall\u00e8le.</p> In\u00a0[58]: Copied! <pre>for epoch in range(epochs):\n    state=None\n    running_loss = 0\n    n=0\n    data_ptr = torch.randint(100,(1,1)).item()\n    # On train sur des s\u00e9quences de seq_len caract\u00e8res et on break si on d\u00e9passe la taille du dataset\n    while True:\n        x = train_data[data_ptr : data_ptr+seq_len]\n        y = train_data[data_ptr+1 : data_ptr+seq_len+1]\n        optimizer.zero_grad()\n        y_pred,state = model.forward(x,state)\n        loss = criterion(y_pred, y)\n        running_loss += loss.item()\n        n+=1\n        loss.backward()\n        optimizer.step()\n        data_ptr+=seq_len\n        # Pour \u00e9viter de sortir de l'index du dataset\n        if data_ptr + seq_len + 1 &gt; len(train_data):\n            break\n    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n))\n</pre> for epoch in range(epochs):     state=None     running_loss = 0     n=0     data_ptr = torch.randint(100,(1,1)).item()     # On train sur des s\u00e9quences de seq_len caract\u00e8res et on break si on d\u00e9passe la taille du dataset     while True:         x = train_data[data_ptr : data_ptr+seq_len]         y = train_data[data_ptr+1 : data_ptr+seq_len+1]         optimizer.zero_grad()         y_pred,state = model.forward(x,state)         loss = criterion(y_pred, y)         running_loss += loss.item()         n+=1         loss.backward()         optimizer.step()         data_ptr+=seq_len         # Pour \u00e9viter de sortir de l'index du dataset         if data_ptr + seq_len + 1 &gt; len(train_data):             break     print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n)) <pre>Epoch: 0 \t Loss: 2.17804336\nEpoch: 1 \t Loss: 1.76270216\nEpoch: 2 \t Loss: 1.62740668\nEpoch: 3 \t Loss: 1.54147145\nEpoch: 4 \t Loss: 1.47995140\nEpoch: 5 \t Loss: 1.43100239\nEpoch: 6 \t Loss: 1.39074463\nEpoch: 7 \t Loss: 1.35526441\nEpoch: 8 \t Loss: 1.32519794\nEpoch: 9 \t Loss: 1.29712536\nEpoch: 10 \t Loss: 1.27268774\nEpoch: 11 \t Loss: 1.24876227\nEpoch: 12 \t Loss: 1.22720749\nEpoch: 13 \t Loss: 1.20663312\nEpoch: 14 \t Loss: 1.18768359\nEpoch: 15 \t Loss: 1.16936996\nEpoch: 16 \t Loss: 1.15179397\nEpoch: 17 \t Loss: 1.13514291\nEpoch: 18 \t Loss: 1.11997525\nEpoch: 19 \t Loss: 1.10359089\n</pre> <p>On peut maintenant \u00e9valuer le loss sur nos donn\u00e9es de test.</p> In\u00a0[59]: Copied! <pre>state=None\nrunning_loss = 0\nn=0\ndata_ptr = torch.randint(100,(1,1)).item()\nwhile True:\n    with torch.no_grad():\n        x = test[data_ptr : data_ptr+seq_len]\n        y = test[data_ptr+1 : data_ptr+seq_len+1]\n        y_pred,state = model.forward(x,state)\n        loss = criterion(y_pred, y)\n    running_loss += loss.item()\n    n+=1\n    data_ptr+=seq_len\n    if data_ptr + seq_len + 1 &gt; len(test):\n        break\nprint(\"Loss de test: {0:.8f}\".format(running_loss/n))\n</pre> state=None running_loss = 0 n=0 data_ptr = torch.randint(100,(1,1)).item() while True:     with torch.no_grad():         x = test[data_ptr : data_ptr+seq_len]         y = test[data_ptr+1 : data_ptr+seq_len+1]         y_pred,state = model.forward(x,state)         loss = criterion(y_pred, y)     running_loss += loss.item()     n+=1     data_ptr+=seq_len     if data_ptr + seq_len + 1 &gt; len(test):         break print(\"Loss de test: {0:.8f}\".format(running_loss/n)) <pre>Loss de test: 1.51168611\n</pre> <p>Le mod\u00e8le overfit pas mal ... Essayez de corriger \u00e7a par vous-m\u00eame.</p> <p>On va maintenant pouvoir test la g\u00e9n\u00e9ration de texte !</p> In\u00a0[62]: Copied! <pre>import torch.nn.functional as F \nmoliere='.'\nsequence_length=250\nstate=None\nfor i in range(sequence_length):\n    x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()\n    y_pred,state = model.forward(x.unsqueeze(0),state)\n    probs=F.softmax(torch.squeeze(y_pred), dim=0)\n    sample=torch.multinomial(probs, 1)\n    moliere+=itos[sample.item()]\nprint(moliere)\n</pre> import torch.nn.functional as F  moliere='.' sequence_length=250 state=None for i in range(sequence_length):     x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()     y_pred,state = model.forward(x.unsqueeze(0),state)     probs=F.softmax(torch.squeeze(y_pred), dim=0)     sample=torch.multinomial(probs, 1)     moliere+=itos[sample.item()] print(moliere) <pre>.\n\u00c7\u00e0 coeuse, et bon enfin l'avoir faire.\n\nMASCARILLE.\n\nEn me donner d vous, Le pas.\n\nMASCARILLE, \u00e0 dans un pour s\u00fbte matinix! cette ma foi.\n\nPANDOLFE.\n\nMa foi, tu te le sy sois touves d'arr\u00eate sa bien sans les bonheur.\n\nMASCARILLE.\n\nMoi, je me suis to\n</pre> <p>La g\u00e9n\u00e9ration est peut-\u00eatre un peu mieux que pour le mod\u00e8le RNN de base mais ce n'est pas encore convaincant. Vous pouvez essayer d'am\u00e9liorer les performances du mod\u00e8le en faisant varier les param\u00e8tres (nombre de couches en s\u00e9rie, hidden dim etc ...)</p>"},{"location":"05_NLP/06_Lstm.html#long-short-term-memory","title":"Long Short-Term Memory\u00b6","text":""},{"location":"05_NLP/06_Lstm.html#quest-ce-quune-couche-lstm","title":"Qu'est ce qu'une couche LSTM ?\u00b6","text":""},{"location":"05_NLP/06_Lstm.html#implementation-pytorch","title":"Impl\u00e9mentation pytorch\u00b6","text":""},{"location":"05_NLP/06_Lstm.html#dataset","title":"Dataset\u00b6","text":""},{"location":"05_NLP/06_Lstm.html#creation-du-modele","title":"Cr\u00e9ation du mod\u00e8le\u00b6","text":""},{"location":"05_NLP/06_Lstm.html#entrainement","title":"Entra\u00eenement\u00b6","text":""},{"location":"05_NLP/06_Lstm.html#generation","title":"G\u00e9n\u00e9ration\u00b6","text":""},{"location":"06_HuggingFace/index.html","title":"\ud83e\udd17 Hugging Face \ud83e\udd17","text":"<p>Ce cours est d\u00e9di\u00e9 \u00e0 une exploration des librarys, des mod\u00e8les, des datasets et autres de Hugging Face. C'est une plateforme regroupant \u00e9normement des mod\u00e8les open source pour une grande vari\u00e9t\u00e9 de t\u00e2ches avec une library pour les impl\u00e9menter rapidement et efficacement en python. Le cours pr\u00e9sente d'abord le site de Hugging Face pour ensuite pr\u00e9senter les fonctionnalit\u00e9s des diff\u00e9rentes librarys (transformers et diffusers principalement) sur diff\u00e9rents cas d'usage. Le dernier notebook pr\u00e9sente bri\u00e8vement gradio, une library pour cr\u00e9er des interfaces simples de d\u00e9mo.</p>"},{"location":"06_HuggingFace/index.html#notebook-1-introduction","title":"Notebook 1\ufe0f\u20e3 : Introduction","text":"<p>Ce notebook introduit la library et le site HuggingFace. En particulier, le site est d\u00e9crit pour permettre de facilement rep\u00e9rer les informations importantes et trouver les informations importante.</p>"},{"location":"06_HuggingFace/index.html#notebook-2-computer-vision-with-transformers","title":"Notebook 2\ufe0f\u20e3 : Computer vision with transformers","text":"<p>Ce notebook montre les possibilit\u00e9s de r\u00e9solution de probl\u00e8mes de computer vision avec la library HuggingFace. En particulier, les probl\u00e8mes de zero-shot object detection, image captionning, zero-shot image classification, segmentation d'images et estimation de profondeur sont pr\u00e9sent\u00e9s.</p>"},{"location":"06_HuggingFace/index.html#notebook-3-nlp-with-transformers","title":"Notebook 3\ufe0f\u20e3 : NLP with transformers","text":"<p>Ce notebook montre les possibilit\u00e9s de r\u00e9solution de probl\u00e8mes de NLP avec la library HuggingFace. En particulier, les probl\u00e8mes de chatbot, traduction, r\u00e9sum\u00e9 de texte et embedding de phrase sont abord\u00e9s.</p>"},{"location":"06_HuggingFace/index.html#notebook-4-audio-with-transformers","title":"Notebook 4\ufe0f\u20e3 : Audio with transformers","text":"<p>Ce notebook montre les possibilit\u00e9s de r\u00e9solution de probl\u00e8mes d'audio avec la library HuggingFace. En particulier, les probl\u00e8mes de zero-shot classification, reconnaissance automatique de la parole et transformation de texte en dialogue sont pr\u00e9sent\u00e9s.</p>"},{"location":"06_HuggingFace/index.html#notebook-5-image-generation-with-diffusers","title":"Notebook 5\ufe0f\u20e3 : Image generation with diffusers","text":"<p>Ce notebook pr\u00e9sente la library diffusers de hugging face pour la g\u00e9n\u00e9ration d'images avec des mod\u00e8les de diffusion.</p>"},{"location":"06_HuggingFace/index.html#notebook-6-demo-avec-gradio","title":"Notebook 6\ufe0f\u20e3 : Demo avec gradio","text":"<p>Ce notebook pr\u00e9sente la library gradio qui permet de cr\u00e9er simplement des interfaces de d\u00e9monstration.</p>"},{"location":"06_HuggingFace/01_introduction.html","title":"Hugging Face Library Introduction","text":"<p>Hugging Face est une entreprise qui a un r\u00f4le tr\u00e8s important dans la communaut\u00e9 open source en intelligence artificiel. C'est d'une part un site qui regroupe des datasets, des mod\u00e8les et des \"spaces\" et d'autre part plusieurs librarys (transformers, diffusers et datasets principalement). </p> <p>Pourquoi utiliser Hugging Face ? Hugging Face permet d'avoir acc\u00e8s \u00e0 des mod\u00e8les complexes de l'\u00e9tat de l'art de mani\u00e8re simple et de pouvoir utiliser des mod\u00e8les complexes rapidement et efficacement. Il est aussi possible de tester des mod\u00e8les via la cat\u00e9gorie \"spaces\" de leur site et de t\u00e9l\u00e9charger des datasets de la communaut\u00e9. C'est un peu le \"github\" du deep learning.</p> <p>Ce cours est moins th\u00e9orique que les pr\u00e9c\u00e9dents mais va permettre de toucher \u00e0 des mod\u00e8les tr\u00e8s performants et de voir les capacit\u00e9s des mod\u00e8les de deep learning actuels. Ce cours s'inspire des ressources disponbles sur le site de Hugging Face et du cours gratuit sur deeplearning.ai intitul\u00e9 \"Open Source Models with Hugging Face\". Je vous invite \u00e0 consulter ce cours qui montre de nombreuses applications en vision, audio et NLP.</p> <p>Le plan de ce cours est le suivant :</p> <ul> <li>Dans l'introduction (ici), nous pr\u00e9sentons le site de Hugging Face avec les 3 principales cat\u00e9gories : Models, Datasets et Spaces.</li> <li>Les 3 notebooks suivants sont d\u00e9di\u00e9s \u00e0 l'utilisation de la library transformers : le premier tra\u00eete des mod\u00e8les de vision, le second du NLP et le troisi\u00e8me de l'audio.</li> <li>Apr\u00e8s cela, un notebook sera dedi\u00e9 \u00e0 la library diffusers qui permet d'utiliser des mod\u00e8les de diffusion (Stable diffusion par exemple) pour g\u00e9nerer des images.</li> <li>Le dernier notebook pr\u00e9sente gradio, une library permettant de faire des interfaces rapidement pour des d\u00e9mos par exemple.</li> </ul> <p>La cat\u00e9gorie la plus ludique et la plus facile d'acc\u00e8s est la cat\u00e9gorie spaces qui regroupe des d\u00e9mos de diff\u00e9rents mod\u00e8les.</p> <p>La page d'accueil se pr\u00e9sente comme cela :</p> <p></p> <p>Les d\u00e9mos (spaces) sont class\u00e9 par utilisation et par \"trending\" mais vous pouvez utiliser la fonction \"Search spaces\" pour chercher un mod\u00e8le particulier que vous souhaiteriez tester. Je vous conseille \u00e9galement de parcourir r\u00e9gulierement les diff\u00e9rents spaces, cela permet de se tenir au courant des nouveaut\u00e9s dans le domaine du deep learning.</p> <p>Si vous entra\u00eenez vous-m\u00eame un mod\u00e8le, vous pouvez ensuite le partager gratuitement dans un space via l'outil de cr\u00e9ation \"Create New Space\". Pour cela, il faut avoir les bases de la library gradio qui nous traiterons dans le dernier notebook de ce cours.</p> <p>Parfois, un mod\u00e8le que vous voudriez tester n'est pas disponible dans les spaces ou alors il est disponible mais vous voulez l'utiliser dans votre propre code. Pour cela, il faudra passer par la cat\u00e9gorie Models du site. La page Models regroupe \u00e9normement de mod\u00e8les open-source.</p> <p>Voici \u00e0 quoi ressemble la page :</p> <p></p> <p>Il y beaucoup d'information sur cette page. D'une part, vous pouvez chercher un mod\u00e8le particulier dont vous connaissez le nom via \"Filter by name\". Vous pouvez \u00e9galement utiliser les filtres \u00e0 gauche pour une recherche de mod\u00e8les par cat\u00e9gorie.</p> <p>Par exemple, imaginons que je cherche un mod\u00e8le de \"zero-shot object detection\", ce qui permet de d\u00e9tecter n'importe quel objet sur l'image \u00e0 partir d'un prompt.</p> <p>***Prompt, qu'est ce que c'est ?** : Un *prompt est une entr\u00e9e qui permet de guider le mod\u00e8le dans sa t\u00e2che. En NLP, un prompt va simplement \u00eatre l'entr\u00e9e du mod\u00e8le, c'est-\u00e0-dire le texte saisi par l'utilisateur. Un prompt peut \u00e9galement \u00eatre une coordon\u00e9e sur une image (pour une t\u00e2che de segmentation) ou m\u00eame une image ou encore une vid\u00e9o. La plupart du temps, lorsqu'on fait r\u00e9f\u00e9rence \u00e0 prompt, c'est pour parler d'un ajout de texte en input du mod\u00e8le. Dans le cas du \"zero-shot object detection\", si je cherche \u00e0 d\u00e9tecter les bananes sur un image, l'entr\u00e9e de mon mod\u00e8le sera l'image et le texte \"banana\".</p> <p>Pour chercher un mod\u00e8le qui pourrait me convenir, j'utilise le filtrage de gauche en s\u00e9l\u00e9ctionnant la cat\u00e9gorie zero-shot object detection et je consulte les mod\u00e8les propos\u00e9s.</p> <p></p> <p>Le mod\u00e8le \"IDEA-Research/grounding-dino-tiny\" me parait bien, je le selectionne et j'arrive sur la page suivante :</p> <p></p> <p>La model card donne une description pr\u00e9cise du mod\u00e8le (son fonctionnement, ses capacit\u00e9s etc ...). Pour obtenir le code permettant de l'utiliser directement en python (via la library transformers ou diffusers en fonction du mod\u00e8le choisi), vous pouvez utiliser le bouton Use this model.</p> <p>Pour le mod\u00e8le choisi, on aura le bout de code suivant :</p> <p></p> <p>Dans les notebooks suivants, nous verrons comment utiliser ce code pour notre t\u00e2che.</p> <p>Si vous souhaitez entra\u00eener votre propre mod\u00e8le, il est n\u00e9cessaire d'avoir un dataset. Vous pouvez choisir de le cr\u00e9er vous-m\u00eame mais il existe beaucoup de datasets open-source que l'on peut notamment trouver sur Hugging Face.</p> <p>La page Datasets se pr\u00e9sente comme la page Models avec les fonctions de filtrage et la fonction de recherche :</p> <p></p> <p>Vous pouvez s\u00e9lectionner un dataset et tomber sur la page suivante :</p> <p></p> <p>Comme pour les mod\u00e8les, la Dataset card permet de visualiser les donn\u00e9es du dataset et d'avoir une description de celui-ci. Pour l'utiliser directement en python, vous pouvez utiliser Use in Datasets library pour obtenir le code python correspondant.</p> <p>Le site est plus complet que \u00e7a mais cette introduction ne vise pas \u00e0 \u00eatre exhaustive. Je vous invite \u00e0 naviguer par vous-m\u00eame sur le site pour trouver des choses qui vous interessent. Dans les notebooks suivants, nous pr\u00e9senterons une vue d'ensemble des types de mod\u00e8les disponibles et leur utilisation en python via Hugging Face.</p>"},{"location":"06_HuggingFace/01_introduction.html#hugging-face-library-introduction","title":"Hugging Face Library Introduction\u00b6","text":""},{"location":"06_HuggingFace/01_introduction.html#quest-ce-que-cest","title":"Qu'est ce que c'est ?\u00b6","text":""},{"location":"06_HuggingFace/01_introduction.html#quest-ce-quon-va-apprendre-dans-ce-cours","title":"Qu'est ce qu'on va apprendre dans ce cours ?\u00b6","text":""},{"location":"06_HuggingFace/01_introduction.html#site-de-hugging-face","title":"Site de Hugging Face\u00b6","text":""},{"location":"06_HuggingFace/01_introduction.html#spaces","title":"Spaces\u00b6","text":""},{"location":"06_HuggingFace/01_introduction.html#models","title":"Models\u00b6","text":""},{"location":"06_HuggingFace/01_introduction.html#datasets","title":"Datasets\u00b6","text":""},{"location":"06_HuggingFace/01_introduction.html#autres-categories","title":"Autres cat\u00e9gories\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html","title":"Computer Vision with Transformers","text":"<p>Dans ce notebook, nous allons utiliser la library transformers de Hugging Face pour tra\u00eeter des images. Pour \u00e9viter de surchager le notebook, certaines fonctions se trouvent dans utils/util.py.</p> <p>La d\u00e9tection d'objets dans une image est une t\u00e2che importante de vision par ordinateur. Les mod\u00e8les zero-shot sont particuli\u00e8rement versatiles pour cette t\u00e2che car ils peuvent d\u00e9tecter n'importe quel objet dans une image sans avoir besoin d'un fine-tuning. Il suffit de donner une image entr\u00e9e ainsi qu'un prompt textuel qui sp\u00e9cifie les classes que l'on veut d\u00e9tecter.</p> <p>Nous avons choisi le mod\u00e8le OWL-ViT de google (google/owlvit-base-patch32) pour cette t\u00e2che car c'est un mod\u00e8le relativement petit et il pourra tourner sur la plupart des ordinateurs. Utilisons le pipeline de Hugging Face :</p> In\u00a0[1]: Copied! <pre>from transformers import pipeline\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nzeroshot = pipeline(\"zero-shot-object-detection\", model=\"google/owlvit-base-patch32\")\n</pre> from transformers import pipeline from PIL import Image import cv2 import numpy as np import matplotlib.pyplot as plt  zeroshot = pipeline(\"zero-shot-object-detection\", model=\"google/owlvit-base-patch32\") <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Regardons \u00e0 quoi ressemble notre image.</p> In\u00a0[119]: Copied! <pre>image=Image.open(\"images/coco.jpg\") # Image extraite de la base de donn\u00e9es COCO (https://cocodataset.org/#home)\n\nplt.imshow(image)\nplt.axis('off')  \nplt.show()\n</pre> image=Image.open(\"images/coco.jpg\") # Image extraite de la base de donn\u00e9es COCO (https://cocodataset.org/#home)  plt.imshow(image) plt.axis('off')   plt.show() <p>Utilisons le mod\u00e8le et dessinons les box pr\u00e9dites.</p> In\u00a0[120]: Copied! <pre>from utils.util import draw_box\n\ntext_prompt = \"surfboard\" # Vous pouvez changer la classe pour d\u00e9tecter autre chose \"person\" ou \"surfboard\"\noutput = zeroshot(image,candidate_labels = [text_prompt])\ncv_image=draw_box(image,output)\n\nplt.imshow(cv_image)\nplt.axis('off') \nplt.show()\n</pre> from utils.util import draw_box  text_prompt = \"surfboard\" # Vous pouvez changer la classe pour d\u00e9tecter autre chose \"person\" ou \"surfboard\" output = zeroshot(image,candidate_labels = [text_prompt]) cv_image=draw_box(image,output)  plt.imshow(cv_image) plt.axis('off')  plt.show() <p>Vous savez maintenant comment impl\u00e9menter un d\u00e9tecteur d'objet zero-shot en quelques lignes de code.</p> <p>La t\u00e2che d'image captionning consiste \u00e0 cr\u00e9er une description d'une image. Le mod\u00e8le prend en entr\u00e9e une image et va cr\u00e9er ou compl\u00e9ter une description.</p> <p>De la m\u00eame mani\u00e8re, on utilise le pipeline de Hugging Face pour charger notre model. Ici, nous utilisons le mod\u00e8le BLIP de salesforce (Salesforce/blip-image-captioning-base).</p> In\u00a0[121]: Copied! <pre>captionner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n</pre> captionner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\") <p>On va utiliser la m\u00eame image pour g\u00e9n\u00e9rer une description.</p> In\u00a0[122]: Copied! <pre>result=captionner(image)\nprint(result[0]['generated_text'])\n</pre> result=captionner(image) print(result[0]['generated_text']) <pre>a man holding a surfboard in a room\n</pre> <p>Nous avons g\u00e9n\u00e9r\u00e9 le texte \"un homme qui tient une planche de surf dans une pi\u00e8ce\" ce qui est parfaitement exact. Vous savez maintenant g\u00e9n\u00e9rer des descriptions d'images. Cela peut \u00eatre tr\u00e8s utile pour g\u00e9n\u00e9rer automatiquement des datasets par exemple.</p> <p>En plus du zero-shot object detection, on peut aussi faire du zero-shot image classification. Le principe de fonctionnement est un peu le m\u00eame sauf que cette fois, on va saisir au minimum deux phrases et le mod\u00e8le va nous renvoyer la probabilit\u00e9 que l'image corresponde \u00e0 une phrase plut\u00f4t qu'une autre.</p> <p>Utilisons une photo de mon chat pour d\u00e9terminer sa race :</p> In\u00a0[123]: Copied! <pre>image=Image.open(\"images/tigrou.png\") # Image extraite de la base de donn\u00e9es COCO (https://cocodataset.org/#home)\n\nplt.imshow(image)\nplt.axis('off')  \nplt.show()\n</pre> image=Image.open(\"images/tigrou.png\") # Image extraite de la base de donn\u00e9es COCO (https://cocodataset.org/#home)  plt.imshow(image) plt.axis('off')   plt.show() <p>On va essayer de voir si le mod\u00e8le est capable de d\u00e9terminer si il s'agit d'un Maine Coon ou d'un chat europ\u00e9en.</p> <p>Nous utilisons le mod\u00e8le CLIP de OpenAI (openai/clip-vit-base-patch32). Pour changer, utilisons d'autres fonctions de la library Hugging Face \u00e0 la place du pipeline.</p> In\u00a0[125]: Copied! <pre>from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n\nprocessor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-base-patch32\")\n</pre> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification  processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\") model = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-base-patch32\") <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n</pre> In\u00a0[133]: Copied! <pre>labels = [\"a photo of a european shorthair\", \"a photo of maine coon\"]\ninputs = processor(text=labels,images=image,return_tensors=\"pt\",padding=True)\noutputs = model(**inputs)\n\n# Transformation des outputs pour obtenir des probabilit\u00e9s\nprint(\"Probabilit\u00e9 de a photo of a european shorthair : \",outputs.logits_per_image.softmax(dim=1)[0][0].item())\nprint(\"Probabilit\u00e9 de a photo of maine coon : \",outputs.logits_per_image.softmax(dim=1)[0][1].item())\n</pre> labels = [\"a photo of a european shorthair\", \"a photo of maine coon\"] inputs = processor(text=labels,images=image,return_tensors=\"pt\",padding=True) outputs = model(**inputs)  # Transformation des outputs pour obtenir des probabilit\u00e9s print(\"Probabilit\u00e9 de a photo of a european shorthair : \",outputs.logits_per_image.softmax(dim=1)[0][0].item()) print(\"Probabilit\u00e9 de a photo of maine coon : \",outputs.logits_per_image.softmax(dim=1)[0][1].item()) <pre>Probabilit\u00e9 de a photo of a european shorthair :  0.9104425311088562\nProbabilit\u00e9 de a photo of maine coon :  0.08955750614404678\n</pre> <p>Le mod\u00e8le est plut\u00f4t confiant sur le fait qu'il s'agit d'un chat europ\u00e9en et comme vous l'avez sans doute devin\u00e9, il a raison.</p> <p>Pour cet exemple, nous allons faire de la segmentation d'image \u00e0 partir du mod\u00e8le SAM de meta qui permet de segmenter n'importe quel objet.</p> In\u00a0[2]: Copied! <pre>sam = pipeline(\"mask-generation\",\"Zigeng/SlimSAM-uniform-77\")\n</pre> sam = pipeline(\"mask-generation\",\"Zigeng/SlimSAM-uniform-77\") In\u00a0[3]: Copied! <pre>image=Image.open(\"images/coco.jpg\") # Image extraite de la base de donn\u00e9es COCO (https://cocodataset.org/#home)\n\nplt.imshow(image)\nplt.axis('off')  \nplt.show()\n</pre> image=Image.open(\"images/coco.jpg\") # Image extraite de la base de donn\u00e9es COCO (https://cocodataset.org/#home)  plt.imshow(image) plt.axis('off')   plt.show() In\u00a0[8]: Copied! <pre># ATTENTION : le traitement peut prendre plusieurs minutes\noutput=sam(image, points_per_batch=32)\n</pre> # ATTENTION : le traitement peut prendre plusieurs minutes output=sam(image, points_per_batch=32) In\u00a0[5]: Copied! <pre>masks=output[\"masks\"]\n</pre> masks=output[\"masks\"] In\u00a0[7]: Copied! <pre>from utils.util import draw_masks\nimage_np=draw_masks(image,masks)\n\nplt.imshow(image_np)\nplt.axis('off') \nplt.show()\n</pre> from utils.util import draw_masks image_np=draw_masks(image,masks)  plt.imshow(image_np) plt.axis('off')  plt.show() <p>Comme vous le voyez, on a segment\u00e9 l'ensemble des objets de l'image. Par contre le temps de traitement \u00e9tait vraiment important... Pour avoir un temps d'inf\u00e9rence plus raisonnable, utilisons un prompt de coordonn\u00e9es d'un point de l'image, cela permet de sp\u00e9cifier le traitement et d'avoir un r\u00e9sultat plus rapidement. Nous ne pouvons pas utiliser le pipeline pour cette t\u00e2che.</p> In\u00a0[9]: Copied! <pre>from transformers import SamModel, SamProcessor\n</pre> from transformers import SamModel, SamProcessor In\u00a0[10]: Copied! <pre>model = SamModel.from_pretrained(\"Zigeng/SlimSAM-uniform-77\")\n\nprocessor = SamProcessor.from_pretrained(\"Zigeng/SlimSAM-uniform-77\")\n</pre> model = SamModel.from_pretrained(\"Zigeng/SlimSAM-uniform-77\")  processor = SamProcessor.from_pretrained(\"Zigeng/SlimSAM-uniform-77\") <p>Cr\u00e9eons notre prompt de coordonn\u00e9es et visualisons le point :</p> In\u00a0[41]: Copied! <pre>input_points = [[[300, 200]]]\nimage_np= np.array(image)\ncv2.circle(image_np,input_points[0][0],radius=3,color=(255,0,0),thickness=5)\nplt.imshow(image_np)\nplt.axis('off')\nplt.show()\n</pre> input_points = [[[300, 200]]] image_np= np.array(image) cv2.circle(image_np,input_points[0][0],radius=3,color=(255,0,0),thickness=5) plt.imshow(image_np) plt.axis('off') plt.show() In\u00a0[44]: Copied! <pre>inputs = processor(image,input_points=input_points,return_tensors=\"pt\")\noutputs = model(**inputs)\npredicted_masks = processor.image_processor.post_process_masks(\n  outputs.pred_masks,\n  inputs[\"original_sizes\"],\n  inputs[\"reshaped_input_sizes\"]\n)\n</pre> inputs = processor(image,input_points=input_points,return_tensors=\"pt\") outputs = model(**inputs) predicted_masks = processor.image_processor.post_process_masks(   outputs.pred_masks,   inputs[\"original_sizes\"],   inputs[\"reshaped_input_sizes\"] ) <p>Le traitement est beaucoup plus rapide !! SAM produit 3 masques par d\u00e9fauts, chaque masque est une possibilit\u00e9 de masking de l'image. Vous pouvez changer la valeur mask_number pour visualiser les diff\u00e9rents masques.</p> In\u00a0[46]: Copied! <pre>mask_number=2 # 0,1 or 2\nmask=predicted_masks[0][:, mask_number] \nimage_np=draw_masks(image,mask)\nplt.imshow(image_np)\nplt.axis('off') \nplt.show()\n</pre> mask_number=2 # 0,1 or 2 mask=predicted_masks[0][:, mask_number]  image_np=draw_masks(image,mask) plt.imshow(image_np) plt.axis('off')  plt.show() <p>Dans cet exemple, on voit que les 3 masques sont pertinents : le premier segmente la personne en entier, le second segmente les vetements et le troisi\u00e8me segmente uniquement le t-shirt. Vous pouvez essayer de changer les coordonn\u00e9es du point et de visualiser les masques g\u00e9n\u00e9r\u00e9s.</p> <p>Une t\u00e2che importante de la vision par ordinateur est l'estimation de la profondeur. C'est tr\u00e8s utile pour des cas d'usage comme la voiture autonome o\u00f9 l'on cherche \u00e0 estimer la distance par rapport au v\u00e9hicule devant nous. Pour l'industrie, c'est aussi quelque chose de tr\u00e8s int\u00e9ressant permettant d'organiser les objets dans un colis en fonction de l'espace restant par exemple. Pour cet exemple, nous utilisons le mod\u00e8le DPT (Intel/dpt-hybrid-midas) qui prend une image en entr\u00e9e et nous renvoie une carte de profondeur.</p> In\u00a0[47]: Copied! <pre>depth_estimator = pipeline(task=\"depth-estimation\",model=\"Intel/dpt-hybrid-midas\")\n</pre> depth_estimator = pipeline(task=\"depth-estimation\",model=\"Intel/dpt-hybrid-midas\") In\u00a0[48]: Copied! <pre>image=Image.open(\"images/coco2.jpg\") # Image extraite de la base de donn\u00e9es COCO (https://cocodataset.org/#home)\n\nplt.imshow(image)\nplt.axis('off')  \nplt.show()\n</pre> image=Image.open(\"images/coco2.jpg\") # Image extraite de la base de donn\u00e9es COCO (https://cocodataset.org/#home)  plt.imshow(image) plt.axis('off')   plt.show() In\u00a0[57]: Copied! <pre>outputs = depth_estimator(image)\noutputs[\"predicted_depth\"].shape\n</pre> outputs = depth_estimator(image) outputs[\"predicted_depth\"].shape Out[57]: <pre>torch.Size([1, 384, 384])</pre> <p>On utilise pytorch pour faire correspondre la dimension de la carte de profondeur pr\u00e9dite \u00e0 celle de notre image de base puis on g\u00e9nere une image de notre carte de profondeur.</p> In\u00a0[60]: Copied! <pre>import torch \nprediction = torch.nn.functional.interpolate(outputs[\"predicted_depth\"].unsqueeze(1),size=image.size[::-1],\n                                             mode=\"bicubic\",align_corners=False)\noutput = prediction.squeeze().numpy()\nformatted = (output * 255 / np.max(output)).astype(\"uint8\")\ndepth = Image.fromarray(formatted)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6)) \nax1.imshow(image)\nax1.axis('off')  \nax2.imshow(depth)\nax2.axis('off')  \nplt.show()\n</pre> import torch  prediction = torch.nn.functional.interpolate(outputs[\"predicted_depth\"].unsqueeze(1),size=image.size[::-1],                                              mode=\"bicubic\",align_corners=False) output = prediction.squeeze().numpy() formatted = (output * 255 / np.max(output)).astype(\"uint8\") depth = Image.fromarray(formatted)  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))  ax1.imshow(image) ax1.axis('off')   ax2.imshow(depth) ax2.axis('off')   plt.show() <p>Sur la carte de profondeur, les couleurs vives representent les objects les plus proches. On a bien la route proche de couleur tr\u00e8s vive et le bus de couleur assez vive. La carte de profondeur est donc pr\u00e9cise.</p>"},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#computer-vision-with-transformers","title":"Computer Vision with Transformers\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#zero-shot-object-detection","title":"Zero-Shot Object Detection\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#image-captionning","title":"Image Captionning\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#zero-shot-image-classification","title":"Zero-Shot Image Classification\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#image-segmentation","title":"Image Segmentation\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#estimation-de-la-profondeur","title":"Estimation de la profondeur\u00b6","text":""},{"location":"06_HuggingFace/02_ComputerVisionWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/03_NlpWithTransformers.html","title":"NLP with Transformers","text":"<p>Dans ce notebook, nous allons utiliser la library transformers de Hugging Face pour le traitement du langage naturel (NLP). Les mod\u00e8les de langages les plus performants (GPT, Llama etc...) sont tr\u00e8s couteux en terme d'espace m\u00e9moire et il est souvent impossible de les faire fonctionner sur un ordinateur portable. Nous allons donc utiliser des mod\u00e8les de taille r\u00e9duite (et donc moins performants) dans ce notebook.</p> <p>L'utilisation la plus commune des mod\u00e8les de langages (LLM) est aujourd'hui le ChatBot ce qui consiste en un genre d'assistant virtuel qui va r\u00e9pondre \u00e0 nos questions. Avec Hugging Face, vous pouvez avoir votre propre ChatBot en local de la mani\u00e8re suivante.</p> <p>Nous utilisons une version r\u00e9duite de BlenderBot de Meta (facebook/blenderbot-400M-distill).</p> In\u00a0[1]: Copied! <pre>from transformers import pipeline\n</pre> from transformers import pipeline <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>chatbot = pipeline(task=\"conversational\",model=\"facebook/blenderbot-400M-distill\")\n</pre> chatbot = pipeline(task=\"conversational\",model=\"facebook/blenderbot-400M-distill\") <p>Ce ChatBot ne g\u00e8re que l'anglais, il faudra donc lui poser des questions en anglais.</p> In\u00a0[3]: Copied! <pre>from transformers import Conversation\nuser_message = \"\"\"What is the best french deep learning course?\"\"\"\nconversation = Conversation(user_message)\nconversation = chatbot(conversation)\nprint(conversation)\n</pre> from transformers import Conversation user_message = \"\"\"What is the best french deep learning course?\"\"\" conversation = Conversation(user_message) conversation = chatbot(conversation) print(conversation) <pre>No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n</pre> <pre>Conversation id: 44c34bd3-ea1b-44b6-bd54-9127133cc941\nuser: What is the best  french deep learning course?\nassistant:  I'm not sure, but I do know that French is one of the most widely spoken languages in the world.\n\n</pre> <p>Comme vous le voyez, le mod\u00e8le est assez mal entra\u00een\u00e9, il ne sait pas que le meilleur cours de Deep Learning est celui-ci.</p> <p>Si vous voulez poser une autre question, la commande suivante permet d'avoir la r\u00e9ponse \u00e0 une question en une seule ligne de code.</p> In\u00a0[4]: Copied! <pre>conversation=Conversation(\"What is the most tasty fruit?\")\nprint(chatbot(conversation))\n</pre> conversation=Conversation(\"What is the most tasty fruit?\") print(chatbot(conversation)) <pre>Conversation id: d258da22-78e4-4621-a0e1-90776454a595\nuser: What is the most tasty fruit?\nassistant:  I would have to say watermelon. It is so juicy and juicy.\n\n</pre> <p>Par contre, si vous souhaitez continuer la conversation d\u00e9j\u00e0 entam\u00e9e, il faut utiliser cette fonction.</p> In\u00a0[13]: Copied! <pre># Il faut sp\u00e9cifier le r\u00f4le (user) et ajouter votre message dans la conversation d\u00e9j\u00e0 existante\nconversation.add_message({\"role\": \"user\",\"content\": \"\"\"What else do you recommend?\"\"\"})\nprint(chatbot(conversation))\n</pre> # Il faut sp\u00e9cifier le r\u00f4le (user) et ajouter votre message dans la conversation d\u00e9j\u00e0 existante conversation.add_message({\"role\": \"user\",\"content\": \"\"\"What else do you recommend?\"\"\"}) print(chatbot(conversation)) <pre>Conversation id: c3e1a64c-5b40-4808-8632-38d9df14ed9d\nuser: What is the most tasty fruit?\nassistant:  I would have to say watermelon. It is so juicy and juicy.\nuser: What else do you recommend?\nassistant:  I would say mangos are pretty good too. They are sweet and tangy.\n\n</pre> <p>Vous savez maintenant comment utiliser un ChatBot avec la library transformers de Hugging Face.</p> <p>Nous allons maintenant regarder comment impl\u00e9menter un traducteur. Pour cela, nous utilisons le mod\u00e8le No Language Left Behind de facebook (facebook/nllb-200-distilled-600M) qui permet de faire depuis n'importe quel langage. Par soucis de m\u00e9moire, nous utilisons une version r\u00e9duite du mod\u00e8le.</p> In\u00a0[14]: Copied! <pre>traducteur = pipeline(task=\"translation\",model=\"facebook/nllb-200-distilled-600M\") \n</pre> traducteur = pipeline(task=\"translation\",model=\"facebook/nllb-200-distilled-600M\")  In\u00a0[24]: Copied! <pre>text = \"\"\"Le meilleur cours de d'apprentissage profond est celui-ci.\"\"\"\ntext_translated = traducteur(text,src_lang=\"fra_Latn\",tgt_lang=\"eng_Latn\")\nprint(\"Le texte en anglais : \", text_translated[0][\"translation_text\"])\ntext_translated = traducteur(text,src_lang=\"fra_Latn\",tgt_lang=\"jpn_Jpan\")\nprint(\"Le texte en japonais : \",text_translated[0][\"translation_text\"])\n</pre> text = \"\"\"Le meilleur cours de d'apprentissage profond est celui-ci.\"\"\" text_translated = traducteur(text,src_lang=\"fra_Latn\",tgt_lang=\"eng_Latn\") print(\"Le texte en anglais : \", text_translated[0][\"translation_text\"]) text_translated = traducteur(text,src_lang=\"fra_Latn\",tgt_lang=\"jpn_Jpan\") print(\"Le texte en japonais : \",text_translated[0][\"translation_text\"]) <pre>Le texte en anglais :  The best course of deep learning is this one.\nLe texte en japonais :  \u6df1\u3044\u5b66\u7fd2\u306e\u6700\u9ad8\u306e\u30b3\u30fc\u30b9\u306f\u3053\u308c\u3067\u3059\n</pre> <p>La traduction est tr\u00e8s bonne (pour l'anglais en tout cas, pour le japonais je n'ai malheuresement pas l'expertise) ! Vous pouvez \u00e9galement tester d'autres combinaisons de langage, il faut juste sp\u00e9cifier le bon code que l'on peut trouver sur cette page.</p> <p>Une autre t\u00e2che utile dans le traitement du langage naturel est le r\u00e9sum\u00e9 de texte. Le mod\u00e8le doit \u00eatre capable de rassembler les informations les plus importantes d'un texte. Pour cela, nous utilisons le mod\u00e8le BART de facebook (facebook/bart-large-cnn).</p> In\u00a0[25]: Copied! <pre>resumeur=pipeline(task=\"summarization\",model=\"facebook/bart-large-cnn\")\n</pre> resumeur=pipeline(task=\"summarization\",model=\"facebook/bart-large-cnn\") In\u00a0[34]: Copied! <pre>text= \"Troyes is a beautiful city. Troyes is a commune and the capital of the department of Aube in the Grand Est region of north-central France. It is located on the Seine river about 140 km (87 mi) south-east of Paris. Troyes is situated within the Champagne wine region and is near to the Orient Forest Regional Natural Park.Troyes had a population of 61,996 inhabitants in 2018. It is the center of the Communaut\u00e9 d'agglom\u00e9ration Troyes Champagne M\u00e9tropole, which was home to 170,145 inhabitants.\"\nsummary = resumeur(text,min_length=10,max_length=100)\nprint(\"Le r\u00e9sum\u00e9 du texte : \",summary[0][\"summary_text\"]) #[\"summary_text\"]\n</pre> text= \"Troyes is a beautiful city. Troyes is a commune and the capital of the department of Aube in the Grand Est region of north-central France. It is located on the Seine river about 140 km (87 mi) south-east of Paris. Troyes is situated within the Champagne wine region and is near to the Orient Forest Regional Natural Park.Troyes had a population of 61,996 inhabitants in 2018. It is the center of the Communaut\u00e9 d'agglom\u00e9ration Troyes Champagne M\u00e9tropole, which was home to 170,145 inhabitants.\" summary = resumeur(text,min_length=10,max_length=100) print(\"Le r\u00e9sum\u00e9 du texte : \",summary[0][\"summary_text\"]) #[\"summary_text\"] <pre>Le r\u00e9sum\u00e9 du texte :  Troyes is a commune and the capital of the department of Aube in the Grand Est region of north-central France. It is located on the Seine river about 140 km (87 mi) south-east of Paris. Troyes had a population of 61,996 inhabitants in 2018.\n</pre> <p>Le r\u00e9sum\u00e9 n'est pas tr\u00e8s bon car il s'agit d'un petit mod\u00e8le mais il a quand m\u00eame su ressortir les informations cl\u00e9s et enlever les informations \"moins importantes\".</p> <p>Un aspect important du langage naturel que nous avons abord\u00e9 dans le cours sur les NLP est l'embedding. Pour rappel, cela consiste \u00e0 projetter nos tokens (mots ou caract\u00e8res par exemple) dans un espace latent. Cela va permettre de faire une premi\u00e8re \u00e9tape de rapprochement de ces mots. Des mots proches comme chiens et chats vont \u00eatre proches dans l'espace latent tandis que \"chien\" et \"est\" vont \u00eatre \u00e9loign\u00e9s. Nous pouvons utiliser ces embeddings pour calculer la similarit\u00e9 entre deux phrases. Pour cela, nous allons utiliser la library sentence_transformers qui permet d'extraire l'embedding \u00e0 partir d'un mod\u00e8le pr\u00e9-entrain\u00e9.</p> <p>Nous utilisons le mod\u00e8le all-MiniLM-L6-v2.</p> In\u00a0[36]: Copied! <pre>from sentence_transformers import SentenceTransformer\nfrom sentence_transformers import util\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n</pre> from sentence_transformers import SentenceTransformer from sentence_transformers import util model = SentenceTransformer(\"all-MiniLM-L6-v2\") <p>Nous allons regarder la similarit\u00e9 entre des phrases diff\u00e9rentes.</p> In\u00a0[42]: Copied! <pre>sentences1 = ['The cat is chasing the mouse','A man is watching the television','The latest movie is awesome']\nsentences2 = ['The dog sleeps in the kitchen','A boy watches TV','The new movie is so great']\nembeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2,convert_to_tensor=True)\ncosine_scores = util.cos_sim(embeddings1,embeddings2)\nfor i in range(len(sentences1)):\n  print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i],\n                                                sentences2[i],\n                                                cosine_scores[i][i]))\n</pre> sentences1 = ['The cat is chasing the mouse','A man is watching the television','The latest movie is awesome'] sentences2 = ['The dog sleeps in the kitchen','A boy watches TV','The new movie is so great'] embeddings1 = model.encode(sentences1, convert_to_tensor=True) embeddings2 = model.encode(sentences2,convert_to_tensor=True) cosine_scores = util.cos_sim(embeddings1,embeddings2) for i in range(len(sentences1)):   print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i],                                                 sentences2[i],                                                 cosine_scores[i][i])) <pre>The cat is chasing the mouse \t\t The dog sleep in the kitchen \t\t Score: 0.0601\nA man is watching the television \t\t A boy watches TV \t\t Score: 0.7207\nThe latest movie is awesome \t\t The new movie is so great \t\t Score: 0.7786\n</pre> <p>Comme vous pouvez le voir, les phrases proches en terme de sens ont des embeddings assez similaires. Ce mod\u00e8le est donc int\u00e9ressant pour extraire des embeddings. Avoir un bon mod\u00e8le extracteur d'embedding est souvent une premi\u00e8re \u00e9tape dans un projet de traitement du langage naturel.</p>"},{"location":"06_HuggingFace/03_NlpWithTransformers.html#nlp-with-transformers","title":"NLP with Transformers\u00b6","text":""},{"location":"06_HuggingFace/03_NlpWithTransformers.html#chatbot","title":"ChatBot\u00b6","text":""},{"location":"06_HuggingFace/03_NlpWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/03_NlpWithTransformers.html#traduction","title":"Traduction\u00b6","text":""},{"location":"06_HuggingFace/03_NlpWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/03_NlpWithTransformers.html#resume-de-texte","title":"R\u00e9sum\u00e9 de texte\u00b6","text":""},{"location":"06_HuggingFace/03_NlpWithTransformers.html#embedding-de-phrase","title":"Embedding de phrase\u00b6","text":""},{"location":"06_HuggingFace/04_AudioWithTransformers.html","title":"Audio with Transformers","text":"<p>Dans ce notebook, nous allons utiliser la library transformers de Hugging Face pour le traitement de l'audio.</p> <p>Commen\u00e7ons par un probl\u00e8me d\u00e9j\u00e0 vu en vision par ordinateur, la clasification zero-shot. Il va s'agir de donner la provenance d'un extrait sonore sans avoir eu d'entra\u00eenement sur des cat\u00e9gories sp\u00e9cifiques.</p> <p>Pour cela, nous utilisons le dataset ESC-50 (ashraq/esc50) qui regroupe des enregistrements de 5 secondes de 50 classes diff\u00e9rentes. Pour le t\u00e9l\u00e9charger, utilisons la library datasets de Hugging Face :</p> In\u00a0[1]: Copied! <pre>from datasets import load_dataset\nfrom transformers import pipeline\nfrom IPython.display import Audio as IPythonAudio\n</pre> from datasets import load_dataset from transformers import pipeline from IPython.display import Audio as IPythonAudio <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[\u00a0]: Copied! <pre>dataset = load_dataset(\"ashraq/esc50\",split=\"train[0:10]\")\n</pre> dataset = load_dataset(\"ashraq/esc50\",split=\"train[0:10]\") <p>On peut observer les m\u00e9tadonn\u00e9es de l'extrait audio. Le sampling_rate est particuli\u00e8rement important. Il faut s'assurer que c'est le m\u00eame que celui des donn\u00e9es d'entra\u00eenement du mod\u00e8le.</p> In\u00a0[6]: Copied! <pre>audio_sample = dataset[0]\naudio_sample\n</pre> audio_sample = dataset[0] audio_sample Out[6]: <pre>{'filename': '1-100038-A-14.wav',\n 'fold': 1,\n 'target': 14,\n 'category': 'chirping_birds',\n 'esc10': False,\n 'src_file': 100038,\n 'take': 'A',\n 'audio': {'path': None,\n  'array': array([-0.01184082, -0.10336304, -0.14141846, ...,  0.06985474,\n          0.04049683,  0.00274658]),\n  'sampling_rate': 44100}}</pre> <p>On peut \u00e9couter l'extrait audio (attention \u00e0 ne pas mettre trop fort le son de votre ordinateur) avec IPython.</p> In\u00a0[7]: Copied! <pre>IPythonAudio(audio_sample[\"audio\"][\"array\"],rate=audio_sample[\"audio\"][\"sampling_rate\"])\n</pre> IPythonAudio(audio_sample[\"audio\"][\"array\"],rate=audio_sample[\"audio\"][\"sampling_rate\"]) Out[7]:                      Your browser does not support the audio element.                  <p>Il est temps d'utiliser le pipeline de Hugging Face pour r\u00e9cuperer notre mod\u00e8le. On utilise le mod\u00e8le CLAP de LAION-AI (laion/clap-htsat-unfused).</p> In\u00a0[9]: Copied! <pre>audio_zero_shot = pipeline(task=\"zero-shot-audio-classification\",model=\"laion/clap-htsat-unfused\")\n</pre> audio_zero_shot = pipeline(task=\"zero-shot-audio-classification\",model=\"laion/clap-htsat-unfused\") <p>Regardons le sampling rate du mod\u00e8le pour voir si il correspond  celui de nos donn\u00e9es.</p> In\u00a0[14]: Copied! <pre>print(\"Sampling rate du mod\u00e8le : \",audio_zero_shot.feature_extractor.sampling_rate)\nprint(\"Sampling rate de notre extrait : \",audio_sample[\"audio\"][\"sampling_rate\"])\n</pre> print(\"Sampling rate du mod\u00e8le : \",audio_zero_shot.feature_extractor.sampling_rate) print(\"Sampling rate de notre extrait : \",audio_sample[\"audio\"][\"sampling_rate\"]) <pre>Sampling rate du mod\u00e8le :  48000\nSampling rate de notre extrait :  44100\n</pre> <p>On va devoir modifier le sampling rate de notre dataset pour l'adapter au mod\u00e8le.</p> In\u00a0[16]: Copied! <pre>from datasets import Audio\ndataset = dataset.cast_column(\"audio\",Audio(sampling_rate=48_000))\naudio_sample = dataset[0]\nprint(\"Sampling rate de notre extrait : \",audio_sample[\"audio\"][\"sampling_rate\"])\n</pre> from datasets import Audio dataset = dataset.cast_column(\"audio\",Audio(sampling_rate=48_000)) audio_sample = dataset[0] print(\"Sampling rate de notre extrait : \",audio_sample[\"audio\"][\"sampling_rate\"]) <pre>Sampling rate de notre extrait :  48000\n</pre> <p>Maintenant que les extraits et le mod\u00e8le sont sur la m\u00eame longueur d'onde, on va pouvoir proc\u00e9der \u00e0 notre classification. On va proposer des labels candidats (comme pour le mod\u00e8le CLIP en vision).</p> In\u00a0[18]: Copied! <pre>candidate_labels = [\"Sound of a dog\",\"Sound of cat\"]\noutputs=audio_zero_shot(audio_sample[\"audio\"][\"array\"],candidate_labels=candidate_labels)\nprint(\"Score de \"+candidate_labels[0],outputs[0][\"score\"])\nprint(\"Score de \"+candidate_labels[1],outputs[1][\"score\"])\n</pre> candidate_labels = [\"Sound of a dog\",\"Sound of cat\"] outputs=audio_zero_shot(audio_sample[\"audio\"][\"array\"],candidate_labels=candidate_labels) print(\"Score de \"+candidate_labels[0],outputs[0][\"score\"]) print(\"Score de \"+candidate_labels[1],outputs[1][\"score\"]) <pre>Score de Sound of a dog 0.9805886149406433\nScore de Sound of cat 0.019411340355873108\n</pre> <p>Le mod\u00e8le est capable d'identifier que l'extrait audio est un aboiement de chien et pas un miaulement de chat. Vous pouvez tester sur vos propres extraits sonores ou sur d'autres extraits du dataset.</p> <p>La detection automatique la parole est une t\u00e2che qui consiste \u00e0 transcrire des paroles en texte. C'est utile pour la prise de note par la parole, pour activer des objets connect\u00e9s ('Ok google', 'Hey Siri') et bien d'autres choses.</p> <p>Dans cet exemple, nous utilisons LibriSpeech ASR corpus qui contient environ 1000 heures de parole en anglais.</p> In\u00a0[19]: Copied! <pre>from datasets import load_dataset\ndataset = load_dataset(\"librispeech_asr\",split=\"train.clean.100\",streaming=True,trust_remote_code=True)\n</pre> from datasets import load_dataset dataset = load_dataset(\"librispeech_asr\",split=\"train.clean.100\",streaming=True,trust_remote_code=True) <pre>Downloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.5k/11.5k [00:00&lt;00:00, 6.94MB/s]\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.2k/10.2k [00:00&lt;00:00, 11.7MB/s]\n</pre> In\u00a0[25]: Copied! <pre>example = next(iter(dataset))\nexample\n</pre> example = next(iter(dataset)) example Out[25]: <pre>{'file': '374-180298-0000.flac',\n 'audio': {'path': '374-180298-0000.flac',\n  'array': array([ 7.01904297e-04,  7.32421875e-04,  7.32421875e-04, ...,\n         -2.74658203e-04, -1.83105469e-04, -3.05175781e-05]),\n  'sampling_rate': 16000},\n 'text': 'CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED',\n 'speaker_id': 374,\n 'chapter_id': 180298,\n 'id': '374-180298-0000'}</pre> In\u00a0[24]: Copied! <pre>from IPython.display import Audio as IPythonAudio\n\nIPythonAudio(example[\"audio\"][\"array\"],rate=example[\"audio\"][\"sampling_rate\"])\n</pre> from IPython.display import Audio as IPythonAudio  IPythonAudio(example[\"audio\"][\"array\"],rate=example[\"audio\"][\"sampling_rate\"]) Out[24]:                      Your browser does not support the audio element.                  <p>Nous allons utiliser le mod\u00e8le Whisper (distil-whisper/distil-small.en) de OpenAI qui est con\u00e7u pour la reconnaissance de paroles anglaises. C'est une version r\u00e9duite du mod\u00e8le original. Construisons maintenant notre pipeline Hugging Face.</p> In\u00a0[27]: Copied! <pre>reco_parole = pipeline(task=\"automatic-speech-recognition\",model=\"distil-whisper/distil-small.en\")\n</pre> reco_parole = pipeline(task=\"automatic-speech-recognition\",model=\"distil-whisper/distil-small.en\") <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> In\u00a0[30]: Copied! <pre>print(\"Sampling rate du mod\u00e8le : \",reco_parole.feature_extractor.sampling_rate)\nprint(\"Sampling rate de notre extrait : \",example['audio']['sampling_rate'])\n</pre> print(\"Sampling rate du mod\u00e8le : \",reco_parole.feature_extractor.sampling_rate) print(\"Sampling rate de notre extrait : \",example['audio']['sampling_rate']) <pre>Sampling rate du mod\u00e8le :  16000\nSampling rate de notre extrait :  16000\n</pre> <p>Les sampling rate sont identiques, pas besoin de modifier quoi que ce soit.</p> In\u00a0[37]: Copied! <pre>output=reco_parole(example[\"audio\"][\"array\"])\nprint(\"Texte transcrit : \",output['text'])\nprint(\"Texte de base : \",example[\"text\"].lower())\n</pre> output=reco_parole(example[\"audio\"][\"array\"]) print(\"Texte transcrit : \",output['text']) print(\"Texte de base : \",example[\"text\"].lower()) <pre>Texte transcrit :   Chapter 16 I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came. I too agree to whatever Marguerite wished.\nTexte de base :  chapter sixteen i might have told you of the beginning of this liaison in a few lines but i wanted you to see every step by which we came i to agree to whatever marguerite wished\n</pre> <p>Comme vous pouvez le voir, la transcription est assez fid\u00e8le \u00e0 l'original.</p> <p>Cette t\u00e2che est l'inverse de la t\u00e2che pr\u00e9c\u00e9dente. Ici, on va donner un texte en entr\u00e9e et le mod\u00e8le va nous ressortir un audio d'une personne disant ce texte.</p> <p>Nous utilisons le mod\u00e8le vits de kakao-enterprise (kakao-enterprise/vits-ljs).</p> In\u00a0[5]: Copied! <pre>text_parole = pipeline(\"text-to-speech\", model=\"kakao-enterprise/vits-ljs\")\n</pre> text_parole = pipeline(\"text-to-speech\", model=\"kakao-enterprise/vits-ljs\") <pre>Some weights of the model checkpoint at kakao-enterprise/vits-ljs were not used when initializing VitsModel: ['flow.flows.0.wavenet.in_layers.0.weight_g', 'flow.flows.0.wavenet.in_layers.0.weight_v', 'flow.flows.0.wavenet.in_layers.1.weight_g', 'flow.flows.0.wavenet.in_layers.1.weight_v', 'flow.flows.0.wavenet.in_layers.2.weight_g', 'flow.flows.0.wavenet.in_layers.2.weight_v', 'flow.flows.0.wavenet.in_layers.3.weight_g', 'flow.flows.0.wavenet.in_layers.3.weight_v', 'flow.flows.0.wavenet.res_skip_layers.0.weight_g', 'flow.flows.0.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.res_skip_layers.1.weight_g', 'flow.flows.0.wavenet.res_skip_layers.1.weight_v', 'flow.flows.0.wavenet.res_skip_layers.2.weight_g', 'flow.flows.0.wavenet.res_skip_layers.2.weight_v', 'flow.flows.0.wavenet.res_skip_layers.3.weight_g', 'flow.flows.0.wavenet.res_skip_layers.3.weight_v', 'flow.flows.1.wavenet.in_layers.0.weight_g', 'flow.flows.1.wavenet.in_layers.0.weight_v', 'flow.flows.1.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.in_layers.1.weight_v', 'flow.flows.1.wavenet.in_layers.2.weight_g', 'flow.flows.1.wavenet.in_layers.2.weight_v', 'flow.flows.1.wavenet.in_layers.3.weight_g', 'flow.flows.1.wavenet.in_layers.3.weight_v', 'flow.flows.1.wavenet.res_skip_layers.0.weight_g', 'flow.flows.1.wavenet.res_skip_layers.0.weight_v', 'flow.flows.1.wavenet.res_skip_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.1.weight_v', 'flow.flows.1.wavenet.res_skip_layers.2.weight_g', 'flow.flows.1.wavenet.res_skip_layers.2.weight_v', 'flow.flows.1.wavenet.res_skip_layers.3.weight_g', 'flow.flows.1.wavenet.res_skip_layers.3.weight_v', 'flow.flows.2.wavenet.in_layers.0.weight_g', 'flow.flows.2.wavenet.in_layers.0.weight_v', 'flow.flows.2.wavenet.in_layers.1.weight_g', 'flow.flows.2.wavenet.in_layers.1.weight_v', 'flow.flows.2.wavenet.in_layers.2.weight_g', 'flow.flows.2.wavenet.in_layers.2.weight_v', 'flow.flows.2.wavenet.in_layers.3.weight_g', 'flow.flows.2.wavenet.in_layers.3.weight_v', 'flow.flows.2.wavenet.res_skip_layers.0.weight_g', 'flow.flows.2.wavenet.res_skip_layers.0.weight_v', 'flow.flows.2.wavenet.res_skip_layers.1.weight_g', 'flow.flows.2.wavenet.res_skip_layers.1.weight_v', 'flow.flows.2.wavenet.res_skip_layers.2.weight_g', 'flow.flows.2.wavenet.res_skip_layers.2.weight_v', 'flow.flows.2.wavenet.res_skip_layers.3.weight_g', 'flow.flows.2.wavenet.res_skip_layers.3.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_g', 'flow.flows.3.wavenet.in_layers.0.weight_v', 'flow.flows.3.wavenet.in_layers.1.weight_g', 'flow.flows.3.wavenet.in_layers.1.weight_v', 'flow.flows.3.wavenet.in_layers.2.weight_g', 'flow.flows.3.wavenet.in_layers.2.weight_v', 'flow.flows.3.wavenet.in_layers.3.weight_g', 'flow.flows.3.wavenet.in_layers.3.weight_v', 'flow.flows.3.wavenet.res_skip_layers.0.weight_g', 'flow.flows.3.wavenet.res_skip_layers.0.weight_v', 'flow.flows.3.wavenet.res_skip_layers.1.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_v', 'flow.flows.3.wavenet.res_skip_layers.2.weight_g', 'flow.flows.3.wavenet.res_skip_layers.2.weight_v', 'flow.flows.3.wavenet.res_skip_layers.3.weight_g', 'flow.flows.3.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.in_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.10.weight_g', 'posterior_encoder.wavenet.in_layers.10.weight_v', 'posterior_encoder.wavenet.in_layers.11.weight_g', 'posterior_encoder.wavenet.in_layers.11.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_g', 'posterior_encoder.wavenet.in_layers.12.weight_v', 'posterior_encoder.wavenet.in_layers.13.weight_g', 'posterior_encoder.wavenet.in_layers.13.weight_v', 'posterior_encoder.wavenet.in_layers.14.weight_g', 'posterior_encoder.wavenet.in_layers.14.weight_v', 'posterior_encoder.wavenet.in_layers.15.weight_g', 'posterior_encoder.wavenet.in_layers.15.weight_v', 'posterior_encoder.wavenet.in_layers.2.weight_g', 'posterior_encoder.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.4.weight_g', 'posterior_encoder.wavenet.in_layers.4.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_g', 'posterior_encoder.wavenet.in_layers.5.weight_v', 'posterior_encoder.wavenet.in_layers.6.weight_g', 'posterior_encoder.wavenet.in_layers.6.weight_v', 'posterior_encoder.wavenet.in_layers.7.weight_g', 'posterior_encoder.wavenet.in_layers.7.weight_v', 'posterior_encoder.wavenet.in_layers.8.weight_g', 'posterior_encoder.wavenet.in_layers.8.weight_v', 'posterior_encoder.wavenet.in_layers.9.weight_g', 'posterior_encoder.wavenet.in_layers.9.weight_v', 'posterior_encoder.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.1.weight_g', 'posterior_encoder.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.res_skip_layers.10.weight_g', 'posterior_encoder.wavenet.res_skip_layers.10.weight_v', 'posterior_encoder.wavenet.res_skip_layers.11.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_v', 'posterior_encoder.wavenet.res_skip_layers.12.weight_g', 'posterior_encoder.wavenet.res_skip_layers.12.weight_v', 'posterior_encoder.wavenet.res_skip_layers.13.weight_g', 'posterior_encoder.wavenet.res_skip_layers.13.weight_v', 'posterior_encoder.wavenet.res_skip_layers.14.weight_g', 'posterior_encoder.wavenet.res_skip_layers.14.weight_v', 'posterior_encoder.wavenet.res_skip_layers.15.weight_g', 'posterior_encoder.wavenet.res_skip_layers.15.weight_v', 'posterior_encoder.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.4.weight_g', 'posterior_encoder.wavenet.res_skip_layers.4.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_g', 'posterior_encoder.wavenet.res_skip_layers.5.weight_v', 'posterior_encoder.wavenet.res_skip_layers.6.weight_g', 'posterior_encoder.wavenet.res_skip_layers.6.weight_v', 'posterior_encoder.wavenet.res_skip_layers.7.weight_g', 'posterior_encoder.wavenet.res_skip_layers.7.weight_v', 'posterior_encoder.wavenet.res_skip_layers.8.weight_g', 'posterior_encoder.wavenet.res_skip_layers.8.weight_v', 'posterior_encoder.wavenet.res_skip_layers.9.weight_g', 'posterior_encoder.wavenet.res_skip_layers.9.weight_v']\n- This IS expected if you are initializing VitsModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing VitsModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of VitsModel were not initialized from the model checkpoint at kakao-enterprise/vits-ljs and are newly initialized: ['flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> <p>Essayons de g\u00e9nerer une phrase en fran\u00e7ais.</p> In\u00a0[6]: Copied! <pre>text = \"\"\"Ce cours de deep learning est incroyable.\"\"\"\ngenerated_parole = text_parole(text)\nIPythonAudio(generated_parole[\"audio\"][0],rate=generated_parole[\"sampling_rate\"])\n</pre> text = \"\"\"Ce cours de deep learning est incroyable.\"\"\" generated_parole = text_parole(text) IPythonAudio(generated_parole[\"audio\"][0],rate=generated_parole[\"sampling_rate\"]) Out[6]:                      Your browser does not support the audio element.                  <p>Comme vous pouvez le constater, ce n'est pas terrible car le mod\u00e8le est entra\u00een\u00e9 sur des donn\u00e9es en anglais. Si vous voulez g\u00e9nerer du fran\u00e7ais, il faudra trouver un mod\u00e8le adapt\u00e9 pour cela. Essayons maintenant sur une phrase en anglais :</p> In\u00a0[7]: Copied! <pre>text = \"\"\"This deep learning course is fantastic.\"\"\"\ngenerated_parole = text_parole(text)\nIPythonAudio(generated_parole[\"audio\"][0],rate=generated_parole[\"sampling_rate\"])\n</pre> text = \"\"\"This deep learning course is fantastic.\"\"\" generated_parole = text_parole(text) IPythonAudio(generated_parole[\"audio\"][0],rate=generated_parole[\"sampling_rate\"]) Out[7]:                      Your browser does not support the audio element.                  <p>C'est beaucoup mieux !</p> <p>Note 1 : Vous pouvez aussi combiner plusieurs mod\u00e8les, par exemple vous prenez votre phrase en fran\u00e7ais, vous la traduisez en anglais puis vous g\u00e9n\u00e9rer l'audio correspondant.</p> <p>Note 2 : Si vous souhaitez g\u00e9n\u00e9rer du son (musique, son d'ambiance, bruit etc ...) qui ne soit pas de la parole, vous devez plut\u00f4t regarder la cat\u00e9gorie Text-to-Audio sur Hugging Face.</p>"},{"location":"06_HuggingFace/04_AudioWithTransformers.html#audio-with-transformers","title":"Audio with Transformers\u00b6","text":""},{"location":"06_HuggingFace/04_AudioWithTransformers.html#classification-zero-shot","title":"Classification zero-shot\u00b6","text":""},{"location":"06_HuggingFace/04_AudioWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/04_AudioWithTransformers.html#reconnaissance-automatique-de-la-parole","title":"Reconnaissance automatique de la parole\u00b6","text":""},{"location":"06_HuggingFace/04_AudioWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/04_AudioWithTransformers.html#transformation-de-texte-en-dialogue","title":"Transformation de texte en dialogue\u00b6","text":""},{"location":"06_HuggingFace/04_AudioWithTransformers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/05_ImageGenerationWithDiffusers.html","title":"Generation d'image avec la library Diffusers","text":"<p>Dans ce notebook, nous allons apprendre \u00e0 utiliser les mod\u00e8les de diffusion \u00e0 l'aide la library diffusers de Hugging Face. Les mod\u00e8les de diffusion \u00e9tant tr\u00e8s couteux en terme de temps de calcul, nous nous contenterons d'un petit exemple sur un mod\u00e8le assez l\u00e9ger.</p> <p>Nous utilisons le mod\u00e8le small-stable-diffusion de OFA-Sys (OFA-Sys/small-stable-diffusion-v0).</p> In\u00a0[14]: Copied! <pre>from diffusers import DiffusionPipeline\n\ndiffuser = DiffusionPipeline.from_pretrained(\"OFA-Sys/small-stable-diffusion-v0\") #\"stabilityai/sdxl-turbo\"\n</pre> from diffusers import DiffusionPipeline  diffuser = DiffusionPipeline.from_pretrained(\"OFA-Sys/small-stable-diffusion-v0\") #\"stabilityai/sdxl-turbo\" <pre>vae/diffusion_pytorch_model.safetensors not found\nLoading pipeline components...:  29%|\u2588\u2588\u258a       | 2/7 [00:01&lt;00:03,  1.48it/s]The config attributes {'predict_epsilon': True} were passed to DPMSolverMultistepScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\nLoading pipeline components...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:01&lt;00:00,  4.05it/s]\n</pre> <p>La g\u00e9n\u00e9ration peut prendre quelques minutes selon votre ordinateur.</p> In\u00a0[15]: Copied! <pre>outputAutre=diffuser(\"A car in the winter\")\n</pre> outputAutre=diffuser(\"A car in the winter\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [03:01&lt;00:00,  3.63s/it]\n</pre> In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\nprint(outputAutre.images[0])\nplt.imshow(outputAutre.images[0])\nplt.axis(\"off\")\nplt.show()\n</pre> import matplotlib.pyplot as plt print(outputAutre.images[0]) plt.imshow(outputAutre.images[0]) plt.axis(\"off\") plt.show() <pre>&lt;PIL.Image.Image image mode=RGB size=512x512 at 0x733D54E7F210&gt;\n</pre> <p>Vous savez maintenant comment g\u00e9n\u00e9rer des images avec la library Diffusers de Hugging Face.</p> <p>Pour aller plus loin : Si vous souhaitez en apprendre plus sur les utilisations possibles des mod\u00e8les de diffusion, je vous conseille le cours gratuit \"Prompt Engineering for Vision Models\" sur deeplearning.ai. Vous pourrez y apprendre comment remplacer un objet sur une image en utilisant SAM et un mod\u00e8le de diffusion.</p>"},{"location":"06_HuggingFace/05_ImageGenerationWithDiffusers.html#generation-dimage-avec-la-library-diffusers","title":"Generation d'image avec la library Diffusers\u00b6","text":""},{"location":"06_HuggingFace/05_ImageGenerationWithDiffusers.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/06_DemoAvecGradio.html","title":"Demo avec Gradio","text":"<p>Ce notebook va pr\u00e9sente, en un exemple, la library gradio qui permet de cr\u00e9er des interfaces de d\u00e9mos tr\u00e8s simplement.</p> <p>Pour cet exemple, nous allons utiliser un mod\u00e8le de d\u00e9tection d'objets dans une image entrain\u00e9 sur les 80 classes du dataset COCO. Nous utilisons le mod\u00e8le DETR de meta (facebook/detr-resnet-50).</p> <p>Tout d'abord, construisons notre pipeline \u00e0 l'aide de la library transformers de Hugging Face.</p> In\u00a0[1]: Copied! <pre>from PIL import Image\nimport matplotlib.pyplot as plt\nfrom transformers import pipeline\nimport cv2\nimport numpy as np\n</pre> from PIL import Image import matplotlib.pyplot as plt from transformers import pipeline import cv2 import numpy as np <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>detector = pipeline(\"object-detection\", \"facebook/detr-resnet-50\")\n</pre> detector = pipeline(\"object-detection\", \"facebook/detr-resnet-50\") <pre>Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n</pre> In\u00a0[3]: Copied! <pre>image = Image.open('images/coco3.jpg')\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n</pre> image = Image.open('images/coco3.jpg') plt.imshow(image) plt.axis('off') plt.show() <p>Proc\u00e9dons \u00e0 la d\u00e9tection et dessinons les boites. Pour avoir un r\u00e9sultat clair, on ne va entourer que les personnes. Vous pouvez enlever ce filtre si vous voulez.</p> In\u00a0[4]: Copied! <pre>def draw_boxes(image,output):\n  cv_image = np.array(image)\n  for bbox in output:\n    box = bbox['box']\n    label = bbox['label']\n    if (label!=\"person\"):\n      continue\n    cv2.rectangle(cv_image, (box['xmin'], box['ymin']), (box['xmax'], box['ymax']), (0, 0, 255), 1)\n    cv2.putText(cv_image, label, (box['xmin'], box['ymin'] - 10), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA)\n  return cv_image\n\noutput = detector(image)\ncv_image=draw_boxes(image,output)\nplt.imshow(cv_image)\nplt.axis('off')\nplt.show()\n</pre> def draw_boxes(image,output):   cv_image = np.array(image)   for bbox in output:     box = bbox['box']     label = bbox['label']     if (label!=\"person\"):       continue     cv2.rectangle(cv_image, (box['xmin'], box['ymin']), (box['xmax'], box['ymax']), (0, 0, 255), 1)     cv2.putText(cv_image, label, (box['xmin'], box['ymin'] - 10), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA)   return cv_image  output = detector(image) cv_image=draw_boxes(image,output) plt.imshow(cv_image) plt.axis('off') plt.show()  <p>C'est maintenant le moment de construire notre d\u00e9mo avec gradio. L'id\u00e9e sera d'avoir une interface qui prend une image en entr\u00e9e et renvoie la m\u00eame image avec les personnes entour\u00e9es.</p> In\u00a0[5]: Copied! <pre>import gradio as gr\n</pre> import gradio as gr In\u00a0[6]: Copied! <pre>def get_pipeline_prediction(pil_image):\n  pipeline_output = detector(pil_image)\n  processed_image = draw_boxes(pil_image,pipeline_output)\n  return processed_image\n</pre> def get_pipeline_prediction(pil_image):   pipeline_output = detector(pil_image)   processed_image = draw_boxes(pil_image,pipeline_output)   return processed_image In\u00a0[7]: Copied! <pre>demo = gr.Interface(\n  fn=get_pipeline_prediction,\n  inputs=gr.Image(label=\"Image d'entr\u00e9e\",type=\"pil\"),\n  outputs=gr.Image(label=\"Image avec les personnes d\u00e9tect\u00e9es\",type=\"pil\")\n)\n</pre> demo = gr.Interface(   fn=get_pipeline_prediction,   inputs=gr.Image(label=\"Image d'entr\u00e9e\",type=\"pil\"),   outputs=gr.Image(label=\"Image avec les personnes d\u00e9tect\u00e9es\",type=\"pil\") ) <pre>IMPORTANT: You are using gradio version 4.24.0, however version 4.29.0 is available, please upgrade.\n--------\n</pre> In\u00a0[8]: Copied! <pre>demo.launch()\n</pre> demo.launch() <pre>Running on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True` in `launch()`.\n</pre> Out[8]: <pre></pre> <p>Et voil\u00e0, vous avez construit votre propre d\u00e9mo. Il est possible de la partager en activant le param\u00e8tre share=True de la m\u00e9thode launch(). Par contre, il faudra garder votre notebook actif sinon la d\u00e9mo disparaitra. Pour faire une d\u00e9mo qui ne vous contraint pas \u00e0 garder votre pc allum\u00e9, vous pouvez cr\u00e9er un space sur le site de Hugging Face (voir notebook 1).</p> <p>Notes : Bien s\u00fbr, gradio poss\u00e8de plus de fonctionnalit\u00e9s que \u00e7a. Je vous invite \u00e0 regarder la documentation et les diff\u00e9rents tutoriels si vous avez un besoin particulier.</p>"},{"location":"06_HuggingFace/06_DemoAvecGradio.html#demo-avec-gradio","title":"Demo avec Gradio\u00b6","text":""},{"location":"06_HuggingFace/06_DemoAvecGradio.html#interface-de-detection-dobjets","title":"Interface de d\u00e9tection d'objets\u00b6","text":""},{"location":"06_HuggingFace/06_DemoAvecGradio.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"06_HuggingFace/06_DemoAvecGradio.html#demo-gradio","title":"D\u00e9mo gradio\u00b6","text":""},{"location":"06_HuggingFace/utils/util.html","title":"Util","text":"In\u00a0[\u00a0]: Copied! <pre>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> import cv2 import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>def draw_box(image,output):\n    cv_image = np.array(image)\n    for bbox in output:\n        box = bbox['box']\n        label = bbox['label']\n        cv2.rectangle(cv_image, (box['xmin'], box['ymin']), (box['xmax'], box['ymax']), (0, 0, 255), 2)\n        cv2.putText(cv_image, label, (box['xmin'], box['ymin'] - 10), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA)\n        return cv_image\n</pre> def draw_box(image,output):     cv_image = np.array(image)     for bbox in output:         box = bbox['box']         label = bbox['label']         cv2.rectangle(cv_image, (box['xmin'], box['ymin']), (box['xmax'], box['ymax']), (0, 0, 255), 2)         cv2.putText(cv_image, label, (box['xmin'], box['ymin'] - 10), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA)         return cv_image In\u00a0[\u00a0]: Copied! <pre>def draw_masks(image,masks):\n    image_np = np.array(image)\n    colors = plt.cm.get_cmap('tab20', 38)\n    for i, mask in enumerate(masks):\n        color = colors(i)[:3] \n        color = tuple(int(c * 255) for c in color)  \n\n        image_np[mask] = image_np[mask] * 0.5 + np.array(color) * 0.5\n        \n    return image_np\n</pre> def draw_masks(image,masks):     image_np = np.array(image)     colors = plt.cm.get_cmap('tab20', 38)     for i, mask in enumerate(masks):         color = colors(i)[:3]          color = tuple(int(c * 255) for c in color)            image_np[mask] = image_np[mask] * 0.5 + np.array(color) * 0.5              return image_np"},{"location":"07_Transformers/index.html","title":"\ud83e\udd16 Transformers \ud83e\udd16","text":"<p>Ce cours est d\u00e9di\u00e9 \u00e0 l'architecture du transformers. Apr\u00e8s avoir vu ses applications dans le cours pr\u00e9c\u00e9dent. Nous allons entrer dans le d\u00e9tail de l'architecture pour en comprendre les m\u00e9canismes. Le premier notebook est grandement inspir\u00e9 de la vid\u00e9o Let's build GPT de Andrej Karpathy et propose une impl\u00e9mentation pas \u00e0 pas d'un encodeur transformers. Le but de ce notebook sera de cr\u00e9er un mod\u00e8le capable de g\u00e9n\u00e9rer du \"Moli\u00e8re\" automatiquement. La seconde partie est une approche plus math\u00e9matique et la pr\u00e9sentation de la partie encodeur du transformers. La troisi\u00e8me partie pr\u00e9sente des architectures de mod\u00e8le reposant sur la couche transformers pour de nombreux cas d'applications (Vision, traduction etc ...). Enfin, une derni\u00e8re partie propose une impl\u00e9mentation du vision transformer \u00e0 partir de l'article original.</p>"},{"location":"07_Transformers/index.html#notebook-1-introduction","title":"Notebook 1\ufe0f\u20e3 : Introduction","text":"<p>Ce notebook introduit bri\u00e8vement l'architecture transformer et pr\u00e9sente un plan de cours.</p>"},{"location":"07_Transformers/index.html#notebook-2-gpt-from-scratch","title":"Notebook 2\ufe0f\u20e3 : GPT from scratch","text":"<p>Ce notebook pr\u00e9sente une impl\u00e9mentation d'un generative pretrained transformer \u00e0 partir de z\u00e9ro.</p>"},{"location":"07_Transformers/index.html#notebook-3-training-our-gpt","title":"Notebook 3\ufe0f\u20e3 : Training our GPT","text":"<p>Ce notebook reprend les \u00e9l\u00e9ments impl\u00e9ment\u00e9s dans le notebook pr\u00e9c\u00e9dent et entra\u00eene un mod\u00e8le GPT pour la pr\u00e9diction du prochain caract\u00e8re.</p>"},{"location":"07_Transformers/index.html#notebook-4-architecture-et-particularites","title":"Notebook 4\ufe0f\u20e3 : Architecture et particularit\u00e9s","text":"<p>Ce notebook d\u00e9crit formellement l'architecture du transformer en pr\u00e9sentant les diff\u00e9rents blocks (encoder et decoder).</p>"},{"location":"07_Transformers/index.html#notebook-5-utilisations-possibles","title":"Notebook 5\ufe0f\u20e3 : Utilisations possibles","text":"<p>Ce notebook pr\u00e9sente diff\u00e9rentes utilisations de l'architecture transformer pour divers probl\u00e8mes dans le domaine du NLP et de la vision.</p>"},{"location":"07_Transformers/index.html#notebook-6-vision-transformer-implementation","title":"Notebook 6\ufe0f\u20e3 : Vision Transformer Implementation","text":"<p>Ce notebook propose une impl\u00e9mentation de l'architecture du Vision Transformer.</p>"},{"location":"07_Transformers/index.html#notebook-7-swin-transformer","title":"Notebook 7\ufe0f\u20e3 : Swin Transformer","text":"<p>Ce notebook propose une impl\u00e9mentation de l'architecture du Swin Transformer.</p>"},{"location":"07_Transformers/01_Introduction.html","title":"Introduction aux transformers","text":"<p>Dans le chapitre pr\u00e9c\u00e9dent, nous avons vu de nombreuses applications de la library transformers de Hugging Face. Comme son nom l'indique, cette library g\u00e8re des mod\u00e8les transformers. Mais alors, qu'est ce qu'un mod\u00e8le transformer ?</p> <p>Jusqu'\u00e0 2017, la plupart des r\u00e9seaux de neurones pour les t\u00e2ches de NLP utilisaient des r\u00e9seaux r\u00e9currents (RNN). En 2017, des chercheurs de google ont publi\u00e9 un papier qui a chang\u00e9 le domaine du NLP puis plus tard les autres domaines du deep learning (vision, audio etc...) en introduisant l'architecture transformer.</p> <p>Ce papier est \"Attention Is All You Need\" et l'architecture du transformer ressemble \u00e0 cela :</p> <p></p> <p>A premi\u00e8re vue, \u00e7a semble bien compliqu\u00e9. La partie de gauche s'appelle l'encodeur et la partie de droite le d\u00e9codeur.</p> <p>La premi\u00e8re partie de ce cours s'inspire grandement de la vid\u00e9o \"Let's build GPT: from scratch, in code, spelled out.\" de Andrej Karpathy et consiste \u00e0 impl\u00e9menter un mod\u00e8le de pr\u00e9diction du prochain caract\u00e8re en se basant sur les caract\u00e8res pr\u00e9c\u00e9dents (c'est un peu la continuit\u00e9 du cours 5 sur les NLP). Cette partie va servir \u00e0 appr\u00e9hender l'int\u00earet de l'architecture transformer et en particulier du d\u00e9codeur. Dans cette partie, nous entrainerons un mod\u00e8le \u00e0 \u00e9crire du \"Moli\u00e8re\" automatiquement.</p> <p>La deuxi\u00e8me partie pr\u00e9sente des concepts un peu plus math\u00e9matiques et pr\u00e9sente \u00e9galement le d\u00e9codeur de l'architecture transformer.</p> <p>Cette troisi\u00e8me partie pr\u00e9sente rapidement des adaptations de l'architecture transformer pour des t\u00e2ches diff\u00e9rente de GPT.</p> <p>Dans la quatri\u00e8me partie, nous impl\u00e9mentons le vision transformer \u00e0 partir du papier An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  et nous l'entra\u00eenons sur le dataset CIFAR-10.</p> <p>Cette cinqui\u00e8me et derni\u00e8re partie propose une explication du papier Swin Transformer: Hierarchical Vision Transformer using Shifted Windows ainsi qu'une impl\u00e9mentation simplifi\u00e9e.</p>"},{"location":"07_Transformers/01_Introduction.html#introduction-aux-transformers","title":"Introduction aux transformers\u00b6","text":""},{"location":"07_Transformers/01_Introduction.html#le-transformer-dou-ca-vient","title":"Le transformer, d'o\u00f9 \u00e7a vient ?\u00b6","text":""},{"location":"07_Transformers/01_Introduction.html#contenu-du-cours","title":"Contenu du cours\u00b6","text":""},{"location":"07_Transformers/01_Introduction.html#premiere-partie-construisons-gpt-from-scratch","title":"Premi\u00e8re partie : construisons GPT from scratch\u00b6","text":""},{"location":"07_Transformers/01_Introduction.html#deuxieme-partie-theorie-et-encodeur","title":"Deuxi\u00e8me partie : Th\u00e9orie et encodeur\u00b6","text":""},{"location":"07_Transformers/01_Introduction.html#troisieme-partie-vit-bert-et-autres-architectures-marquantes","title":"Troisi\u00e8me partie : ViT, BERT et autres architectures marquantes\u00b6","text":""},{"location":"07_Transformers/01_Introduction.html#quatrieme-partie-implementation-du-vision-transformer","title":"Quatri\u00e8me partie : Impl\u00e9mentation du Vision Transformer\u00b6","text":""},{"location":"07_Transformers/01_Introduction.html#cinquieme-partie-implementation-du-swin-transformer","title":"Cinqui\u00e8me partie : Impl\u00e9mentation du Swin Transformer\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html","title":"Construisons GPT \u00e0 partir de rien","text":"<p>Ce notebook va pr\u00e9senter la cr\u00e9ation, \u00e0 partir de z\u00e9ro, d'un mod\u00e8le de langage pour pr\u00e9dire le prochain caract\u00e8re qui se base sur l'architecture du transformer (d\u00e9codeur en particulier). Pour cela, nous utilons un fichier texte moliere.txt qui regroupe l'int\u00e9gralit\u00e9 des dialogues des pi\u00e8ces de Moli\u00e8re. Ce dataset a \u00e9t\u00e9 cr\u00e9e \u00e0 partir des oeuvres compl\u00e8tes de Moli\u00e8re disponibles sur le site Gutenberg.org. J'ai nettoy\u00e9 un peu les donn\u00e9es pour ne garder que les dialogues.</p> In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# Pour utiliser le GPU automatiquement si vous en avez un \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n</pre> import torch import torch.nn as nn from torch.nn import functional as F  # Pour utiliser le GPU automatiquement si vous en avez un  device = 'cuda' if torch.cuda.is_available() else 'cpu' <p>Commen\u00e7ons par ouvrir et par visualiser un peu ce que contient notre dataset.</p> In\u00a0[3]: Copied! <pre>with open('moliere.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n</pre> with open('moliere.txt', 'r', encoding='utf-8') as f:     text = f.read() In\u00a0[4]: Copied! <pre>print(\"Nombre de caract\u00e8res dans le dataset : \", len(text))\n</pre> print(\"Nombre de caract\u00e8res dans le dataset : \", len(text)) <pre>Nombre de caract\u00e8res dans le dataset :  1687290\n</pre> <p>Affichons les 250 premiers caract\u00e8res :</p> In\u00a0[5]: Copied! <pre>print(text[:250])\n</pre> print(text[:250]) <pre>VAL\u00c8RE.\n\nEh bien, Sabine, quel conseil me donnes-tu?\n\nSABINE.\n\nVraiment, il y a bien des nouvelles. Mon oncle veut r\u00e9sol\u00fbment que ma\ncousine \u00e9pouse Villebrequin, et les affaires sont tellement avanc\u00e9es,\nque je crois qu'ils eussent \u00e9t\u00e9 mari\u00e9s d\u00e8s aujo\n</pre> <p>Utilisons set() pour r\u00e9cuperer les caract\u00e8res uniques pr\u00e9sent dans le dataset.</p> In\u00a0[6]: Copied! <pre>chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(\"Nombre de caract\u00e8res diff\u00e9rents : \", vocab_size)\n</pre> chars = sorted(list(set(text))) vocab_size = len(chars) print(''.join(chars)) print(\"Nombre de caract\u00e8res diff\u00e9rents : \", vocab_size) <pre>\n !'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijlmnopqrstuvxyz\u00ab\u00bb\u00c7\u00c8\u00c9\u00ca\u00cf\u00e0\u00e2\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ee\u00ef\u00f2\u00f4\u00f9\u00fb\u0152\u0153\nNombre de caract\u00e8res diff\u00e9rents :  85\n</pre> <p>Comme dans le cours 5, nous allons cr\u00e9er un mapping pour passer de caract\u00e8res \u00e0 entier. Le mapping que nous faisons ici est une forme de tokenization la plus simple possible.</p> <p>La tokenization, qu'est ce que c'est ? : La tokenization est le processus de conversion d'un texte en s\u00e9quence d'entier o\u00f9 chaque entier peut correspondre \u00e0 un caract\u00e8re, un groupe de caract\u00e8re ou un mot selon les m\u00e9thodes employ\u00e9es.</p> <p>Balance entre Vocabulaire et taille de s\u00e9quence : Un bon tokenizer trouve une balance entre la taille du vocabulaire (26 pour toutes les lettres de l'alphabet et ~100 000 pour les nombre de mots de la langue fran\u00e7aise). Plus on a une taille de vocabulaire petite, plus les s\u00e9quences seront longues (le mot \"Bonjour\" est encod\u00e9 par 7 tokens si notre vocabulaire est au niveau du caract\u00e8re et un seul token si notre vocabulaire regroupe tous les mots de la langue fran\u00e7aise) et inversement. En pratique, les deux extr\u00e8mes sont probl\u00e9matiques et on cherche le juste milieu.</p> <p>Tokenizer de la litt\u00e9rature : Les tokenizers sont une part importante du bon fonctionnement d'un mod\u00e8le de langage. La fa\u00e7on de cr\u00e9er un bon tokenizer d\u00e9pend de la m\u00e9thode et des donn\u00e9es d'entra\u00eenement. Parmi les tokenizers les plus utilis\u00e9s, on retrouve SentencePiece de Google et tiktoken de OpenAI.</p> In\u00a0[7]: Copied! <pre># Creation d'un mapping de caract\u00e8re \u00e0 entiers et inversement\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n\nprint(encode(\"Bonjour \u00e0 tous\"))\nprint(decode(encode(\"Bonjour \u00e0 Tous\")))\n</pre> # Creation d'un mapping de caract\u00e8re \u00e0 entiers et inversement stoi = { ch:i for i,ch in enumerate(chars) } itos = { i:ch for i,ch in enumerate(chars) } encode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string  print(encode(\"Bonjour \u00e0 tous\")) print(decode(encode(\"Bonjour \u00e0 Tous\"))) <pre>[13, 50, 49, 46, 50, 56, 53, 1, 68, 1, 55, 50, 56, 54]\nBonjour \u00e0 Tous\n</pre> <p>On va transformer notre dataset en s\u00e9quence d'entier et le stocker sous forme de tenseur pytorch.</p> In\u00a0[8]: Copied! <pre>data = torch.tensor(encode(text), dtype=torch.long)\nprint(data[:250]) # Les 250 premiers caract\u00e8res encod\u00e9\n</pre> data = torch.tensor(encode(text), dtype=torch.long) print(data[:250]) # Les 250 premiers caract\u00e8res encod\u00e9 <pre>tensor([33, 12, 23, 64, 29, 16,  8,  0,  0, 16, 44,  1, 38, 45, 41, 49,  6,  1,\n        30, 37, 38, 45, 49, 41,  6,  1, 52, 56, 41, 47,  1, 39, 50, 49, 54, 41,\n        45, 47,  1, 48, 41,  1, 40, 50, 49, 49, 41, 54,  7, 55, 56, 11,  0,  0,\n        30, 12, 13, 20, 25, 16,  8,  0,  0, 33, 53, 37, 45, 48, 41, 49, 55,  6,\n         1, 45, 47,  1, 59,  1, 37,  1, 38, 45, 41, 49,  1, 40, 41, 54,  1, 49,\n        50, 56, 57, 41, 47, 47, 41, 54,  8,  1, 24, 50, 49,  1, 50, 49, 39, 47,\n        41,  1, 57, 41, 56, 55,  1, 53, 73, 54, 50, 47, 82, 48, 41, 49, 55,  1,\n        52, 56, 41,  1, 48, 37,  0, 39, 50, 56, 54, 45, 49, 41,  1, 73, 51, 50,\n        56, 54, 41,  1, 33, 45, 47, 47, 41, 38, 53, 41, 52, 56, 45, 49,  6,  1,\n        41, 55,  1, 47, 41, 54,  1, 37, 42, 42, 37, 45, 53, 41, 54,  1, 54, 50,\n        49, 55,  1, 55, 41, 47, 47, 41, 48, 41, 49, 55,  1, 37, 57, 37, 49, 39,\n        73, 41, 54,  6,  0, 52, 56, 41,  1, 46, 41,  1, 39, 53, 50, 45, 54,  1,\n        52, 56,  3, 45, 47, 54,  1, 41, 56, 54, 54, 41, 49, 55,  1, 73, 55, 73,\n         1, 48, 37, 53, 45, 73, 54,  1, 40, 72, 54,  1, 37, 56, 46, 50])\n</pre> <p>On va maintenant d\u00e9couper notre texte en une partie training et une partie validation. Prenons un ratio de 0.9-0.1.</p> In\u00a0[9]: Copied! <pre>n = int(0.9*len(data)) # 90% pour le train et 10% pour la validation\ntrain_data = data[:n]\nval_data = data[n:]\n</pre> n = int(0.9*len(data)) # 90% pour le train et 10% pour la validation train_data = data[:n] val_data = data[n:] <p>Pour notre mod\u00e8le de langage, on va \u00e9galement d\u00e9finir une taille de contexte block_size.</p> In\u00a0[10]: Copied! <pre>block_size = 8\ntrain_data[:block_size+1]\n</pre> block_size = 8 train_data[:block_size+1] Out[10]: <pre>tensor([33, 12, 23, 64, 29, 16,  8,  0,  0])</pre> <p>Ici, les 8 premiers caract\u00e8res represente le contexte et le 9\u00e8me est le label. Ce simple exemple regroupe en fait une multitude d'exemples car notre mod\u00e8le doit \u00eatre capable de pr\u00e9dire le prochain caract\u00e8re peu importe le contexte qu'il a en amont. Dans cette liste, on a donc 8 exemples qui sont les suivants :</p> In\u00a0[11]: Copied! <pre>x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"Quand l'entr\u00e9e est {context.numpy()} le label est : {target}\")\n</pre> x = train_data[:block_size] y = train_data[1:block_size+1] for t in range(block_size):     context = x[:t+1]     target = y[t]     print(f\"Quand l'entr\u00e9e est {context.numpy()} le label est : {target}\") <pre>Quand l'entr\u00e9e est [33] le label est : 12\nQuand l'entr\u00e9e est [33 12] le label est : 23\nQuand l'entr\u00e9e est [33 12 23] le label est : 64\nQuand l'entr\u00e9e est [33 12 23 64] le label est : 29\nQuand l'entr\u00e9e est [33 12 23 64 29] le label est : 16\nQuand l'entr\u00e9e est [33 12 23 64 29 16] le label est : 8\nQuand l'entr\u00e9e est [33 12 23 64 29 16  8] le label est : 0\nQuand l'entr\u00e9e est [33 12 23 64 29 16  8  0] le label est : 0\n</pre> <p>On sait maintenant comme cr\u00e9er un ensemble de entr\u00e9e/label \u00e0 partir d'un seul exemple. Adaptons cette m\u00e9thode pour un traitement en batch :</p> In\u00a0[12]: Copied! <pre>batch_size = 4 # La taille de batch (les s\u00e9quences calcul\u00e9s en parall\u00e8les)\nblock_size = 8 # La taille de contexte maximale pour une pr\u00e9diction du mod\u00e8le\n\ndef get_batch(split):\n    # On genere un batch de donn\u00e9es (sur train ou val)\n    data = train_data if split == 'train' else val_data\n    #On g\u00e9n\u00e9re batch_size indice de d\u00e9but de s\u00e9quence pris au hasard dans le dataset\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    # On stocke dans notre tenseur torch\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device) # On met les sur le GPU si on en a un \n    return x, y\n\nxb, yb = get_batch('train')\nprint('Entr\u00e9e : ')\nprint(xb.shape)\nprint(xb)\nprint('Labels :')\nprint(yb.shape)\nprint(yb)\n</pre> batch_size = 4 # La taille de batch (les s\u00e9quences calcul\u00e9s en parall\u00e8les) block_size = 8 # La taille de contexte maximale pour une pr\u00e9diction du mod\u00e8le  def get_batch(split):     # On genere un batch de donn\u00e9es (sur train ou val)     data = train_data if split == 'train' else val_data     #On g\u00e9n\u00e9re batch_size indice de d\u00e9but de s\u00e9quence pris au hasard dans le dataset     ix = torch.randint(len(data) - block_size, (batch_size,))     # On stocke dans notre tenseur torch     x = torch.stack([data[i:i+block_size] for i in ix])     y = torch.stack([data[i+1:i+block_size+1] for i in ix])     x, y = x.to(device), y.to(device) # On met les sur le GPU si on en a un      return x, y  xb, yb = get_batch('train') print('Entr\u00e9e : ') print(xb.shape) print(xb) print('Labels :') print(yb.shape) print(yb) <pre>Entr\u00e9e : \ntorch.Size([4, 8])\ntensor([[53, 69, 39, 41,  2,  0,  0, 27],\n        [53,  1, 56, 49,  1, 39, 84, 56],\n        [54, 11,  0,  0, 24, 12, 30, 14],\n        [ 1, 51, 72, 53, 41,  8,  0,  0]], device='cuda:0')\nLabels :\ntorch.Size([4, 8])\ntensor([[69, 39, 41,  2,  0,  0, 27, 19],\n        [ 1, 56, 49,  1, 39, 84, 56, 53],\n        [11,  0,  0, 24, 12, 30, 14, 12],\n        [51, 72, 53, 41,  8,  0,  0, 33]], device='cuda:0')\n</pre> <p>Chacun de ces 4 exemples regroupe 8 exemples distincts (comme expliqu\u00e9 pr\u00e9cedemment), cela fait donc un total de 32 exemples.</p> <p>Dans le cours 5 sur les NLP, nous avons vu le bigramme qui peut \u00eatre consid\u00e9r\u00e9 comme le mod\u00e8le de langage le plus simple et qui consiste \u00e0 pr\u00e9dire la prochain caract\u00e8re \u00e0 partir d'un unique caract\u00e8re de contexte. Notons $B$ pour le batch_size, $T$ pour le block_size et $C$ pour le vocab_size. Pour voir sa performance sur le dataset moliere.txt, impl\u00e9mentons le rapidement en pytorch :</p> In\u00a0[13]: Copied! <pre>class BigramLanguageModel(nn.Module):\n  def __init__(self, vocab_size):\n    super().__init__()\n    # Chaque token va directement lire la valeur du prochain \u00e0 partir d'une look-up table entrain\u00e9\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n  def forward(self, idx, targets=None):\n    # Taille (B,T)\n    logits = self.token_embedding_table(idx) \n    # Taille (B,T,C)\n    \n    # Pour g\u00e9rer le cas de la g\u00e9n\u00e9ration (pas de target)\n    if targets is None:\n      loss = None\n    else: # Cas de l'entra\u00eenement\n      B, T, C = logits.shape\n      logits = logits.view(B*T, C)\n      targets = targets.view(B*T)\n      loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\n  def generate(self, idx, max_new_tokens):\n    # idx est de la taille (B,T) avec T le contexte actuel\n    for _ in range(max_new_tokens):\n      # Forward du mod\u00e8le pour r\u00e9cuperer les pr\u00e9dictions\n      logits, _ = self(idx)\n      # On prend uniquement le dernier caract\u00e8re\n      logits = logits[:, -1, :] # devient (B, C)\n      # On applique la softmax pour r\u00e9cuperer les probabilit\u00e9s\n      probs = F.softmax(logits, dim=-1) # (B, C)\n      # On sample avec torch.multinomial\n      idx_next = torch.multinomial(probs, num_samples=1) # devient (B, 1)\n      # On ajouter l'\u00e9l\u00e9ment sample \u00e0 la s\u00e9quence actuelle\n      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size).to(device)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n</pre> class BigramLanguageModel(nn.Module):   def __init__(self, vocab_size):     super().__init__()     # Chaque token va directement lire la valeur du prochain \u00e0 partir d'une look-up table entrain\u00e9     self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)    def forward(self, idx, targets=None):     # Taille (B,T)     logits = self.token_embedding_table(idx)      # Taille (B,T,C)          # Pour g\u00e9rer le cas de la g\u00e9n\u00e9ration (pas de target)     if targets is None:       loss = None     else: # Cas de l'entra\u00eenement       B, T, C = logits.shape       logits = logits.view(B*T, C)       targets = targets.view(B*T)       loss = F.cross_entropy(logits, targets)      return logits, loss    def generate(self, idx, max_new_tokens):     # idx est de la taille (B,T) avec T le contexte actuel     for _ in range(max_new_tokens):       # Forward du mod\u00e8le pour r\u00e9cuperer les pr\u00e9dictions       logits, _ = self(idx)       # On prend uniquement le dernier caract\u00e8re       logits = logits[:, -1, :] # devient (B, C)       # On applique la softmax pour r\u00e9cuperer les probabilit\u00e9s       probs = F.softmax(logits, dim=-1) # (B, C)       # On sample avec torch.multinomial       idx_next = torch.multinomial(probs, num_samples=1) # devient (B, 1)       # On ajouter l'\u00e9l\u00e9ment sample \u00e0 la s\u00e9quence actuelle       idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)     return idx  m = BigramLanguageModel(vocab_size).to(device) logits, loss = m(xb, yb) print(logits.shape) print(loss) <pre>torch.Size([32, 85])\ntensor(4.6802, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)\n</pre> <p>Le mod\u00e8le est implement\u00e9 mais non entra\u00een\u00e9, si on le teste comme \u00e7a on obtient des r\u00e9sultats catastrophiques :</p> In\u00a0[14]: Copied! <pre>base=torch.zeros((1, 1), dtype=torch.long).to(device) # Le premier \u00e9l\u00e9ment est un 0 (token de retour \u00e0 la ligne)\n# On g\u00e9n\u00e8re 100 \u00e9l\u00e9ments\nprint(decode(m.generate(idx = base , max_new_tokens=100)[0].tolist()))\n</pre> base=torch.zeros((1, 1), dtype=torch.long).to(device) # Le premier \u00e9l\u00e9ment est un 0 (token de retour \u00e0 la ligne) # On g\u00e9n\u00e8re 100 \u00e9l\u00e9ments print(decode(m.generate(idx = base , max_new_tokens=100)[0].tolist())) <pre>\nCZjb!DzPG\u0152R?'h\u00f4.\u00f9\ncddhhf,s\u00e9\u00c7qmp.\u00c9Mj\u00f4C\u00f9\u00caF:TAFY\u00e8L  \u00e0P;zbVm\u00ebtuPipL.\u00f4HtSE\u00e9,t:\u00e6\u00e9\u00c9Y\u00c8\u00ec\u00ef\u00eb?VGYxo\u00f9y\u00e7n\u00ef'lp\u00f4H\u00e0!\u00f4\n</pre> <p>C'est tout simplement al\u00e9atoire et c'est logique car le mod\u00e8le est initialis\u00e9 al\u00e9atoirement.</p> <p>On va maintenant entrainer le mod\u00e8le :</p> In\u00a0[15]: Copied! <pre>optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\nbatch_size = 32\nsteps=10000\nfor step in range(steps): # Nombre d'\u00e9tape d'entra\u00eenement (\u00e9lements trait\u00e9s = steps*batch_size)\n\n    # On r\u00e9cup\u00e8re un batch de donn\u00e9es al\u00e9atoires\n    xb, yb = get_batch('train')\n    # On calcule le loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    # Retropropagation\n    loss.backward()\n    # Mise \u00e0 jour des poids du mod\u00e8le\n    optimizer.step()\n\nprint(loss.item())\n</pre> optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) batch_size = 32 steps=10000 for step in range(steps): # Nombre d'\u00e9tape d'entra\u00eenement (\u00e9lements trait\u00e9s = steps*batch_size)      # On r\u00e9cup\u00e8re un batch de donn\u00e9es al\u00e9atoires     xb, yb = get_batch('train')     # On calcule le loss     logits, loss = m(xb, yb)     optimizer.zero_grad(set_to_none=True)     # Retropropagation     loss.backward()     # Mise \u00e0 jour des poids du mod\u00e8le     optimizer.step()  print(loss.item()) <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <pre>2.2493152618408203\n</pre> <p>G\u00e9n\u00e9rons \u00e0 partir de notre mod\u00e8le entrain\u00e9 :</p> In\u00a0[16]: Copied! <pre>print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=300)[0].tolist()))\n</pre> print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=300)[0].tolist())) <pre>\n\n\nELASGOX\u00fb\u00cf\u00ef!\nANDann donde se ns ntrar pous fa \u00e0TEn!.\n\nTELITEL'enomouv\u00fb\u00fbKbeue\nSGAvore oue mesontre\nt de pou n qur quvabou qude dente je p\u00e8re e em'eni\n\nLa d'euh\u00e8mpon, j'es en paiqus de rau plenoil\u00e0 jonont DARLysontausqus es ei voisangur s ve.\n\n\n\nDO lar dire tr\u00e9 quseuqu'arme \u00e0 ai? t pe ne ndome l pa, \n</pre> <p>On constate une am\u00e9lioration dans la structuration des donn\u00e9es et certains mots semblent presque correct mais \u00e7a reste catastrophique. En soit, on s'attendait \u00e0 ce r\u00e9sultat car le bigramme est un mod\u00e8le trop simple.</p> <p>Nous allons maintenant pr\u00e9senter pas \u00e0 pas le concept de self-attention qui est un concept cl\u00e9 de l'architecture d'un transformer.</p> <p>On va commencer par une id\u00e9e simple. On a un tenseur de taille $(B,T,C)$, on veut que chaque \u00e9l\u00e9ment T soit la moyenne de l'\u00e9l\u00e9ment actuel et des \u00e9l\u00e9ments pr\u00e9c\u00e9dents mais sans tenir compte des \u00e9l\u00e9ments suivants. C'est la fa\u00e7on la plus triviale de donner une importance aux \u00e9l\u00e9ments pr\u00e9c\u00e9dents pour pr\u00e9dire la valeur actuelle (ce qui est l'id\u00e9e derri\u00e8re le m\u00e9canisme d'attention).</p> <p>En python, on peut impl\u00e9menter l'id\u00e9e de cette mani\u00e8re :</p> In\u00a0[20]: Copied! <pre># Cr\u00e9ation de notre tenseur random\nB,T,C = 4,4,2\nx = torch.randn(B,T,C)\nx.shape\n</pre> # Cr\u00e9ation de notre tenseur random B,T,C = 4,4,2 x = torch.randn(B,T,C) x.shape Out[20]: <pre>torch.Size([4, 4, 2])</pre> In\u00a0[21]: Copied! <pre># Calcul de la moyenne des \u00e9l\u00e9ments pr\u00e9c\u00e9dents (incluant l'\u00e9l\u00e9ment actuel) pour chaque valeur.\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\nprint(x[0])\nprint(xbow[0])\n</pre> # Calcul de la moyenne des \u00e9l\u00e9ments pr\u00e9c\u00e9dents (incluant l'\u00e9l\u00e9ment actuel) pour chaque valeur. xbow = torch.zeros((B,T,C)) for b in range(B):     for t in range(T):         xprev = x[b,:t+1] # (t,C)         xbow[b,t] = torch.mean(xprev, 0) print(x[0]) print(xbow[0]) <pre>tensor([[ 1.5023, -0.5911],\n        [ 1.0199, -0.2976],\n        [-1.7581,  0.0969],\n        [ 0.7444, -0.3360]])\ntensor([[ 1.5023, -0.5911],\n        [ 1.2611, -0.4443],\n        [ 0.2547, -0.2639],\n        [ 0.3771, -0.2819]])\n</pre> <p>On a bien ce qu'on voulait, si vous faites les calculs chaque \u00e9l\u00e9ment correspond aux \u00e0 la moyenne de l'\u00e9l\u00e9ment actuel avec les \u00e9l\u00e9ments pr\u00e9c\u00e9dents.</p> <p>Par contre, on sait que les boucles for sont inefficaces lors du calcul. On voudrait plut\u00f4t une op\u00e9ration matricielle pour effectuer la m\u00eame op\u00e9ration.</p> <p>Multiplication Matricielle : Matrice $(3 \\times 3)$ par Matrice $(3 \\times 2)$ Matrices de d\u00e9part</p> <p>Soit la matrice $A$ de dimensions $(3 \\times 3)$ :</p> <p>$A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{pmatrix}$</p> <p>et la matrice $B$ de dimensions $(3 \\times 2)$ :</p> <p>$B = \\begin{pmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\\\ b_{31} &amp; b_{32} \\end{pmatrix}$</p> <p>La multiplication matricielle $C = A \\times B$ donne une matrice $C$ de dimensions $(3 \\times 2)$ :</p> <p>$C = \\begin{pmatrix} c_{11} &amp; c_{12} \\\\ c_{21} &amp; c_{22} \\\\ c_{31} &amp; c_{32} \\end{pmatrix}$</p> <p>o\u00f9 chaque \u00e9l\u00e9ment $c_{ij}$ est calcul\u00e9 comme suit :</p> <p>$c_{ij} = \\sum_{k=1}^{3} a_{ik} \\cdot b_{kj}$</p> <p>C'est-\u00e0-dire :</p> <ul> <li>$c_{11} = a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}$</li> <li>$c_{12} = a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32}$</li> <li>$c_{21} = a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31}$</li> <li>$c_{22} = a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}$</li> <li>$c_{31} = a_{31}b_{11} + a_{32}b_{21} + a_{33}b_{31}$</li> <li>$c_{32} = a_{31}b_{12} + a_{32}b_{22} + a_{33}b_{32}$</li> </ul> <p>Voici un exemple en python qui illustre cela :</p> In\u00a0[25]: Copied! <pre>a = torch.ones(3, 3)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n</pre> a = torch.ones(3, 3) b = torch.randint(0,10,(3,2)).float() c = a @ b print('a=') print(a) print('--') print('b=') print(b) print('--') print('c=') print(c) <pre>a=\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n--\nb=\ntensor([[7., 6.],\n        [5., 0.],\n        [1., 8.]])\n--\nc=\ntensor([[13., 14.],\n        [13., 14.],\n        [13., 14.]])\n</pre> <p>C'est maitenant que la magie op\u00e9re. Lorsque, au lieu d'une matrice de 1, on prend une matrice triangulaire inf\u00e9rieure et qu'on refait le calcul :</p> In\u00a0[26]: Copied! <pre>a = torch.tril(torch.ones(3, 3))\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n</pre> a = torch.tril(torch.ones(3, 3)) b = torch.randint(0,10,(3,2)).float() c = a @ b print('a=') print(a) print('--') print('b=') print(b) print('--') print('c=') print(c) <pre>a=\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n--\nb=\ntensor([[1., 2.],\n        [1., 4.],\n        [6., 6.]])\n--\nc=\ntensor([[ 1.,  2.],\n        [ 2.,  6.],\n        [ 8., 12.]])\n</pre> <p>Chaque valeur de la matrice est la somme de la valeur actuelle et des valeurs pr\u00e9c\u00e9dentes. C'est presque ce que l'on veut ! Il suffit alors de normaliser selon les lignes :</p> In\u00a0[27]: Copied! <pre>a = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n</pre> a = torch.tril(torch.ones(3, 3)) a = a / torch.sum(a, 1, keepdim=True) b = torch.randint(0,10,(3,2)).float() c = a @ b print('a=') print(a) print('--') print('b=') print(b) print('--') print('c=') print(c) <pre>a=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[1., 2.],\n        [8., 6.],\n        [9., 8.]])\n--\nc=\ntensor([[1.0000, 2.0000],\n        [4.5000, 4.0000],\n        [6.0000, 5.3333]])\n</pre> <p>Et voil\u00e0, le tour est jou\u00e9 ! On a remplac\u00e9 notre double boucle for par une simple multiplication matricielle et une normalisation des valeurs.</p> <p>On va maintenant l'utiliser pour calculer xbow et comparer sa valeur avec la valeur que l'on avait calcul\u00e9 avec notre double boucle :</p> In\u00a0[29]: Copied! <pre>wei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C) fonctionne gr\u00e2ce au broadcasting de pytorch\ntorch.allclose(xbow, xbow2) # V\u00e9rifie que tous les \u00e9l\u00e9ments sont identiques\n</pre> wei = torch.tril(torch.ones(T, T)) wei = wei / wei.sum(1, keepdim=True) xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C) fonctionne gr\u00e2ce au broadcasting de pytorch torch.allclose(xbow, xbow2) # V\u00e9rifie que tous les \u00e9l\u00e9ments sont identiques Out[29]: <pre>True</pre> <p>A la place de la normalisation, on peut utiliser la fonction softmax.</p> In\u00a0[38]: Copied! <pre>tril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\n# On met toutes les valeurs \u00e9gales \u00e0 0 \u00e0 la valeur -inf\nwei = wei.masked_fill(tril == 0, float('-inf'))\nprint(wei)\n</pre> tril = torch.tril(torch.ones(T, T)) wei = torch.zeros((T,T)) # On met toutes les valeurs \u00e9gales \u00e0 0 \u00e0 la valeur -inf wei = wei.masked_fill(tril == 0, float('-inf')) print(wei) <pre>tensor([[0., -inf, -inf, -inf],\n        [0., 0., -inf, -inf],\n        [0., 0., 0., -inf],\n        [0., 0., 0., 0.]])\n</pre> <p>On peut maintenant appliquer la softmax sur la matrice et TADAAA :</p> In\u00a0[39]: Copied! <pre>wei = F.softmax(wei, dim=-1)\nprint(wei)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n</pre> wei = F.softmax(wei, dim=-1) print(wei) xbow3 = wei @ x torch.allclose(xbow, xbow3) <pre>tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500]])\n</pre> Out[39]: <pre>True</pre> <p>En pratique, la version avec softmax est utilis\u00e9e pour la couche self-attention.</p> <p>Actuellement, la matrice $wei$ contient des valeurs uniformes sur chaque ligne ce qui ne donne aucune r\u00e9elle information sur l'importance des informations pr\u00e9c\u00e9dente.</p> <p>C'est l\u00e0 que le concept de self-attention intervient. Ce qu'on voudrait, c'est une matrice $wei$ que l'on peut entra\u00eener.</p> <p>On va cr\u00e9er 3 valeurs \u00e0 partir de notre valeur de $x$ :</p> <p>query : Qu'est ce que je recherche ? Cette valeur repr\u00e9sente ce que chaque position de la s\u00e9quence essaye de trouver dans les autres positions.</p> <p>key : Qu'est ce que je contiens ? Cette valeur repr\u00e9sente ce que chaque position de la s\u00e9quence contient comme information qui pourrait \u00eatre pertinente pour d'autres positions.</p> <p>value : Quelle est ma valeur ? Cette valeur repr\u00e9sente l'information r\u00e9elle \u00e0 extraire de chaque position de la s\u00e9quence si elle est jug\u00e9e pertinente.</p> <p>Pour extraire les valeurs query, key et value, on utilise une couche lin\u00e9aire qui projette l'entr\u00e9e dans une dimension head_size.</p> <p>Pour calculer l'importance d'un \u00e9l\u00e9ments pr\u00e9c\u00e9dent de la s\u00e9quence par rapport \u00e0 un l'\u00e9l\u00e9ment actuel. On effectue le produit scalaire entre les query Q et les key K(tranpos\u00e9e) : $wei = QK^T$</p> <p>Pour obtenir des poids d'attention (somme \u00e9gale \u00e0 1), on applique la softmax et on multiplie par les value V : $Output = \\text{softmax}\\left(wei\\right) \\cdot V$</p> <p></p> <p>En python, on l'impl\u00e9mente de cette mani\u00e8re :</p> In\u00a0[40]: Copied! <pre>B,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n\nhead_size = 16 # Valeur de head_size (projection de x)\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\nwei = wei.masked_fill(tril == 0, float('-inf')) # Pour appliquer le softmax, il faut des valeurs -inf\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n\nout.shape\n</pre> B,T,C = 4,8,32 # batch, time, channels x = torch.randn(B,T,C)   head_size = 16 # Valeur de head_size (projection de x) key = nn.Linear(C, head_size, bias=False) query = nn.Linear(C, head_size, bias=False) value = nn.Linear(C, head_size, bias=False) k = key(x)   # (B, T, 16) q = query(x) # (B, T, 16) wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)  tril = torch.tril(torch.ones(T, T)) wei = wei.masked_fill(tril == 0, float('-inf')) # Pour appliquer le softmax, il faut des valeurs -inf wei = F.softmax(wei, dim=-1)  v = value(x) out = wei @ v  out.shape Out[40]: <pre>torch.Size([4, 8, 16])</pre> <p>Notre matrice $wei$ est donc maintenant enti\u00e8rement entra\u00eenable et il est donc possible d'utiliser cette couche pour l'entra\u00eenement d'un r\u00e9seau de neurones.</p> <p>**Notes sur la couche *self-attention*** :</p> <ul> <li>L'attention est un m\u00e9canisme de communication qui peut \u00eatre vu comme un graphe avec des connexions entre les noeuds (dans notre cas, les noeuds de fin sont connect\u00e9s \u00e0 l'ensemble des noeuds pr\u00e9c\u00e9dents).</li> <li>Dans la couche d'attention, il n'y a aucune notion de la position des \u00e9l\u00e9ments les uns par rapport aux autres. Pour combler ce probl\u00e8me, il faudra rajouter un positionnal_embedding (voir suite du cours).</li> <li>Pour pr\u00e9cision, il n'y a aucune interaction le long de la dimension batch, chaque \u00e9l\u00e9ment du batch est trait\u00e9 ind\u00e9pendamment des autres. C'est un peu comme si on avait batch_size graphes ind\u00e9pendants.</li> <li>Ce block d'attention est appel\u00e9 decoder block. Il a la particularit\u00e9 que chaque \u00e9l\u00e9ment ne communique qu'avec le pass\u00e9 (gr\u00e2ce \u00e0 la matrice triangulaire inf\u00e9rieure). Cependant, il existe d'autres couches d'attention (encoder) qui permettent la communication de tous les \u00e9l\u00e9ments les uns avec les autres (pour la traduction, l'analyse de sentiments ou encore le traitement d'images)</li> <li>On parle de self-attention parce que les query, key and value viennent de la m\u00eame source. Il est possible d'avoir des query, key et value qui proviennent de sources diff\u00e9rentes, on parle alors de cross-attention.</li> <li>Si vous lisez le papier Attention is all you need, vous constaterez qu'il y a une normalisation par la racine de la head_size :  Cela permet une stabilit\u00e9 de la fonction softmax lors de l'initialisation des poids en particulier.</li> </ul> <p>Impl\u00e9mentons maintenant une classe head qui va effectuer les op\u00e9ration de la self-attention. C'est simplement ce que l'on a vu au dessus sous forme de classe.</p> In\u00a0[\u00a0]: Copied! <pre>class Head(nn.Module):\n    \"\"\" Couche de self-attention unique \"\"\"\n\n    def __init__(self, head_size,n_embd,dropout=0.2):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        # Ajout de dropout pour la regularization\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # Le * C**-0.5 correspond \u00e0 la normalisation par la racine de head_size\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n</pre> class Head(nn.Module):     \"\"\" Couche de self-attention unique \"\"\"      def __init__(self, head_size,n_embd,dropout=0.2):         super().__init__()         self.key = nn.Linear(n_embd, head_size, bias=False)         self.query = nn.Linear(n_embd, head_size, bias=False)         self.value = nn.Linear(n_embd, head_size, bias=False)         self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))          # Ajout de dropout pour la regularization         self.dropout = nn.Dropout(dropout)      def forward(self, x):         B,T,C = x.shape         k = self.key(x)   # (B,T,C)         q = self.query(x) # (B,T,C)         # Le * C**-0.5 correspond \u00e0 la normalisation par la racine de head_size         wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)         wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)         wei = F.softmax(wei, dim=-1) # (B, T, T)         wei = self.dropout(wei)         v = self.value(x) # (B,T,C)         out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)         return out <p>Dans la papier Attention is all you need, une variante de la self-attention est propos\u00e9e. Cette variante se nomme multi-head attention et consiste simplement \u00e0 avoir plusieurs couches de self-attention en parall\u00e8le. Le but de cette couche est de parall\u00e9liser le traitement pour que celui-ci soit plus rapide sur GPU.</p> <p></p> <p>L'impl\u00e9mentation est assez simple puisqu'il s'agit juste de plusieurs couches head.</p> In\u00a0[41]: Copied! <pre>class MultiHeadAttention(nn.Module):\n    \"\"\" Plusieurs couches de self attention en parall\u00e8le\"\"\"\n\n    def __init__(self, num_heads, head_size,n_embd,dropout):\n        super().__init__()\n        # Cr\u00e9ation de num_head couches head de taille head_size\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Couche pour Linear (voir schema) apr\u00e8s concatenation\n        self.proj = nn.Linear(n_embd, n_embd)\n        # Dropout si besoin\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n</pre> class MultiHeadAttention(nn.Module):     \"\"\" Plusieurs couches de self attention en parall\u00e8le\"\"\"      def __init__(self, num_heads, head_size,n_embd,dropout):         super().__init__()         # Cr\u00e9ation de num_head couches head de taille head_size         self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])         # Couche pour Linear (voir schema) apr\u00e8s concatenation         self.proj = nn.Linear(n_embd, n_embd)         # Dropout si besoin         self.dropout = nn.Dropout(dropout)      def forward(self, x):         out = torch.cat([h(x) for h in self.heads], dim=-1)         out = self.dropout(self.proj(out))         return out <p>Un dernier \u00e9l\u00e9ment du transformer que l'on peut voir dans le papier Attention is all you need est la couche Feed Forward qui est simplement un petit fully connected network.</p> <p>On l'impl\u00e9mente en python comme cela :</p> In\u00a0[44]: Copied! <pre>class FeedFoward(nn.Module):\n\n    def __init__(self, n_embd,dropout):\n        super().__init__()\n        self.net = nn.Sequential(\n            # 4*n_embd comme dans le papier\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</pre> class FeedFoward(nn.Module):      def __init__(self, n_embd,dropout):         super().__init__()         self.net = nn.Sequential(             # 4*n_embd comme dans le papier             nn.Linear(n_embd, 4 * n_embd),             nn.ReLU(),             nn.Linear(4 * n_embd, n_embd),             nn.Dropout(dropout),         )      def forward(self, x):         return self.net(x) <p>On a maitenant tous les \u00e9l\u00e9ments pour impl\u00e9menter notre couche transformer qui va utiliser multi-head attention et feed forward. Sur la figure principale du papier, on remarque \u00e9galement qu'il a des connexions r\u00e9siduelles entre l'input et l'output des couches d'attention et de feed forward. Ces connexions permettent de faciliter l'entra\u00eenement d'un mod\u00e8le profond (plus de d\u00e9tails dans la papier Deep Residual Learning for Image Recognition). On va donc \u00e9galement impl\u00e9menter ces connexions r\u00e9siduelles. Pour ce qui est de la layer norm, nous n'allons pas entrer dans les d\u00e9tails ici mais on peut comparer son utilit\u00e9 \u00e0 une couche de batch norm (plus de d\u00e9tails dans ce blogpost). Nous utilisons donc simplement l'impl\u00e9mentation pytorch de la layer norm.</p> <p>Voici l'impl\u00e9mentation python :</p> In\u00a0[43]: Copied! <pre>class TransformerBlock(nn.Module):\n    \"\"\" Block transformer\"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x)) # x+ car c'est une connexion r\u00e9siduelle\n        x = x + self.ffwd(self.ln2(x))\n        return x\n</pre> class TransformerBlock(nn.Module):     \"\"\" Block transformer\"\"\"      def __init__(self, n_embd, n_head):         super().__init__()         head_size = n_embd // n_head         self.sa = MultiHeadAttention(n_head, head_size)         self.ffwd = FeedFoward(n_embd)         self.ln1 = nn.LayerNorm(n_embd)         self.ln2 = nn.LayerNorm(n_embd)      def forward(self, x):         x = x + self.sa(self.ln1(x)) # x+ car c'est une connexion r\u00e9siduelle         x = x + self.ffwd(self.ln2(x))         return x <p>Note : On applique la layer norm avant les couches (contrairement au papier). C'est la seule partie du transformer qui a \u00e9t\u00e9 modifi\u00e9e depuis la publication du papier et qui am\u00e9liorer les performances.</p> <p>Pour plus de clart\u00e9, nous allons cr\u00e9er notre mod\u00e8le et l'optimiser dans le notebook suivant.</p>"},{"location":"07_Transformers/02_GptFromScratch.html#construisons-gpt-a-partir-de-rien","title":"Construisons GPT \u00e0 partir de rien\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#lecture-du-dataset","title":"Lecture du dataset\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#creation-de-notre-dataset-dentrainement","title":"Cr\u00e9ation de notre dataset d'entra\u00eenement\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#point-rapide-sur-la-tokenization","title":"Point rapide sur la tokenization\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#modele-bigramme","title":"Mod\u00e8le bigramme\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#self-attention","title":"Self-Attention\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#quest-ce-que-que-lon-veut-faire","title":"Qu'est ce que que l'on veut faire ?\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#rappel-sur-la-multiplication-entre-deux-matrices","title":"Rappel sur la multiplication entre deux matrices\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#lastuce-mathematique-pour-le-self-attention","title":"L'astuce math\u00e9matique pour le self-attention\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#self-attention-le-coeur-du-transformer","title":"Self-Attention : le coeur du transformer\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#multi-head-attention","title":"Multi-Head Attention\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#feed-forward-layer","title":"Feed Forward layer\u00b6","text":""},{"location":"07_Transformers/02_GptFromScratch.html#couche-transformer","title":"Couche transformer\u00b6","text":""},{"location":"07_Transformers/03_TrainingOurGpt.html","title":"Entra\u00eenons notre mod\u00e8le GPT","text":"<p>Ce notebook s'appuie sur le notebook pr\u00e9c\u00e9dent. Il est fortement conseill\u00e9 de faire le notebook pr\u00e9c\u00e9dent avant celui-ci.</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n</pre> import torch import torch.nn as nn from torch.nn import functional as F In\u00a0[2]: Copied! <pre>batch_size = 64 \nblock_size = 128 # Longueur du contexte \nmax_iters = 5000 # Nombre d'it\u00e9rations d'entra\u00eenement\neval_interval = 500 # Intervalle pour l'\u00e9valuation sur les donn\u00e9es de validation\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 128 # Dimension de la couche d'attention\nn_head = 4 # Nombre de head d'attention (pour la multi-head attention)\nn_layer = 3 # Nombre de couches d'attention\ndropout = 0.2\n</pre> batch_size = 64  block_size = 128 # Longueur du contexte  max_iters = 5000 # Nombre d'it\u00e9rations d'entra\u00eenement eval_interval = 500 # Intervalle pour l'\u00e9valuation sur les donn\u00e9es de validation learning_rate = 3e-4 device = 'cuda' if torch.cuda.is_available() else 'cpu' eval_iters = 200 n_embd = 128 # Dimension de la couche d'attention n_head = 4 # Nombre de head d'attention (pour la multi-head attention) n_layer = 3 # Nombre de couches d'attention dropout = 0.2 In\u00a0[3]: Copied! <pre>with open('moliere.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # 90% pour le train et 10% pour la validation\ntrain_data = data[:n]\nval_data = data[n:]\ndef get_batch(split):\n    # On genere un batch de donn\u00e9es (sur train ou val)\n    data = train_data if split == 'train' else val_data\n    #On g\u00e9n\u00e9re batch_size indice de d\u00e9but de s\u00e9quence pris au hasard dans le dataset\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    # On stocke dans notre tenseur torch\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device) # On met les sur le GPU si on en a un \n    return x, y\n\n@torch.no_grad()\n# Fonction pour estimer le loss plus pr\u00e9cisement\ndef estimate_loss(model):\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            _, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n</pre> with open('moliere.txt', 'r', encoding='utf-8') as f:     text = f.read() chars = sorted(list(set(text))) vocab_size = len(chars) stoi = { ch:i for i,ch in enumerate(chars) } itos = { i:ch for i,ch in enumerate(chars) } encode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string data = torch.tensor(encode(text), dtype=torch.long) n = int(0.9*len(data)) # 90% pour le train et 10% pour la validation train_data = data[:n] val_data = data[n:] def get_batch(split):     # On genere un batch de donn\u00e9es (sur train ou val)     data = train_data if split == 'train' else val_data     #On g\u00e9n\u00e9re batch_size indice de d\u00e9but de s\u00e9quence pris au hasard dans le dataset     ix = torch.randint(len(data) - block_size, (batch_size,))     # On stocke dans notre tenseur torch     x = torch.stack([data[i:i+block_size] for i in ix])     y = torch.stack([data[i+1:i+block_size+1] for i in ix])     x, y = x.to(device), y.to(device) # On met les sur le GPU si on en a un      return x, y  @torch.no_grad() # Fonction pour estimer le loss plus pr\u00e9cisement def estimate_loss(model):     out = {}     model.eval()     for split in ['train', 'val']:         losses = torch.zeros(eval_iters)         for k in range(eval_iters):             X, Y = get_batch(split)             _, loss = model(X, Y)             losses[k] = loss.item()         out[split] = losses.mean()     model.train()     return out In\u00a0[4]: Copied! <pre>class Head(nn.Module):\n    \"\"\" Couche de self-attention unique \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        # Ajout de dropout pour la regularization\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # Le * C**-0.5 correspond \u00e0 la normalisation par la racine de head_size\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" Plusieurs couches de self attention en parall\u00e8le\"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        # Cr\u00e9ation de num_head couches head de taille head_size\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Couche pour Linear (voir schema) apr\u00e8s concatenation\n        self.proj = nn.Linear(n_embd, n_embd)\n        # Dropout si besoin\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            # 4*n_embd comme dans le papier\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GeLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass TransformerBlock(nn.Module):\n    \"\"\" Block transformer\"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x)) # x+ car c'est une connexion r\u00e9siduelle\n        x = x + self.ffwd(self.ln2(x))\n        return x\n</pre> class Head(nn.Module):     \"\"\" Couche de self-attention unique \"\"\"      def __init__(self, head_size):         super().__init__()         self.key = nn.Linear(n_embd, head_size, bias=False)         self.query = nn.Linear(n_embd, head_size, bias=False)         self.value = nn.Linear(n_embd, head_size, bias=False)         self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))          # Ajout de dropout pour la regularization         self.dropout = nn.Dropout(dropout)      def forward(self, x):         B,T,C = x.shape         k = self.key(x)   # (B,T,C)         q = self.query(x) # (B,T,C)         # Le * C**-0.5 correspond \u00e0 la normalisation par la racine de head_size         wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)         wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)         wei = F.softmax(wei, dim=-1) # (B, T, T)         wei = self.dropout(wei)         v = self.value(x) # (B,T,C)         out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)         return out  class MultiHeadAttention(nn.Module):     \"\"\" Plusieurs couches de self attention en parall\u00e8le\"\"\"      def __init__(self, num_heads, head_size):         super().__init__()         # Cr\u00e9ation de num_head couches head de taille head_size         self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])         # Couche pour Linear (voir schema) apr\u00e8s concatenation         self.proj = nn.Linear(n_embd, n_embd)         # Dropout si besoin         self.dropout = nn.Dropout(dropout)      def forward(self, x):         out = torch.cat([h(x) for h in self.heads], dim=-1)         out = self.dropout(self.proj(out))         return out  class FeedFoward(nn.Module):      def __init__(self, n_embd):         super().__init__()         self.net = nn.Sequential(             # 4*n_embd comme dans le papier             nn.Linear(n_embd, 4 * n_embd),             nn.GeLU(),             nn.Linear(4 * n_embd, n_embd),             nn.Dropout(dropout),         )      def forward(self, x):         return self.net(x)  class TransformerBlock(nn.Module):     \"\"\" Block transformer\"\"\"      def __init__(self, n_embd, n_head):         super().__init__()         head_size = n_embd // n_head         self.sa = MultiHeadAttention(n_head, head_size)         self.ffwd = FeedFoward(n_embd)         self.ln1 = nn.LayerNorm(n_embd)         self.ln2 = nn.LayerNorm(n_embd)      def forward(self, x):         x = x + self.sa(self.ln1(x)) # x+ car c'est une connexion r\u00e9siduelle         x = x + self.ffwd(self.ln2(x))         return x <p>Il est temps d'impl\u00e9menter notre propre GPT. Dans l'id\u00e9e, il s'agit d'un agencement de block transformer avec \u00e9galement des matrices d'embeddings pour convertir les tokens en embeddings.</p> In\u00a0[5]: Copied! <pre>class GPT(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # Chaque token recupere son embedding \u00e0 partir d'une look-up table (entrainable)\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Agencement de n_layer TransformerBlock de taille n_embed avec n_head heads.\n        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # Une mani\u00e8re optimile d'initialiser les poids (\u00e0 regarder plus en d\u00e9tails si \u00e7a vous interesse)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx et targets sont des tenseurs d'entier de taille (B,T)\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        # Position embedding pour ajouter une information spatiale sur les \u00e9l\u00e9ments\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    # Fonction pour g\u00e9n\u00e9rer du texte : tr\u00e8s proche de celle utilis\u00e9e dans le mod\u00e8le bigramme du notebook pr\u00e9c\u00e9dent\n    def generate(self, idx, max_new_tokens):\n        # idx contient des entiers dans un tenseur de taille (B, T), c'est le contexte actuel\n        for _ in range(max_new_tokens):\n            # On limite le contexte \u00e0 block_size (maximum que le r\u00e9seau peut prendre)\n            idx_cond = idx[:, -block_size:]\n            # On calcule la pr\u00e9dictions du prochain token\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] # becomes (B, C)\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # On sample depuis les probabilit\u00e9s obtenues avec le softmax\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # On ajouter l'\u00e9l\u00e9ment sample \u00e0 notre texte\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n</pre> class GPT(nn.Module):      def __init__(self):         super().__init__()         # Chaque token recupere son embedding \u00e0 partir d'une look-up table (entrainable)         self.token_embedding_table = nn.Embedding(vocab_size, n_embd)         self.position_embedding_table = nn.Embedding(block_size, n_embd)         # Agencement de n_layer TransformerBlock de taille n_embed avec n_head heads.         self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])         self.ln_f = nn.LayerNorm(n_embd) # final layer norm         self.lm_head = nn.Linear(n_embd, vocab_size)          # Une mani\u00e8re optimile d'initialiser les poids (\u00e0 regarder plus en d\u00e9tails si \u00e7a vous interesse)         self.apply(self._init_weights)      def _init_weights(self, module):         if isinstance(module, nn.Linear):             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)             if module.bias is not None:                 torch.nn.init.zeros_(module.bias)         elif isinstance(module, nn.Embedding):             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)      def forward(self, idx, targets=None):         B, T = idx.shape          # idx et targets sont des tenseurs d'entier de taille (B,T)         tok_emb = self.token_embedding_table(idx) # (B,T,C)         # Position embedding pour ajouter une information spatiale sur les \u00e9l\u00e9ments         pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)         x = tok_emb + pos_emb # (B,T,C)         x = self.blocks(x) # (B,T,C)         x = self.ln_f(x) # (B,T,C)         logits = self.lm_head(x) # (B,T,vocab_size)          if targets is None:             loss = None         else:             B, T, C = logits.shape             logits = logits.view(B*T, C)             targets = targets.view(B*T)             loss = F.cross_entropy(logits, targets)          return logits, loss      # Fonction pour g\u00e9n\u00e9rer du texte : tr\u00e8s proche de celle utilis\u00e9e dans le mod\u00e8le bigramme du notebook pr\u00e9c\u00e9dent     def generate(self, idx, max_new_tokens):         # idx contient des entiers dans un tenseur de taille (B, T), c'est le contexte actuel         for _ in range(max_new_tokens):             # On limite le contexte \u00e0 block_size (maximum que le r\u00e9seau peut prendre)             idx_cond = idx[:, -block_size:]             # On calcule la pr\u00e9dictions du prochain token             logits, _ = self(idx_cond)             logits = logits[:, -1, :] # becomes (B, C)             probs = F.softmax(logits, dim=-1) # (B, C)             # On sample depuis les probabilit\u00e9s obtenues avec le softmax             idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)             # On ajouter l'\u00e9l\u00e9ment sample \u00e0 notre texte             idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)         return idx <p>Il est maintenant temps d'entra\u00eener le mod\u00e8le. L'entra\u00eenement peut \u00eatre assez long selon votre ordinateur et les hyperparam\u00e8tres que vous avez d\u00e9fini.</p> In\u00a0[6]: Copied! <pre>model = GPT()\nm = model.to(device)\n# print the number of parameters in the model\nprint(\"Nombre de param\u00e8tres du mod\u00e8le : \",sum(p.numel() for p in m.parameters()))\n\n# Optimizer AdamW\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # On evalue le loss sur train et validation de temps en temps (tous les eval_interval it\u00e9rations)\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss(model)\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # On recup\u00e8re un batch al\u00e9atoire du dataset\n    xb, yb = get_batch('train')\n\n    # Calcul du loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n</pre> model = GPT() m = model.to(device) # print the number of parameters in the model print(\"Nombre de param\u00e8tres du mod\u00e8le : \",sum(p.numel() for p in m.parameters()))  # Optimizer AdamW optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  for iter in range(max_iters):      # On evalue le loss sur train et validation de temps en temps (tous les eval_interval it\u00e9rations)     if iter % eval_interval == 0 or iter == max_iters - 1:         losses = estimate_loss(model)         print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")      # On recup\u00e8re un batch al\u00e9atoire du dataset     xb, yb = get_batch('train')      # Calcul du loss     logits, loss = model(xb, yb)     optimizer.zero_grad(set_to_none=True)     loss.backward()     optimizer.step() <pre>Nombre de param\u00e8tres du mod\u00e8le :  632149\n</pre> <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <pre>step 0: train loss 4.4823, val loss 4.4809\nstep 500: train loss 1.9838, val loss 2.0682\nstep 1000: train loss 1.6177, val loss 1.7505\nstep 1500: train loss 1.4865, val loss 1.6591\nstep 2000: train loss 1.4180, val loss 1.6133\nstep 2500: train loss 1.3637, val loss 1.5702\nstep 3000: train loss 1.3256, val loss 1.5459\nstep 3500: train loss 1.3022, val loss 1.5306\nstep 4000: train loss 1.2784, val loss 1.5076\nstep 4500: train loss 1.2622, val loss 1.4943\nstep 4999: train loss 1.2467, val loss 1.4835\n</pre> <p>On peut maintenant g\u00e9n\u00e9rer du \"Moli\u00e8re\" automatiquement !</p> In\u00a0[9]: Copied! <pre># Generation d'un texte par le mod\u00e8le\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n</pre> # Generation d'un texte par le mod\u00e8le context = torch.zeros((1, 1), dtype=torch.long, device=device) print(decode(m.generate(context, max_new_tokens=500)[0].tolist())) <pre>\nFMEROT.\n\nIl a rien qu'il mieux a plus que me le sortir engage.\n\nDONE ELVIRE.\n\nAh! ot, et vous bons aime aussi seule abuer?\n\nSGANARELLE.\n\nVous plaissez. Vous savez non pas!\n\nLE MONCTAGNE.\n\nVous \u00eates, cet autres que changus!\n\nDON JUAN.\n\nNe sont point tout ce qui ne vous d\u00e9sie.\n\nVAL\u00c8RE.\n\nUne son m\u00e9clai que exp\u00e8re\nQue je suis sous-moi qu'en le consie,\nQue je tiens peurt\u00e9 aun la r\u00e9gle, que je suis en me ducens mois.\nAu comoqu'on disant fait ce le vite avisenve\nQui voir de cettte r\u00e9joustoisse dis, et \n</pre> <p>Comme vous le voyez, \u00e7a ressemble (de loin) \u00e0 un texte de Moli\u00e8re. En augmentant le nombre de param\u00e8tres du mod\u00e8le, on peut obtenir beaucoup mieux mais le temps de traitement sera tr\u00e8s lent (surtout si vous n'avez pas de GPU).</p>"},{"location":"07_Transformers/03_TrainingOurGpt.html#entrainons-notre-modele-gpt","title":"Entra\u00eenons notre mod\u00e8le GPT\u00b6","text":""},{"location":"07_Transformers/03_TrainingOurGpt.html#hyperparametres","title":"Hyperparam\u00e8tres\u00b6","text":""},{"location":"07_Transformers/03_TrainingOurGpt.html#dataset","title":"Dataset\u00b6","text":""},{"location":"07_Transformers/03_TrainingOurGpt.html#couches-head-multi-head-feed-forward-et-transformerblock","title":"Couches Head, Multi-Head, Feed Forward et TransformerBlock\u00b6","text":""},{"location":"07_Transformers/03_TrainingOurGpt.html#implementation-de-notre-modele","title":"Impl\u00e9mentation de notre mod\u00e8le\u00b6","text":""},{"location":"07_Transformers/03_TrainingOurGpt.html#entrainement-du-modele","title":"Entrainement du mod\u00e8le\u00b6","text":""},{"location":"07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html","title":"Architecture et particularit\u00e9s du transformer","text":"<p>Jusqu'\u00e0 pr\u00e9sent, nous avons regard\u00e9 l'architecture d\u00e9codeur du transformer et uniquement la partie avec la masked multi-head self-attention. Dans cette partie, nous allons voir l'intuition derri\u00e8re le block encodeur et la multi-head cross-attention.</p> <p>Avant de pr\u00e9senter le block encodeur, il est n\u00e9cessaire de bien comprendre pourquoi nous n'avions besoin que du block d\u00e9codeur les notebooks pr\u00e9c\u00e9dents. Dans la figure suivante, l'encadr\u00e9 rouge indique la partie du mod\u00e8le que nous avons utilis\u00e9 (sans la seconde multi-head attention).</p> <p></p> <p>Le texte a la particularit\u00e9 de se lire de gauche \u00e0 droite. Il est donc logique de supposer qu'un mod\u00e8le qui g\u00e9n\u00e9re du texte va aussi prendre les informations de gauche \u00e0 droite. C'est ce qu'on appelle un mod\u00e8le autoregressif. On va pr\u00e9dire le token suivant compte tenu des tokens pr\u00e9c\u00e9dents (compris dans le contexte). C'est pourquoi, lors de l'entra\u00eenement, on utilise un masque pour ne pas \"voir\" les tokens situ\u00e9s apr\u00e8s le token actuel.</p> <p>Cependant, cette approche n'a du sens que dans des cas de \"g\u00e9n\u00e9ration de gauche \u00e0 droite\". Il y a \u00e9normement de cas o\u00f9 l'approche autoregressive n'est pas l'approche optimale. Parmis ces cas, on retrouve certaines approches de NLP comme la traduction ou l'analyse des \u00e9motions mais \u00e9galement des approches de vision. Actuellement, l'architecture du transformer a fait ces preuves dans la majorit\u00e9 des domaines utilisant le deep learning et dans bon nombre de cas, il ne s'agit pas de l'approche autogressive.</p> <p></p> <p>AR-LLM signifie auto regressive large language model.</p> <p>Le block encodeur est le block de gauche represent\u00e9 sur la figure de l'architecture (encadr\u00e9 en rouge \u00e0 nouveau).</p> <p></p> <p>La seule diff\u00e9rence avec le block utilis\u00e9 pr\u00e9cedemment est que la couche multi-head attention n'est pas masked. Pour faire l'analogie avec les notebooks pr\u00e9c\u00e9dents, cela signifie qu'on utilise une matrice compl\u00e8te au lieu d'une matrice triangulaire inf\u00e9rieure pour le calcul de l'attention.</p> <p>Concr\u00e9tement, cela signifie que chaque token de la s\u00e9quence d'entr\u00e9e peut interragir avec tous les autres tokens (situ\u00e9s avant ou apr\u00e8s). Dans des cas comme l'analyse de sentiments, c'est l'approche privil\u00e9gi\u00e9e car on a une s\u00e9quence connue en entr\u00e9e et on cherche \u00e0 pr\u00e9dire un label (positif, n\u00e9gatif ou neutre).</p> <p>Point sur l'analyse de sentiments : En NLP, l'analyse de sentiments consiste \u00e0 donner au mod\u00e8le un texte et lui demander de ressortir le sentiment associ\u00e9 \u00e0 ce texte. Par exemple, pour une critique de film, on veut que le mod\u00e8le pr\u00e9dise n\u00e9gatif pour la critique \"Ce film est un vrai navet\" et positif pour la critique \"Pour moi, c'est le meilleur film de tout les temps\".</p> <p>Point important \u00e0 consid\u00e9rer : Pour pr\u00e9dire le sentiment associ\u00e9 \u00e0 une phrase, on a besoin de ne passer qu'une seule fois dans le transformer. Pour la g\u00e9n\u00e9ration de texte, on doit faire appel au mod\u00e8le apr\u00e8s chaque token g\u00e9n\u00e9r\u00e9 (donc on passe 10 fois dans le mod\u00e8le pour g\u00e9n\u00e9rer 10 tokens).</p> <p>Les mod\u00e8les transformer bas\u00e9s uniquement sur le block encodeur ont de nombreuse utilisations en plus de la d\u00e9tection de sentiments : D\u00e9tection de spam, classification de documents, extraction d'entit\u00e9es nomm\u00e9es et recommendation de contenu. Nous verrons aussi que pour le traitement d'images, on utilise une variante du transformer pouvant s'apparenter \u00e0 un encodeur. Pour r\u00e9sumer, on pourrait dire que le block encoder est adapt\u00e9 aux t\u00e2ches de classification (d\u00e9tection et segmentation \u00e9galement pour les images).</p> <p>Il nous reste \u00e0 comprendre l'utilit\u00e9 de l'architecture compl\u00e8te. Pour rappel, l'article \"Attention Is All You Need\" qui introduit le transformer est un article de traduction automatique.</p> <p>Analysons le probl\u00e8me de traduction avant de comprendre le fonctionnement de l'architecture. En traduction, on dispose d'un texte dans une langue et on veut g\u00e9n\u00e9rer le m\u00eame texte dans une autre langue. On a donc d'une part, une partie g\u00e9n\u00e9ration (donc d\u00e9codeur) et d'autre part, une partie d'encodage de l'information que l'on a a disposition.</p> <p>Il faut imaginer que la partie d\u00e9codeur qui g\u00e9n\u00e8re les tokens va le faire avec d'une part les tokens g\u00e9n\u00e9r\u00e9s pr\u00e9cedemment mais \u00e9galement en interrogeant la partie encod\u00e9 via la couche de cross-attention.</p> <p>L'encoder du transformer prend en entr\u00e9e la s\u00e9quence source $x$ et produit une repr\u00e9sentation contextuelle pour chaque token de cette s\u00e9quence : $E = \\text{Encoder}(x)$ Ici, $E$ est une matrice de repr\u00e9sentations contextuelles pour la s\u00e9quence source $x$. Chaque ligne de $E$ correspond \u00e0 une repr\u00e9sentation contextuelle $e_i$ pour le token $x_i$.</p> <p>Le decoder du transformer prend en entr\u00e9e les repr\u00e9sentations contextuelles $E$ de l'encoder et g\u00e9n\u00e8re la s\u00e9quence cible $y$. \u00c0 chaque \u00e9tape de g\u00e9n\u00e9ration, le decoder produit un token $y_t$ en se basant sur les tokens g\u00e9n\u00e9r\u00e9s pr\u00e9c\u00e9demment et en interrogeant l'encoder via la couche de cross-attention : $y_t = \\text{Decoder}(y_{&lt;t}, E) $ Ici, $y_{&lt;t}$ repr\u00e9sente les tokens g\u00e9n\u00e9r\u00e9s pr\u00e9c\u00e9demment jusqu'\u00e0 l'\u00e9tape $t-1$. Le processus de g\u00e9n\u00e9ration utilise \u00e0 la fois le self-attention (pour capturer les d\u00e9pendances s\u00e9quentielles dans la s\u00e9quence cible) et la cross-attention (pour incorporer des informations de l'encoder $E$).</p> <p>La cross-attention permet au decoder de consulter les repr\u00e9sentations contextuelles $E$ de l'encoder pour obtenir des informations pertinentes lors de la g\u00e9n\u00e9ration de chaque token $y_t$. Elle est calcul\u00e9e comme : $ \\text{Cross-Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $ o\u00f9 $Q$ (query) sont les embeddings des tokens g\u00e9n\u00e9r\u00e9s pr\u00e9c\u00e9demment par le decoder, $K$(key) et $V$(value) sont les embeddings de l'encoder $E$, et $d_k$ est la dimension des embeddings $K$ pour la normalisation.</p> <p>En combinant ces \u00e9l\u00e9ments, le mod\u00e8le transformer peut efficacement traduire une s\u00e9quence de tokens d'une langue source \u00e0 une langue cible en utilisant des m\u00e9canismes d'attention et des postionnal embedding pour maintenir l'ordre s\u00e9quentiel et capturer les d\u00e9pendances \u00e0 long terme.</p> <p>Notes : Le mod\u00e8le d\u00e9codeur essaye de g\u00e9n\u00e9rer un token pertinent compte tenu des tokens de l'encodeur. Il va \u00e9mettre une requ\u00eate (query) et regarder les cl\u00e9s (key) et valeurs (value) transmises par l'encodeur via la couche de cross-attention. Dans un deuxi\u00e8me temps, il va \u00e9mettre une requ\u00eate, une cl\u00e9 et une valeur pour trouver un token coh\u00e9rent par rapport aux tokens qu'il a g\u00e9n\u00e9r\u00e9 pr\u00e9cedemment via la couche de self-attention.</p> <p>L'architecture compl\u00e8te va \u00eatre utilis\u00e9e dans des cas o\u00f9 l'on cherche \u00e0 g\u00e9n\u00e9rer un texte \u00e0 partir d'un autre texte. Les cas d'applications les plus communs sont : la traduction, le r\u00e9sum\u00e9 de texte, correction automatique et g\u00e9n\u00e9ration de texte guid\u00e9e.</p> <p>Notes : Vous avez sans doute constat\u00e9 que ChatGpt est capable de faire de la traduction et du r\u00e9sum\u00e9 de texte. En effet, le mod\u00e8le est tellement puissant qu'il arrive \u00e0 r\u00e9aliser des t\u00e2ches qui sont en th\u00e9orie difficiles pour lui. Il est entra\u00een\u00e9 \u00e0 pr\u00e9dire le prochain token et en y reflechissant un peu, on peut facilement imager que la t\u00e2che de pr\u00e9diction du prochain token permette de r\u00e9aliser toutes les t\u00e2ches de NLP. Cependant, pour des mod\u00e8les plus restreints, il faut mieux s'en tenir une architecture adapt\u00e9e \u00e0 notre probl\u00e8me.</p>"},{"location":"07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html#architecture-et-particularites-du-transformer","title":"Architecture et particularit\u00e9s du transformer\u00b6","text":""},{"location":"07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html#particularite-du-texte","title":"Particularit\u00e9 du texte\u00b6","text":""},{"location":"07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html#le-block-encodeur","title":"Le block encodeur\u00b6","text":""},{"location":"07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html#architecture-complete-avec-cross-attention","title":"Architecture compl\u00e8te avec cross-attention\u00b6","text":""},{"location":"07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html#formalisation-mathematique","title":"Formalisation math\u00e9matique\u00b6","text":""},{"location":"07_Transformers/04_ArchitectureEtParticularit%C3%A9s.html#exemples-dutilisation","title":"Exemples d'utilisation\u00b6","text":""},{"location":"07_Transformers/05_UtilisationsPossibles.html","title":"Utilisations possibles de l'architecture Transformers","text":"<p>Dans les parties pr\u00e9c\u00e9dentes, nous avons demontr\u00e9 les capacit\u00e9s des transformers par une application de pr\u00e9diction du prochain token (GPT). Nous avons \u00e9galement mentionn\u00e9 la diff\u00e9rence entre encodeur, d\u00e9codeur et architecture compl\u00e8te pour les t\u00e2ches de NLP.</p> <p>Ce qui est g\u00e9nial avec l'architecture transformers est que celle-ci est tr\u00e8s g\u00e9n\u00e9raliste, c'est \u00e0 dire qu'elle peut s'appliquer \u00e0 \u00e9normement de probl\u00e8mes diff\u00e9rents. Ce n'est, par exemple, pas le cas avec les couches de convolutions qui sont biais\u00e9es (ce qui les rend aussi tr\u00e8s performantes rapidement sur les images).</p> <p>Dans ce cours, nous allons pr\u00e9senter rapidement quelques architectures classiques de transformers dans diff\u00e9rents domaines (NLP et Vision principalement).</p> <p>Le papier BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding propose une fa\u00e7on d'entra\u00eener un mod\u00e8le de langage de type encodeur de mani\u00e8re non supervis\u00e9e.</p> <p>Point sur l'entra\u00eenement non supervis\u00e9 pour les NLP : Une des forces des mod\u00e8les de langage (LLM) comme GPT et BERT, c'est qu'on peut les entra\u00eener sur des grosses quantit\u00e9s de donn\u00e9es sans avoir \u00e0 fournir une annotation de ces donn\u00e9es. Pour GPT, il faut imaginer qu'on prend un document texte, qu'on cache la fin du document \u00e0 notre mod\u00e8le GPT et qu'on lui demande de g\u00e9n\u00e9rer la fin. Pour calculer le loss, on va comparer la g\u00e9n\u00e9ration de notre mod\u00e8le au texte original (c'est ce que nous avons fait pour g\u00e9n\u00e9rer du Moli\u00e8re). Pour BERT, l'approche est un peu diff\u00e9rente.</p> <p>BERT est un mod\u00e8le encodeur, c'est \u00e0 dire qu'il prend en compte le contexte des mots \u00e0 la fois \u00e0 droite et \u00e0 gauche (avant et apr\u00e8s le mot actuel). Pour l'entra\u00eener, on ne peut pas faire comme GPT et se contenter de pr\u00e9dire les mots suivants.</p> <p>Masked Langage Model (MLM) : BERT est ce qu'on appelle un Masked Langage Model (MLM), pendant l'entra\u00eenement, on va masquer certains mots d'une phrase (\u00e0 des positions al\u00e9atoires) et on va demander au mod\u00e8le de les pr\u00e9dire en se servant du contexte autour du mot masqu\u00e9.</p> <p></p> <p>Figure extraite de blogpost</p> <p>Next Sentence Prediction (NSP) : BERT est \u00e9galement pr\u00e9-entra\u00een\u00e9 \u00e0 si une phrase B suit une phrase A dans le texte, ce qui aide le mod\u00e8le \u00e0 comprendre les relations entre les phrases.</p> <p>Note : Pour en savoir plus sur BERT et pour apprendre \u00e0 le finetune vous pouvez consulter le cours 10 sur BERT.</p> <p>BERT et les autres mod\u00e8les de langage encodeur (RoBERTa, ALBERT etc ...) sont utilis\u00e9s comme base pour des t\u00e2ches plus pr\u00e9cises. On va ensuite les finetune pour d'autres t\u00e2ches et en particulier les t\u00e2ches \u00e9voqu\u00e9es dans le notebook pr\u00e9c\u00e9dent (analyse de sentiments, classification de texte etc ...).</p> <p>Note : On va vu comment entra\u00eener les mod\u00e8les encodeur et d\u00e9codeur de mani\u00e8re non supervis\u00e9e pour les t\u00e2ches de NLP (BERT et GPT). Il est aussi possible d'entra\u00eener un mod\u00e8le complet (encodeur, d\u00e9codeur et cross attention) de mani\u00e8re non supervis\u00e9e. C'est le cas du mod\u00e8le T5. Nous n'allons pas d\u00e9crire le fonctionnement dans ce notebook mais pour en savoir plus, vous pouvez consulter le blogpost.</p> <p>Quelques ann\u00e9es apr\u00e8s le BOOM des transformers dans le domaine du NLP, l'utilisation de cette architecture dans le domaine de la vision par ordinateur a \u00e9galement boulevers\u00e9 le domaine. Le papier An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  introduit une application d'un transformer de type encodeur adapt\u00e9 au traitement des images.</p> <p>Ce papier introduit le Vision Transformer (ViT) qui se base sur une d\u00e9coupe de l'image en patch qui vont \u00eatre ensuite donn\u00e9 en entr\u00e9e au transformer comme des tokens.</p> <p></p> <p>Comme on peut le voir \u00e0 droite de la figure, l'architecture correspond \u00e0 une architecture de type encodeur (la seule chose qui change par rapport \u00e0 Attention Is All You Need est l'application des normes avant les couches plut\u00f4t qu'apr\u00e8s).</p> <p>Dans le mod\u00e8le Vision Transformer (ViT), chaque image est d\u00e9coup\u00e9e en patchs de taille fixe, par exemple 16x16 pixels. Chaque patch est transform\u00e9 en un vecteur en l'aplatissant, puis ce vecteur est projet\u00e9 dans un espace d'embedding \u00e0 l'aide d'une couche de projection lin\u00e9aire, similaire \u00e0 celle utilis\u00e9e dans les mod\u00e8les de traitement de texte comme BERT ou GPT (couche Embedding). Cette repr\u00e9sentation vectorielle capture les informations spatiales et structurelles de l'image, tout comme les embeddings dans les mod\u00e8les NLP capturent le sens et les relations entre les mots. Le titre de l'article \"An Image is Worth 16x16 Words\" refl\u00e8te cette analogie : chaque patch d'image est trait\u00e9 comme un \"mot\" projet\u00e9 dans un espace d'embedding pour permettre l'apprentissage par avec l'architecture transformer.</p> <p>Note :</p> <ul> <li>Le Vision Transformer du papier original est entra\u00een\u00e9 de mani\u00e8re supervis\u00e9 sur des t\u00e2ches de classification d'objets. Les r\u00e9sultats de ce papier sont impressionnants et d\u00e9montrent sa capacit\u00e9 \u00e0 surpasser les mod\u00e8le convolutifs.</li> <li>Une am\u00e9lioration notable de l'architecture ViT pour les t\u00e2ches de vision (avec entra\u00eenement supervis\u00e9) est le Swin Transformer. Ce transformer a une architecture hierarchique (pouvant rappeler les CNN) permettant de capturer les relations spatiales plus efficacement.</li> </ul> <p>Dans le domaine du NLP, les mod\u00e8les de fondations (entrain\u00e9 de mani\u00e8re non supervis\u00e9e) ont permis des avanc\u00e9es spectaculaires. Cr\u00e9er un mod\u00e8le de fondation pour les images est aussi une t\u00e2che tr\u00e8s attrayantes. Cela permettrait d'avoir un mod\u00e8le que l'on peut finetune simplement sur des t\u00e2ches pr\u00e9cises  et avec de bons r\u00e9sultats. Pour cette t\u00e2che, plusieurs approches ont \u00e9t\u00e9 propos\u00e9es \u00e0 partir d'images uniquement. Nous allons en pr\u00e9senter deux dans la suite de cette partie.</p> <p>BEIT : BEIT: BERT Pre-Training of Image Transformers propose d'utiliser le m\u00eame mode d'entra\u00eenement que BERT mais dans le contexte des images. Cela va consister \u00e0 masquer certains patchs de l'image que l'on va essayer de pr\u00e9dire pendant l'entra\u00eenement. Cependant, \u00e0 l'inverse des mots, les possibilit\u00e9s d'images sont presque infinie (si on veut pr\u00e9dire une image RGB de taille $3 \\times 8 \\times 8$, il y a $(256 \\times 256 \\times 256)^{8 \\times 8} = (16777216)^{64}$ de possiblit\u00e9s ce qui est plus que le nombre d'atomes dans l'univers) donc on ne peut pas directement pr\u00e9dire les pixels. Pour remedier \u00e0 ce probl\u00e8me, on utilise un VQ-VAE qui permet de discr\u00e9tiser une representation de l'image. Cette version discr\u00e8te correspond \u00e0 des valeurs issues d'un dictionnaire de taille fixe et il est donc possible de pr\u00e9dire cette representation discr\u00e8te.</p> <p></p> <p>Image GPT : L'article Generative Pretraining from Pixels introduit un \u00e9quivalent de GPT mais pour les pixels. Il s'agit d'un mod\u00e8le autoregressif qui va g\u00e9n\u00e9rer les pixels d'une image un par un comme le fait un mod\u00e8le autoregressif de NLP avec les tokens. Cela permet d'avoir un entra\u00eenement non supervis\u00e9 mais il y a quand m\u00eame de nombreux d\u00e9fauts :</p> <ul> <li>La g\u00e9n\u00e9ration prend \u00e9normement de temps car on g\u00e9n\u00e9re un pixel \u00e0 la fois. On doit donc appliquer une r\u00e9duction de dimension au pr\u00e9alable.</li> <li>G\u00e9n\u00e9rer de gauche \u00e0 droite n'a pas de sens pour une image, pourquoi de gauche \u00e0 droite et pas de droite \u00e0 gauche ? Ou en commen\u00e7ant du milieu ?</li> </ul> <p></p> <p>Il existe d'autres fa\u00e7ons d'entra\u00eener des transformers de vision (ou autre mod\u00e8le de vision) de mani\u00e8re non supervis\u00e9e comme les Masked Autoencoders ou les mod\u00e8les associant texte et image.</p> <p>Les mod\u00e8les transformer associants texte et image se sont rev\u00e9l\u00e9s d'une grande aide pour la cr\u00e9ation de mod\u00e8le de fondations. Ces mod\u00e8les sont souvent des captionners, c'est-\u00e0-dire qu'on les entra\u00eene \u00e0 g\u00e9n\u00e9rer la description d'une image.</p> <p>Dans cette partie, nous allons pr\u00e9senter le fonctionnement du mod\u00e8le CLIP introduit dans le papier Learning Transferable Visual Models From Natural Language Supervision. Nous pr\u00e9senterons \u00e9galement l'int\u00earet de ce type de mod\u00e8le et ses capacit\u00e9s dans le cadre de nombreuses t\u00e2ches.</p> <p>Architecture de CLIP : L'entra\u00eenement de CLIP repose sur une m\u00e9thode contrastive. Cette m\u00e9thode d'entra\u00eenement consiste \u00e0 pr\u00e9senter au mod\u00e8le deux exemples : un exemple positif qui correspond au label donn\u00e9 et un exemple n\u00e9gatif qui ne correspond pas au label. Le but est de pousser le mod\u00e8le \u00e0 associer correctement l'exemple positif au label tout en dissociant l'exemple n\u00e9gatif du label. Ainsi, cette approche permet de d\u00e9finir une fronti\u00e8re claire entre ce qui est pertinent (positif) et ce qui ne l'est pas (n\u00e9gatif), en maximisant la s\u00e9paration entre les deux.</p> <p>En pratique, CLIP utilise \u00e0 la fois un encodeur textuel et un encodeur d'image, tous deux bas\u00e9s sur des architectures de transformers. Le mod\u00e8le encode des descriptions textuelles et des images pour ensuite les associer de mani\u00e8re correcte pendant l'entra\u00eenement. L'objectif principal est de maximiser la corr\u00e9lation entre les descriptions et les images correspondantes, tout en minimisant cette corr\u00e9lation pour les paires qui ne correspondent pas. Cela permet au mod\u00e8le d'apprendre \u00e0 repr\u00e9senter efficacement les relations entre texte et image dans un espace d'embedding commun, facilitant ainsi la compr\u00e9hension et la g\u00e9n\u00e9ration de texte \u00e0 partir d'images, et vice versa.</p> <p>Lors de la phase de test, on peut demander au mod\u00e8le de g\u00e9n\u00e9rer une description adapt\u00e9e pour notre image.</p> <p></p> <p>Utilisation du mod\u00e8le : Au d\u00e9l\u00e0 d'\u00eatre un simple captionner, CLIP permet aussi de faire de la classification zero-shot, c'est \u00e0 dire qu'on va pouvoir classifier une image sans avoir entra\u00een\u00e9 le mod\u00e8le sp\u00e9cifiquement sur cette t\u00e2che. Dans le cas de CLIP, cela va permettre de donner un score \u00e0 chaque description qu'on lui fournit. On va lui donner deux descriptions \"A photo of a cat\" et \"A photo of a dog\" et il va nous renvoyer des scores de probabilit\u00e9s d'association de notre image actuelle avec chacune des deux descriptions.</p> <p>Autres utilisations : Cette m\u00e9thode d'entra\u00eenement a \u00e9galement permis de cr\u00e9er des mod\u00e8les de d\u00e9tection zero-shot comme OWL-ViT, des mod\u00e8les de transfert de style ou encore des mod\u00e8les de g\u00e9n\u00e9rations d'images.</p> <p>Dataset d'images avec description : On peut \u00e9galement se demander si une description d'image n'est pas \u00e9quivalente \u00e0 un label et qu'on aurait donc besoin d'une annotation laborieuse pour entra\u00eener ce type de mod\u00e8le (qui demandent des milliards d'images pour \u00eatre performants). En r\u00e9alit\u00e9, il est possible de r\u00e9colter des images avec description assez simplement sur internet gr\u00e2ce au \"alt\" de l'image en code HTML. Il s'agit d'une description de l'image que les gens ajoutent \u00e0 leur image dans le code HTML. Bien s\u00fbr, ces donn\u00e9es ne sont pas forc\u00e9ment fiables mais la quantit\u00e9 est plus int\u00e9ressante que la qualit\u00e9 dans ce type de mod\u00e8le.</p> <p>De plus il existe maintenant des bases de donn\u00e9es open-source contenant plusieurs milliards de paires image/description. Le plus connu \u00e9tant LAION-5B.</p>"},{"location":"07_Transformers/05_UtilisationsPossibles.html#utilisations-possibles-de-larchitecture-transformers","title":"Utilisations possibles de l'architecture Transformers\u00b6","text":""},{"location":"07_Transformers/05_UtilisationsPossibles.html#bert","title":"BERT\u00b6","text":""},{"location":"07_Transformers/05_UtilisationsPossibles.html#entrainement-du-modele","title":"Entra\u00eenement du mod\u00e8le\u00b6","text":""},{"location":"07_Transformers/05_UtilisationsPossibles.html#utilite-de-bert","title":"Utilit\u00e9 de BERT\u00b6","text":""},{"location":"07_Transformers/05_UtilisationsPossibles.html#transformers-pour-le-traitement-dimages","title":"Transformers pour le tra\u00eetement d'images\u00b6","text":""},{"location":"07_Transformers/05_UtilisationsPossibles.html#vit-vision-transformer","title":"ViT : Vision Transformer\u00b6","text":""},{"location":"07_Transformers/05_UtilisationsPossibles.html#apprentissage-non-supervise-pour-la-vision","title":"Apprentissage non supervis\u00e9 pour la vision\u00b6","text":""},{"location":"07_Transformers/05_UtilisationsPossibles.html#transformers-associant-texte-et-image","title":"Transformers associant texte et image\u00b6","text":""},{"location":"07_Transformers/05_UtilisationsPossibles.html#clip-connecter-images-et-texte","title":"CLIP : Connecter images et texte\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html","title":"Vision transformer implementation","text":"<p>Dans ce notebook, nous allons impl\u00e9menter le vision transformer et la tester le petit dataset CIFAR-10. L'impl\u00e9mentation reprend des \u00e9l\u00e9ments du notebook 2 \"GptFromScratch\" donc il est n\u00e9cessaire de les faire dans l'ordre.</p> <p>L'impl\u00e9mentation se base sur le papier An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</p> <p>Voici la figure importante de cet article que l'on va impl\u00e9menter petit \u00e0 petit :</p> <p></p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torchvision.datasets as datasets\nimport matplotlib.pyplot as plt\n\n# Detection automatique du GPU\ndevice = \"cpu\"\nif torch.cuda.is_available():\n    device = \"cuda\"\nprint(f\"using device: {device}\")\n</pre> import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as T import torchvision.datasets as datasets import matplotlib.pyplot as plt  # Detection automatique du GPU device = \"cpu\" if torch.cuda.is_available():     device = \"cuda\" print(f\"using device: {device}\") <pre>using device: cpu\n</pre> <p>Dans un premier temps, on va reprendre le code du notebook 2 de ce cours en y apportant quelques modifications.</p> <p>Si vous vous souvenez bien, dans le notebook 2 nous avons impl\u00e9ment\u00e9 la couche masked multi-head attention pour entra\u00eener un transformer de type decoder. Pour les images, on veut un transformer de type encoder, il va donc falloir changer notre impl\u00e9mentation. C'est en fait tr\u00e8s simple : on avait une multiplication par une matrice triangulaire inf\u00e9rieure pour masquer le \"futur\" dans le decoder alors que dans l'encoder on ne veut pas masquer le futur, il suffit donc de supprimer cette multiplication par la matrice.</p> <p>Voici le code python ajust\u00e9 :</p> In\u00a0[2]: Copied! <pre>class Head_enc(nn.Module):\n    \"\"\" Couche de self-attention unique \"\"\"\n\n    def __init__(self, head_size,n_embd,dropout=0.2):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # Le * C**-0.5 correspond \u00e0 la normalisation par la racine de head_size\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        # On a supprimer le masquage du futur\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n</pre> class Head_enc(nn.Module):     \"\"\" Couche de self-attention unique \"\"\"      def __init__(self, head_size,n_embd,dropout=0.2):         super().__init__()         self.key = nn.Linear(n_embd, head_size, bias=False)         self.query = nn.Linear(n_embd, head_size, bias=False)         self.value = nn.Linear(n_embd, head_size, bias=False)         self.dropout = nn.Dropout(dropout)      def forward(self, x):         B,T,C = x.shape         k = self.key(x)   # (B,T,C)         q = self.query(x) # (B,T,C)         # Le * C**-0.5 correspond \u00e0 la normalisation par la racine de head_size         wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)         # On a supprimer le masquage du futur         wei = F.softmax(wei, dim=-1) # (B, T, T)         wei = self.dropout(wei)         v = self.value(x) # (B,T,C)         out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)         return out <p>Pour avoir plusieurs head, on va simplement reprendre notre classe du notebook 2 mais en utilisant Head_enc au lieu de Head :</p> In\u00a0[3]: Copied! <pre>class MultiHeadAttention(nn.Module):\n    \"\"\" Plusieurs couches de self attention en parall\u00e8le\"\"\"\n\n    def __init__(self, num_heads, head_size,n_embd,dropout):\n        super().__init__()\n        # Cr\u00e9ation de num_head couches head_enc de taille head_size\n        self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n</pre> class MultiHeadAttention(nn.Module):     \"\"\" Plusieurs couches de self attention en parall\u00e8le\"\"\"      def __init__(self, num_heads, head_size,n_embd,dropout):         super().__init__()         # Cr\u00e9ation de num_head couches head_enc de taille head_size         self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])         self.proj = nn.Linear(n_embd, n_embd)         self.dropout = nn.Dropout(dropout)      def forward(self, x):         out = torch.cat([h(x) for h in self.heads], dim=-1)         out = self.dropout(self.proj(out))         return out <p>On r\u00e9utilise \u00e9galement notre impl\u00e9mentation de la feed forward layer, on change juste la fonction d'activation ReLU en GeLU comme d\u00e9crit dans le papier :</p> In\u00a0[4]: Copied! <pre>class FeedFoward(nn.Module):\n\n    def __init__(self, n_embd,dropout):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GELU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</pre> class FeedFoward(nn.Module):      def __init__(self, n_embd,dropout):         super().__init__()         self.net = nn.Sequential(             nn.Linear(n_embd, 4 * n_embd),             nn.GELU(),             nn.Linear(4 * n_embd, n_embd),             nn.Dropout(dropout),         )      def forward(self, x):         return self.net(x) <p>Et enfin, on peut construire notre block de transformer encorder correspondant \u00e0 celui que l'on voit sur la figure plus haut :</p> In\u00a0[5]: Copied! <pre>class TransformerBlock(nn.Module):\n    \"\"\" Block transformer\"\"\"\n\n    def __init__(self, n_embd, n_head,dropout=0.):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)\n        self.ffwd = FeedFoward(n_embd,dropout)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n</pre> class TransformerBlock(nn.Module):     \"\"\" Block transformer\"\"\"      def __init__(self, n_embd, n_head,dropout=0.):         super().__init__()         head_size = n_embd // n_head         self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)         self.ffwd = FeedFoward(n_embd,dropout)         self.ln1 = nn.LayerNorm(n_embd)         self.ln2 = nn.LayerNorm(n_embd)      def forward(self, x):         x = x + self.sa(self.ln1(x))         x = x + self.ffwd(self.ln2(x))         return x <p>Note : Ici, je suis all\u00e9 tr\u00e8s rapidement sur ces couches car elles ont \u00e9t\u00e9 impl\u00e9ment\u00e9es en d\u00e9tail dans le notebook 2. Je vous invite \u00e0 vous y r\u00e9f\u00e9rer en cas d'incompr\u00e9hension.</p> <p>Nous allons maintenant faire l'impl\u00e9mentation du r\u00e9seau pas \u00e0 pas.</p> <p>La premi\u00e8re \u00e9tape d\u00e9crite dans la papier est la division de l'image en patch : Chaque image est d\u00e9coup\u00e9 en $N$ patchs de taille $p \\times p$ puis les patchs sont ensuite applatis (flatten). On passe d'une dimension de l'image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ \u00e0 une s\u00e9quence de patch $\\mathbf{x}_p \\in \\mathbb{R}^{N \\times (P^2 \\cdot C)}$.</p> <p></p> <p>Pour r\u00e9aliser cela, nous allons r\u00e9cuperer une image du dataset CIFAR-10 comme exemple ce qui nous permettra de visualiser si notre code fonctionne.</p> In\u00a0[6]: Copied! <pre>transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement\ndataset = datasets.CIFAR10(root='./../data', train=True, download=True,transform=transform)\n</pre> transform=T.ToTensor() # Pour convertir les \u00e9l\u00e9ments en tensor torch directement dataset = datasets.CIFAR10(root='./../data', train=True, download=True,transform=transform) <pre>Files already downloaded and verified\n</pre> <p>R\u00e9cuperons une simple image de ce dataset pour faire nos magouilles :</p> In\u00a0[7]: Copied! <pre>image=dataset[0][0]\nprint(image.shape)\nplt.imshow(dataset[0][0].permute(1,2,0).numpy())\nplt.axis(\"off\")\nplt.show()\n</pre> image=dataset[0][0] print(image.shape) plt.imshow(dataset[0][0].permute(1,2,0).numpy()) plt.axis(\"off\") plt.show() <pre>torch.Size([3, 32, 32])\n</pre> <p>Une magnifique grenouille !</p> <p>Pour choisir la dimension d'un patch, il faut prendre une dimension divisible par 32. Prenons par exemple $8 \\times 8$ ce qui nous fera 16 patchs. Laissons cette valeur comme \u00e9tant un param\u00e8tre que l'on peut choisir.</p> <p>Dans un premier temps, on peut penser qu'il faut faire deux boucles sur la largeur et la hauteur en r\u00e9cuperant un patch \u00e0 chaque fois de cette mani\u00e8re :</p> In\u00a0[8]: Copied! <pre>patch_size = 8\nlist_of_patches = []\nfor i in range(0,image.shape[1],patch_size):\n    for j in range(0,image.shape[2],patch_size):\n        patch=image[:,i:i+patch_size,j:j+patch_size]\n        list_of_patches.append(patch)\ntensor_patches = torch.stack(list_of_patches)\nprint(tensor_patches.shape)\n</pre> patch_size = 8 list_of_patches = [] for i in range(0,image.shape[1],patch_size):     for j in range(0,image.shape[2],patch_size):         patch=image[:,i:i+patch_size,j:j+patch_size]         list_of_patches.append(patch) tensor_patches = torch.stack(list_of_patches) print(tensor_patches.shape) <pre>torch.Size([16, 3, 8, 8])\n</pre> <p>Ce n'est pas du tout efficace en terme de code. Avec pytorch, on peut en fait faire beaucoup plus simple avec view() et unfold(). Cette \u00e9tape est un peu compliqu\u00e9 mais n\u00e9cessaire pour des raisons de continuit\u00e9 en m\u00e9moire pour que la fonction view() fonctionne correctement. Faire simplement <code>patches = image.view(-1, C, patch_size, patch_size)</code> ne fonctionnerait pas (vous pouvez essayer pour vous en assurer).</p> In\u00a0[9]: Copied! <pre>C,H,W = image.shape\n# On utilise la fonction unfold pour d\u00e9couper l'image en patch contigus\n# Le premier unfold d\u00e9coupe la premi\u00e8re dimension (H) en ligne\n# Le deuxi\u00e8me unfold d\u00e9coupe chacune des lignes en patch_size colonnes \n# Ce qui donne une image de taille (C, H//patch_size, W//patch_size,patch_size, patch_size)\npatches = image.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n# Permute pour avoir les dimensions dans le bon ordre\npatches = patches.permute(1, 2, 0, 3, 4).contiguous()\npatches = patches.view(-1, C, patch_size, patch_size)\nprint(patches.shape)\n# On peut v\u00e9rifier que \u00e7a fait bien la m\u00eame chose\nprint((patches==tensor_patches).all())\n</pre> C,H,W = image.shape # On utilise la fonction unfold pour d\u00e9couper l'image en patch contigus # Le premier unfold d\u00e9coupe la premi\u00e8re dimension (H) en ligne # Le deuxi\u00e8me unfold d\u00e9coupe chacune des lignes en patch_size colonnes  # Ce qui donne une image de taille (C, H//patch_size, W//patch_size,patch_size, patch_size) patches = image.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size) # Permute pour avoir les dimensions dans le bon ordre patches = patches.permute(1, 2, 0, 3, 4).contiguous() patches = patches.view(-1, C, patch_size, patch_size) print(patches.shape) # On peut v\u00e9rifier que \u00e7a fait bien la m\u00eame chose print((patches==tensor_patches).all()) <pre>torch.Size([16, 3, 8, 8])\ntensor(True)\n</pre> <p>Maitenant on va applatir nos patchs pour avoir notre r\u00e9sultat final.</p> In\u00a0[10]: Copied! <pre>nb_patches = patches.shape[0]\nprint(nb_patches)\npatches_flat = patches.flatten(1, 3)\nprint(patches_flat.shape)\n</pre> nb_patches = patches.shape[0] print(nb_patches) patches_flat = patches.flatten(1, 3) print(patches_flat.shape) <pre>16\ntorch.Size([16, 192])\n</pre> <p>D\u00e9finissons une fonction pour faire ces transformations :</p> In\u00a0[11]: Copied! <pre># La fonction a \u00e9t\u00e9 modifi\u00e9e pour prendre en compte le batch\ndef image_to_patches(image, patch_size):\n    # On rajoute une dimension pour le batch\n    B,C,_,_ = image.shape\n    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n    patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()\n    patches = patches.view(B,-1, C, patch_size, patch_size)\n    patches_flat = patches.flatten(2, 4)\n    return patches_flat\n</pre> # La fonction a \u00e9t\u00e9 modifi\u00e9e pour prendre en compte le batch def image_to_patches(image, patch_size):     # On rajoute une dimension pour le batch     B,C,_,_ = image.shape     patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)     patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()     patches = patches.view(B,-1, C, patch_size, patch_size)     patches_flat = patches.flatten(2, 4)     return patches_flat  <p>Nous y sommes ! La premi\u00e8re \u00e9tape est termin\u00e9e :)</p> <p>Il est temps de passer \u00e0 la deuxi\u00e8me \u00e9tape qui est la projection lin\u00e9aire des patchs dans un espace latent.</p> <p></p> <p>Cette \u00e9tape est l'\u00e9quivalent de l'\u00e9tape de conversion des tokens \u00e0 l'aide la table d'embedding. Cette fois-ci, on va convertir nos patchs applatis en vecteur de dimension fixe pour que ces vecteurs puissent \u00eatre trait\u00e9 par le transformer. D\u00e9finissons notre dimension d'embedding et notre couche de projection :</p> In\u00a0[12]: Copied! <pre>n_embd = 64\nproj_layer = nn.Linear(C*patch_size*patch_size, n_embd)\n</pre> n_embd = 64 proj_layer = nn.Linear(C*patch_size*patch_size, n_embd) <p>C'est tout, ce n'est pas l'\u00e9tape la plus compliqu\u00e9e.</p> <p>Passons \u00e0 la derni\u00e8re \u00e9tape avant les couches transformers (qui sont d\u00e9j\u00e0 impl\u00e9ment\u00e9es). Cette \u00e9tape contient en fait 2 \u00e9tapes distinctes :</p> <ul> <li>L'ajout d'un embedding de position : comme dans le GPT, le transformer n'a pas d'information pr\u00e9alable sur la position du patch dans l'image. Pour cela, on va simplement ajouter un embedding d\u00e9di\u00e9 \u00e0 cela qui permettra au r\u00e9seau d'avoir une notion de position relative des patchs.</li> <li>L'ajout d'un class token : Cette \u00e9tape est nouvelle car elle n'\u00e9tait pas n\u00e9cessaire dans GPT. L'id\u00e9e vient en fait de BERT et est une technique pour faire de la classification \u00e0 l'aide d'un transformer sans avoir \u00e0 sp\u00e9cifier de taille de s\u00e9quence fixe. Sans class token, pour obtenir notre classification, on aurait besoin soit de coller un r\u00e9seau fully connected \u00e0 l'ensemble des sorties du transformers (ce qui va imposer une taille de s\u00e9quence fixe) ou de coller un r\u00e9seau fully connected \u00e0 une sortie du transformer choisie au hasard (une sortie correspond \u00e0 un patch, mais alors comment choisir ce patch sans biais ?). L'ajout du class token permet de r\u00e9pondre \u00e0 ce probl\u00e8me en ajoutant un token d\u00e9di\u00e9 sp\u00e9cifiquement \u00e0 la classification.</li> </ul> <p>Note : Pour pour les CNNs, une mani\u00e8re d'\u00e9viter le probl\u00e8me de la dimension fixe de l'entr\u00e9e est d'utiliser un global average pooling en sortie (couche de pooling avec taille de sortie fixe). Cette technique peut aussi \u00eatre utilis\u00e9e pour un vision transformer \u00e0 la place du class token.</p> <p></p> In\u00a0[13]: Copied! <pre># Pour le positional encoding, +1 pour le cls token\npos_emb = nn.Embedding(nb_patches+1, n_embd)\n# On ajoute un token cls\ncls_token = torch.zeros(1, 1, n_embd)\n# On ajoutera ce token cls au d\u00e9but de chaque s\u00e9quence\n</pre> # Pour le positional encoding, +1 pour le cls token pos_emb = nn.Embedding(nb_patches+1, n_embd) # On ajoute un token cls cls_token = torch.zeros(1, 1, n_embd) # On ajoutera ce token cls au d\u00e9but de chaque s\u00e9quence <p>Maintenant, passons \u00e0 la fin du ViT, c'est \u00e0 dire le r\u00e9seau MLP de classification. Si vous avez suivi l'int\u00earet du class token, vous comprenez que ce r\u00e9seau de classification prend en entr\u00e9e uniquement ce token pour nous sortir la classe pr\u00e9dite.</p> <p></p> <p>A nouveau, c'est encore une impl\u00e9mentation assez simple. Dans l'article, ils disent qu'ils utilisent un r\u00e9seau d'une couche cach\u00e9e pour l'entra\u00eenement et uniquement une couche pour un fine-tuning (voir cours 10 pour des pr\u00e9cisions sur le fine tuning). Par soucis de simplicit\u00e9, nous utilisons une seule couche lin\u00e9aire pour projeter le class token de sortie dans la dimension du nombre de classes.</p> In\u00a0[14]: Copied! <pre>classi_head = nn.Linear(n_embd, 10)\n</pre> classi_head = nn.Linear(n_embd, 10) <p>Nous disposons maitenant de tous les \u00e9l\u00e9ments pour construire notre ViT et l'entra\u00eener !</p> <p>On peut maintenant rassembler les morceaux et cr\u00e9er notre vision transformer.</p> In\u00a0[15]: Copied! <pre>class ViT(nn.Module):\n    def __init__(self, n_embed,patch_size,C,n_head,n_layer,nb_patches,dropout=0.) -&gt; None:\n        super().__init__()\n        self.proj_layer = nn.Linear(C*patch_size*patch_size, n_embed)\n        self.pos_emb = nn.Embedding(nb_patches+1, n_embed)\n        # Permet de cr\u00e9er cls_token comme un param\u00e8tre du r\u00e9seau\n        self.register_parameter(name='cls_token', param=torch.nn.Parameter(torch.zeros(1, 1, n_embed)))\n        self.transformer=nn.Sequential(*[TransformerBlock(n_embed, n_head,dropout) for _ in range(n_layer)])\n        self.classi_head = nn.Linear(n_embed, 10)\n    \n    def forward(self,x):\n        B,_,_,_=x.shape\n        # On d\u00e9coupe l'image en patch et on les applatit\n        x = image_to_patches(x, patch_size)\n        # On projette dans la dimension n_embed\n        x = self.proj_layer(x)\n        # On ajoute le token cls\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        # On ajoute le positional encoding\n        pos_emb = self.pos_emb(torch.arange(x.shape[1], device=x.device))\n        x = x + pos_emb\n        # On applique les blocks transformer\n        x = self.transformer(x)\n        # On r\u00e9cup\u00e8re le token cls\n        cls_tokens = x[:, 0]\n        # On applique la derni\u00e8re couche de classification\n        x = self.classi_head(cls_tokens)\n        return x        \n</pre> class ViT(nn.Module):     def __init__(self, n_embed,patch_size,C,n_head,n_layer,nb_patches,dropout=0.) -&gt; None:         super().__init__()         self.proj_layer = nn.Linear(C*patch_size*patch_size, n_embed)         self.pos_emb = nn.Embedding(nb_patches+1, n_embed)         # Permet de cr\u00e9er cls_token comme un param\u00e8tre du r\u00e9seau         self.register_parameter(name='cls_token', param=torch.nn.Parameter(torch.zeros(1, 1, n_embed)))         self.transformer=nn.Sequential(*[TransformerBlock(n_embed, n_head,dropout) for _ in range(n_layer)])         self.classi_head = nn.Linear(n_embed, 10)          def forward(self,x):         B,_,_,_=x.shape         # On d\u00e9coupe l'image en patch et on les applatit         x = image_to_patches(x, patch_size)         # On projette dans la dimension n_embed         x = self.proj_layer(x)         # On ajoute le token cls         cls_tokens = self.cls_token.expand(B, -1, -1)         x = torch.cat((cls_tokens, x), dim=1)         # On ajoute le positional encoding         pos_emb = self.pos_emb(torch.arange(x.shape[1], device=x.device))         x = x + pos_emb         # On applique les blocks transformer         x = self.transformer(x)         # On r\u00e9cup\u00e8re le token cls         cls_tokens = x[:, 0]         # On applique la derni\u00e8re couche de classification         x = self.classi_head(cls_tokens)         return x         <p>On va entra\u00eener notre mod\u00e8le ViT sur le dataset CIFAR-10. A noter que les param\u00e8tres que nous avons d\u00e9fini sont adapt\u00e9s pour des images de petites taille (n_embed et patch_size). Pour traiter des images plus grande, il faudra adapter ces param\u00e8tres. Le code fonctionne avec des tailles diff\u00e9rentes dans que la taille de l'image est divisible par le taille du patch.</p> <p>Chargons le dataset CIFAR-10 et cr\u00e9ons nos dataloaders :</p> <p>Note : Vous pouvez s\u00e9l\u00e9ctionner une sous-partie du dataset pour acc\u00e9lerer l'entra\u00eenement.</p> In\u00a0[16]: Copied! <pre>classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n# Transformation des donn\u00e9es, normalisation et transformation en tensor pytorch\ntransform = T.Compose([T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# T\u00e9l\u00e9chargement et chargement du dataset\ndataset = datasets.CIFAR10(root='./../data', train=True,download=True, transform=transform)\ntestdataset = datasets.CIFAR10(root='./../data', train=False,download=True, transform=transform)\nprint(\"taille d'une image : \",dataset[0][0].shape)\n\n\n#Cr\u00e9ation des dataloaders pour le train, validation et test\ntrain_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\nprint(\"taille du train dataset : \",len(train_dataset))\nprint(\"taille du val dataset : \",len(val_dataset))\nprint(\"taille du test dataset : \",len(testdataset))\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2)\nval_loader= torch.utils.data.DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2)\n</pre> classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck') # Transformation des donn\u00e9es, normalisation et transformation en tensor pytorch transform = T.Compose([T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # T\u00e9l\u00e9chargement et chargement du dataset dataset = datasets.CIFAR10(root='./../data', train=True,download=True, transform=transform) testdataset = datasets.CIFAR10(root='./../data', train=False,download=True, transform=transform) print(\"taille d'une image : \",dataset[0][0].shape)   #Cr\u00e9ation des dataloaders pour le train, validation et test train_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2]) print(\"taille du train dataset : \",len(train_dataset)) print(\"taille du val dataset : \",len(val_dataset)) print(\"taille du test dataset : \",len(testdataset)) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2) val_loader= torch.utils.data.DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2) test_loader = torch.utils.data.DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2) <pre>Files already downloaded and verified\nFiles already downloaded and verified\ntaille d'une image :  torch.Size([3, 32, 32])\ntaille du train dataset :  40000\ntaille du val dataset :  10000\ntaille du test dataset :  10000\n</pre> <p>On va maintenant d\u00e9finir nos hyperparam\u00e8tres d'entra\u00eenement et pour les sp\u00e9cificit\u00e9s du mod\u00e8le :</p> In\u00a0[17]: Copied! <pre>patch_size = 8\nnb_patches = (32//patch_size)**2\nn_embed = 64\nn_head = 4\nn_layer = 4\nepochs = 10\nC=3 # Nombre de canaux\nlr = 1e-3\nmodel = ViT(n_embed,patch_size,C,n_head,n_layer,nb_patches).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n</pre> patch_size = 8 nb_patches = (32//patch_size)**2 n_embed = 64 n_head = 4 n_layer = 4 epochs = 10 C=3 # Nombre de canaux lr = 1e-3 model = ViT(n_embed,patch_size,C,n_head,n_layer,nb_patches).to(device) optimizer = torch.optim.Adam(model.parameters(), lr=lr) <p>Il est finalement temps d'entra\u00eener notre mod\u00e8le !</p> In\u00a0[18]: Copied! <pre>for epoch in range(epochs):\n    model.train()\n    loss_train = 0\n    for i, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        output = model(images)\n        loss = F.cross_entropy(output, labels)\n        loss_train += loss.item()\n        loss.backward()\n        optimizer.step()\n        \n    model.eval()\n    correct = 0\n    total = 0\n    loss_val = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss_val += F.cross_entropy(outputs, labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Epoch {epoch}, loss train {loss_train/len(train_loader)}, loss val {loss_val/len(val_loader)},pr\u00e9cision {100 * correct / total}\")\n</pre> for epoch in range(epochs):     model.train()     loss_train = 0     for i, (images, labels) in enumerate(train_loader):         images, labels = images.to(device), labels.to(device)         optimizer.zero_grad()         output = model(images)         loss = F.cross_entropy(output, labels)         loss_train += loss.item()         loss.backward()         optimizer.step()              model.eval()     correct = 0     total = 0     loss_val = 0     with torch.no_grad():         for images, labels in val_loader:             images, labels = images.to(device), labels.to(device)             outputs = model(images)             loss_val += F.cross_entropy(outputs, labels).item()             _, predicted = torch.max(outputs.data, 1)             total += labels.size(0)             correct += (predicted == labels).sum().item()      print(f\"Epoch {epoch}, loss train {loss_train/len(train_loader)}, loss val {loss_val/len(val_loader)},pr\u00e9cision {100 * correct / total}\") <pre>Epoch 0, loss train 1.6522698682546615, loss val 1.4414834783554078,pr\u00e9cision 47.97\nEpoch 1, loss train 1.3831321718215943, loss val 1.3656272639274598,pr\u00e9cision 50.69\nEpoch 2, loss train 1.271412028503418, loss val 1.2726070711135864,pr\u00e9cision 55.17\nEpoch 3, loss train 1.1935315937042237, loss val 1.2526390438556672,pr\u00e9cision 55.52\nEpoch 4, loss train 1.1144725002408027, loss val 1.2377954412460328,pr\u00e9cision 55.66\nEpoch 5, loss train 1.0520227519154548, loss val 1.2067877051830291,pr\u00e9cision 56.82\nEpoch 6, loss train 0.9839000009179115, loss val 1.2402711957931518,pr\u00e9cision 56.93\nEpoch 7, loss train 0.9204218792438507, loss val 1.2170260044574737,pr\u00e9cision 58.23\nEpoch 8, loss train 0.853291154640913, loss val 1.2737546770095824,pr\u00e9cision 57.65\nEpoch 9, loss train 0.7962572723925113, loss val 1.2941821083545684,pr\u00e9cision 58.26\n</pre> <p>L'entra\u00eenement s'est bien pass\u00e9, on obtient une pr\u00e9cision de 58% sur les donn\u00e9es de validation. Regardons maintenant nos r\u00e9sultats sur les donn\u00e9es de test :</p> In\u00a0[19]: Copied! <pre>model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Pr\u00e9cision {100 * correct / total}\")\n</pre> model.eval() correct = 0 total = 0 with torch.no_grad():     for images, labels in test_loader:         images, labels = images.to(device), labels.to(device)         outputs = model(images)         _, predicted = torch.max(outputs.data, 1)         total += labels.size(0)         correct += (predicted == labels).sum().item()  print(f\"Pr\u00e9cision {100 * correct / total}\") <pre>Pr\u00e9cision 58.49\n</pre> <p>La pr\u00e9cision est du m\u00eame ordre sur les donn\u00e9es de test !</p> <p>Note : Ce r\u00e9sultat peut paraitre assez m\u00e9diocre mais il ne faut pas oublier que nous utilisons un petit transformer entra\u00een\u00e9 sur peu d'epochs. Vous pouvez essayer d'am\u00e9liorer ce r\u00e9sultat en jouant sur les hyperparam\u00e8tres.</p> <p>Note2 : Les auteurs du papier pr\u00e9cisent que le transformer n'a pas de \"inductive bias\" sur les images contrairement aux CNN et cela provient de l'architecture. Les couches d'un CNN sont invariantes par translation et capturent le voisinage de chaque pixel tandis que les transformers utilisent principalement l'information globale. En pratique, on constate que sur des \"petits\" datasets (jusqu'\u00e0 1 million d'images), les CNN performent mieux mais pour des plus grosses quantit\u00e9 de donn\u00e9es, les transformers sont plus performants.</p>"},{"location":"07_Transformers/06_VisionTransformerImplementation.html#vision-transformer-implementation","title":"Vision transformer implementation\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#reprise-du-code-precedent","title":"Reprise du code pr\u00e9c\u00e9dent\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#couche-de-self-attention","title":"Couche de self-attention\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#multi-head-self-attention","title":"Multi-head self-attention\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#feed-forward-layer","title":"Feed forward layer\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#transformer-encoder-block","title":"Transformer encoder block\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#implementation-du-reseau","title":"Impl\u00e9mentation du r\u00e9seau\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#separation-de-limage-en-patch","title":"S\u00e9paration de l'image en patch\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#projection-lineaire-des-patchs","title":"Projection lin\u00e9aire des patchs\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#embedding-de-position-et-class-token","title":"Embedding de position et class token\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#reseau-fully-connected-de-classification","title":"R\u00e9seau fully connected de classification\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#creation-du-modele-vit","title":"Cr\u00e9ation du mod\u00e8le ViT\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#entrainement-de-notre-vit","title":"Entrainement de notre ViT\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#chargement-des-datasets-train-val-et-test","title":"Chargement des datasets : train, val et test\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#hyperparametres-et-creation-du-modele","title":"Hyperparam\u00e8tres et cr\u00e9ation du mod\u00e8le\u00b6","text":""},{"location":"07_Transformers/06_VisionTransformerImplementation.html#entrainement-du-modele","title":"Entrainement du mod\u00e8le\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html","title":"Swin transformer","text":"<p>Ce notebook analyse le papier Swin Transformer: Hierarchical Vision Transformer using Shifted Windows qui propose une am\u00e9lioration de l'archicture transformer avec un design hierarchique sp\u00e9cifique aux images pouvant rappeler les r\u00e9seaux de neurones convolutifs. La premi\u00e8re partie du notebook explique les propositions de l'article une par une et la seconde partie est une impl\u00e9mentation simplifi\u00e9e de l'architecture.</p> <p></p> <p>L'id\u00e9e principale du papier est d'appliquer l'attention de mani\u00e8re hierarchique sur des parties de plus en plus grande de l'image. Cette approche a plusieurs fondements : Tout d'abord, l'analyse des images en regardant d'abord les d\u00e9tails locaux avant de regarder la relation entre tous les pixels de l'image est intuitivement logique (c'est pourquoi les CNNs sont si performants). Ensuite, le fait que les tokens (patch) ne communique pas avec tous les autres permet d'am\u00e9liorer le temps de calcul.</p> <p>L'architecture hierarchique du swin transformer est r\u00e9sum\u00e9e dans cette figure :</p> <p></p> <p>Nous avons vu, dans notre impl\u00e9mentation, que le mod\u00e8le ViT convertit les patchs en tokens et applique simplement un transformer encoder sur tous les \u00e9l\u00e9ments. C'est une architecture tr\u00e8s simple et sans aucun biais sur les donn\u00e9es (c'est l'architecture de base du transformer qui peut s'appliquer sur un peu tous les types de donn\u00e9es).</p> <p>L'architecture swin ajoute un biais destin\u00e9e \u00e0 la rendre plus performante sur les images et plus rapide en terme de traitement. Comme on le voit dans la figure, l'image est d'abord s\u00e9par\u00e9 en tr\u00e8s petits patchs (taille $4 \\times 4$ dans le papier) et les patchs sont regroup\u00e9s en fen\u00eatre. La couche d'attention est ensuite appliqu\u00e9e uniquement sur chaque fen\u00eatre de mani\u00e8re ind\u00e9pendante. Plus on va profond dans le r\u00e9seau, plus la dimension C, taille des patchs (relative \u00e0 l'image) et des fen\u00eatres augmente jusqu'\u00e0 avoir une fen\u00eatre de toute l'image et le m\u00eame nombre de patchs que l'architecture ViT. A la mani\u00e8re d'un CNN, le r\u00e9seau tra\u00eete d'abord les informations locales puis au fur et \u00e0 mesure (avec l'augmentation du receptive field) des informations de plus en plus globales. Cela se fait en augmentant le nombre de filtres et en diminuant la r\u00e9solution de l'image.</p> <p>Les nouveaux blocks de transformer correspondants sont appel\u00e9s Window Multi-Head Self-Attention (W-MSA dans le papier, attention le M signifie Multi-Head et pas Masked).</p> <p>Dans leur analogie avec le CNN, les auteurs se sont rendu compte qu'il peut \u00eatre probl\u00e9matique de s\u00e9parer en fen\u00eatres \u00e0 des positions arbitraires car cela brise la connexion entre des pixels voisins situ\u00e9s \u00e0 des extremit\u00e9s de fen\u00eatres.</p> <p>Pour corriger ce probl\u00e8me, les auteurs proposent d'utiliser un syst\u00e8me de fen\u00eatre glissante (shifting window) dans chaque block swin. Les blocks swin sont agenc\u00e9s par paires comme d\u00e9crit dans la figure du d\u00e9but du notebook.</p> <p>Voici \u00e0 quoi ressemble la fen\u00eatre glissante :</p> <p></p> <p>Comme vous pouvez le constater, avec cette technique, on passe de $2 \\times 2$ patchs \u00e0 $3 \\times 3$ patchs (de mani\u00e8re g\u00e9n\u00e9rale de $n \\times n$ patchs \u00e0 $(n+1) \\times (n+1)$) ce qui est probl\u00e9matique pour le traitement par le r\u00e9seau en particulier en batch.</p> <p>Les auteurs proposent d'incorporer un cyclic shift qui consiste \u00e0 faire cette op\u00e9ration sur l'image pour permettre un traitement plus efficace :</p> <p></p> <p>A noter que pour utiliser cette m\u00e9thode, il est n\u00e9cessaire de masquer les informations de patchs ne provenant pas d'une m\u00eame partie de l'image. Les parties blanches, jaunes, vertes et bleues de la figure ne communique pas ensemble gr\u00e2ce \u00e0 une couche d'attention masqu\u00e9e.</p> <p>L'architecture ViT utilisait un position embedding absolu pour ajouter une information de position sur les diff\u00e9rents patchs. Le probl\u00e8me de position embedding est qu'il ne capture par les relations entre les patchs et est donc moins performant si on donne au transformer des images de r\u00e9solutions diff\u00e9rentes.</p> <p>Le swin tranformer utilise un biais de positions relative pour compenser cela. Ce biais va d\u00e9pendre de la distance relative entre les diff\u00e9rents patchs. Ce biais est ajout\u00e9 lorsque l'attention est calcul\u00e9e entre deux patchs. Ce biais a plusieurs effets mais son principal int\u00earet est qu'il am\u00e9liorer la capture des relations spatiales et qu'il permet de s'adapter \u00e0 des images de r\u00e9solutions diff\u00e9rentes.</p> <ul> <li><p>Comme on le voit sur la premi\u00e8re figure du notebook, il y a plus de couches dans le stage 3 du swin transformer. Lorsqu'on augmente le nombre de couches du r\u00e9seaux, c'est uniquement les couches du stage 3 qui vont \u00eatre augment\u00e9es, les autres couches restent fixes. Cela permet de b\u00e9neficier de l'architecture swin (shifting etc ...) tout en \u00e9tant suffisament profond et performant en terme de temps de traitement.</p> </li> <li><p>Supposons que chaque fen\u00eatre contienne des patchs de $M \\times M$. La complexit\u00e9 computationnelle d'une couche multi-head self-attention (MSA) et celle d'une couche window multi-head self-attention (W-MSA) une image de $h \\times w$ patchs sont: $\\Omega(\\text{MSA}) = 4hwC^2 + 2(h w)^2 C$ $\\Omega(\\text{W-MSA}) = 4hwC^2 + 2M^2hwC$ Le premier est de complexit\u00e9 quadratique tandis que le second est lin\u00e9aire si $M$ est fixe. L'architecture swin permet de gagner en vitesse de traitement.</p> </li> </ul> <p>Passons maintenant \u00e0 l'impl\u00e9mentation en pytorch du swin transformer. Certaines parties sont assez complexes en terme d'impl\u00e9mentation et nous n'allons pas les couvrir dans cette partie : la partie fen\u00eatre glissante et la partie relative position bias. Nous allons donc nous contenter d'impl\u00e9menter l'architecture hierarchique.</p> <p>Si vous souhaitez regarder l'impl\u00e9mentation compl\u00e8te du swin transformer par les auteurs, vous pouvez aller voir leur github. Notre impl\u00e9mentation s'inspire du code des auteurs et reprend notre impl\u00e9mentation du ViT.</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torchvision.datasets as datasets\nimport matplotlib.pyplot as plt\n\n# Detection automatique du GPU\ndevice = \"cpu\"\nif torch.cuda.is_available():\n    device = \"cuda\"\nprint(f\"using device: {device}\")\n</pre> import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as T import torchvision.datasets as datasets import matplotlib.pyplot as plt  # Detection automatique du GPU device = \"cpu\" if torch.cuda.is_available():     device = \"cuda\" print(f\"using device: {device}\") <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <pre>using device: cuda\n</pre> <p>Pour la conversion de l'image en patch, on reprend notre fonction du notebook pr\u00e9c\u00e9dent :</p> In\u00a0[2]: Copied! <pre>def image_to_patches(image, patch_size):\n    # On rajoute une dimension pour le batch\n    B,C,_,_ = image.shape\n    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n    patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()\n    patches = patches.view(B,-1, C, patch_size, patch_size)\n    patches_flat = patches.flatten(2, 4)\n    return patches_flat\n</pre> def image_to_patches(image, patch_size):     # On rajoute une dimension pour le batch     B,C,_,_ = image.shape     patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)     patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()     patches = patches.view(B,-1, C, patch_size, patch_size)     patches_flat = patches.flatten(2, 4)     return patches_flat <p>Dans l'impl\u00e9mentation du swin, la couche multi-head self-attention ne change pas par rapport \u00e0 l'impl\u00e9mentation du ViT. C'est essentiellement la m\u00eame couche et ce qui va changer c'est la mani\u00e8re de l'utiliser dans le swin block. Reprenons donc notre code du notebook pr\u00e9c\u00e9dent :</p> In\u00a0[3]: Copied! <pre>class Head_enc(nn.Module):\n    \"\"\" Couche de self-attention unique \"\"\"\n    def __init__(self, head_size,n_embd,dropout=0.2):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape   \n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # Le * C**-0.5 correspond \u00e0 la normalisation par la racine de head_size\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        # On a supprimer le masquage du futur\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" Plusieurs couches de self attention en parall\u00e8le\"\"\"\n\n    def __init__(self, num_heads, head_size,n_embd,dropout):\n        super().__init__()\n        # Cr\u00e9ation de num_head couches head_enc de taille head_size\n        self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n</pre> class Head_enc(nn.Module):     \"\"\" Couche de self-attention unique \"\"\"     def __init__(self, head_size,n_embd,dropout=0.2):         super().__init__()         self.key = nn.Linear(n_embd, head_size, bias=False)         self.query = nn.Linear(n_embd, head_size, bias=False)         self.value = nn.Linear(n_embd, head_size, bias=False)         self.dropout = nn.Dropout(dropout)      def forward(self, x):         B,T,C = x.shape            k = self.key(x)   # (B,T,C)         q = self.query(x) # (B,T,C)         # Le * C**-0.5 correspond \u00e0 la normalisation par la racine de head_size         wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)         # On a supprimer le masquage du futur         wei = F.softmax(wei, dim=-1) # (B, T, T)         wei = self.dropout(wei)         v = self.value(x) # (B,T,C)         out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)         return out  class MultiHeadAttention(nn.Module):     \"\"\" Plusieurs couches de self attention en parall\u00e8le\"\"\"      def __init__(self, num_heads, head_size,n_embd,dropout):         super().__init__()         # Cr\u00e9ation de num_head couches head_enc de taille head_size         self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])         self.proj = nn.Linear(n_embd, n_embd)         self.dropout = nn.Dropout(dropout)      def forward(self, x):         out = torch.cat([h(x) for h in self.heads], dim=-1)         out = self.dropout(self.proj(out))         return out <p>Note : Si on voulait impl\u00e9menter le relative position bias, on aurait besoin de modifier la fonction car ce bias s'ajoute directement lors du calcul de l'attention (voir code source pour aller plus loin).</p> <p>C'est pareil pour la feed forward layer qui reste la m\u00eame :</p> In\u00a0[4]: Copied! <pre>class FeedFoward(nn.Module):\n\n    def __init__(self, n_embd,dropout):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GELU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</pre> class FeedFoward(nn.Module):      def __init__(self, n_embd,dropout):         super().__init__()         self.net = nn.Sequential(             nn.Linear(n_embd, 4 * n_embd),             nn.GELU(),             nn.Linear(4 * n_embd, n_embd),             nn.Dropout(dropout),         )      def forward(self, x):         return self.net(x) <p>Commen\u00e7ons par impl\u00e9menter la fonction pour partionner notre image en fen\u00eatre. Pour cela, on va reconvertir notre x en dimension $B \\times H \\times W \\times C$ plut\u00f4t que $B \\times T \\times C$. Puis on va ensuite transformer notre tenseur en plusieurs fen\u00eatres qui passeront dans la dimension batch (pour tra\u00eeter chaque fen\u00eatre ind\u00e9pendamment).</p> In\u00a0[5]: Copied! <pre>def window_partition(x, window_size,input_resolution):\n    B,_,C = x.shape\n    H,W = input_resolution\n    x = x.view(B, H, W, C)\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n</pre> def window_partition(x, window_size,input_resolution):     B,_,C = x.shape     H,W = input_resolution     x = x.view(B, H, W, C)     B, H, W, C = x.shape     x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)     windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)     return windows <p>Pour l'exemple, supposons que, comme dans l'impl\u00e9mentation du papier, on divise notre image de taille 224 et patchs de taille $4 \\times 4$. Cela nous donnera $224/4 \\times 224/4$ patchs donc 3136 qui seront ensuite projet\u00e9 dans une dimension d'embedding $C$ de taille 96 (pour swin-T et swin-S). On va s\u00e9parer en $M=7$ fen\u00eatres ce qui nous donnera ce tenseur :</p> In\u00a0[6]: Copied! <pre># Pour un batch de taille 2\nwindow_size = 7\nn_embed = 96\ndummy=torch.randn(2,3136,n_embed)\nwindows=window_partition(dummy,window_size,(56,56))\nprint(windows.shape)\n</pre> # Pour un batch de taille 2 window_size = 7 n_embed = 96 dummy=torch.randn(2,3136,n_embed) windows=window_partition(dummy,window_size,(56,56)) print(windows.shape) <pre>torch.Size([128, 7, 7, 96])\n</pre> <p>Avant de le passer \u00e0 la couche d'attention, on va devoir le remettre dans une dimension $B \\times T \\times C$.</p> In\u00a0[7]: Copied! <pre>windows=windows.view(-1, window_size * window_size, n_embed)\nprint(windows.shape)\n</pre> windows=windows.view(-1, window_size * window_size, n_embed) print(windows.shape) <pre>torch.Size([128, 49, 96])\n</pre> <p>On pourra ensuite appliquer notre couche d'attention pour effectuer le self-attention sur toutes les fen\u00eatres ind\u00e9pendamment. Une fois que c'est fait, il faut appliquer la transform\u00e9e inverse pour revenir dans un format sans fen\u00eatres :</p> In\u00a0[8]: Copied! <pre>def window_reverse(windows, window_size,input_resolution):\n    H,W=input_resolution\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\nwindows=window_reverse(windows,window_size,(56,56))\nprint(windows.shape)\n# et revenir en format BxTxC\nwindows=windows.view(2,3136,n_embed)\nprint(windows.shape)\n</pre> def window_reverse(windows, window_size,input_resolution):     H,W=input_resolution     B = int(windows.shape[0] / (H * W / window_size / window_size))     x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)     x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)     return x  windows=window_reverse(windows,window_size,(56,56)) print(windows.shape) # et revenir en format BxTxC windows=windows.view(2,3136,n_embed) print(windows.shape) <pre>torch.Size([2, 56, 56, 96])\ntorch.Size([2, 3136, 96])\n</pre> <p>Nous venons d'impl\u00e9menter les \u00e9l\u00e9ments fondamentaux pour le traitement en fen\u00eatre (hierarchique du swin transformer). On va maintenant pouvoir construire notre block swin regroupant toutes ces transformations :</p> In\u00a0[9]: Copied! <pre>class swinblock(nn.Module):\n  def __init__(self, n_embd,n_head,input_resolution,window_size,dropout=0.) -&gt; None:\n    super().__init__()\n    head_size = n_embd // n_head\n    self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)\n    self.ffwd = FeedFoward(n_embd,dropout)\n    self.ln1 = nn.LayerNorm(n_embd)\n    self.ln2 = nn.LayerNorm(n_embd)\n    self.input_resolution = input_resolution\n    self.window_size = window_size\n    self.n_embd = n_embd\n    \n  def forward(self,x):\n    B,T,C = x.shape\n    x=window_partition(x, self.window_size,self.input_resolution)\n    x=self.ln1(x)\n    x=x.view(-1, self.window_size * self.window_size, self.n_embd)\n    x=self.sa(x)\n    x=window_reverse(x,self.window_size,self.input_resolution)\n    x=x.view(B,T,self.n_embd)\n    x=x+self.ffwd(self.ln2(x))\n    return x\n</pre> class swinblock(nn.Module):   def __init__(self, n_embd,n_head,input_resolution,window_size,dropout=0.) -&gt; None:     super().__init__()     head_size = n_embd // n_head     self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)     self.ffwd = FeedFoward(n_embd,dropout)     self.ln1 = nn.LayerNorm(n_embd)     self.ln2 = nn.LayerNorm(n_embd)     self.input_resolution = input_resolution     self.window_size = window_size     self.n_embd = n_embd        def forward(self,x):     B,T,C = x.shape     x=window_partition(x, self.window_size,self.input_resolution)     x=self.ln1(x)     x=x.view(-1, self.window_size * self.window_size, self.n_embd)     x=self.sa(x)     x=window_reverse(x,self.window_size,self.input_resolution)     x=x.view(B,T,self.n_embd)     x=x+self.ffwd(self.ln2(x))     return x <p>Dans l'architecture hierarchique du swin transformer, \u00e0 chaque fois l'on augmente notre receptive field donc que l'on diminue le nombre de fen\u00eatres, on va concatener les 4 patchs adjacents de taille $C$ en dimension $4C$ puis appliquer une couche lin\u00e9aire pour revenir \u00e0 une dimension plus petite de $2C$. Cela va r\u00e9duire le nombre de tokens par 4 \u00e0 chaque fois que l'on r\u00e9duit le nombre de fen\u00eatres. On peut r\u00e9cuperer les patchs adjacents de cette mani\u00e8re :</p> In\u00a0[10]: Copied! <pre># Reprenons un exemple de nos 56x56 patchs\ndummy=torch.randn(2,3136,n_embed)\nB,T,C = dummy.shape\nH,W=T**0.5,T**0.5\ndummy=dummy.view(2,56,56,n_embed)\n# En python, 0::2 prend un \u00e9l\u00e9ment sur 2 \u00e0 partir de 0, 1::2 prend un \u00e9l\u00e9ment sur 2 \u00e0 partir de 1\n# De cette mani\u00e8re, on peut r\u00e9cup\u00e9rer les \u00e0 intervalles r\u00e9guliers \ndummy0 = dummy[:, 0::2, 0::2, :]  # B H/2 W/2 C\ndummy1 = dummy[:, 1::2, 0::2, :]  # B H/2 W/2 C\ndummy2 = dummy[:, 0::2, 1::2, :]  # B H/2 W/2 C\ndummy3 = dummy[:, 1::2, 1::2, :]  # B H/2 W/2 C\nprint(dummy0.shape)\n</pre> # Reprenons un exemple de nos 56x56 patchs dummy=torch.randn(2,3136,n_embed) B,T,C = dummy.shape H,W=T**0.5,T**0.5 dummy=dummy.view(2,56,56,n_embed) # En python, 0::2 prend un \u00e9l\u00e9ment sur 2 \u00e0 partir de 0, 1::2 prend un \u00e9l\u00e9ment sur 2 \u00e0 partir de 1 # De cette mani\u00e8re, on peut r\u00e9cup\u00e9rer les \u00e0 intervalles r\u00e9guliers  dummy0 = dummy[:, 0::2, 0::2, :]  # B H/2 W/2 C dummy1 = dummy[:, 1::2, 0::2, :]  # B H/2 W/2 C dummy2 = dummy[:, 0::2, 1::2, :]  # B H/2 W/2 C dummy3 = dummy[:, 1::2, 1::2, :]  # B H/2 W/2 C print(dummy0.shape) <pre>torch.Size([2, 28, 28, 96])\n</pre> <p>On va ensuite concatener nos patchs adjacents :</p> In\u00a0[11]: Copied! <pre>dummy = torch.cat([dummy0, dummy1, dummy2, dummy3], -1)  # B H/2 W/2 4*C\nprint(dummy.shape)\n# On repasse en BxTxC\ndummy = dummy.view(B, -1, 4 * C)\nprint(dummy.shape)\n</pre> dummy = torch.cat([dummy0, dummy1, dummy2, dummy3], -1)  # B H/2 W/2 4*C print(dummy.shape) # On repasse en BxTxC dummy = dummy.view(B, -1, 4 * C) print(dummy.shape) <pre>torch.Size([2, 28, 28, 384])\ntorch.Size([2, 784, 384])\n</pre> <p>On a bien divis\u00e9 par quatre le nombre de patchs tout en augmentant les channels par 4. On applique maintenant la couche lin\u00e9aire pour diminuer le nombre de channels.</p> In\u00a0[12]: Copied! <pre>layer = nn.Linear(4 * C, 2 * C, bias=False)\ndummy = layer(dummy)\nprint(dummy.shape)\n</pre> layer = nn.Linear(4 * C, 2 * C, bias=False) dummy = layer(dummy) print(dummy.shape) <pre>torch.Size([2, 784, 192])\n</pre> <p>Et voil\u00e0, on a tous les \u00e9l\u00e9ments pour construire notre couche de merging :</p> In\u00a0[13]: Copied! <pre>class PatchMerging(nn.Module):\n\n    def __init__(self, input_resolution, in_channels, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.in_channels = in_channels\n        self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)\n        self.norm = norm_layer(4 * in_channels)\n\n    def forward(self, x):\n\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n        x = self.norm(x)\n        x = self.reduction(x)\n        return x\n</pre> class PatchMerging(nn.Module):      def __init__(self, input_resolution, in_channels, norm_layer=nn.LayerNorm):         super().__init__()         self.input_resolution = input_resolution         self.in_channels = in_channels         self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)         self.norm = norm_layer(4 * in_channels)      def forward(self, x):          H, W = self.input_resolution         B, L, C = x.shape         assert L == H * W, \"input feature has wrong size\"         assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"          x = x.view(B, H, W, C)         x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C         x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C         x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C         x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C         x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C         x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C         x = self.norm(x)         x = self.reduction(x)         return x <p>Pour le swin transformer, il est compliqu\u00e9 d'ajouter un cls_token dans l'impl\u00e9mentation. C'est pourquoi nous allons utiliser l'autre m\u00e9thode mentionn\u00e9e dans le notebook pr\u00e9c\u00e9dent c'est \u00e0 dire l'adaptive average pooling. Cela nous permet d'avoir une sortie de taille fixe peu importe la taille de l'image d'entr\u00e9e.</p> In\u00a0[14]: Copied! <pre># 3 blocs de 2 couches au lieu de 4 car CIFAR-10 a de plus petites images\nclass SwinTransformer(nn.Module):\n  def __init__(self,n_embed,patch_size,C,window_size,num_heads,img_dim=[16,8,4],depths=[2,2,2]) -&gt; None:\n    super().__init__()\n    self.patch_size = patch_size\n    self.proj_layer = nn.Linear(C*patch_size*patch_size, n_embed)\n    input_resolution = [(img_dim[0],img_dim[0]),(img_dim[1],img_dim[1]),(img_dim[2],img_dim[2])]\n    self.blocks1 = nn.Sequential(*[swinblock(n_embed,num_heads,input_resolution[0],window_size) for _ in range(depths[0])])\n    self.down1 = PatchMerging(input_resolution[0],in_channels=n_embed)\n    self.blocks2 = nn.Sequential(*[swinblock(n_embed*2,num_heads,input_resolution[1],window_size) for _ in range(depths[1])])\n    self.down2 = PatchMerging(input_resolution[1],in_channels=n_embed*2)\n    self.blocks3 = nn.Sequential(*[swinblock(n_embed*4,num_heads,input_resolution[2],window_size) for _ in range(depths[2])])\n    self.classi_head = nn.Linear(n_embed*4, 10)\n    self.avgpool = nn.AdaptiveAvgPool1d(1)\n  \n  def forward(self,x):\n    x = image_to_patches(x,self.patch_size)\n    x = self.proj_layer(x)\n    x = self.blocks1(x)\n    x = self.down1(x)\n    x = self.blocks2(x)\n    x = self.down2(x)\n    x = self.blocks3(x)\n    x = self.avgpool(x.transpose(1, 2)).flatten(1)\n    x = self.classi_head(x)\n    return x\n</pre> # 3 blocs de 2 couches au lieu de 4 car CIFAR-10 a de plus petites images class SwinTransformer(nn.Module):   def __init__(self,n_embed,patch_size,C,window_size,num_heads,img_dim=[16,8,4],depths=[2,2,2]) -&gt; None:     super().__init__()     self.patch_size = patch_size     self.proj_layer = nn.Linear(C*patch_size*patch_size, n_embed)     input_resolution = [(img_dim[0],img_dim[0]),(img_dim[1],img_dim[1]),(img_dim[2],img_dim[2])]     self.blocks1 = nn.Sequential(*[swinblock(n_embed,num_heads,input_resolution[0],window_size) for _ in range(depths[0])])     self.down1 = PatchMerging(input_resolution[0],in_channels=n_embed)     self.blocks2 = nn.Sequential(*[swinblock(n_embed*2,num_heads,input_resolution[1],window_size) for _ in range(depths[1])])     self.down2 = PatchMerging(input_resolution[1],in_channels=n_embed*2)     self.blocks3 = nn.Sequential(*[swinblock(n_embed*4,num_heads,input_resolution[2],window_size) for _ in range(depths[2])])     self.classi_head = nn.Linear(n_embed*4, 10)     self.avgpool = nn.AdaptiveAvgPool1d(1)      def forward(self,x):     x = image_to_patches(x,self.patch_size)     x = self.proj_layer(x)     x = self.blocks1(x)     x = self.down1(x)     x = self.blocks2(x)     x = self.down2(x)     x = self.blocks3(x)     x = self.avgpool(x.transpose(1, 2)).flatten(1)     x = self.classi_head(x)     return x <p>Pour tester notre mod\u00e8le, nous allons \u00e0 nouveau utiliser CIFAR-10 m\u00eame si la petite taille des images ne se pr\u00eate pas forc\u00e9ment bien \u00e0 l'architecture hierarchique.</p> <p>Note : Vous pouvez s\u00e9l\u00e9ctionner une sous-partie du dataset pour acc\u00e9lerer l'entra\u00eenement.</p> In\u00a0[15]: Copied! <pre>import torchvision.transforms as T\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\n\nclasses = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n# Transformation des donn\u00e9es, normalisation et transformation en tensor pytorch\ntransform = T.Compose([T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ndataset = datasets.CIFAR10(root='./../data', train=True,download=False, transform=transform)\n# indices = torch.randperm(len(dataset))[:5000]\n# dataset = torch.utils.data.Subset(dataset, indices)\n\ntestdataset = datasets.CIFAR10(root='./../data', train=False,download=False, transform=transform)\n# indices = torch.randperm(len(testdataset))[:1000]\n# testdataset = torch.utils.data.Subset(testdataset, indices)\nprint(\"taille d'une image : \",dataset[0][0].shape)\n\n\n#Cr\u00e9ation des dataloaders pour le train, validation et test\ntrain_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\nprint(\"taille du train dataset : \",len(train_dataset))\nprint(\"taille du val dataset : \",len(val_dataset))\nprint(\"taille du test dataset : \",len(testdataset))\ntrain_loader = DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2)\nval_loader= DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2)\ntest_loader = DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2)\n</pre> import torchvision.transforms as T import torchvision.datasets as datasets from torch.utils.data import DataLoader  classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck') # Transformation des donn\u00e9es, normalisation et transformation en tensor pytorch transform = T.Compose([T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) dataset = datasets.CIFAR10(root='./../data', train=True,download=False, transform=transform) # indices = torch.randperm(len(dataset))[:5000] # dataset = torch.utils.data.Subset(dataset, indices)  testdataset = datasets.CIFAR10(root='./../data', train=False,download=False, transform=transform) # indices = torch.randperm(len(testdataset))[:1000] # testdataset = torch.utils.data.Subset(testdataset, indices) print(\"taille d'une image : \",dataset[0][0].shape)   #Cr\u00e9ation des dataloaders pour le train, validation et test train_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2]) print(\"taille du train dataset : \",len(train_dataset)) print(\"taille du val dataset : \",len(val_dataset)) print(\"taille du test dataset : \",len(testdataset)) train_loader = DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2) val_loader= DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2) test_loader = DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2) <pre>taille d'une image :  torch.Size([3, 32, 32])\ntaille du train dataset :  40000\ntaille du val dataset :  10000\ntaille du test dataset :  10000\n</pre> In\u00a0[16]: Copied! <pre>patch_size = 2\nn_embed = 24\nn_head = 4\nC=3 \nwindow_size = 4\n\nepochs = 10\nlr = 0.0001 #1e-3\n\nmodel = SwinTransformer(n_embed,patch_size,C,window_size,n_head,img_dim=[16,8,4],depths=[2,2,2]).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n</pre> patch_size = 2 n_embed = 24 n_head = 4 C=3  window_size = 4  epochs = 10 lr = 0.0001 #1e-3  model = SwinTransformer(n_embed,patch_size,C,window_size,n_head,img_dim=[16,8,4],depths=[2,2,2]).to(device) optimizer = torch.optim.Adam(model.parameters(), lr=lr)  In\u00a0[17]: Copied! <pre>for epoch in range(epochs):\n    model.train()\n    loss_train = 0\n    for i, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        output = model(images)\n        loss = F.cross_entropy(output, labels)\n        loss_train += loss.item()\n        loss.backward()\n        optimizer.step()\n        \n    model.eval()\n    correct = 0\n    total = 0\n    loss_val = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss_val += F.cross_entropy(outputs, labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Epoch {epoch}, loss train {loss_train/len(train_loader)}, loss val {loss_val/len(val_loader)},pr\u00e9cision {100 * correct / total}\")\n</pre> for epoch in range(epochs):     model.train()     loss_train = 0     for i, (images, labels) in enumerate(train_loader):         images, labels = images.to(device), labels.to(device)         optimizer.zero_grad()         output = model(images)         loss = F.cross_entropy(output, labels)         loss_train += loss.item()         loss.backward()         optimizer.step()              model.eval()     correct = 0     total = 0     loss_val = 0     with torch.no_grad():         for images, labels in val_loader:             images, labels = images.to(device), labels.to(device)             outputs = model(images)             loss_val += F.cross_entropy(outputs, labels).item()             _, predicted = torch.max(outputs.data, 1)             total += labels.size(0)             correct += (predicted == labels).sum().item()      print(f\"Epoch {epoch}, loss train {loss_train/len(train_loader)}, loss val {loss_val/len(val_loader)},pr\u00e9cision {100 * correct / total}\") <pre>Epoch 0, loss train 1.9195597559928894, loss val 1.803518475151062,pr\u00e9cision 33.94\nEpoch 1, loss train 1.7417401003360748, loss val 1.6992134885787964,pr\u00e9cision 37.84\nEpoch 2, loss train 1.651085284280777, loss val 1.6203388486862182,pr\u00e9cision 40.53\nEpoch 3, loss train 1.5808091670751572, loss val 1.5558069843292237,pr\u00e9cision 43.03\nEpoch 4, loss train 1.522760990524292, loss val 1.5169190183639527,pr\u00e9cision 44.3\nEpoch 5, loss train 1.4789127678394318, loss val 1.4665142657279968,pr\u00e9cision 47.02\nEpoch 6, loss train 1.4392719486951828, loss val 1.4568698994636535,pr\u00e9cision 47.65\nEpoch 7, loss train 1.4014943064451217, loss val 1.4456377569198609,pr\u00e9cision 48.14\nEpoch 8, loss train 1.3745941290140151, loss val 1.4345624563694,pr\u00e9cision 48.38\nEpoch 9, loss train 1.3492228104948998, loss val 1.398228020954132,pr\u00e9cision 50.04\n</pre> <p>L'entra\u00eenement est termin\u00e9, on obtient une pr\u00e9cision de 50% sur les donn\u00e9es de validation.</p> <p>Regardons maintenant sur nos donn\u00e9es de test :</p> In\u00a0[18]: Copied! <pre>model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Pr\u00e9cision {100 * correct / total}\")\n</pre> model.eval() correct = 0 total = 0 with torch.no_grad():     for images, labels in test_loader:         images, labels = images.to(device), labels.to(device)         outputs = model(images)         _, predicted = torch.max(outputs.data, 1)         total += labels.size(0)         correct += (predicted == labels).sum().item()  print(f\"Pr\u00e9cision {100 * correct / total}\") <pre>Pr\u00e9cision 49.6\n</pre> <p>La pr\u00e9cision est \u00e0 peu pr\u00e8s similaire que sur les donn\u00e9es de validation !</p> <p>Note : Les r\u00e9sultats ne sont pas tr\u00e8s bons pour plusieurs raisons. Tout d'abord, nous traitons des petites images et l'architecture hierarchique du swin transformer est plut\u00f4t con\u00e7u pour traiter des images de plus grandes dimensions. Ensuite, notre impl\u00e9mentation est vraiment minimaliste puisqu'il manque deux \u00e9l\u00e9ments cl\u00e9s de l'architecture swin : la partie fen\u00eatre glissante et la partie relative position bias. Le but de ce notebook \u00e9tait de vous donner une intuition sur le fonctionnement de l'architecture swin et pas de vous proposer une impl\u00e9mentation parfaite ;)</p>"},{"location":"07_Transformers/07_SwinTransformer.html#swin-transformer","title":"Swin transformer\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#analyse-de-larticle","title":"Analyse de l'article\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#architecture-hierarchique","title":"Architecture hierarchique\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#fenetre-glissante","title":"Fen\u00eatre glissante\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#relative-position-bias","title":"Relative position bias\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#details-supplementaires-sur-larchitecture","title":"D\u00e9tails suppl\u00e9mentaires sur l'architecture\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#implementation-simplifiee","title":"Impl\u00e9mentation simplifi\u00e9e\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#conversion-de-limage-en-patch","title":"Conversion de l'image en patch\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#multi-head-self-attention","title":"Multi-head self-attention\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#feed-foward-layer","title":"Feed foward layer\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#implementation-du-swin-block","title":"Impl\u00e9mentation du swin block\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#patch-merging","title":"Patch merging\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#construction-du-modele-swin","title":"Construction du mod\u00e8le swin\u00b6","text":""},{"location":"07_Transformers/07_SwinTransformer.html#entrainement-sur-imagenette","title":"Entrainement sur Imagenette\u00b6","text":""},{"location":"08_DetectionEtYolo/index.html","title":"\ud83d\udd0d Detection \ud83d\udd0d","text":"<p>Ce cours pr\u00e9sente le fonctionnement de la d\u00e9tection d'objets sur des images. L'introduction pr\u00e9sente ce qu'est la d\u00e9tection et les deux m\u00e9thodes classiques (two-stage et one-stage). Le notebook suivant propose une description pr\u00e9cise du fonctionnement de YOLO et le dernier notebook pr\u00e9sente la library ultralytics qui permet d'acc\u00e8der aux mod\u00e8les YOLO tr\u00e8s simplement.</p>"},{"location":"08_DetectionEtYolo/index.html#notebook-1-introduction","title":"Notebook 1\ufe0f\u20e3 : Introduction","text":"<p>Ce notebook introduit les concepts cl\u00e9s de la d\u00e9tection d'objet avec deep learning (two-stage et one-stage detector, NMS etc ...)</p>"},{"location":"08_DetectionEtYolo/index.html#notebook-2-yolo-en-detail","title":"Notebook 2\ufe0f\u20e3 : Yolo en d\u00e9tail","text":"<p>Ce notebook pr\u00e9sente l'architecture You Only Look Once (YOLO) en d\u00e9tail et pr\u00e9sente ses variantes.</p>"},{"location":"08_DetectionEtYolo/index.html#notebook-3-ultralytics","title":"Notebook 3\ufe0f\u20e3 : Ultralytics","text":"<p>Ce notebook pr\u00e9sente la library ultralytics qui permet d'utiliser les mod\u00e8les YOLO et SAM tr\u00e8s simplement.</p>"},{"location":"08_DetectionEtYolo/01_Introduction.html","title":"Introduction \u00e0 la d\u00e9tection d'objets dans les images","text":"<p>Nous avons vu pr\u00e9c\u00e9demment que le traitement d'images regroupe trois grandes cat\u00e9gories :</p> <ul> <li>La classification : Est ce qu'un objet est sur l'image ? (Est-ce une photo de chien ?)</li> <li>La d\u00e9tection : Ou se situe un objet sur une image (si celui-ci est pr\u00e9sent) ? (Quelle est la position du chien sur cette image ?)</li> <li>La segmentation : Quels sont les pixels qui appartiennent \u00e0 un objet ? (Quels sont pr\u00e9cisement les pixels du chien sur l'image?)</li> </ul> <p></p> <p>Image extraite de ce site.</p> <p>Dans le cours sur les CNN, nous avons vu des probl\u00e8mes de classification avec une architecture de CNN classique qui se termine par une couche Fully Connected et nous avons \u00e9galement vu un probl\u00e8me de segmentation avec l'utilisation du mod\u00e8le U-Net.</p> <p>La t\u00e2che de d\u00e9tection \u00e9tant un peu plus complexe \u00e0 expliquer, ce cours est d\u00e9di\u00e9 \u00e0 proposer une description des m\u00e9thodes existantes et une description pr\u00e9cise du mod\u00e8le YOLO.</p> <p>Dans cette introduction, nous allons d'abord expliquer les diff\u00e9rences entre les deux principales cat\u00e9gories de d\u00e9tecteur :</p> <ul> <li>M\u00e9thodes en deux \u00e9tapes (Two-Stage Detectors) regroupant la famille des RCNN(Region-based Convolutional Neural Networks).</li> <li>M\u00e9thodes en une \u00e9tape (Single-Stage Detectors) regroupant la famille des YOLO(You Only Look Once).</li> </ul> <p>Comme son nom l'indique, le two-stage detector a deux \u00e9tapes dans la d\u00e9tection d'objets :</p> <ul> <li>Une premi\u00e8re \u00e9tape de proposition de region (region proposal) qui genere des propositions de r\u00e9gion o\u00f9 des objets d'int\u00earet pourraient se trouver.</li> <li>Une seconde \u00e9tape consiste \u00e0 affiner la d\u00e9tection, c'est \u00e0 dire associer la classe de l'objet et sp\u00e9cifier la bounding box plus pr\u00e9cisement (si un objet est bien pr\u00e9sent).</li> </ul> <p></p> <p>Image extraite de l'article.</p> <p>De mani\u00e8re g\u00e9n\u00e9rale, les two-stage detectors sont tr\u00e8s pr\u00e9cis et permettent des d\u00e9tections complexes mais sont assez lent et ne permettent pas le tra\u00eetement en temps r\u00e9el.</p> <p>Les r\u00e9seaux two-stage les plus connus sont la famille des RCNN. Pour en savoir plus, je vous invite \u00e0 consulter ce blogpost.</p> <p>Le one-stage detector ne n\u00e9cessite qu'une seule \u00e9tape pour g\u00e9n\u00e9rer les bounding box avec les labels correspondants. Le r\u00e9seau divise l'image en une grille et pour chaque cellule de la grille, il va pr\u00e9dire plusieurs bounding box et leurs probabilit\u00e9s correspondantes.</p> <p></p> <p>Figure extraite de l'article.</p> <p>Les one-stage detectors sont en g\u00e9n\u00e9ral moins pr\u00e9cis que les two-stage detectors mais ils sont beaucoup plus rapides et permettent un traitement en temps r\u00e9el. C'est la famille de d\u00e9tecteurs la plus utilis\u00e9e aujourd'hui.</p> <p>Lorsque l'on fait la d\u00e9tection d'objets avec notre mod\u00e8le, l'architecture ne permet pas d'\u00e9viter que plusieurs bounding box se retrouvent sur le m\u00eame objet (avec des positions qui se chevauchent). Avant de remonter les d\u00e9tections \u00e0 l'utilisateur du mod\u00e8le, on voudrait avoir une unique d\u00e9tection par objet de l'image et que \u00e7a soit la plus pertinente.</p> <p>C'est l\u00e0 qu'intervient la non-maximum suppression, l'algorithme ne sera pas d\u00e9taill\u00e9 dans ce cours mais vous pouvez regarder les ressources suivantes pour entrer dans le d\u00e9tail : blogpost et site.</p> <p></p> <p>Les ancres sont des bounding boxes pr\u00e9d\u00e9finies plac\u00e9es sur une grille r\u00e9guli\u00e8re qui recouvre l'image. Ces ancres peuvent avoir diff\u00e9rents ratio (longueur/hauteur) et sont de taille variable pour couvrir le maximum de taille d'objets possible. Ces ancres permettent en fait de diminuer le nombre de positions \u00e0 \u00e9tudier pour le mod\u00e8le. Ce que le mod\u00e8le va pr\u00e9dire quand on utilise des ancres, c'est le d\u00e9calage par rapport \u00e0 l'ancre pr\u00e9-g\u00e9n\u00e9r\u00e9e et la probabilit\u00e9 d'appartenance \u00e0 un objet.</p> <p>Cette m\u00e9thode a montr\u00e9 de bons r\u00e9sultats pour l'am\u00e9lioration de la qualit\u00e9 des d\u00e9tections. Pour en savoir plus, vous pouvez consulter le blogpost.</p> <p>En pratique, il y a souvent \u00e9normement d'ancres. La figure suivante montre 1% des ancres du mod\u00e8le retinaNet :</p> <p></p> <p>Plus r\u00e9cemment, l'architecture du transformer a \u00e9t\u00e9 adapt\u00e9e pour la d\u00e9tection d'objets. Le mod\u00e8le DETR propose une approche utilisant \u00e0 la fois un mod\u00e8le CNN pour l'extraction de caract\u00e9ristiques visuelles. Ces features sont ensuite pass\u00e9es \u00e0 travers un transformer encoder (avec l'ajout d'un positional embedding) pour d\u00e9terminer les relations spatiales entre les caract\u00e8ristiques gr\u00e2ce au m\u00e9canisme d'attention. Enfin, un transformer decoder (pas le m\u00eame type de decoder qu'en NLP) prend en entr\u00e9e la sortie de l'encoder (keys et values) et des embeddings de label d'objets (queries) ce qui va convertir les embeddings en pr\u00e9diction. Finalement, une derni\u00e8re couche lin\u00e9aire va tra\u00eeter la sortie du d\u00e9codeur et pr\u00e9dire label et bounding box.</p> <p>Si vous souhaitez en savoir plus, vous pouvez lire l'article ou lire ce blogpost.</p> <p></p> <p>Cette m\u00e9thode pr\u00e9sente de nombreux avantages :</p> <ul> <li>Pas besoin de NMS, d'ancres ou de region proposal. Cela simplifie grandement l'architecture et le pipeline d'entra\u00eenement.</li> <li>Le mod\u00e8le a une compr\u00e9hension globale de la sc\u00e8ne plus pouss\u00e9e gr\u00e2ce au m\u00e9canisme d'attention. Mais aussi quelques inconv\u00e9nients :</li> <li>Les transformers sont assez gourmands en calcul donc ce mod\u00e8le est moins rapide qu'un one-stage detector comme YOLO.</li> <li>L'apprentissage est souvent plus long que pour un d\u00e9tecteur bas\u00e9 uniquement sur un CNN</li> </ul> <p>Note : Les transformers utilis\u00e9s dans le domaine de la vision ont souvent des temps d'entra\u00eenement sup\u00e9rieur \u00e0 ceux des CNN. Une explication potentielle de cet \u00e9cart est le biais des CNN qui fait qu'ils sont particuli\u00e8rement bien adapt\u00e9s aux images et ont donc besoin d'un temps d'entra\u00eenement plus court. Les transformers sont des mod\u00e8les g\u00e9n\u00e9ralistes qui n'ont pas de biais et ils doivent donc apprendre depuis z\u00e9ro.</p>"},{"location":"08_DetectionEtYolo/01_Introduction.html#introduction-a-la-detection-dobjets-dans-les-images","title":"Introduction \u00e0 la d\u00e9tection d'objets dans les images\u00b6","text":""},{"location":"08_DetectionEtYolo/01_Introduction.html#two-stage-detectors","title":"Two-Stage Detectors\u00b6","text":""},{"location":"08_DetectionEtYolo/01_Introduction.html#one-stage-detectors","title":"One-Stage Detectors\u00b6","text":""},{"location":"08_DetectionEtYolo/01_Introduction.html#non-maximum-suppression-et-ancres","title":"Non-Maximum Suppression et Ancres\u00b6","text":""},{"location":"08_DetectionEtYolo/01_Introduction.html#nms-non-maximum-supression","title":"NMS (Non-Maximum Supression)\u00b6","text":""},{"location":"08_DetectionEtYolo/01_Introduction.html#ancres-anchor-boxes","title":"Ancres (Anchor boxes)\u00b6","text":""},{"location":"08_DetectionEtYolo/01_Introduction.html#bonus-detection-dobjets-avec-larchitecture-transformer","title":"Bonus : Detection d'objets avec l'architecture transformer\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html","title":"Yolo en d\u00e9tail","text":"<p>Dans ce notebook, nous allons analyser en d\u00e9tails le fonctionnement du mod\u00e8le YOLO.</p> <p>Peu apr\u00e8s le fameux papier ayant propuls\u00e9 le deep learning sur le devant de la sc\u00e8ne. Les chercheurs en traitement d'images du monde entier ont commenc\u00e9 \u00e0 travailler sur des mod\u00e8les de Deep Learning.</p> <p>Comme indiqu\u00e9 dans le notebook pr\u00e9c\u00e9dent, il y a trois principales cat\u00e9gories d'algorithme de traitement d'images : la classication, la d\u00e9tection et la segmentation.</p> <p>C\u00f4t\u00e9 classification, l'approche est assez directe d\u00e8s lors qu'on a les ressources pour impl\u00e9menter un mod\u00e8le de deep learning profond. Par contre, pour la d\u00e9tection, il faut \u00eatre plus inventif.</p> <p>En 2014, un groupe de chercheur propose le papier Rich feature hierarchies for accurate object detection and semantic segmentation plus connu sous le nom R-CNN. Ce papier, qui a eu une grande influence, introduit une architecture en deux \u00e9tapes pour la d\u00e9tection d'objets et offre des performances remarquables. Le principal probl\u00e8me de cette approche est son temps de tra\u00eetement trop lent qui ne permet pas de faire de la d\u00e9tection en temps r\u00e9el. De nombreuses m\u00e9thodes ont essay\u00e9 de r\u00e9soudre ce probl\u00e8me de temps en proposant des architectures diff\u00e9rentes comme fast R-CNN, faster R-CNN et mask R-CNN. Ces m\u00e9thodes am\u00e9liorent grandement le R-CNN de base mais ne sont pas suffisantes pour du temps r\u00e9el dans la plupart des cas.</p> <p>En 2015, un article va provoquer un chamboulement majeur dans le domaine de la d\u00e9tection d'objets. Ce papier est You Only Look Once: Unified, Real-Time Object Detection.</p> <p>Les approches pr\u00e9c\u00e9dentes se basaient sur une proposition de r\u00e9gions suivie d'une classification. Il s'agissait donc d'un fa\u00e7on d'utiliser les puissants classifieurs dans une t\u00e2che de d\u00e9tection d'objets.</p> <p>Le papier You Only Look Once (YOLO) propose de pr\u00e9dire les bounding box et les probabilit\u00e9s d'appartenance \u00e0 une classe directement gr\u00e2ce \u00e0 un unique r\u00e9seau de neurones. Cette architecture est beaucoup plus rapide et permet d'atteindre des vitesses de traitement allant jusqu'\u00e0 45 images par seconde.</p> <p>C'est une r\u00e9volution dans le domaine de la d\u00e9tection d'objets !</p> <p>Mais alors, comment \u00e7a marche ?</p> <p>Cette partie d\u00e9crit l'architecture de YOLO en s'inspirant du blogpost qui je vous invite \u00e0 consulter. Les images utilis\u00e9es sont tir\u00e9es du blogpost ou du papier original.</p> <p>Le principe de base de YOLO est de diviser l'image en plus petites images \u00e0 l'aide d'une grille de dimension $S \\times S$ de cette mani\u00e8re :</p> <p></p> <p>La cellule contenant le centre d'un objet (par exemple le chien ou le v\u00e9lo) est la cellule responsable de la d\u00e9tection de cet objet (pour le calcul du loss). Chaque cellule de la grille va pr\u00e9dire $B$ bounding boxes (param\u00e8trable, 2 sur le papier original) et un score de confiance pour chacune des bounding box pr\u00e9dites. La bounding box pr\u00e9dite contient les valeurs $x,y,w,h,c$ ou $(x,y)$ sont la position du centre dans la grille, $(w,h)$ sont la dimension de la box en pourcentage de l'image enti\u00e8re et $c$ est la confiance du mod\u00e8le (probabilit\u00e9).</p> <p>Pour calculer la pr\u00e9cision de notre bounding box lors de l'entra\u00eenement (composante du loss), on utilise l'intersection over union qui est d\u00e9finie comme : $\\frac{pred_{box}\\cap label_{box}}{pred_{box} \\cup label_{box}}$</p> <p></p> <p>En plus de pr\u00e9dire la bounding box et la confiance, chaque cellule va aussi pr\u00e9dire la classe de l'objet. Cette classe est represent\u00e9e par un vecteur one_hot (qui contient uniquement des 0 sauf un 1 dans la bonne classe) dans les annotations. Il est important de pr\u00e9ciser que chaque cellule peut pr\u00e9dire plusieurs bounding box mais une seule classe. C'est une des limitations de l'algorithme : si il y a plusieurs objets dans la m\u00eame cellule, le mod\u00e8le ne pourra pas pr\u00e9dire correctement chaque objet.</p> <p>Maitenant qu'on a toutes les informations, on peut calculer la dimension de sortie du r\u00e9seau. On a $S \\times S$ cellules, chaque cellule va pr\u00e9dire $B$ bouding box et $C$ probabilit\u00e9 (avec $C$ le nombre de classe). La pr\u00e9diction du mod\u00e8le est donc de taille : $S \\times S \\times (C +B \\times 5) $</p> <p>Cela nous am\u00e8ne \u00e0 la figure suivante :</p> <p></p> <p>La figure du centre en haut represente les bounding box pr\u00e9dites par le mod\u00e8le (celles avec un trait plus gros sont les scores de confiance \u00e9l\u00e9v\u00e9s) et la figure du centre en bas repr\u00e9sente la classe pr\u00e9dite dans chaque cellule (en bleu la classe chien, en jaune la classe v\u00e9lo et en rose la classe voiture).</p> <p>L'architecture du mod\u00e8le YOLO en terme d'agencement des couches est aussi assez particuli\u00e8re. Il y a 3 composants principals : head, neck and backbone.</p> <ul> <li>Backbone : C'est la partie la plus importante du r\u00e9seau qui est consistu\u00e9e d'une s\u00e9rie de convolutions pour d\u00e9tecter les features les plus importantes. Typiquement, cette partie est souvent pr\u00e9-entra\u00een\u00e9 sur un dataset de classification.</li> <li>Neck and Head : Ces couches sont responsables du traitement de l'output des couches de convolutions pour sortir une pr\u00e9diction de taille $S \\times S \\times (C +B \\times 5)$</li> </ul> <p>Dans le papier original de YOLO, la grille est de taille 7x7, il y a 20 classes (Pascal VOC) et on pr\u00e9dit deux bounding box par cellule. Cela nous donne une pr\u00e9diction de taille $7 \\times 7 \\times (20 +2 \\times 5) = 1470$</p> <p>Les valeurs d'entra\u00eenement du mod\u00e8le (taille d'image, epochs, nombres de couches, batch_size etc ...) sont d\u00e9taill\u00e9s dans le papier original et nous n'allons pas entrer dans les d\u00e9tails ici.</p> <p>Par contre, il est int\u00e9ressant de s'attarder un peu sur la fonction de loss. L'id\u00e9e logique de base serait d'utiliser simplement le loss MSE entre nos pr\u00e9dictions et les labels. Cependant, \u00e7a ne fonctionne pas tel quel car le mod\u00e8le donnerait une importance similaire \u00e0 la qualit\u00e9 de la localisation et \u00e0 la veracit\u00e9 de la pr\u00e9diction. En pratique, on utilise une pond\u00e9ration sur les loss $\\lambda_{coord}$ et $\\lambda_{noobj}$. Les valeurs du papier original sont d\u00e9fini \u00e0 5 pour $\\lambda_{coord}$ et 0.5 pour $\\lambda_{noobj}$. A noter que $\\lambda_{noobj}$ est utilis\u00e9 uniquement sur les cellules o\u00f9 il n'y a pas d'objets pour \u00e9viter que son score de confiance d'une valeur proche de 0 puisse trop impacter les cellules contenant des objets.</p> <p>Nous avons d\u00e9j\u00e0 \u00e9voqu\u00e9 sa principale limitation qui est de ne pr\u00e9dire qu'un nombre limit\u00e9 de bounding box par cellule et de ne pas permettre la d\u00e9tection d'objets de diff\u00e9rentes cat\u00e9gories dans la m\u00eame cellule. Cela pose probl\u00e8me lorsque l'on veut d\u00e9tecter des personnes dans une foule par exemple.</p> <p>Nous avons vu que YOLO est un mod\u00e8le tr\u00e8s performant et rapide pour la d\u00e9tection d'objets dans les images. C'est pourquoi, de nombreux chercheurs ont cherch\u00e9 \u00e0 l'am\u00e9liorer en proposant diverses optimisations. Aujourd'hui encore, de nouvelles versions de Yolo sortent r\u00e9gulierement.</p> <p>Cette partie pr\u00e9sente chronologiquement les diff\u00e9rentes version de YOLO.</p>"},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolo-en-detail","title":"Yolo en d\u00e9tail\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#detection-dobjets","title":"D\u00e9tection d'objets\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#histoire-de-la-detection-dobjets","title":"Histoire de la d\u00e9tection d'objets\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#you-only-look-once","title":"You Only Look Once\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolo-comment-ca-marche","title":"YOLO : Comment \u00e7a marche ?\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#separation-en-grille","title":"S\u00e9paration en grille\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#architecture-du-modele","title":"Architecture du mod\u00e8le\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#entrainement-du-modele","title":"Entra\u00eenement du mod\u00e8le\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#limitations-de-yolo","title":"Limitations de YOLO\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#amelioration-de-yolo","title":"Am\u00e9lioration de YOLO\u00b6","text":""},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolov2-2017-aussi-connu-sous-le-nom-de-yolo9000","title":"YOLOv2 (2017) - aussi connu sous le nom de YOLO9000\u00b6","text":"<p>Papier : YOLO9000: Better, Faster, Stronger</p> <p>Innovations :</p> <ul> <li>Introduction de l'id\u00e9e d'ancres (anchors) pour am\u00e9liorer la pr\u00e9cision des pr\u00e9dictions de bo\u00eete.</li> <li>Passer de la r\u00e9solution d'entr\u00e9e de 224x224 \u00e0 416x416 pour am\u00e9liorer la d\u00e9tection d'objets de petite taille.</li> </ul>"},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolov3-2018","title":"YOLOv3 (2018)\u00b6","text":"<p>Papier : YOLOv3: An Incremental Improvement</p> <p>Innovations :</p> <ul> <li>Utilisation d'un mod\u00e8le plus profond avec une architecture Darknet-53, un r\u00e9seau de neurones convolutifs r\u00e9siduel.</li> <li>D\u00e9tection multi-\u00e9chelle, avec des pr\u00e9dictions faites \u00e0 trois niveaux de granularit\u00e9 diff\u00e9rents (feature maps de diff\u00e9rentes tailles).</li> </ul>"},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolov4-2020","title":"YOLOv4 (2020)\u00b6","text":"<p>Papier : YOLOv4: Optimal Speed and Accuracy of Object Detection</p> <p>Innovations :</p> <ul> <li>Utilisation du backbone CSPDarknet53 pour une meilleure performance.</li> <li>Am\u00e9liorations des t\u00eates de d\u00e9tection avec PANet (Path Aggregation Network) pour am\u00e9liorer les flux d'informations.</li> <li>Introduction du concept de Mosaic Data Augmentation pour enrichir la diversit\u00e9 des donn\u00e9es d'entra\u00eenement.</li> <li>Ajout de diverses techniques modernes comme DropBlock, Mish activation, et SPP (Spatial Pyramid Pooling).</li> </ul>"},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolov5-2020","title":"YOLOv5 (2020)\u00b6","text":"<p>D\u00e9velopp\u00e9 par Ultralytics</p> <p>Innovations :</p> <ul> <li>Pas de papier officiel, mais des am\u00e9liorations pratiques dans l'impl\u00e9mentation et la performance.</li> <li>Mod\u00e8le plus l\u00e9ger et plus facile \u00e0 entra\u00eener avec une meilleure gestion des d\u00e9pendances.</li> </ul>"},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolov6-2022","title":"YOLOv6 (2022)\u00b6","text":"<p>Innovations :</p> <ul> <li>Nouveau Backbone, YOLOv6S, optimis\u00e9 pour les performances en temps r\u00e9el.</li> <li>Techniques avanc\u00e9es de r\u00e9duction de la latence.</li> <li>Am\u00e9liorations dans les m\u00e9thodes d'augmentation des donn\u00e9es et l'optimisation des hyperparam\u00e8tres.</li> </ul>"},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolov7-2022","title":"YOLOv7 (2022)\u00b6","text":"<p>Papier : YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</p> <p>Innovations :</p> <ul> <li>Int\u00e9gration de bag of freebies pour am\u00e9liorer la pr\u00e9cision sans augmenter le temps d'inf\u00e9rence.</li> <li>Architecture optimis\u00e9e pour un compromis optimal entre vitesse et pr\u00e9cision.</li> <li>Ajout de diverses techniques de r\u00e9gularisation pour am\u00e9liorer la performance g\u00e9n\u00e9rale.</li> </ul>"},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolov8-2023","title":"YOLOv8 (2023)\u00b6","text":"<p>D\u00e9velopp\u00e9 par Ultralytics</p> <p>Innovations :</p> <ul> <li>Encore plus optimis\u00e9 pour les performances en temps r\u00e9el et l'int\u00e9gration mobile.</li> <li>Architecture flexible permettant des ajustements pour divers cas d'utilisation, y compris la d\u00e9tection, la segmentation, et la classification.</li> </ul>"},{"location":"08_DetectionEtYolo/02_YoloEnDetail.html#yolo-world-2024","title":"YOLO-World (2024)\u00b6","text":"<p>Papier : YOLO-World: Real-Time Open-Vocabulary Object Detection</p> <p>Innovations :</p> <ul> <li>Utilisation d'un transformer encoder pour le texte permettant la d\u00e9tection open-vocabulary.</li> </ul>"},{"location":"08_DetectionEtYolo/03_Ultralytics.html","title":"Ultralytics","text":"<p>Ultralytics est une library python tr\u00e8s utile lorsque l'on a besoin d'utiliser ou d'entra\u00eener des mod\u00e8les YOLO. La documentation sur leur site est tr\u00e8s compl\u00e8te mais nous allons quand m\u00eame pr\u00e9senter deux mod\u00e8les YOLO dans ce notebook. Nous verrons dans une derni\u00e8re partie que la library offre aussi la possibilit\u00e9 d'utiliser SAM (segment anything).</p> <p>En regardant la liste des mod\u00e8les YOLO, on peut \u00eatre vite perdu en voyant la vari\u00e9t\u00e9 de mod\u00e8les (YOLOv4, YOLOv5, YOLO-NAS etc ...). De mani\u00e8re g\u00e9n\u00e9rale, j'aurai tendance \u00e0 vous inviter \u00e0 tester les diff\u00e9rents mod\u00e8les et \u00e0 vous faire une id\u00e9e par vous-m\u00eame. Les auteurs des diff\u00e9rentes version de YOLO ne sont pas les m\u00eames donc c'est difficile de dire avec certitude quelle version est la meilleure.</p> <p>Pour la plupart des mod\u00e8les YOLO (sauf YOLO-World), le fa\u00e7on de charger le mod\u00e8le et de faire l'inf\u00e9rence est la m\u00eame. En arrivant sur le page de docs de YOLOv8, vous pourrez constater qu'il y a plusieurs mod\u00e8les \u00e0 disposition.</p> <p></p> <p>YOLOv8 permet de r\u00e9aliser des t\u00e2ches autre que la d\u00e9tection :</p> <ul> <li>La segmentation s\u00e9mantique</li> <li>L'estimation de la pose : La position d'une personne dans l'image (son squelette)</li> <li>La d\u00e9tection orient\u00e9e : Les bounding box de d\u00e9tection peuvent \u00eatre orient\u00e9e</li> <li>La classification</li> </ul> <p>Dans ce notebook, nous allons regarder uniquement un exemple de d\u00e9tection.</p> <p>Pour commencer, vous devez d'abord t\u00e9l\u00e9charger un mod\u00e8le. Le choix du mod\u00e8le va d\u00e9pendre des ressources de votre machine et de la vitesse de traitement d\u00e9sir\u00e9e. Plus le mod\u00e8le est \"gros\", plus il est performant mais moins il est rapide.</p> <p>Prenons le mod\u00e8le le plus rapide, yolov8n. Voici comment vous pouvez le charger avec la library ultralytics :</p> In\u00a0[1]: Copied! <pre>from ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\n\n# On charge le mod\u00e8le pre-entrain\u00e9 YOLOv8n\nmodel = YOLO(\"yolov8n.pt\")\n\n# Affiche les informations du mod\u00e8le\nmodel.info()\n\n# On fait une pr\u00e9diction sur une image\nresults = model(\"images/coco.jpg\")\n</pre> from ultralytics import YOLO import cv2 import matplotlib.pyplot as plt  # On charge le mod\u00e8le pre-entrain\u00e9 YOLOv8n model = YOLO(\"yolov8n.pt\")  # Affiche les informations du mod\u00e8le model.info()  # On fait une pr\u00e9diction sur une image results = model(\"images/coco.jpg\") <pre>YOLOv8n summary: 225 layers, 3157200 parameters, 0 gradients, 8.9 GFLOPs\n\nimage 1/1 /Users/simonthomine/Documents/CoursDeepLearning/08_WIP_DetectionEtYolo/images/coco.jpg: 448x640 6 persons, 2 cars, 1 backpack, 3 handbags, 5 cell phones, 79.1ms\nSpeed: 1.1ms preprocess, 79.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n</pre> In\u00a0[2]: Copied! <pre># Seuil de d\u00e9tection\nthreshold=0.5\n# Les noms des classes\nnames={0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\nimage=cv2.imread(\"images/coco.jpg\")\nboxes = results[0].boxes.xyxy.cpu().numpy()\nconfidences=results[0].boxes.conf.cpu().numpy()\nlabels=results[0].boxes.cls.cpu().numpy()\n# Affichage des r\u00e9sultats\nfor box,conf,label in zip(boxes,confidences,labels):\n    box = box.astype(int)\n    if conf&gt;threshold:\n        x1, y1, x2, y2 = box[:4]\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 1) \n        cv2.putText(image, names[label]+str(conf)[:4], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n#Conversion de l'image en RGB\nimage=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n</pre> # Seuil de d\u00e9tection threshold=0.5 # Les noms des classes names={0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'} image=cv2.imread(\"images/coco.jpg\") boxes = results[0].boxes.xyxy.cpu().numpy() confidences=results[0].boxes.conf.cpu().numpy() labels=results[0].boxes.cls.cpu().numpy() # Affichage des r\u00e9sultats for box,conf,label in zip(boxes,confidences,labels):     box = box.astype(int)     if conf&gt;threshold:         x1, y1, x2, y2 = box[:4]         cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 1)          cv2.putText(image, names[label]+str(conf)[:4], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) #Conversion de l'image en RGB image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.imshow(image) plt.axis('off') plt.show() <p>En quelque ligne de code, on a pu utiliser le mod\u00e8le YOLOv8 !</p> <p>YOLO-World est une nouvelle version de YOLO qui permet de faire de la d\u00e9tection d'objets open-vocabulary. Les versions ant\u00e9rieures de YOLO sont entrain\u00e9es sur des classes sp\u00e9cifiques (en g\u00e9n\u00e9ral les 80 classes de COCO) et on a donc besoin des les r\u00e9-entra\u00eener ou finetuner pour les utiliser sur d'autres classes. YOLO-World est open-vocabulary ce qui signifie qu'il peut d\u00e9tecter des objets de n'importe quelle classe. Il suffit de donner un prompt de la classe que l'on souhaite en m\u00eame temps que l'image en entr\u00e9e du mod\u00e8le.</p> <p></p> <p>Pour l'utiliser, c'est assez similaire \u00e0 un autre mod\u00e8le YOLO mis \u00e0 part qu'il faut rajouter notre prompt textuel.</p> In\u00a0[3]: Copied! <pre># On charge le mod\u00e8le pre-entrain\u00e9 YOLOv8s-world\nmodel = YOLO(\"yolov8s-worldv2.pt\") \n\n# On d\u00e9finit les classes du mod\u00e8le\nmodel.set_classes([\"person\", \"surfboard\"])\nnames={0: 'person', 1: 'surfboard'}\n\n# On fait une pr\u00e9diction sur notre image\nresults = model.predict(\"images/coco2.jpg\")\n</pre> # On charge le mod\u00e8le pre-entrain\u00e9 YOLOv8s-world model = YOLO(\"yolov8s-worldv2.pt\")   # On d\u00e9finit les classes du mod\u00e8le model.set_classes([\"person\", \"surfboard\"]) names={0: 'person', 1: 'surfboard'}  # On fait une pr\u00e9diction sur notre image results = model.predict(\"images/coco2.jpg\") <pre>\nimage 1/1 /Users/simonthomine/Documents/CoursDeepLearning/08_WIP_DetectionEtYolo/images/coco2.jpg: 640x384 2 persons, 1 surfboard, 155.2ms\nSpeed: 2.6ms preprocess, 155.2ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 384)\n</pre> In\u00a0[4]: Copied! <pre># Seuil de d\u00e9tection\nthreshold=0.5\n# Les noms des classes\nimage=cv2.imread(\"images/coco2.jpg\")\nboxes = results[0].boxes.xyxy.cpu().numpy()\nconfidences=results[0].boxes.conf.cpu().numpy()\nlabels=results[0].boxes.cls.cpu().numpy()\n# Affichage des r\u00e9sultats\nfor box,conf,label in zip(boxes,confidences,labels):\n    box = box.astype(int)\n    if conf&gt;threshold:\n        x1, y1, x2, y2 = box[:4]\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 1) \n        cv2.putText(image, names[label]+str(conf)[:4], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\nimage=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n</pre> # Seuil de d\u00e9tection threshold=0.5 # Les noms des classes image=cv2.imread(\"images/coco2.jpg\") boxes = results[0].boxes.xyxy.cpu().numpy() confidences=results[0].boxes.conf.cpu().numpy() labels=results[0].boxes.cls.cpu().numpy() # Affichage des r\u00e9sultats for box,conf,label in zip(boxes,confidences,labels):     box = box.astype(int)     if conf&gt;threshold:         x1, y1, x2, y2 = box[:4]         cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 1)          cv2.putText(image, names[label]+str(conf)[:4], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.imshow(image) plt.axis('off') plt.show() <p>On d\u00e9tecte bien la personne et la planche de surf.</p> <p>SAM est un mod\u00e8le de segmentation qui permet de segmenter n'importe quel objet. Nous avons d\u00e9j\u00e0 utilis\u00e9 ce mod\u00e8le dans le cours 6 sur HuggingFace. Il est \u00e9galement possible d'utilise ce mod\u00e8le via la library ultralytics. A vous de voir ce que vous preferez.</p> <p>Voici comment l'utiliser avec ultralytics en python :</p> In\u00a0[5]: Copied! <pre>from PIL import Image\nraw_image = Image.open(\"images/coco2.jpg\")\nplt.imshow(raw_image)\nplt.axis('off')\nplt.show()\n</pre> from PIL import Image raw_image = Image.open(\"images/coco2.jpg\") plt.imshow(raw_image) plt.axis('off') plt.show()  <p>Nous utilisons FastSAM qui est une version plus rapide de SAM. Vous pouvez t\u00e9l\u00e9charger les poids sur ce lien.</p> In\u00a0[6]: Copied! <pre>from ultralytics import FastSAM\n\nresized_image=raw_image.resize((1024,1024))\nmodel = FastSAM('./FastSAM-s.pt')\n</pre> from ultralytics import FastSAM  resized_image=raw_image.resize((1024,1024)) model = FastSAM('./FastSAM-s.pt') In\u00a0[7]: Copied! <pre>results = model(resized_image,retina_masks=True)\n</pre> results = model(resized_image,retina_masks=True) <pre>\n0: 640x640 30 objects, 292.8ms\nSpeed: 4.2ms preprocess, 292.8ms inference, 199.2ms postprocess per image at shape (1, 3, 640, 640)\n</pre> In\u00a0[39]: Copied! <pre>import numpy as np\n\ndef draw_masks(image,masks):\n    image_np = np.array(image)\n    plt.cm.get_cmap('tab20', 38)\n    colors = plt.cm.get_cmap('tab20', 38)\n    for i, mask in enumerate(masks):\n        color = colors(i)[:3] \n        color = tuple(int(c * 255) for c in color) \n        mask =mask.data.squeeze().numpy()&gt;0.5\n        image_np[mask] = image_np[mask] * 0.5 + np.array(color) * 0.5       \n    return image_np\n\nimage_np = draw_masks(resized_image,results[0].masks)\nplt.imshow(image_np)\nplt.axis('off')\nplt.show()\n</pre> import numpy as np  def draw_masks(image,masks):     image_np = np.array(image)     plt.cm.get_cmap('tab20', 38)     colors = plt.cm.get_cmap('tab20', 38)     for i, mask in enumerate(masks):         color = colors(i)[:3]          color = tuple(int(c * 255) for c in color)          mask =mask.data.squeeze().numpy()&gt;0.5         image_np[mask] = image_np[mask] * 0.5 + np.array(color) * 0.5            return image_np  image_np = draw_masks(resized_image,results[0].masks) plt.imshow(image_np) plt.axis('off') plt.show() <p>Les r\u00e9sultats sont assez moyens mais il ne faut pas oublier qu'il s'agit du plus petit mod\u00e8le de FastSAM et qu'il est tr\u00e8s rapide (vous pouvez comparer au temps de traitement du notebook 2 du cours 6 sur Hugging Face).</p> <p>Notes : Il est \u00e9galement possible de faire des prompts qui sont des points, des box ou du texte (voir la documentation pour plus de d\u00e9tails).</p> <p>Vous savez maintenant comment utiliser des mod\u00e8les rapides de d\u00e9tection et de segmentation d'images avec la library ultralytics !</p>"},{"location":"08_DetectionEtYolo/03_Ultralytics.html#ultralytics","title":"Ultralytics\u00b6","text":""},{"location":"08_DetectionEtYolo/03_Ultralytics.html#yolo","title":"YOLO\u00b6","text":""},{"location":"08_DetectionEtYolo/03_Ultralytics.html#yolov8","title":"YOLOv8\u00b6","text":""},{"location":"08_DetectionEtYolo/03_Ultralytics.html#yolo-world","title":"YOLO-World\u00b6","text":""},{"location":"08_DetectionEtYolo/03_Ultralytics.html#segment-anything-sam","title":"Segment Anything (SAM)\u00b6","text":""},{"location":"09_EntrainementContrastif/index.html","title":"\ud83c\udfaf Entrainement contrastif \ud83c\udfaf","text":"<p>Ce cours pr\u00e9sente le concept de l'entra\u00eenement contrastif. Un premier notebook pr\u00e9sente ce qu'est l'entra\u00eenement contrastif en se basant sur l'impl\u00e9mentation d'un article de \"face verification\". Le second notebook pr\u00e9sente la place de l'entra\u00eenment contrastif dans le deep learning r\u00e9cent et notamment son int\u00earet pour l'entrainement non supervis\u00e9. </p>"},{"location":"09_EntrainementContrastif/index.html#notebook-1-face-verification","title":"Notebook 1\ufe0f\u20e3 : Face verification","text":"<p>Ce notebook pr\u00e9sente le concept d'entra\u00eenement contrastif en prenant l'exemple de la v\u00e9rification de visage.</p>"},{"location":"09_EntrainementContrastif/index.html#notebook-2-non-supervise","title":"Notebook 2\ufe0f\u20e3 : Non supervis\u00e9","text":"<p>Ce notebook pr\u00e9sente diverses approches non supervis\u00e9es utilisant l'entra\u00eenement contrastif.</p>"},{"location":"09_EntrainementContrastif/01_FaceVerification.html","title":"Face Verification","text":"<p>La t\u00e2che de face verification (ou v\u00e9rification faciale) consiste \u00e0 v\u00e9rifier l'identit\u00e9 d'une personne \u00e0 partir d'une photo \"exemple\". On peut imaginer une base de donn\u00e9es contenant des une photo de chaque employ\u00e9 d'une entreprise. Lorsque quelqu'un se pr\u00e9sente \u00e0 la porte du batiment avec son badge, on va v\u00e9rifier son identit\u00e9 en comparant la photo de la personne \u00e0 la porte \u00e0 la photo stock\u00e9e en base de donn\u00e9es.</p> <p>Comme vous devez l'imaginer, cette t\u00e2che pr\u00e9sente plusieurs contraintes :</p> <ul> <li>Tout d'abord, on n'a pas forc\u00e9ment acc\u00e8s \u00e0 des centaines de photos de chaque personne de l'entreprise pour l'entrainement.</li> <li>Ensuite, on voudrait \u00e9viter de r\u00e9-entra\u00eener le mod\u00e8le d\u00e8s qu'une nouvelle personne rejoint l'entreprise.</li> </ul> <p>Pour ce probl\u00e8me, nous allons suivre la m\u00e9thode introduite dans le papier Learning a Similarity Metric Discriminatively, with Application to Face Verification qui propose d'entra\u00eener une mesure de similarit\u00e9 \u00e0 partir des donn\u00e9es.</p> <p>Cette m\u00e9thode est tr\u00e8s performante pour des datasets qui contiennent un nombre tr\u00e8s importants de cat\u00e9gories (pour les visages, chaque personne est une cat\u00e9gorie) et/ou dans les cas o\u00f9 l'on ne dispose pas de toutes les cat\u00e9gories lors de l'entra\u00eenement du mod\u00e8le (nouvelle personne qui rejoint l'entreprise).</p> <p>Commen\u00e7ons par analyser quelques points th\u00e9oriques de l'article pour comprendre le fonctionnement de la m\u00e9thode puis nous passerons \u00e0 la partie impl\u00e9mentation.</p> <p>L'id\u00e9e principale de l'article est d'entra\u00eener le mod\u00e8le de mani\u00e8re contrastive sur des paires d'images. Pour les paires d'images, deux sc\u00e9narios sont possibles, soit les deux images proviennent de la m\u00eame cat\u00e9gorie, soit elles proviennent d'une cat\u00e9gorie diff\u00e9rente.</p> <p>Ce qu'on veut, c'est avoir un mod\u00e8le $G_w$ qui va mapper une image dans un espace latent o\u00f9 la distance euclidienne (ou cosinus) permettent de diff\u00e9rencier efficacement les \u00e9l\u00e9ments.</p> <p>Plus formellement, on vaut avoir : $D_W(X_1,X_2) = \\lVert G_W(X_1) - G_W(X_2)\\rVert $  petit si $X_1$ et $X_2$ sont dans la m\u00eame cat\u00e9gorie et grand si ils sont dans des cat\u00e9gories diff\u00e9rentes.</p> <p>Le mod\u00e8le est entra\u00een\u00e9 sur des paires d'images et les poids de $G_W$ sont bien entendu partag\u00e9s. L'architecture ressemble \u00e0 cela :</p> <p></p> <p>Figure extraite de l'article original (le $E_W$ de la figure correspond \u00e0 notre $D_W$)</p> <p>Ou (de mani\u00e8re plus simple) \u00e0 cela :</p> <p></p> <p>Mais alors, comment est ce que l'on peut d\u00e9finir notre fonction de loss pour cet objectif ? Prenons $Y$ le label de notre paire d'image. Si $Y$ vaut 0 alors les images proviennent de la m\u00eame cat\u00e9gorie, sinon $Y$ vaut 1. La fonction de loss contrastif s'exprime alors de la mani\u00e8re suivante : $L(Y,X_1,X_2)= (1-Y)\\frac{1}{2}(D_W)^2 + (Y)\\frac{1}{2}(max(0,m-D_W))\u00b2$</p> <p>La fonction de loss fait un peu peur vue comme \u00e7a mais elle est en fait assez simple. Regardons pour les deux valeurs possibles de $Y$ :</p> <ul> <li>Si $Y = 0$, alors  $L(Y,X_1,X_2)= \\frac{1}{2}(D_W)^2$ ce qui correspond simplement \u00e0 la distance $D_W$ que l'on a d\u00e9finit plus haut.</li> <li>Si $Y = 1$, alors $L(Y,X_1,X_2)= \\frac{1}{2}(max(0,m-D_W))\u00b2$. Cette partie demande un peu plus d'explication. On pourrait supposer que simplement utiliser $\\frac{1}{D_W}$ permettrait de maximiser la distance (ce que l'on veut). En pratique, cela ne fonctionne pas car le mod\u00e8le pourrait apprendre \u00e0 s\u00e9parer les \u00e9l\u00e9ments d'une marge tr\u00e8s petite ce qui pourrait poser des probl\u00e8mes de g\u00e9n\u00e9ralisation. Le param\u00e8tre $m$ (margin) permet de sp\u00e9cifier une marge minimale de s\u00e9paration entre les \u00e9l\u00e9ments diff\u00e9rents. Cela rend le mod\u00e8le plus robuste et stabilise l'entra\u00eenement.</li> </ul> <p>Note : Pourquoi ne pas simplement minimiser $D_W(X_1,X_2)$ sur des paires d'images de la m\u00eame cat\u00e9gorie uniquement ? Si on fait cela, le mod\u00e8le peut collapse et apprendre un solution triviale qui consiste \u00e0 faire de $G_W$ une fonction constante si bien que le loss sera toujours \u00e9gal \u00e0 0. A noter qu'en pratique, le mod\u00e8le va toujours aller au plus simple, il faut donc veiller \u00e0 bien d\u00e9finir ses fonctions de loss pour \u00e9viter que le mod\u00e8le trouve un raccourci trivial.</p> <p>$G_W$ peut \u00eatre n'importe quel r\u00e9seau de neurones. L'article propose d'utiliser un r\u00e9seau convolutifs pour ses capacit\u00e9s sur les images et sa robustesse aux distorsions g\u00e9om\u00e9triques (qui sont courantes pour la face verification).</p> <p>Nous n'allons pas reproduire strictement l'architecture de l'article mais nous allons plut\u00f4t concevoir notre propre architecture et l'entra\u00eener sur un dataset plus cons\u00e9quent que ceux de l'article.</p> <p>Pour l'entra\u00eenement, l'article pr\u00e9conise d'utiliser 50% de paires positives et 50% de paires n\u00e9gatives.</p> In\u00a0[15]: Copied! <pre>import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as datasets\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n</pre> import torch  import torch.nn as nn import torch.nn.functional as F import torchvision.datasets as datasets import torchvision.transforms as T from torch.utils.data import DataLoader import matplotlib.pyplot as plt device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') <p>Il est temps d'impl\u00e9menter notre mod\u00e8le !</p> <p>Comme dataset, nous utilisons LFW qui regroupe 13233 images de 5749 personnes avec 1680 ayant au moins 2 images (n\u00e9cessaire pour les exemples positifs). Nous utilisons la LFWPairs de torchvision qui regroupe d\u00e9j\u00e0 les images par paires positives ou n\u00e9gatives en proportion \u00e9gales.</p> In\u00a0[100]: Copied! <pre>transform = T.Compose([\n    T.ToTensor(),\n    T.Resize((128,128)), # On r\u00e9duit la taille des images pour acc\u00e9l\u00e9rer l'entra\u00eenement\n])\ntrain_data=datasets.LFWPairs(root='./../data',download=False,split= 'train',transform=transform)\ntest_data=datasets.LFWPairs(root='./../data',download=False,split= 'test',transform=transform)\n</pre> transform = T.Compose([     T.ToTensor(),     T.Resize((128,128)), # On r\u00e9duit la taille des images pour acc\u00e9l\u00e9rer l'entra\u00eenement ]) train_data=datasets.LFWPairs(root='./../data',download=False,split= 'train',transform=transform) test_data=datasets.LFWPairs(root='./../data',download=False,split= 'test',transform=transform) In\u00a0[101]: Copied! <pre>print('Nombre de paires pour le training: ',len(train_data))\nprint('Nombre de paires pour le test:',len(test_data))\nprint('Taille image',train_data[0][0].shape)\n\nfig, axs = plt.subplots(2, 2, figsize=(7, 7))\n\naxs[0, 0].imshow(train_data[0][0].permute(1, 2, 0))\naxs[0, 0].axis('off')\n\naxs[0, 1].imshow(train_data[0][1].permute(1, 2, 0))\naxs[0, 1].axis('off')\nfor data in train_data:\n  if data[2]==0:\n    axs[1, 0].imshow(data[0].permute(1,2,0))\n    axs[1, 0].axis('off')\n    axs[1, 1].imshow(data[1].permute(1,2,0))\n    axs[1, 1].axis('off')\n    break\naxs[0, 0].set_title(\"Paire positive\", fontsize=14, ha='center')\naxs[1, 0].set_title(\"Paire n\u00e9gative\", fontsize=14, ha='center');\n</pre> print('Nombre de paires pour le training: ',len(train_data)) print('Nombre de paires pour le test:',len(test_data)) print('Taille image',train_data[0][0].shape)  fig, axs = plt.subplots(2, 2, figsize=(7, 7))  axs[0, 0].imshow(train_data[0][0].permute(1, 2, 0)) axs[0, 0].axis('off')  axs[0, 1].imshow(train_data[0][1].permute(1, 2, 0)) axs[0, 1].axis('off') for data in train_data:   if data[2]==0:     axs[1, 0].imshow(data[0].permute(1,2,0))     axs[1, 0].axis('off')     axs[1, 1].imshow(data[1].permute(1,2,0))     axs[1, 1].axis('off')     break axs[0, 0].set_title(\"Paire positive\", fontsize=14, ha='center') axs[1, 0].set_title(\"Paire n\u00e9gative\", fontsize=14, ha='center');  <pre>Nombre de paires pour le training:  2200\nNombre de paires pour le test: 1000\nTaille image torch.Size([3, 128, 128])\n</pre> <p>On remarque que dans le dataset, $Y=1$ correspond \u00e0 une paire positive tandis que $Y=0$ correspond \u00e0 une paire n\u00e9gative. C'est l'inverse des notations de l'article. On va inverser \u00e7a pour plus de clart\u00e9.</p> In\u00a0[102]: Copied! <pre># On ne peut pas modifier les donn\u00e9es directement, on doit les transformer en listes\n# Pour les donn\u00e9es de training\ntrain_data_list = [list(data) for data in train_data]\nfor data in train_data_list:\n    data[2] = 1 - data[2]\ntrain_data = [tuple(data) for data in train_data_list]\n\n# Pour les donn\u00e9es de test\ntest_data_list = [list(data) for data in test_data]\nfor data in test_data_list:\n    data[2] = 1 - data[2]\ntest_data = [tuple(data) for data in test_data_list]\n</pre> # On ne peut pas modifier les donn\u00e9es directement, on doit les transformer en listes # Pour les donn\u00e9es de training train_data_list = [list(data) for data in train_data] for data in train_data_list:     data[2] = 1 - data[2] train_data = [tuple(data) for data in train_data_list]  # Pour les donn\u00e9es de test test_data_list = [list(data) for data in test_data] for data in test_data_list:     data[2] = 1 - data[2] test_data = [tuple(data) for data in test_data_list] <p>On va maintenant pouvoir cr\u00e9er nos dataloaders :</p> In\u00a0[103]: Copied! <pre>train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n</pre> train_loader = DataLoader(train_data, batch_size=16, shuffle=True) test_loader = DataLoader(test_data, batch_size=16, shuffle=False) <p>Il est temps de d\u00e9finir notre fonction de loss. Pour cela, on va simplement reprendre la formule que l'on a introduite plus haut. Pour $D_W$ on prend la distance euclidienne : $D_W (X_1,X_2) = \\lVert X_1 - X_2 \\rVert_2 = \\sqrt{\\sum_{i=1}^{n} (X_{1,i} - X_{2,i})^2}$</p> In\u00a0[104]: Copied! <pre>class ContrastiveLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        # Distance euclidienne entre les deux sorties du r\u00e9seau\n        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n        \n        # Calcul du loss contrastif\n        loss_contrastive = torch.mean(\n            (1 - label) * torch.pow(euclidean_distance, 2) +\n            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n        )\n        \n        return loss_contrastive\n</pre> class ContrastiveLoss(nn.Module):     def __init__(self, margin=1.0):         super(ContrastiveLoss, self).__init__()         self.margin = margin      def forward(self, output1, output2, label):         # Distance euclidienne entre les deux sorties du r\u00e9seau         euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)                  # Calcul du loss contrastif         loss_contrastive = torch.mean(             (1 - label) * torch.pow(euclidean_distance, 2) +             (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)         )                  return loss_contrastive <p>Pour cette t\u00e2che nous allons construire un r\u00e9seau convolutif classique et assez petit. Le but du mod\u00e8le est de projetter les donn\u00e9es dans un espace o\u00f9 la distance euclidienne est une distance pertinente.</p> In\u00a0[142]: Copied! <pre>class SiameseNetwork(torch.nn.Module):\n\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n\n        # Extraction des features pertinentes\n        self.conv_net = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=7, stride=2, padding=3),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n        )\n        \n        # Projection des features dans un espace de dimension r\u00e9duite\n        self.linear=nn.Sequential(\n            nn.Linear(128 * 4 * 4, 256),\n            nn.ReLU(),\n            nn.Linear(256, 32), \n            )\n    \n    def forward(self, x1, x2):\n        x1 = self.conv_net(x1)\n        x2 = self.conv_net(x2)\n        x1 = x1.view(x1.size(0), -1)\n        x2 = x2.view(x2.size(0), -1)\n        x1=self.linear(x1)\n        x2=self.linear(x2)\n        return x1,x2\n</pre> class SiameseNetwork(torch.nn.Module):      def __init__(self):         super(SiameseNetwork, self).__init__()          # Extraction des features pertinentes         self.conv_net = nn.Sequential(             nn.Conv2d(3, 16, kernel_size=7, stride=2, padding=3),             nn.ReLU(),             nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),             nn.ReLU(),             nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),             nn.ReLU(),             nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),             nn.ReLU(),             nn.MaxPool2d(2, 2),         )                  # Projection des features dans un espace de dimension r\u00e9duite         self.linear=nn.Sequential(             nn.Linear(128 * 4 * 4, 256),             nn.ReLU(),             nn.Linear(256, 32),              )          def forward(self, x1, x2):         x1 = self.conv_net(x1)         x2 = self.conv_net(x2)         x1 = x1.view(x1.size(0), -1)         x2 = x2.view(x2.size(0), -1)         x1=self.linear(x1)         x2=self.linear(x2)         return x1,x2 <p>D\u00e9finissons nos hyperparam\u00e8tres d'entrainement :</p> In\u00a0[134]: Copied! <pre>epochs = 30\nlr = 0.001\nmargin = 1.0\nmodel = SiameseNetwork().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\ncriterion=ContrastiveLoss(margin=margin)\n</pre> epochs = 30 lr = 0.001 margin = 1.0 model = SiameseNetwork().to(device) optimizer = torch.optim.SGD(model.parameters(), lr=lr) criterion=ContrastiveLoss(margin=margin) In\u00a0[135]: Copied! <pre>for epoch in range(epochs):\n  loss_train = 0\n  for x1, x2, label in train_loader:\n    x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n    optimizer.zero_grad()\n    out1, out2 = model(x1, x2)\n    loss = criterion(out1, out2, label)\n    loss_train+=loss.item()\n    loss.backward()\n    optimizer.step()\n  print(f\"Epoch {epoch+1}/{epochs} Loss: {loss_train/len(train_loader)}\")\n</pre> for epoch in range(epochs):   loss_train = 0   for x1, x2, label in train_loader:     x1, x2, label = x1.to(device), x2.to(device), label.to(device)     optimizer.zero_grad()     out1, out2 = model(x1, x2)     loss = criterion(out1, out2, label)     loss_train+=loss.item()     loss.backward()     optimizer.step()   print(f\"Epoch {epoch+1}/{epochs} Loss: {loss_train/len(train_loader)}\") <pre>Epoch 1/30 Loss: 0.4855058027998261\nEpoch 2/30 Loss: 0.47506297098985617\nEpoch 3/30 Loss: 0.44379603841166565\nEpoch 4/30 Loss: 0.36472982092612033\nEpoch 5/30 Loss: 0.31656959955243097\nEpoch 6/30 Loss: 0.3091158025722573\nEpoch 7/30 Loss: 0.2988005231903947\nEpoch 8/30 Loss: 0.2873344306928524\nEpoch 9/30 Loss: 0.28030256138763565\nEpoch 10/30 Loss: 0.27428370077108993\nEpoch 11/30 Loss: 0.27216234900381253\nEpoch 12/30 Loss: 0.2714559338662935\nEpoch 13/30 Loss: 0.2713507852260617\nEpoch 14/30 Loss: 0.27069854628348694\nEpoch 15/30 Loss: 0.26960040553324466\nEpoch 16/30 Loss: 0.26909896256267163\nEpoch 17/30 Loss: 0.2681442526155624\nEpoch 18/30 Loss: 0.26780268636302673\nEpoch 19/30 Loss: 0.2663887916267782\nEpoch 20/30 Loss: 0.26515361407528754\nEpoch 21/30 Loss: 0.26577359880658163\nEpoch 22/30 Loss: 0.266544328543587\nEpoch 23/30 Loss: 0.2644324919235879\nEpoch 24/30 Loss: 0.26508791971465817\nEpoch 25/30 Loss: 0.26529018699690915\nEpoch 26/30 Loss: 0.26407808313767117\nEpoch 27/30 Loss: 0.26463793779628864\nEpoch 28/30 Loss: 0.26539979831895966\nEpoch 29/30 Loss: 0.264066638721936\nEpoch 30/30 Loss: 0.2643573438559753\n</pre> <p>Maintenant que l'entra\u00eenement est termin\u00e9, il faut \u00e9valuer le mod\u00e8le. Mais alors comment faire ? Ce n'est pas le type de mod\u00e8le que nous avons l'habitude d'\u00e9valuer.</p> <p>On a notre dataset de test et on peut calculer les distances entre chaque paire de ce dataset. Donc dans l'id\u00e9e, on peut avoir un score de similarit\u00e9 pour chaque \u00e9l\u00e9ment du dataset de test.</p> <p>La courbe ROC : La courbe ROC (Receiver Operating Characteristic) est un graphique qui illustre les performances d'un mod\u00e8le de classification binaire (pour nous, positif ou n\u00e9gatif) \u00e0 diff\u00e9rents seuils de d\u00e9cision. La courbe ROC contient les \u00e9l\u00e9ments suivants :</p> <ul> <li>Axe des X est de le taux de faux positifs (ou taux de fausse alarme). C'est le nombre d'\u00e9l\u00e9ments n\u00e9gatifs incorrectement class\u00e9s comme positifs, divis\u00e9 par le nombre total d'\u00e9l\u00e9ments n\u00e9gatifs.</li> <li>Axe des Y est le taux de vrais positifs (rappel ou sensibilit\u00e9). C'est le nombre d'\u00e9l\u00e9ments positifs correctement class\u00e9s comme positifs, divis\u00e9 par le nombre total d'\u00e9l\u00e9ments positifs.</li> </ul> <p>Chaque point de la courbe va repr\u00e9senter un seuil de d\u00e9cision diff\u00e9rent pour la classification des \u00e9l\u00e9ments.</p> <p></p> <p>Figure extraite de blogpost.</p> <p>Pour juger la qualit\u00e9 d'un mod\u00e8le, on peut calculer l'aire sous la courbe ROC (AUROC). Dans le cas du random classifier on aura un AUROC de 0.5 alors que si il s'agit du classifieur parfait, l'AUROC sera de 1.</p> <p>On peut le calculer pour notre mod\u00e8le :</p> In\u00a0[139]: Copied! <pre>from sklearn.metrics import roc_curve, auc\n\nmodel.eval()\nlabels = []\ndistances = []\nwith torch.no_grad():\n    for x1, x2, label in test_loader:\n        x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n        out1, out2 = model(x1, x2)\n        dist = torch.nn.functional.pairwise_distance(out1, out2)\n        distances.extend(dist.cpu().numpy())\n        labels.extend(label.cpu().numpy())\n        \nfpr, tpr, thresholds = roc_curve(labels, distances)\nroc_auc = auc(fpr, tpr)\nprint(f\"ROC AUC: {roc_auc}\")\n</pre> from sklearn.metrics import roc_curve, auc  model.eval() labels = [] distances = [] with torch.no_grad():     for x1, x2, label in test_loader:         x1, x2, label = x1.to(device), x2.to(device), label.to(device)         out1, out2 = model(x1, x2)         dist = torch.nn.functional.pairwise_distance(out1, out2)         distances.extend(dist.cpu().numpy())         labels.extend(label.cpu().numpy())          fpr, tpr, thresholds = roc_curve(labels, distances) roc_auc = auc(fpr, tpr) print(f\"ROC AUC: {roc_auc}\")  <pre>ROC AUC: 0.6163399999999999\n</pre> <p>On obtient un AUROC de 0.62 environ ce qui n'est vraiment pas terrible. On peut aussi tracer la courbe pour voir de quoi \u00e7a a l'air.</p> In\u00a0[141]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Taux de faux positifs')\nplt.ylabel('Taux de vrais positifs')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.figure() plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc) plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('Taux de faux positifs') plt.ylabel('Taux de vrais positifs') plt.title('Receiver Operating Characteristic') plt.legend(loc=\"lower right\") plt.show() <p>On constate que le mod\u00e8le n'est pas tr\u00e8s performant. Le dataset est sans doute trop complexe par rapport \u00e0 la taille de notre mod\u00e8le. Vous pouvez essayer d'am\u00e9liorer les performances du mod\u00e8les en jouant sur l'architecture et les param\u00e8tres d'entrainement.</p> <p>Dans le domaine du traitement des visages, on a deux cat\u00e9gories principales : face verification et face recognition. Pour comprendre la nuance entre les deux, reprenons le cas de notre employ\u00e9 d'entreprise :</p> <ul> <li>Face verification : Dans le cas de la face verification, un employ\u00e9 va vouloir entrer dans le batiment avec son badge. Il passe son badge dans la machine et le mod\u00e8le de face verification va v\u00e9rifier qu'il s'agit bien de l'employ\u00e9 poss\u00e9dant ce badge.</li> <li>Face recognition : Ici, il faut plut\u00f4t imaginer que l'employ\u00e9 se rend au batiment sans son badge et que le mod\u00e8le est capable de le reconna\u00eetre parmi la base de donn\u00e9es de tous les employ\u00e9s pour le laisser entrer dans la batiment.</li> </ul> <p>Comme vous pouvez l'imaginer, la seconde t\u00e2che est plus complexe que ce soit en terme de difficult\u00e9 pure ou m\u00eame en terme de contraintes de temps de traitement (on ne va pas faire attendre l'employ\u00e9 \u00e0 la porte pendant 1 heure le temps de comparer sa photo aux centaines de photos de notre base de donn\u00e9es).</p> <p>Pour la t\u00e2che de face recognition, un article FaceNet: A Unified Embedding for Face Recognition and Clustering a introduit un nouveau type de loss qui ressemble au loss contrastif. Il s'agit du Triplet Loss que nous allons d\u00e9crire bri\u00e8vement.</p> <p>L'objectif de ce loss est similaire \u00e0 celui du loss contrastif : il vise \u00e0 apprendre une repr\u00e9sentation de sorte que les vecteurs de caract\u00e9ristiques (ou embeddings) de points similaires soient rapproch\u00e9s dans l'espace latent, tandis que ceux de points diff\u00e9rents soient \u00e9loign\u00e9s les uns des autres.</p> <p>Contrairement au loss contrastif qui se base sur des paires d'exemples. Le triplet loss se base sur des ** roulements de tambours ** triplets d'exemples !</p> <p>Chaque exemple a son r\u00f4le particulier :</p> <ul> <li>Anchor (A) : L'exemple de r\u00e9f\u00e9rence dont on cherche \u00e0 apprendre une repr\u00e9sentation</li> <li>Positive(P) : Un exemple similaire \u00e0 l'anchor (m\u00eame personne par exemple)</li> <li>Negative(N) : Un exemple diff\u00e9rent de l'anchor (une personne diff\u00e9rente)</li> </ul> <p>La fonction du loss est d\u00e9finie comme : $L_{\\text{triplet}} = \\max\\left( d(A, P) - d(A, N) + \\alpha, 0 \\right)$ o\u00f9 $d$ est la distance et $\\alpha$ la marge (margin).</p> <p>Ce loss minimise la distance entre $P$ et $A$ tout en maximisant celle entre $N$ et $A$.</p> <p></p> <p>Figure extraite de l'article original.</p> <p>Cela peut sembler redondant avec le loss contrastif et c'est un peu le cas. Cependant, le triplet loss a demontr\u00e9 des meilleurs r\u00e9sultats lorsqu'il s'agit de faire une distinction fine entre des classes assez similaire (typiquement le cas de la reconnaissance faciale). Aujourd'hui, son utilisation est pr\u00e9f\u00e9r\u00e9e \u00e0 celle du loss contrastif pour les t\u00e2ches de traitement des visages.</p>"},{"location":"09_EntrainementContrastif/01_FaceVerification.html#face-verification","title":"Face Verification\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#analyse-theorique-de-larticle","title":"Analyse th\u00e9orique de l'article\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#intuition","title":"Intuition\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#loss-contrastif","title":"Loss contrastif\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#architecture-du-modele-et-entrainement","title":"Architecture du mod\u00e8le et entra\u00eenement\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#dataset","title":"Dataset\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#fonction-de-loss-contrastif","title":"Fonction de loss contrastif\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#creation-de-notre-modele","title":"Cr\u00e9ation de notre mod\u00e8le\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#entrainement-du-modele","title":"Entrainement du mod\u00e8le\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#evaluation-du-modele","title":"Evaluation du mod\u00e8le\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#triplet-loss","title":"Triplet loss\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#face-recognition","title":"Face recognition\u00b6","text":""},{"location":"09_EntrainementContrastif/01_FaceVerification.html#facenet","title":"FaceNet\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html","title":"Entrainement contrastif non supervis\u00e9","text":"<p>Aujourd'hui, une grande partie de la recherche en deep learning est tourn\u00e9e du c\u00f4t\u00e9 de l'entrainement non supervis\u00e9. Pour rappel (cf cours 4 sur les autoencodeurs), l'entra\u00eenement non supervis\u00e9 consiste \u00e0 entra\u00eener un mod\u00e8le sans donn\u00e9es labelis\u00e9es. L'avantage principal de ce type d'entrainement est que \u00e7a limite \u00e9normement le co\u00fbt et la main d'oeuvre n\u00e9cessaires \u00e0 la pr\u00e9paration des donn\u00e9es. C'est ce type d'entra\u00eenement qui a propuls\u00e9 les NLP sur le devant de la sc\u00e8ne et qui permet une g\u00e9n\u00e9ration d'images impressionnantes avec Dall-E ou de vid\u00e9os avec SORA.</p> <p>Nous avons vu dans le cours 5 sur les NLP la fa\u00e7on de faire de l'entra\u00eenement non supervis\u00e9 sur du texte (il suffit de r\u00e9cuperer n'importe quel texte et d'entrainer le mod\u00e8le \u00e0 pr\u00e9dire le prochain caract\u00e8re). Pour ce qui est du traitement d'images, nous n'avons pas vraiment vu comment faire si ce n'est une l\u00e9g\u00e8re r\u00e9f\u00e9rence \u00e0 CLIP dans le cours 7 sur les transformers.</p> <p>Nous avons d\u00e9j\u00e0 vu le mod\u00e8le CLIP dans le cours 7 qui est entrain\u00e9 de mani\u00e8re contrastive au sein du batch. C'est une approche un peu diff\u00e9rente de ce que l'on a vu dans le notebook pr\u00e9c\u00e9dent, un vision transformer prend en entr\u00e9e les images du batch et un text transformer prend en entr\u00e9e les descriptions de chaque image. Le mod\u00e8le est alors entra\u00een\u00e9 \u00e0 associer correctement les images et les descriptions de mani\u00e8re contrastive (minimisation de la distance entre les embeddings de la m\u00eame paire et maximisation pour les embeddings de paires diff\u00e9rentes).</p> <p></p> <p>CLIP est un mod\u00e8le utilisant le loss contrastif mais il n'est pas r\u00e9ellement non supervis\u00e9. Il se base sur des paires d'images/textes qui constituent les labels d'entra\u00eenement.</p> <p>Quand on parle de non supervis\u00e9, nous voulons qu'aucune donn\u00e9e \u00e9tiquet\u00e9e ne soit requise. La m\u00e9thode SimCLR introduit une technique d'utilisation de l'apprentissage contrastif dans un cas non supervis\u00e9. L'id\u00e9e est \u00e0 nouveau de tra\u00eeter un batch de donn\u00e9es o\u00f9 chaque \u00e9l\u00e9ment du batch est une paire d'image. Mais la particularit\u00e9 de cette paire d'image c'est qu'il s'agit de la m\u00eame image sur laquelle on a appliqu\u00e9 une transformation (voir cours bonus sur data augmentation). Chaque image est pass\u00e9e dans un r\u00e9seau identique (siamois) et le mod\u00e8le va alors \u00eatre entrain\u00e9 \u00e0 minimiser la distance entre les representations des images provenant de la m\u00eame paire et de maximiser la distance entre les images provenant des paires diff\u00e9rentes.</p> <p>Voici l'architecture du mod\u00e8le :</p> <p></p> <p>Voici des exemples de transformations appliqu\u00e9es aux images :</p> <p></p> <p>Les deux figures ci-dessus sont extraites de l'article original</p> <p>Dans cette m\u00e9thode, l'aspect data augmentation est crucial et il est important de ne pas n\u00e9gliger le panel de transformations possibles.</p> <p>Pour faire l'analogie avec les positive pairs et negative pairs :</p> <ul> <li>Positive pairs : Les deux images transform\u00e9es $x_i$ et $x_j$ provenant de la m\u00eame image $x$.</li> <li>Negative pairs : Deux images transform\u00e9es $x_i$ et $x'_j$ provenant d'images diff\u00e9rentes $x$ et $x'$.</li> </ul> <p>Gr\u00e2ce \u00e0 cette m\u00e9thode, le mod\u00e8le est capable d'apprendre des representations pertinentes des images sans avoir besoin de labels. Le mod\u00e8le va pouvoir distinguer les images representant des objets diff\u00e9rents sans pour autant savoir ce que ces objets representent.</p> <p>On peut se demander l'int\u00earet d'entra\u00eener un tel mod\u00e8le. A quoi ce mod\u00e8le va bien pouvoir servir une fois entra\u00een\u00e9 ?</p> <p>Pour cela, on peut faire une analogie aux mod\u00e8les de langages. Ces mod\u00e8les sont d'abord pr\u00e9-entrain\u00e9 sur une bonne partie d'internet et ensuite on les fine-tune sur une t\u00e2che pr\u00e9cise (chatbot pour chatGPT par exemple). Pour les images c'est \u00e0 peu pr\u00e8s pareil, les mod\u00e8les entrain\u00e9s de mani\u00e8re contrastive sur des milliards d'images peuvent servir de mod\u00e8les g\u00e9n\u00e9riques que l'on pourra ensuite fine-tune sur des t\u00e2ches plus sp\u00e9cifiques comme la classification.</p> <p>Note : Le fine-tuning et le transfer learning sont abord\u00e9s en d\u00e9tail dans le cours suivant. En gros, ce sont des techniques pour r\u00e9-utiliser un mod\u00e8le d\u00e9j\u00e0 entra\u00een\u00e9 sur des t\u00e2ches diff\u00e9rentes.</p> <p>Il est important de mentionner que l'entra\u00eenement contrastif n'est pas la seule mani\u00e8re de faire de l'entra\u00eenement non supervis\u00e9 sur des images.</p> <p>Nous avons d\u00e9j\u00e0 pr\u00e9sent\u00e9 les autoencodeurs dans le cours 4 qui peuvent permettre d'apprendre des r\u00e9presentations d'images pertinentes. L'article Masked Autoencoders Are Scalable Vision Learners d\u00e9montre que les masked autoencodeurs peuvent servir \u00e0 apprendre des representations d'images tr\u00e8s utiles.</p> <p></p> <p>Dans ce cours, nous n'avons pas encore parl\u00e9 des GAN. Rapidement, il s'agit de r\u00e9seaux s'entrainant de mani\u00e8re adversaire, un g\u00e9n\u00e9rateur va g\u00e9n\u00e9rer de fausses images et un discriminateur va devoir faire la diff\u00e9rence entre une image r\u00e9elle et une image construite par le g\u00e9n\u00e9rateur. En s'entra\u00eenant ensemble, on peut avoir un g\u00e9n\u00e9rateur capable de g\u00e9n\u00e9rer des images tr\u00e8s r\u00e9alistes sans avoir eu besoin d'un dataset lab\u00e9lis\u00e9. Il y a quelques ann\u00e9es, c'\u00e9tait la m\u00e9thode la plus utilis\u00e9es pour la g\u00e9n\u00e9ration d'images (depuis, on utilise plut\u00f4t les mod\u00e8les de diffusion qui sont d'ailleurs non supervis\u00e9 \u00e9galement).</p> <p></p> <p>Une autre approche consiste \u00e0 pr\u00e9dire une transformation que l'on a appliqu\u00e9 \u00e0 l'image. On peut par exemple faire une rotation de l'image (RotNet) et entra\u00eener le mod\u00e8le \u00e0 la pr\u00e9dire ou bien m\u00e9langer notre image \u00e0 la mani\u00e8re d'un puzzle et entra\u00eener notre mod\u00e8le \u00e0 reconstruire l'image (JigSaw).</p> <p></p> <p>Plus r\u00e9cemment, des m\u00e9thodes bas\u00e9es sur la distillation des connaissances utilisent \u00e0 nouveau des transformations d'images comme dans l'entra\u00eenement contrastif mais n'utilisent pas de paires n\u00e9gatives. Pour \u00e9viter que le mod\u00e8le ne collapse, diverses techniques sont utilis\u00e9es. Pour en savoir plus, vous pouvez lire l'article DINO.</p> <p>Note : Le concept de distillation des connaissances sera abord\u00e9 dans le cours suivant.</p> <p>Voici un aper\u00e7u des representations apprises par le mod\u00e8le DINO :</p> <p></p> <p>Figure extraite de ce blogpost.</p> <p>Note : La liste des m\u00e9thodes non supervis\u00e9es n'est pas exhaustive mais vous avez maintenant une bonne id\u00e9e des m\u00e9thodes existantes. Aussi, les GAN et mod\u00e8les de diffusion sont non supervis\u00e9s mais ne sont pas utilis\u00e9s pour cr\u00e9er des mod\u00e8les de base pouvant \u00eatre fine-tune sur des t\u00e2ches plus sp\u00e9cifiques.</p>"},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#entrainement-contrastif-non-supervise","title":"Entrainement contrastif non supervis\u00e9\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#comment-adapter-lentrainement-contrastif-au-non-supervise","title":"Comment adapter l'entra\u00eenement contrastif au non supervis\u00e9 ?\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#modele-clip","title":"Mod\u00e8le CLIP\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#entrainement-non-supervise-pour-les-images","title":"Entra\u00eenement non supervis\u00e9 pour les images\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#avantages-de-cette-approche","title":"Avantages de cette approche\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#alternative-a-lentrainement-contrastif-pour-le-non-supervise","title":"Alternative \u00e0 l'entra\u00eenement contrastif pour le non supervis\u00e9\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#autoencodeurs","title":"Autoencodeurs\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#generative-adversarial-network-gan","title":"Generative adversarial network (GAN)\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#prediction-de-transformation","title":"Pr\u00e9diction de transformation\u00b6","text":""},{"location":"09_EntrainementContrastif/02_NonSupervis%C3%A9.html#distillation-auto-supervisee","title":"Distillation auto-supervis\u00e9e\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/index.html","title":"\ud83e\udd1d Transfer learning et distillation \ud83e\udd1d","text":"<p>Ce cours pr\u00e9sente deux concepts majeurs en deep learning : le transfer learning et la distillation des connaissances. La premi\u00e8re partie de ce cours pr\u00e9sente le transfer learning dans sa globalit\u00e9 puis propose une impl\u00e9mentation pratique. La seconde partie pr\u00e9sente le concept de distillation des connaissances et ses variantes puis propose un cas d'application de la distillation des connaissances pour la d\u00e9tection d'anomalies non supervis\u00e9e.</p>"},{"location":"10_TransferLearningEtDistillation/index.html#notebook-1-transfer-learning","title":"Notebook 1\ufe0f\u20e3 : Transfer learning","text":"<p>Ce notebook introduit le concept de transfer learning et les utilisations possibles du transfer learning.</p>"},{"location":"10_TransferLearningEtDistillation/index.html#notebook-2-transfer-learning-pytorch","title":"Notebook 2\ufe0f\u20e3 : Transfer learning pytorch","text":"<p>Ce notebook pr\u00e9sente un exemple de transfer learning pour la classification \u00e0 partir d'un dataset contenant un nombre limit\u00e9 d'images.</p>"},{"location":"10_TransferLearningEtDistillation/index.html#notebook-3-distillation","title":"Notebook 3\ufe0f\u20e3 : Distillation","text":"<p>Ce notebook introduit le concept de distillation des conaissances en deep learning ainsi que les utilisations possibles de cette approche.</p>"},{"location":"10_TransferLearningEtDistillation/index.html#notebook-4-distillationanomalie","title":"Notebook 4\ufe0f\u20e3 : DistillationAnomalie","text":"<p>Ce notebook montre un exemple de distillation des connaissances pour la d\u00e9tection d'anomalies non supervis\u00e9e.</p>"},{"location":"10_TransferLearningEtDistillation/index.html#notebook-5-fine-tuning-llm","title":"Notebook 5\ufe0f\u20e3 : Fine Tuning LLM","text":"<p>Ce notebook pr\u00e9sente en d\u00e9tail l'architecture de BERT, un mod\u00e8le con\u00e7u pour \u00eatre finetune sur des t\u00e2ches annexes.</p>"},{"location":"10_TransferLearningEtDistillation/index.html#notebook-6-fine-tuning-bert-hf","title":"Notebook 6\ufe0f\u20e3 : Fine Tuning Bert HF","text":"<p>Ce notebook propose des exemples de finetuning du mod\u00e8le BERT avec Hugging Face pour des t\u00e2ches de token-level prediction et sentence-level prediction.</p>"},{"location":"10_TransferLearningEtDistillation/01_TransferLearning.html","title":"Transfer Learning","text":"<p>Le transfer learning (ou apprentissage par transfert) est une technique bien connue des chercheurs et ing\u00e9nieurs en deep learning. Le principe de base consiste \u00e0 r\u00e9-utiliser les poids d'un r\u00e9seau d\u00e9j\u00e0 entra\u00een\u00e9 comme base pour l'entra\u00eenement de notre r\u00e9seau.</p> <p>En pratique, cela apporte plusieurs avantages :</p> <ul> <li>Si les deux t\u00e2ches sont assez \"similaires\", l'entra\u00eenement du nouveau mod\u00e8le sera plus rapide.</li> <li>Les performances du nouveau mod\u00e8le seront meilleures que si on l'avait entra\u00een\u00e9 \u00e0 partir de z\u00e9ro.</li> <li>On a besoin de moins de donn\u00e9es que pour entra\u00eener un mod\u00e8le \u00e0 partir de z\u00e9ro.</li> </ul> <p></p> <p>Figure extraite de ce blogpost.</p> <p>Il y a souvent une confusion entre ces deux termes et c'est normal car ils sont tr\u00e8s proches. En pratique, le fine-tuning est une forme de transfer learning qui consiste \u00e0 ne r\u00e9-entra\u00eener qu'une partie des couches du mod\u00e8les que l'on r\u00e9utilise.</p> <p>Pour d\u00e9finir les termes clairement :</p> <ul> <li>Transfer Learning : Entra\u00eener un mod\u00e8le en prenant comme base des poids d'un mod\u00e8le d\u00e9j\u00e0 entra\u00een\u00e9 sur une autre t\u00e2che (on peut r\u00e9entrainer l'ensemble du mod\u00e8le ou certaines couches).</li> <li>Fine-Tuning : R\u00e9-entra\u00eener certaines couches (les derni\u00e8res en g\u00e9n\u00e9ral) d'un mod\u00e8le d\u00e9j\u00e0 entra\u00een\u00e9 sur une autre t\u00e2che pour le rendre plus sp\u00e9cifique \u00e0 notre t\u00e2che actuelle.</li> </ul> <p>Le fine-tuning consiste \u00e0 r\u00e9-entra\u00eener certaines couches d'un mod\u00e8le d\u00e9j\u00e0 entra\u00een\u00e9 pour le rendre sp\u00e9cifique \u00e0 notre t\u00e2che. Il faut donc choisir le nombre de couches que l'on va vouloir r\u00e9-entra\u00eener.</p> <p>Comment choisir ce nombre de couches ? Il y a quelques consid\u00e9rations \u00e0 prendre en compte pour ce choix mais il n'y a pas de formule fixe. En g\u00e9n\u00e9ral, on se base sur notre intuition et sur les r\u00e8gles suivantes pour choisir le nombre de couches \u00e0 r\u00e9-entra\u00eener :</p> <ul> <li>Moins on a de donn\u00e9es d'entra\u00eenement pour notre nouvelle t\u00e2che, moins on va r\u00e9-entra\u00eener de couches (si on a vraiment peu de donn\u00e9es, on r\u00e9-entraine juste la derni\u00e8re couche alors que si on a beaucoup de donn\u00e9es on peut r\u00e9-entrainer presque toutes les couches).</li> <li>Plus les t\u00e2ches sont similaires, moins on va r\u00e9-entrainer de couches (Si on d\u00e9tecte les chats, chiens et lapins et qu'on veut d\u00e9tecter les hamsters en plus, les t\u00e2ches sont tr\u00e8s similaires. A l'inverse, si on souhaite d\u00e9tecter les maladies sur une image de radio \u00e0 partir du mod\u00e8le chat/chien/lapin, les t\u00e2ches sont tr\u00e8s diff\u00e9rentes).</li> </ul> <p>De mani\u00e8re g\u00e9n\u00e9rale, utiliser un mod\u00e8le pr\u00e9-entrain\u00e9 comme base pour notre entra\u00eenement est toujours b\u00e9n\u00e9fique (sauf peut-\u00eatre dans des cas o\u00f9 les domaines n'ont absolument rien \u00e0 voir). Je vous conseillerais donc de l'utiliser d\u00e8s que vous en avez l'occasion. Cependant, cela impose quelques contraintes :</p> <ul> <li>L'architecture du mod\u00e8le ne peut plus \u00eatre modifi\u00e9e comme on le souhaite (les couches que l'on ne r\u00e9-entraine pas).</li> <li>Il faut disposer des poids d'un mod\u00e8le d\u00e9j\u00e0 entrain\u00e9 (en pratique, on en trouve beaucoup sur le net, cf cours 6 sur HuggingFace).</li> </ul> <p>Note : Pour les probl\u00e8mes de classification d'images, on va souvent utiliser un mod\u00e8le pr\u00e9-entrain\u00e9 sur ImageNet car les 1000 classes de ce dataset permettent de rendre le mod\u00e8le assez g\u00e9n\u00e9raliste dans les features qu'il apprend.</p> <p>Lorsqu'on fine-tune un mod\u00e8le, on peut avoir deux objectifs en t\u00eate :</p> <ul> <li>Cas 1 : Entrainer un mod\u00e8le sur une t\u00e2che compl\u00e8tement diff\u00e9rente de celle pour laquelle il a \u00e9t\u00e9 pr\u00e9-entrain\u00e9 (exemple : on veut classifier les dinosaures de Jurassic park alors que le mod\u00e8le est entra\u00een\u00e9 sur des mammif\u00e8res). Dans ce cas, le mod\u00e8le peut \"oublier\" sa t\u00e2che d'origine sans que cela nous pose probl\u00e8me.</li> <li>Cas 2 : Entrainer un mod\u00e8le sur une t\u00e2che compl\u00e9mentaire \u00e0 celle pour laquelle il a \u00e9t\u00e9 pr\u00e9-entrain\u00e9 (exemple : on veut d\u00e9tecter les oiseaux tout en restant performant sur les mammif\u00e8res). Dans ce cas, on veut que le mod\u00e8le reste performant sur la t\u00e2che d'origine.</li> </ul> <p>Selon le cas, on ne va pas utiliser les m\u00eames donn\u00e9es pour l'entra\u00eenement. Pour le cas 1, notre dataset d'entra\u00eenement va comporter uniquement les images nouvelles que l'on souhaite classifier (uniquement images de dinosaures). A l'inverse, pour le cas 2, on veut avoir des donn\u00e9es de l'ancien dataset et du nouveau pour \u00eatre sur que le mod\u00e8le reste performant sur les anciennes donn\u00e9es. En g\u00e9n\u00e9ral, on va prendre 50/50 mais cela peut varier selon les situations (moiti\u00e9 mammif\u00e8res, moiti\u00e9 oiseaux).</p> <p>Note : Selon ce principe, le vrai open-source voudrait dire qu'on laisse accessible \u00e0 la fois le code, les poids et les donn\u00e9es d'entra\u00eenement du mod\u00e8le. Car si il nous manque un des 3, on ne va pas pouvoir fine-tuner efficacement notre mod\u00e8le. C'est vrai en particulier pour les mod\u00e8les de langage LLM.</p> <p>Les mod\u00e8les de fondations sont des mod\u00e8les entra\u00een\u00e9s sur des grosses quantit\u00e9s de donn\u00e9es (g\u00e9n\u00e9ralement non labelis\u00e9es) et que l'on va utiliser comme base pour un fine-tuning ou transfer learning.</p> <p>Mod\u00e8le de fondations pour le NLP : Pour la t\u00e2che de NLP, il existe de nombreux mod\u00e8les de fondations tel que GPT, BLOOM, Llama, Gemini etc... Ces mod\u00e8les vont \u00eatre fine-tun\u00e9 pour des t\u00e2ches diverses. Par exemple, chatGPT est une version fine-tun\u00e9 de GPT sur des conversations type chatbot.</p> <p>Mod\u00e8le de fondations pour les images : Pour les images, le terme mod\u00e8le de fondations est assez d\u00e9batu car ce n'est pas aussi \u00e9vident que pour le NLP. Par exemple, on peut citer ViT, DINO, CLIP etc ...</p> <p>Mod\u00e8le de fondations pour le son : Pour le son, le mod\u00e8le CLAP est un exemple de mod\u00e8le de fondation.</p>"},{"location":"10_TransferLearningEtDistillation/01_TransferLearning.html#transfer-learning","title":"Transfer Learning\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/01_TransferLearning.html#transfer-learning-ou-fine-tuning","title":"Transfer learning ou fine-tuning ?\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/01_TransferLearning.html#comment-utiliser-le-fine-tuning","title":"Comment utiliser le fine-tuning ?\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/01_TransferLearning.html#quand-utiliser-le-transfer-learning-ou-le-fine-tuning","title":"Quand utiliser le transfer learning ou le fine-tuning ?\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/01_TransferLearning.html#dataset-dentrainement-comment-faire","title":"Dataset d'entra\u00eenement, comment faire ?\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/01_TransferLearning.html#modeles-de-fondations","title":"Mod\u00e8les de fondations\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/02_TransferLearningPytorch.html","title":"Transfer Learning avec Pytorch","text":"<p>Pour illustrer l'int\u00earet du transfer learning, nous allons utiliser un dataset assez petit contenant des images de fourmis et d'abeilles. On va chercher \u00e0 entrainer un mod\u00e8le pour classifier si l'insecte sur l'image est une fourmi ou une abeille. Ce code s'inspire de l'exemple de fine-tuning de pytorch. Vous pouvez t\u00e9l\u00e9charger le dataset en cliquant sur ce lien.</p> In\u00a0[46]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport os\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n</pre> import torch import torch.nn as nn import torch.optim as optim import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import os  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") <p>Commen\u00e7ons par charger notre dataset :</p> In\u00a0[47]: Copied! <pre># Transformation des donn\u00e9es\ntransforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # moyenne et ecart type utilis\u00e9 pour le pre-entrainement\n    ])\n\n# Chemin vers les donn\u00e9es\ndata_dir = '../data/hymenoptera_data' \n\ntrain_data = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms)\nval_data= datasets.ImageFolder(os.path.join(data_dir, 'val'), transforms)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=4,shuffle=True, num_workers=4)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=4,shuffle=True, num_workers=4)\n\n\nclass_names = train_data.classes\nprint(\"Classes du dataset : \",class_names)\n\nprint(\"Nombre d'images d'entrainement : \",len(train_data))\nprint(\"Nombre d'images de validation : \",len(val_data))\n</pre> # Transformation des donn\u00e9es transforms = transforms.Compose([         transforms.Resize(256),         transforms.CenterCrop(224),         transforms.ToTensor(),         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # moyenne et ecart type utilis\u00e9 pour le pre-entrainement     ])  # Chemin vers les donn\u00e9es data_dir = '../data/hymenoptera_data'   train_data = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms) val_data= datasets.ImageFolder(os.path.join(data_dir, 'val'), transforms)  train_loader = torch.utils.data.DataLoader(train_data, batch_size=4,shuffle=True, num_workers=4) val_loader = torch.utils.data.DataLoader(val_data, batch_size=4,shuffle=True, num_workers=4)   class_names = train_data.classes print(\"Classes du dataset : \",class_names)  print(\"Nombre d'images d'entrainement : \",len(train_data)) print(\"Nombre d'images de validation : \",len(val_data))  <pre>Classes du dataset :  ['ants', 'bees']\nNombre d'images d'entrainement :  244\nNombre d'images de validation :  153\n</pre> <p>Comme vous le voyez, on dispose de tr\u00e8s peu d'images. On peut visualiser quelques \u00e9l\u00e9ments de notre dataset :</p> In\u00a0[79]: Copied! <pre>def imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    plt.title(title)\n\ninputs, classes = next(iter(train_loader))\n\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n</pre> def imshow(inp, title=None):     inp = inp.numpy().transpose((1, 2, 0))     mean = np.array([0.485, 0.456, 0.406])     std = np.array([0.229, 0.224, 0.225])     inp = std * inp + mean     inp = np.clip(inp, 0, 1)     plt.imshow(inp)     plt.title(title)  inputs, classes = next(iter(train_loader))  out = torchvision.utils.make_grid(inputs)  imshow(out, title=[class_names[x] for x in classes]) <p>Le dataset a l'air assez compliqu\u00e9, parfois les insectes occupent une tr\u00e8s petite place dans l'image.</p> <p>Pour le mod\u00e8le, on utilise l'architecture de resnet18 qui est une architecture assez l\u00e9g\u00e9re et tr\u00e8s performante pour les probl\u00e8mes de classification.</p> In\u00a0[62]: Copied! <pre>class modified_resnet18(nn.Module):\n  def __init__(self,weights=None,out_class=2):\n    super(modified_resnet18, self).__init__()\n    # On charge le mod\u00e8le pr\u00e9-entrain\u00e9. Si weights=None, on charge le mod\u00e8le sans les poids pr\u00e9-entrain\u00e9s\n    self.resnet18 = models.resnet18(weights=weights) \n    # On remplace la derni\u00e8re couche du mod\u00e8le pour correspondre au nombre de classes de notre probl\u00e8me\n    self.resnet18.fc = nn.Linear(512, out_class)\n  \n  def forward(self,x):\n    return self.resnet18(x)\n</pre> class modified_resnet18(nn.Module):   def __init__(self,weights=None,out_class=2):     super(modified_resnet18, self).__init__()     # On charge le mod\u00e8le pr\u00e9-entrain\u00e9. Si weights=None, on charge le mod\u00e8le sans les poids pr\u00e9-entrain\u00e9s     self.resnet18 = models.resnet18(weights=weights)      # On remplace la derni\u00e8re couche du mod\u00e8le pour correspondre au nombre de classes de notre probl\u00e8me     self.resnet18.fc = nn.Linear(512, out_class)      def forward(self,x):     return self.resnet18(x) <p>Dans un premier temps, nous allons essayer d'entra\u00eener notre mod\u00e8le \u00e0 partir de z\u00e9ro.</p> In\u00a0[64]: Copied! <pre>model = modified_resnet18(weights=None,out_class=len(class_names)) #weights=None pour charger le mod\u00e8le sans les poids pr\u00e9-entrain\u00e9s\nmodel = model.to(device)\n\nlr=0.001\nepochs=10\n\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model.parameters(), lr=lr)\n</pre> model = modified_resnet18(weights=None,out_class=len(class_names)) #weights=None pour charger le mod\u00e8le sans les poids pr\u00e9-entrain\u00e9s model = model.to(device)  lr=0.001 epochs=10  criterion = nn.CrossEntropyLoss() optimizer_ft = optim.SGD(model.parameters(), lr=lr) In\u00a0[65]: Copied! <pre>for epoch in range(epochs):\n  loss_train=0\n  for inputs, labels in train_loader:\n    inputs, labels = inputs.to(device), labels.to(device)\n    optimizer_ft.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer_ft.step()\n    loss_train+=loss.item()\n  \n  \n  model.eval()\n  with torch.no_grad():\n    loss_val=0\n    for inputs, labels in val_loader:\n      inputs, labels = inputs.to(device), labels.to(device)\n      outputs = model(inputs)\n      loss = criterion(outputs, labels)\n      loss_val+=loss.item()\n    print(f\"Epoch {epoch+1}/{epochs} : train loss : {loss_train/len(train_loader)} val loss : {loss_val/len(val_loader)}\")\n</pre> for epoch in range(epochs):   loss_train=0   for inputs, labels in train_loader:     inputs, labels = inputs.to(device), labels.to(device)     optimizer_ft.zero_grad()     outputs = model(inputs)     loss = criterion(outputs, labels)     loss.backward()     optimizer_ft.step()     loss_train+=loss.item()         model.eval()   with torch.no_grad():     loss_val=0     for inputs, labels in val_loader:       inputs, labels = inputs.to(device), labels.to(device)       outputs = model(inputs)       loss = criterion(outputs, labels)       loss_val+=loss.item()     print(f\"Epoch {epoch+1}/{epochs} : train loss : {loss_train/len(train_loader)} val loss : {loss_val/len(val_loader)}\") <pre>Epoch 1/10 : train loss : 0.7068140135436761 val loss : 0.6533934359367077\nEpoch 2/10 : train loss : 0.7156724578044453 val loss : 0.7215747200907805\nEpoch 3/10 : train loss : 0.6751646028190362 val loss : 0.6428722785069392\nEpoch 4/10 : train loss : 0.5965930917223946 val loss : 0.7239674238058237\nEpoch 5/10 : train loss : 0.6105695530527928 val loss : 0.5773208579764917\nEpoch 6/10 : train loss : 0.5515003006477825 val loss : 0.8412383454732406\nEpoch 7/10 : train loss : 0.5839061943478272 val loss : 0.6010858137638141\nEpoch 8/10 : train loss : 0.5389361244733216 val loss : 0.6349012469634031\nEpoch 9/10 : train loss : 0.5367097518727427 val loss : 0.5850474796234033\nEpoch 10/10 : train loss : 0.5234740852821068 val loss : 0.782087844151717\n</pre> <p>On calcule la pr\u00e9cision sur nos donn\u00e9es de validation :</p> In\u00a0[66]: Copied! <pre>correct = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\nprint('Pr\u00e9cision sur les images de validation: %d %%' % (100 * correct / total))\n</pre> correct = 0 total = 0 with torch.no_grad():     for inputs, labels in val_loader:         inputs, labels = inputs.to(device), labels.to(device)         outputs = model(inputs)         _, predicted = torch.max(outputs, 1)         total += labels.size(0)         correct += (predicted == labels).sum().item() print('Pr\u00e9cision sur les images de validation: %d %%' % (100 * correct / total)) <pre>Pr\u00e9cision sur les images de validation: 58 %\n</pre> <p>La pr\u00e9cision est vraiment mauvaise (avec un mod\u00e8le al\u00e9atoire, \u00e7a serait 50%)...</p> <p>On peut visualiser les pr\u00e9dictions :</p> In\u00a0[71]: Copied! <pre>def visualize_model(model, num_images=6):\n    model.eval()\n    images_so_far = 0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(val_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'predicted: {class_names[preds[j]]}')\n                imshow(inputs.cpu().data[j], title=f'predicted: {class_names[preds[j]]}')\n\n                if images_so_far == num_images:\n                    return\nvisualize_model(model)\n</pre> def visualize_model(model, num_images=6):     model.eval()     images_so_far = 0     with torch.no_grad():         for i, (inputs, labels) in enumerate(val_loader):             inputs = inputs.to(device)             labels = labels.to(device)             outputs = model(inputs)             _, preds = torch.max(outputs, 1)                          for j in range(inputs.size()[0]):                 images_so_far += 1                 ax = plt.subplot(num_images//2, 2, images_so_far)                 ax.axis('off')                 ax.set_title(f'predicted: {class_names[preds[j]]}')                 imshow(inputs.cpu().data[j], title=f'predicted: {class_names[preds[j]]}')                  if images_so_far == num_images:                     return visualize_model(model) <p>On voit que le mod\u00e8le fait un peu n'importe quoi. En soit, ce n'est pas \u00e9tonnant, on dispose de tr\u00e8s peu d'images et on sait que, pour entra\u00eener un mod\u00e8le performant \u00e0 partir de z\u00e9ro, il nous faut beaucoup plus d'images (surtout quand il s'agit d'un mod\u00e8le assez profond).</p> <p>Regardons maintenant ce que l'on peut faire en utilisant du transfer learning. La seule chose qui va changer par rapport au code pr\u00e9c\u00e9dent c'est le param\u00e8tre weights de notre mod\u00e8le (et esp\u00e9rons le, les r\u00e9sultats aussi).</p> In\u00a0[72]: Copied! <pre>model = modified_resnet18(weights='IMAGENET1K_V1',out_class=len(class_names)) #On charge les poids pr\u00e9-entrain\u00e9s sur ImageNet\nmodel = model.to(device)\n\nlr=0.001\nepochs=10\n\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model.parameters(), lr=lr)\n</pre> model = modified_resnet18(weights='IMAGENET1K_V1',out_class=len(class_names)) #On charge les poids pr\u00e9-entrain\u00e9s sur ImageNet model = model.to(device)  lr=0.001 epochs=10  criterion = nn.CrossEntropyLoss() optimizer_ft = optim.SGD(model.parameters(), lr=lr) In\u00a0[73]: Copied! <pre>for epoch in range(epochs):\n  loss_train=0\n  for inputs, labels in train_loader:\n    inputs, labels = inputs.to(device), labels.to(device)\n    optimizer_ft.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer_ft.step()\n    loss_train+=loss.item()\n  \n  \n  model.eval()\n  with torch.no_grad():\n    loss_val=0\n    for inputs, labels in val_loader:\n      inputs, labels = inputs.to(device), labels.to(device)\n      outputs = model(inputs)\n      loss = criterion(outputs, labels)\n      loss_val+=loss.item()\n    print(f\"Epoch {epoch+1}/{epochs} : train loss : {loss_train/len(train_loader)} val loss : {loss_val/len(val_loader)}\")\n</pre> for epoch in range(epochs):   loss_train=0   for inputs, labels in train_loader:     inputs, labels = inputs.to(device), labels.to(device)     optimizer_ft.zero_grad()     outputs = model(inputs)     loss = criterion(outputs, labels)     loss.backward()     optimizer_ft.step()     loss_train+=loss.item()         model.eval()   with torch.no_grad():     loss_val=0     for inputs, labels in val_loader:       inputs, labels = inputs.to(device), labels.to(device)       outputs = model(inputs)       loss = criterion(outputs, labels)       loss_val+=loss.item()     print(f\"Epoch {epoch+1}/{epochs} : train loss : {loss_train/len(train_loader)} val loss : {loss_val/len(val_loader)}\") <pre>Epoch 1/10 : train loss : 0.6442642182600303 val loss : 0.5642786813087952\nEpoch 2/10 : train loss : 0.30489559746423706 val loss : 0.26585672435183555\nEpoch 3/10 : train loss : 0.10015173801831657 val loss : 0.22248221815635377\nEpoch 4/10 : train loss : 0.03893961609325937 val loss : 0.23963456177481043\nEpoch 5/10 : train loss : 0.017503870887773446 val loss : 0.21813779352352214\nEpoch 6/10 : train loss : 0.011329375068107467 val loss : 0.24817544420903476\nEpoch 7/10 : train loss : 0.008011038282496824 val loss : 0.22638171303939694\nEpoch 8/10 : train loss : 0.005813347443854284 val loss : 0.2239722229714971\nEpoch 9/10 : train loss : 0.004845750937047491 val loss : 0.23538081515699816\nEpoch 10/10 : train loss : 0.003927258039885735 val loss : 0.24088036894728504\n</pre> <p>Calculons la pr\u00e9cision sur nos donn\u00e9es de validation :</p> In\u00a0[74]: Copied! <pre>correct = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\nprint('Pr\u00e9cision sur les images de validation: %d %%' % (100 * correct / total))\n</pre> correct = 0 total = 0 with torch.no_grad():     for inputs, labels in val_loader:         inputs, labels = inputs.to(device), labels.to(device)         outputs = model(inputs)         _, predicted = torch.max(outputs, 1)         total += labels.size(0)         correct += (predicted == labels).sum().item() print('Pr\u00e9cision sur les images de validation: %d %%' % (100 * correct / total)) <pre>Pr\u00e9cision sur les images de validation: 91 %\n</pre> <p>Le r\u00e9sultat n'a rien \u00e0 voir. On est pass\u00e9 de 58% de pr\u00e9cision \u00e0 91% juste gr\u00e2ce \u00e0 l'utilisation de poids pr\u00e9-entrain\u00e9.</p> <p>On peut \u00e0 nouveau visualiser les pr\u00e9dictions :</p> In\u00a0[75]: Copied! <pre>visualize_model(model)\n</pre> visualize_model(model) <p>C'est beaucoup mieux ! J'esp\u00e8re que ce cours vous a convaincu de la puissance du transfer learning !</p>"},{"location":"10_TransferLearningEtDistillation/02_TransferLearningPytorch.html#transfer-learning-avec-pytorch","title":"Transfer Learning avec Pytorch\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/02_TransferLearningPytorch.html#dataset","title":"Dataset\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/02_TransferLearningPytorch.html#creation-du-modele","title":"Cr\u00e9ation du mod\u00e8le\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/02_TransferLearningPytorch.html#entrainement-sans-transfer-learning","title":"Entra\u00eenement sans transfer learning\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/02_TransferLearningPytorch.html#entrainement-avec-transfer-learning","title":"Entra\u00eenement avec transfer learning\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/03_Distillation.html","title":"Distillation des connaissances","text":"<p>Le concept de distillation des connaissances a \u00e9t\u00e9 introduit dans le papier Distilling the Knowledge in a Neural Network en 2015. L'id\u00e9e de la distillation est d'utiliser un mod\u00e8le appel\u00e9 teacher (un mod\u00e8le profond et d\u00e9j\u00e0 entrain\u00e9) pour transf\u00e9rer les connaissances apprises dans un mod\u00e8le plus petit appel\u00e9 student.</p> <p>En pratique, notre mod\u00e8le student va \u00eatre entrain\u00e9 sur deux objectifs :</p> <ul> <li>Il va minimiser la distance entre sa pr\u00e9diction et la pr\u00e9diction du teacher sur le m\u00eame \u00e9l\u00e9ment.</li> <li>Il va \u00e9galement minimiser la distance entre sa pr\u00e9diction et le label de l'entr\u00e9e.</li> </ul> <p>Ces deux loss sont combin\u00e9s avec un facteur de pond\u00e9ration $\\alpha$ que l'on peut choisir. De cette mani\u00e8re, le mod\u00e8le student dispose d'une part du label de l'image et d'autre part de la pr\u00e9diction du mod\u00e8le teacher (qui est une distribution de probabilit\u00e9).</p> <p>Note : En pratique, pour la premi\u00e8re partie du loss, on ne compare pas les probabilit\u00e9s apr\u00e8s la fonction softmax mais plut\u00f4t les logits avant l'application du softmax. Pour plus de clart\u00e9, on continuera \u00e0 dire \"pr\u00e9dictions\" au lieu de \"logits\".</p> <p></p> <p>On peut se demander pourquoi est ce que cela fonctionne mieux que d'entra\u00eener directement le student avec le loss classique pr\u00e9diction/label. Il y a plusieurs raisons \u00e0 cela :</p> <ul> <li>Transfert des connaissances implicites : La fait d'utiliser les pr\u00e9dictions du teacher permet au mod\u00e8le student d'apprendre des connaissances implicites sur les donn\u00e9es. La pr\u00e9diction du mod\u00e8le est une distribution de probabilit\u00e9s qui va indiquer la similarit\u00e9 entre plusieurs classes par exemple.</li> <li>Conservation des relations complexes : Le mod\u00e8le teacher est tr\u00e8s complexe et peut capturer des structures complexes dans les donn\u00e9es ce qui n'est pas forc\u00e9ment le cas pour un mod\u00e8le plus petit entra\u00een\u00e9 \u00e0 partir de z\u00e9ro. La distillation permet au student d'apprendre ces relations complexes plus simplement tout en am\u00e9liorant la vitesse et en r\u00e9duisant l'utilisation m\u00e9moire (car c'est un mod\u00e8le plus petit).</li> <li>Stabilisation de l'entra\u00eenement : En pratique, on constate \u00e9galement que l'entra\u00eenement est plus stable pour le student si on utilise cette m\u00e9thode de distillation.</li> <li>Att\u00e9nuation des probl\u00e8mes d'annotations : Le mod\u00e8le teacher a appris \u00e0 g\u00e9n\u00e9raliser et est donc capable de pr\u00e9dire correctement mais si il a \u00e9t\u00e9 entrain\u00e9 sur ces m\u00eames images avec des labels incorrectes. Dans le cadre de la distillation, la diff\u00e9rence importante entre la sortie du teacher et le label donne une information suppl\u00e9mentaire au student sur la qualit\u00e9 de cette donn\u00e9e.</li> </ul> <p>En pratique, il est possible de transf\u00e9rer la connaissance d'un mod\u00e8le performant dans un mod\u00e8le plus petit avec peu ou pas de perte de qualit\u00e9 de pr\u00e9dictions. C'est tr\u00e8s pratique lorsqu'on cherche \u00e0 r\u00e9duire la taille de nos mod\u00e8les pour, par exemple, faire de l'embarqu\u00e9 ou du traitement sur CPU. Il est aussi possible de distiller plusieurs teachers dans un unique student. En faisant cela, il peut arriver que le student soit plus performant que chaque teacher pris individuellement.</p> <p>C'est une technique \u00e0 conna\u00eetre et qui peut vous \u00eatre utile dans beaucoup de situations.</p> <p>Depuis son invention, la distillation des connaissances a \u00e9t\u00e9 adapt\u00e9 \u00e0 la r\u00e9solution de toute sorte de probl\u00e8mes. Nous allons en pr\u00e9senter deux ici : l'am\u00e9lioration de la classification avec NoisyStudent et la d\u00e9tection d'anomalies non supervis\u00e9e avec STPM.</p> <p>Pendant longtemps, la course \u00e0 la performance sur le dataset ImageNet a \u00e9t\u00e9 au coeur de la recherche en deep learning. Le but \u00e9tant d'am\u00e9liorer encore et encore les performances sur ce fameux dataset. En 2020, l'article Self-training with Noisy Student improves ImageNet classification propose une utilisation de la distillation pour entra\u00een\u00e9 un mod\u00e8le student plus performant que le teacher \u00e0 chaque it\u00e9ration.</p> <p>Un mod\u00e8le student est entra\u00een\u00e9 \u00e0 partir de pseudo-labels extrait d'un mod\u00e8le teacher (labels cr\u00e9es par le mod\u00e8le teacher sur des images non annot\u00e9es). Lors de l'entra\u00eenement, du bruit est ajout\u00e9 pour augmenter sa robustesse. Une fois que le student est entra\u00een\u00e9, on l'utilise pour obtenir de nouveaux pseudo labels et entra\u00eener un autre student. On r\u00e9p\u00e9te le processus un certain nombre de fois et on finit par obtenir un mod\u00e8le bien plus performant que le teacher de base.</p> <p></p> <p>Un exemple tr\u00e8s int\u00e9ressant de l'utilisation de la distillation des connaissances est son utilisation pour la d\u00e9tection d'anomalies non supervis\u00e9e. L'article Student-Teacher Feature Pyramid Matching for Anomaly Detection adapte la distillation des connaissances pour ce cas d'usage.</p> <p>Cette fois-ci, le mod\u00e8le teacher et le mod\u00e8le student ont la m\u00eame architecture. Au lieu de regarder les pr\u00e9dictions et d'entra\u00eener le mod\u00e8le sur cela, on va plut\u00f4t s'int\u00e9resser aux feature map des couches du milieu du r\u00e9seau. Dans l'id\u00e9e, lors de l'entra\u00eenement, on dispose de donn\u00e9es sans anomalies. Le mod\u00e8le teacher est pr\u00e9-entrain\u00e9 sur ImageNet (par exemple) et est frozen pendant l'entra\u00eenement. Le mod\u00e8le student est initialis\u00e9 al\u00e9atoirement et c'est lui qu'on entra\u00eene. Plus pr\u00e9cisement, on l'entraine \u00e0 reproduire les feature map du teacher sur des donn\u00e9es sans d\u00e9fauts. A la fin de l'entrainement, le student et le teacher vont avoir des feature map identiques sur un \u00e9l\u00e9ment sans d\u00e9faut.</p> <p>Lorsqu'il est temps de tester notre mod\u00e8le, on teste sur des donn\u00e9es sans d\u00e9faut et des donn\u00e9es avec d\u00e9fauts. Sur les donn\u00e9es sans d\u00e9faut, le mod\u00e8le student va imiter le teacher parfaitement tandis que sur des donn\u00e9es defectueuses, les feature map du student et du teacher seront diff\u00e9rentes ce qui nous permettra de calculer un score de similarit\u00e9 (qui nous servira de score d'anomalie).</p> <p></p> <p>En pratique, cette m\u00e9thode fait partie des m\u00e9thodes les plus performantes pour la d\u00e9tection d'anomalies non supervis\u00e9e. C'est cette m\u00e9thode que nous allons impl\u00e9menter dans le notebook suivant.</p>"},{"location":"10_TransferLearningEtDistillation/03_Distillation.html#distillation-des-connaissances","title":"Distillation des connaissances\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/03_Distillation.html#comment-ca-fonctionne","title":"Comment \u00e7a fonctionne ?\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/03_Distillation.html#pourquoi-est-ce-que-ca-fonctionne","title":"Pourquoi est-ce que \u00e7a fonctionne ?\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/03_Distillation.html#utilisation-pratique","title":"Utilisation pratique\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/03_Distillation.html#autres-applications-de-la-distillation-des-connaissances","title":"Autres applications de la distillation des connaissances\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/03_Distillation.html#noisy-student","title":"Noisy Student\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/03_Distillation.html#stpm","title":"STPM\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html","title":"Distillation des connaissances pour la d\u00e9tection d'anomalies non supervis\u00e9e","text":"<p>Dans ce notebook d'application, nous allons mettre en pratique la distillation des connaissances dans le cas particulier de la d\u00e9tection d'anomalies non supervis\u00e9e.</p> <p>Pour cela, nous allons nous inspirer du papier Student-Teacher Feature Pyramid Matching for Anomaly Detection et du code correspondant. Voici la figure, extraite de l'article, d\u00e9crivant le fonctionnement de la m\u00e9thode :</p> <p></p> <p>L'article utilise resnet18 comme architecture de r\u00e9seau avec le mod\u00e8le teacher pr\u00e9-entrain\u00e9 sur ImageNet. Le student utilise la m\u00eame architecture mais le r\u00e9seau est initialis\u00e9 al\u00e9atoirement.</p> <p>Comme d\u00e9crit dans la figure ci-dessus le loss est calcul\u00e9 sur les sorties des 3 premiers groupes de couches de resnet18 (un groupe de couche correspond \u00e0 l'ensemble des couches qui op\u00e9rent \u00e0 la m\u00eame r\u00e9solution d'image). C'est \u00e0 dire que le r\u00e9seau student est entra\u00een\u00e9 \u00e0 reproduire les feature map du r\u00e9seau teacher sur ces 3 sorties uniquement. Le score d'anomalie sera aussi calcul\u00e9 sur ces sorties uniquement.</p> <p>Pour ce qui est de la fonction de loss, il s'agit simplement du loss MSE qui nous avons vu pr\u00e9cedemment. Ce loss est calcul\u00e9 sur chaque feature map puis somm\u00e9 pour obtenir le loss total.</p> <p>Le dataset utilis\u00e9 dans l'article est le dataset MVTEC AD qui regroupe 15 cat\u00e9gories dont 10 objets et 5 textures. Il y a environ 350 images sans d\u00e9fauts pour l'entra\u00eenement et une centaine d'images d\u00e9fectueuses pour le test (par cat\u00e9gorie bien s\u00fbr). Voici un aper\u00e7u des images du dataset :</p> <p></p> <p>Vous pouvez t\u00e9l\u00e9charger le dataset ici. Pour notre impl\u00e9mentation, nous utiliserons la cat\u00e9gorie hazelnut (noisette).</p> <p>Certaines fonctions sont un peu compliqu\u00e9e et pas n\u00e9cessaire \u00e0 comprendre pour appr\u00e9hender le concept (chargement du dataset etc ...). Pour plus de clart\u00e9, ces fonctions et classes ont \u00e9t\u00e9 d\u00e9plac\u00e9es dans le fichier utils.py que vous pouvez consulter si besoin.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom utils import MVTecDataset,cal_anomaly_maps\nimport torch\nimport torch.nn as nn\nimport timm\nimport torch.nn.functional as F\nimport numpy as np\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</pre> import matplotlib.pyplot as plt from utils import MVTecDataset,cal_anomaly_maps import torch import torch.nn as nn import timm import torch.nn.functional as F import numpy as np device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>On va commencer par charger notre dataset et regarder rapidement ce qu'il contient. Cette fois, on va vraiment s\u00e9parer notre training en train et validation pour pouvoir \u00e9valuer le mod\u00e8le pendant l'entra\u00eenement.</p> In\u00a0[2]: Copied! <pre>train_dataset = MVTecDataset(root_dir=\"../data/mvtec/hazelnut/train/good\",resize_shape=[256,256],phase='train')\ntest_dataset = MVTecDataset(root_dir=\"../data/mvtec/hazelnut/test/\",resize_shape=[256,256],phase='test')\nprint(\"taille du dataset d'entrainement : \",len(train_dataset))\nprint(\"taille du dataset de test : \",len(test_dataset))\nprint(\"taille d'une image : \",train_dataset[0]['imageBase'].shape)\n\n# S\u00e9paration du dataset d'entrainement en train et validation\nimg_nums = len(train_dataset)\nvalid_num = int(img_nums * 0.2)\ntrain_num = img_nums - valid_num\ntrain_data, val_data = torch.utils.data.random_split(train_dataset, [train_num, valid_num])\n\n# Cr\u00e9ation des dataloaders\ntrain_loader=torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True)\nval_loader=torch.utils.data.DataLoader(val_data, batch_size=4, shuffle=True)\ntest_loader=torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n</pre> train_dataset = MVTecDataset(root_dir=\"../data/mvtec/hazelnut/train/good\",resize_shape=[256,256],phase='train') test_dataset = MVTecDataset(root_dir=\"../data/mvtec/hazelnut/test/\",resize_shape=[256,256],phase='test') print(\"taille du dataset d'entrainement : \",len(train_dataset)) print(\"taille du dataset de test : \",len(test_dataset)) print(\"taille d'une image : \",train_dataset[0]['imageBase'].shape)  # S\u00e9paration du dataset d'entrainement en train et validation img_nums = len(train_dataset) valid_num = int(img_nums * 0.2) train_num = img_nums - valid_num train_data, val_data = torch.utils.data.random_split(train_dataset, [train_num, valid_num])  # Cr\u00e9ation des dataloaders train_loader=torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True) val_loader=torch.utils.data.DataLoader(val_data, batch_size=4, shuffle=True) test_loader=torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)  <pre>taille du dataset d'entrainement :  391\ntaille du dataset de test :  110\ntaille d'une image :  (3, 256, 256)\n</pre> <p>On peut visualiser quelques d\u00e9fauts :</p> In\u00a0[3]: Copied! <pre>num_defects_displayed = 0\nfig, axes = plt.subplots(2, 2, figsize=(5, 5))\n\nfor sample in test_loader:\n  image = sample['imageBase']\n  has_defect = sample['has_anomaly']\n  if has_defect:\n    row = num_defects_displayed // 2\n    col = num_defects_displayed % 2\n    \n    axes[row, col].imshow(image.squeeze().permute(1, 2, 0).numpy())\n    axes[row, col].axis('off')  \n    \n    num_defects_displayed += 1\n    \n    if num_defects_displayed == 4:\n      break\nplt.tight_layout()\nplt.show()\n</pre> num_defects_displayed = 0 fig, axes = plt.subplots(2, 2, figsize=(5, 5))  for sample in test_loader:   image = sample['imageBase']   has_defect = sample['has_anomaly']   if has_defect:     row = num_defects_displayed // 2     col = num_defects_displayed % 2          axes[row, col].imshow(image.squeeze().permute(1, 2, 0).numpy())     axes[row, col].axis('off')            num_defects_displayed += 1          if num_defects_displayed == 4:       break plt.tight_layout() plt.show() <p>Pour nos mod\u00e8les, nous allons utiliser la m\u00eame classe et mettre en param\u00e8tre les sp\u00e9cificit\u00e9s de chaque mod\u00e8le. Pour faciliter l'utilisation d'un backbone d\u00e9j\u00e0 existant, nous utilisons la library timm (pytorch image models). C'est une library tr\u00e8s int\u00e9ressante pour acc\u00e9der \u00e0 des backbones et des mod\u00e8les d\u00e9j\u00e0 entra\u00een\u00e9. Il y a \u00e9galement une certain flexibilit\u00e9 sur la manipulation du r\u00e9seau.</p> In\u00a0[4]: Copied! <pre>class resnet18timm(nn.Module):\n    def __init__(self,backbone_name=\"resnet18\",out_indices=[1,2,3],pretrained=True):\n        super(resnet18timm, self).__init__()     \n        # Features only permet permet de ne r\u00e9cup\u00e9rer que les features et pas la sortie du r\u00e9seau, out_indices permet de choisir les couches \u00e0 r\u00e9cup\u00e9rer\n        self.feature_extractor = timm.create_model(backbone_name,pretrained=pretrained,features_only=True,out_indices=out_indices)\n        if pretrained:\n            # Si le mod\u00e8le est pr\u00e9-entrain\u00e9 (donc c'est le teacher), on g\u00e8le les poids\n            self.feature_extractor.eval() \n            for param in self.feature_extractor.parameters():\n                param.requires_grad = False   \n        \n    def forward(self, x):\n        features = self.feature_extractor(x)\n        return features\n</pre> class resnet18timm(nn.Module):     def __init__(self,backbone_name=\"resnet18\",out_indices=[1,2,3],pretrained=True):         super(resnet18timm, self).__init__()              # Features only permet permet de ne r\u00e9cup\u00e9rer que les features et pas la sortie du r\u00e9seau, out_indices permet de choisir les couches \u00e0 r\u00e9cup\u00e9rer         self.feature_extractor = timm.create_model(backbone_name,pretrained=pretrained,features_only=True,out_indices=out_indices)         if pretrained:             # Si le mod\u00e8le est pr\u00e9-entrain\u00e9 (donc c'est le teacher), on g\u00e8le les poids             self.feature_extractor.eval()              for param in self.feature_extractor.parameters():                 param.requires_grad = False                 def forward(self, x):         features = self.feature_extractor(x)         return features <p>On peut maintenant cr\u00e9er notre teacher et notre student :</p> In\u00a0[5]: Copied! <pre>student=resnet18timm(backbone_name=\"resnet18\",out_indices=[1,2,3],pretrained=False).to(device)\nteacher=resnet18timm(backbone_name=\"resnet18\",out_indices=[1,2,3],pretrained=True).to(device)\n</pre> student=resnet18timm(backbone_name=\"resnet18\",out_indices=[1,2,3],pretrained=False).to(device) teacher=resnet18timm(backbone_name=\"resnet18\",out_indices=[1,2,3],pretrained=True).to(device) <p>La fonction de loss utilse la distance euclidienne (MSE) d\u00e9finie comme ceci : $D(I_1, I_2) = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left( I_1(i,j) - I_2(i,j) \\right)^2}$ o\u00f9 $I_1$ et $I_2$ sont nos deux images.</p> <p>Notre impl\u00e9mentation du loss va donc utiliser cette distance pour comparer les feature map entre elles pour les 3 paires de feature map :</p> In\u00a0[6]: Copied! <pre>class loss_kdad:\n  def __init__(self):\n    pass\n  # fs_list : liste des features du student et ft_list : liste des features du teacher\n  def __call__(self,fs_list, ft_list):\n    t_loss = 0\n    N = len(fs_list)\n    for i in range(N):\n      fs = fs_list[i]\n      ft = ft_list[i]\n      _, _, h, w = fs.shape\n      # Normaliser les features am\u00e9liore les r\u00e9sultats\n      fs_norm = F.normalize(fs, p=2)\n      ft_norm = F.normalize(ft, p=2)\n\n      # Calcul de la distance euclidienne\n      f_loss = 0.5 * (ft_norm - fs_norm) ** 2\n      # On prend la moyenne de la loss sur tous les pixels\n      f_loss = f_loss.sum() / (h * w)\n      t_loss += f_loss\n\n    return t_loss / N\n</pre> class loss_kdad:   def __init__(self):     pass   # fs_list : liste des features du student et ft_list : liste des features du teacher   def __call__(self,fs_list, ft_list):     t_loss = 0     N = len(fs_list)     for i in range(N):       fs = fs_list[i]       ft = ft_list[i]       _, _, h, w = fs.shape       # Normaliser les features am\u00e9liore les r\u00e9sultats       fs_norm = F.normalize(fs, p=2)       ft_norm = F.normalize(ft, p=2)        # Calcul de la distance euclidienne       f_loss = 0.5 * (ft_norm - fs_norm) ** 2       # On prend la moyenne de la loss sur tous les pixels       f_loss = f_loss.sum() / (h * w)       t_loss += f_loss      return t_loss / N <p>D\u00e9finissons nos hyperparam\u00e8tres :</p> In\u00a0[7]: Copied! <pre>epochs= 20\nlr=0.0004\ncriterion = loss_kdad()\noptimizer = torch.optim.Adam(student.parameters(), lr=lr)\n</pre> epochs= 20 lr=0.0004 criterion = loss_kdad() optimizer = torch.optim.Adam(student.parameters(), lr=lr) <p>Il est temps d'entra\u00eener le mod\u00e8le ! L'entrainement peut prendre un certain temps.</p> In\u00a0[8]: Copied! <pre>for epoch in range(epochs):\n    student.train()\n    train_loss = 0.0\n    for data in train_loader:\n        image = data['imageBase'].to(device)\n        optimizer.zero_grad()\n        outputs_student = student(image)\n        outputs_teacher = teacher(image)\n        loss = criterion(outputs_student,outputs_teacher)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    student.eval()\n    val_loss = 0.0\n    for data in val_loader:\n        image = data['imageBase'].to(device)\n        outputs_student = student(image)\n        outputs_teacher = teacher(image)\n        loss = criterion(outputs_student,outputs_teacher)\n        val_loss += loss.item()\n    print(\"Epoch {} - train loss : {} - val loss : {}\".format(epoch,train_loss/len(train_loader),val_loss/len(val_loader)))\n</pre> for epoch in range(epochs):     student.train()     train_loss = 0.0     for data in train_loader:         image = data['imageBase'].to(device)         optimizer.zero_grad()         outputs_student = student(image)         outputs_teacher = teacher(image)         loss = criterion(outputs_student,outputs_teacher)         loss.backward()         optimizer.step()         train_loss += loss.item()      student.eval()     val_loss = 0.0     for data in val_loader:         image = data['imageBase'].to(device)         outputs_student = student(image)         outputs_teacher = teacher(image)         loss = criterion(outputs_student,outputs_teacher)         val_loss += loss.item()     print(\"Epoch {} - train loss : {} - val loss : {}\".format(epoch,train_loss/len(train_loader),val_loss/len(val_loader))) <pre>Epoch 0 - train loss : 1.6731480107277255 - val loss : 1.3833315640687942\nEpoch 1 - train loss : 0.8773692731238618 - val loss : 0.7371394574642182\nEpoch 2 - train loss : 0.5567233881241158 - val loss : 0.5115290269255638\nEpoch 3 - train loss : 0.4170471341172351 - val loss : 0.4163943402469158\nEpoch 4 - train loss : 0.33714089627507365 - val loss : 0.3523293524980545\nEpoch 5 - train loss : 0.29374887162371527 - val loss : 0.28786116763949393\nEpoch 6 - train loss : 0.2577011583349373 - val loss : 0.2747397504746914\nEpoch 7 - train loss : 0.2373752082827725 - val loss : 0.22077917009592057\nEpoch 8 - train loss : 0.21801123603046696 - val loss : 0.2255100306123495\nEpoch 9 - train loss : 0.2012799475577813 - val loss : 0.19499738812446593\nEpoch 10 - train loss : 0.1874875887473927 - val loss : 0.20737174898386002\nEpoch 11 - train loss : 0.17415884736029408 - val loss : 0.254209216684103\nEpoch 12 - train loss : 0.16137944416532032 - val loss : 0.15532575249671937\nEpoch 13 - train loss : 0.14706034665998025 - val loss : 0.1503308217972517\nEpoch 14 - train loss : 0.13900368472066107 - val loss : 0.14076187387108802\nEpoch 15 - train loss : 0.1303630452367324 - val loss : 0.12805806174874307\nEpoch 16 - train loss : 0.1270840932862668 - val loss : 0.3456251971423626\nEpoch 17 - train loss : 0.1299520534333549 - val loss : 0.12075391858816147\nEpoch 18 - train loss : 0.11812143749262713 - val loss : 0.11526557803153992\nEpoch 19 - train loss : 0.1135895169233974 - val loss : 0.11893145311623812\n</pre> <p>On peut maintenant \u00e9valuer notre mod\u00e8le. Pour cela, nous utilisons la mesure AUROC (voir cours 9 pour un rappel).</p> In\u00a0[11]: Copied! <pre>scores = []\ntest_imgs = []\ngt_list = []\nfor sample in test_loader:\n    label=sample['has_anomaly']\n    image = sample['imageBase'].to(device)\n    test_imgs.extend(image.cpu())\n    gt_list.extend(label.cpu().numpy())\n    with torch.set_grad_enabled(False):\n        \n        outputs_student = student(image)\n        outputs_teacher = teacher(image) \n\n        # La fonction cal_anomaly_maps permet de calculer la carte d'anomalie (\u00e7a sera utile pour la visualisation apr\u00e8s)\n        score =cal_anomaly_maps(outputs_student,outputs_teacher,256) \n        \n    scores.append(score)\nscores = np.asarray(scores)\ngt_list = np.asarray(gt_list)\n\nfrom sklearn.metrics import roc_auc_score\n\nmap_scores = np.asarray(scores)\n\nmax_anomaly_score = map_scores.max()\nmin_anomaly_score = map_scores.min()\nmap_scores = (map_scores - min_anomaly_score) / (max_anomaly_score - min_anomaly_score)\nimg_scores = map_scores.reshape(map_scores.shape[0], -1).max(axis=1)\ngt_list = np.asarray(gt_list)\nimg_roc_auc = roc_auc_score(gt_list, img_scores)\nprint(\" image hazelnut ROCAUC : %.3f\" % (img_roc_auc))\n</pre> scores = [] test_imgs = [] gt_list = [] for sample in test_loader:     label=sample['has_anomaly']     image = sample['imageBase'].to(device)     test_imgs.extend(image.cpu())     gt_list.extend(label.cpu().numpy())     with torch.set_grad_enabled(False):                  outputs_student = student(image)         outputs_teacher = teacher(image)           # La fonction cal_anomaly_maps permet de calculer la carte d'anomalie (\u00e7a sera utile pour la visualisation apr\u00e8s)         score =cal_anomaly_maps(outputs_student,outputs_teacher,256)               scores.append(score) scores = np.asarray(scores) gt_list = np.asarray(gt_list)  from sklearn.metrics import roc_auc_score  map_scores = np.asarray(scores)  max_anomaly_score = map_scores.max() min_anomaly_score = map_scores.min() map_scores = (map_scores - min_anomaly_score) / (max_anomaly_score - min_anomaly_score) img_scores = map_scores.reshape(map_scores.shape[0], -1).max(axis=1) gt_list = np.asarray(gt_list) img_roc_auc = roc_auc_score(gt_list, img_scores) print(\" image hazelnut ROCAUC : %.3f\" % (img_roc_auc))  <pre> image hazelnut ROCAUC : 0.990\n</pre> <p>On obtient un tr\u00e8s bon AUROC, notre mod\u00e8le est tr\u00e8s bon pour d\u00e9tecter les d\u00e9fauts sur les noisettes.</p> <p>Pour calculer le score, nous avons compar\u00e9 les feature map du teacher \u00e0 celle du student sur les 3 sorties (de dimension diff\u00e9rente). A l'aide de la fonction cal_anomaly_maps, nous avons effectu\u00e9 les comparaisons et reconstruit une carte d'anomalie de la taille de l'image de base. On peut visualiser cette carte d'anomalie pour obtenir une localisation du d\u00e9faut.</p> In\u00a0[12]: Copied! <pre>fig, axs = plt.subplots(2, 2, figsize=(5, 5))\n\nfor i,(img,mask) in enumerate(zip(test_imgs,scores)):\n    img_act=img.squeeze().permute(1, 2, 0).numpy()\n    row = i // 2\n    col = i % 2\n    axs[row, col].imshow(img_act) \n    axs[row, col].imshow(mask, cmap='jet', alpha=0.5)\n    axs[row, col].axis('off')\n    if i==3:\n        break\n    \nplt.tight_layout()\nplt.show()\n</pre> fig, axs = plt.subplots(2, 2, figsize=(5, 5))  for i,(img,mask) in enumerate(zip(test_imgs,scores)):     img_act=img.squeeze().permute(1, 2, 0).numpy()     row = i // 2     col = i % 2     axs[row, col].imshow(img_act)      axs[row, col].imshow(mask, cmap='jet', alpha=0.5)     axs[row, col].axis('off')     if i==3:         break      plt.tight_layout() plt.show()  <p>On constate que la localisation est assez pr\u00e9cise bien que \u00e7a ne soit pas le but premier de notre mod\u00e8le.</p>"},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#distillation-des-connaissances-pour-la-detection-danomalies-non-supervisee","title":"Distillation des connaissances pour la d\u00e9tection d'anomalies non supervis\u00e9e\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#choix-du-backbone-studentteacher-et-dataset","title":"Choix du backbone student/teacher et dataset\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#backbone-et-loss","title":"Backbone et loss\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#dataset","title":"Dataset\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#implementation-pytorch-et-timm","title":"Impl\u00e9mentation pytorch et timm\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#dataset","title":"Dataset\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#creation-de-nos-deux-modeles-teacher-et-student","title":"Cr\u00e9ation de nos deux mod\u00e8les teacher et student\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#fonction-de-loss","title":"Fonction de loss\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#entrainement-du-modele","title":"Entra\u00eenement du mod\u00e8le\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/04_DistillationAnomalie.html#visualisation-de-la-carte-danomalies","title":"Visualisation de la carte d'anomalies\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/05_FineTuningLLM.html","title":"Fine Tuning LLM","text":"<p>Dans ce cours, nous allons pr\u00e9senter en d\u00e9tail l'article BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding que nous avons d\u00e9j\u00e0 mentionn\u00e9 dans le cours 7 sur les transformers. La plupart des LLM (GPT, BERT etc ...) sont pr\u00e9-entrain\u00e9 sur une t\u00e2che de pr\u00e9diction du prochain mot ou pr\u00e9diction de mots masqu\u00e9s et sont ensuite finetune sur des t\u00e2ches plus sp\u00e9cifiques. Les LLM sans fine-tuning ne sont en g\u00e9n\u00e9ral pas tr\u00e8s utiles.</p> <p>Note : Lorsque l'on finetune un LLM, on va r\u00e9entrainer tous les param\u00e8tres du mod\u00e8le. Lorsque l'on finetune des mod\u00e8les de vision comme des CNN, il est courant de ne r\u00e9-entra\u00eener qu'une partie des couches (uniquement la derni\u00e8re parfois).</p> <p>Dans le cours sur les transformers, nous avons longuement introduit GPT et nous l'avons implement\u00e9. La particularit\u00e9 de GPT est qu'il est unidirectionnel, c'est \u00e0 dire que pour pr\u00e9dire le token de droite, on n'utilise que les tokens de gauche. Cependant, ce type de g\u00e9n\u00e9ration n'est pas opimale pour de nombreuses t\u00e2ches. Bien souvent, on a besoin du contexte de toute la phrase pour notre t\u00e2che. BERT propose une alternative \u00e0 cela en utilisant un transformer bidirectionnel c'est \u00e0 dire que l'on utilise le contexte de droit et de gauche pour notre pr\u00e9diction. Ce transformer a une architecture permettant d'\u00eatre finetune sur deux types de t\u00e2ches :</p> <ul> <li>sentence-level prediction :  On veut pr\u00e9dire la classe de la phrase (par exemple dans le cas d'analyse de sentiment)</li> <li>token-level prediction : On veut pr\u00e9dire la classe de chaque token (named entity recognition par exemple)</li> </ul> <p>Contrairement \u00e0 GPT, l'architecture de BERT est bas\u00e9 sur le block encoder du transformer et pas le block decoder (voir cours 7 pour rappel).</p> <p>Tout d'abord, il faut savoir que le token [CLS] est rajout\u00e9 \u00e0 chaque s\u00e9quence d'entr\u00e9e. Nous verrons son utilisation plus tard dans la partie sur le finetuning du mod\u00e8le. De plus, lors de son pre-entrainement, BERT prend en entr\u00e9e 2 phrases que l'on va s\u00e9parer par un [SEP] token. En plus de la s\u00e9paration, on ajoute \u00e9galement un segment embedding  \u00e0 chaque token embedding qui indique l'indice de la phrase de provenance (1 ou 2). Bien sur, comme pour GPT, on utilise un position embedding que l'on ajoute \u00e9galement \u00e0 chaque token embedding.</p> <p></p> <p>Note : L'utilisation de phrase n'est pas \u00e0 comprendre dans le sens linguistique du terme mais plut\u00f4t comme une s\u00e9quence de tokens qui se suivent.</p> <p>Pour GPT, nous avions vu que pour entra\u00eener le mod\u00e8le, il suffisait de masquer le futur (le token \u00e0 pr\u00e9dire et les tokens \u00e0 droite) pour entra\u00eener le mod\u00e8le. Comme BERT est bidirectionnel, on ne peut pas utiliser cette m\u00e9thode. A la place, les auteurs proposent de masquer une partie des tokens (15% en pratique) de mani\u00e8re al\u00e9atoire et d'entra\u00eener le mod\u00e8le \u00e0 pr\u00e9dire ces mots. BERT est alors appel\u00e9 un Masked Language Model (MLM). L'id\u00e9e va \u00eatre de remplacer ces tokens par des tokens [MSK].</p> <p>Lors du fine-tuning, il n'y aura aucun [MSK] token donc pour compenser ce probl\u00e8me, les auteurs proposent de ne pas convertir l'int\u00e9gralit\u00e9 des 15% de tokens en [MSK] tokens mais plut\u00f4t de faire de la mani\u00e8re suivante :</p> <p>Sur les 15% de tokens choisis :</p> <ul> <li>80% sont convertis en [MSK] tokens</li> <li>10% sont convertis en un autre token random</li> <li>10% restent identiques</li> </ul> <p>Cette technique va permettre un fine-tuning beaucoup plus efficace.</p> <p>Note : Attention \u00e0 la confusion avec le terme masked. Le Masked Language Model (MLM) n'utilise pas de couche masked self-attention alors que GPT (qui n'est pas un MLM) en utilise.</p> <p>Note 2 : Un parall\u00e8le tr\u00e8s int\u00e9ressant est fait entre BERT et un denoising autoencoder. En effet, quand on y refl\u00e9chit, l'id\u00e9e de BERT est de corrompre le texte d'entr\u00e9e en masquant certains tokens et de pr\u00e9dire le texte r\u00e9el. Pour le denoising autoencoder, on va corrompre l'image en rajoutant du bruit et pr\u00e9dire l'image r\u00e9elle. L'id\u00e9e est vraiment similaire mais en pratique, il y a quand m\u00eame une diff\u00e9rence : les denoising autoencoders vont reconstruire l'image enti\u00e8re alors que BERT va se contenter de pr\u00e9dire les tokens manquants sans toucher aux autres tokens de l'input.</p> <p>De nombreuses t\u00e2ches de NLP se basent sur des relations entre 2 phrases. Ce type de relation n'est pas directement captur\u00e9 par le language modeling et il est donc int\u00e9ressant d'ajouter un objectif sp\u00e9cifique pour la compr\u00e9hension de ces relations.</p> <p>Pour cela, BERT ajouter une pr\u00e9diction binaire de next sentence prediction. On a une phrase A et une phrase B, s\u00e9par\u00e9es par un [SEP] token, 50% du temps les phrases A et B vont \u00eatre des phrases qui se suivent dans le texte original et 50% du temps \u00e7a ne sera pas le cas. BERT va alors devoir pr\u00e9dire si ces phrases se suivent.</p> <p>Ce simple ajout d'objectif d'entra\u00eenement est tr\u00e8s b\u00e9n\u00e9fique si l'on souhaite finetune BERT sur des t\u00e2ches de r\u00e9ponses aux questions par exemple.</p> <p>L'article indique \u00e9galements les donn\u00e9es utilis\u00e9es pour l'entra\u00eenement. Avoir cette information est de plus en plus rare de nos jours. BERT a \u00e9t\u00e9 entra\u00een\u00e9 sur ces deux datasets :</p> <ul> <li>BooksCorpus (800 millions de mots) : Un dataset qui contient environ 7000 livres.</li> <li>English Wikipedia (2500 millions de mots) : Un dataset qui contient les textes issus de la version anglaise de wikipedia (juste le texte et pas les listes etc ...).</li> </ul> <p>Le finetuning de BERT est assez direct. On va simplement utiliser les inputs et outputs de la t\u00e2che que l'on souhaite faire et r\u00e9-entra\u00eener l'ensemble des param\u00e8tres du mod\u00e8le. On a deux grandes familles de t\u00e2ches :</p> <ul> <li>sentence-level prediction : Pour ces t\u00e2ches, on va utiliser le [CLS] token pour extraire la classification de la phrase. L'utilisation du [CLS] token permet de faire fonctionner le mod\u00e8le peu importe la taille de phrase d'entr\u00e9e (dans la limite du contexte bien s\u00fbr) et de ne pas avoir un biais  en fonction du choix du tokens. Sans le [CLS] token, on serait oblig\u00e9 d'utiliser une des ces 2 m\u00e9thodes : connecter l'ensemble des embeddings de sortie \u00e0 une couche fully connected pour obtenir la pr\u00e9diction (mais cela ne fonctionnera pas pour une taille de s\u00e9quence arbitraire) ou pr\u00e9dire \u00e0 partir de l'embedding d'un token de la phrase choisi au hasard (mais cela pourrait biaiser le r\u00e9sultat en fonction du token que l'on selectionne).</li> <li>token-level prediction : Pour cette t\u00e2che, on va simplement pr\u00e9dire une classe pour tous les embeddings de tokens que nous avons car on souhaite un label par token.</li> </ul> <p>Note : Finetune BERT ou un autre LLM est beaucoup moins couteux que de pr\u00e9-entrain\u00e9 le mod\u00e8le. Une fois que l'on a un mod\u00e8le pr\u00e9-entrain\u00e9, on peut le r\u00e9utiliser sur un grand nombre de t\u00e2ches \u00e0 moindre co\u00fbt.</p>"},{"location":"10_TransferLearningEtDistillation/05_FineTuningLLM.html#fine-tuning-llm","title":"Fine Tuning LLM\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/05_FineTuningLLM.html#bert-differences-avec-gpt","title":"BERT : diff\u00e9rences avec GPT\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/05_FineTuningLLM.html#tokens-et-embeddings","title":"Tokens et embeddings\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/05_FineTuningLLM.html#comment-pre-entrainer-bert","title":"Comment pr\u00e9-entra\u00eener BERT ?\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/05_FineTuningLLM.html#tache-1-prediction-des-mots-masques","title":"T\u00e2che 1 : Pr\u00e9diction des mots masqu\u00e9s\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/05_FineTuningLLM.html#tache-2-prediction-de-la-prochaine-phrase","title":"T\u00e2che 2 : Pr\u00e9diction de la prochaine phrase\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/05_FineTuningLLM.html#donnees-utilisees-pour-lentrainement","title":"Donn\u00e9es utilis\u00e9es pour l'entra\u00eenement\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/05_FineTuningLLM.html#comment-finetune-bert","title":"Comment finetune BERT ?\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/06_FineTuningBertHF.html","title":"Fine tuning BERT avec Hugging Face","text":"<p>La library transformers de Hugging Face propose des outils pour finetune les mod\u00e8les de mani\u00e8re simple et efficace. Dans ce notebook, nous allons montrer l'utilisation de Hugging Face pour finetune BERT sur deux t\u00e2ches : named-entity recognition (token-level classification) et analyse de sentiment (sentence-level classification).</p> <p>Commen\u00e7ons par faire de la token-level classification sur une t\u00e2che de NER. Pour cela, nous utilisons le dataset CONLL. Pour l'exemple, nous allons prendre uniquement 1000 \u00e9l\u00e9ments de ce dataset.</p> In\u00a0[27]: Copied! <pre>from datasets import load_dataset\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments,AutoModelForTokenClassification,AutoModelForSequenceClassification\nfrom transformers import DataCollatorForTokenClassification,DataCollatorWithPadding\nimport numpy as np\nimport evaluate\n</pre> from datasets import load_dataset from transformers import AutoTokenizer, Trainer, TrainingArguments,AutoModelForTokenClassification,AutoModelForSequenceClassification from transformers import DataCollatorForTokenClassification,DataCollatorWithPadding import numpy as np import evaluate In\u00a0[4]: Copied! <pre>dataset = load_dataset(\"eriktks/conll2003\",trust_remote_code=True)\n\n# 1000 \u00e9l\u00e9ments pour l'entra\u00eenement\nsub_train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))\n\n# 500 \u00e9l\u00e9ments pour l'\u00e9valuation\nsub_val_dataset = dataset['validation'].shuffle(seed=42).select(range(500)) # 500 examples for evaluation\n\nprint(sub_train_dataset['tokens'][0])\nprint(sub_train_dataset['ner_tags'][0])\n</pre> dataset = load_dataset(\"eriktks/conll2003\",trust_remote_code=True)  # 1000 \u00e9l\u00e9ments pour l'entra\u00eenement sub_train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))  # 500 \u00e9l\u00e9ments pour l'\u00e9valuation sub_val_dataset = dataset['validation'].shuffle(seed=42).select(range(500)) # 500 examples for evaluation  print(sub_train_dataset['tokens'][0]) print(sub_train_dataset['ner_tags'][0]) <pre>['\"', 'Neither', 'the', 'National', 'Socialists', '(', 'Nazis', ')', 'nor', 'the', 'communists', 'dared', 'to', 'kidnap', 'an', 'American', 'citizen', ',', '\"', 'he', 'shouted', ',', 'in', 'an', 'oblique', 'reference', 'to', 'his', 'extradition', 'to', 'Germany', 'from', 'Denmark', '.', '\"']\n[0, 0, 0, 7, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0]\n</pre> <p>On a notre s\u00e9quence de mot et la s\u00e9quence de labels correspondantes. Maintenant, associons nos labels aux classes : En NER, si plusieurs mots appartiennent \u00e0 la m\u00eame entit\u00e9, le premier mot aura la classe \"B-XXX\" et les mots suivants de l'entit\u00e9 la classe \"I-XXX\".</p> In\u00a0[3]: Copied! <pre># On va associer les labels \u00e0 des entiers\nitos={0: 'O', 1:'B-PER', 2:'I-PER',  3:'B-ORG',  4:'I-ORG',  5:'B-LOC',  6:'I-LOC', 7:'B-MISC', 8:'I-MISC'}\nstoi = {v: k for k, v in itos.items()}\nprint(stoi)\nprint(itos)\nlabel_names=list(itos.values())\n</pre> # On va associer les labels \u00e0 des entiers itos={0: 'O', 1:'B-PER', 2:'I-PER',  3:'B-ORG',  4:'I-ORG',  5:'B-LOC',  6:'I-LOC', 7:'B-MISC', 8:'I-MISC'} stoi = {v: k for k, v in itos.items()} print(stoi) print(itos) label_names=list(itos.values()) <pre>{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n</pre> <p>On va maintenant charger le tokenizer de BERT. C'est la classe qui permettra de convertir notre phrase en s\u00e9quence de tokens.</p> In\u00a0[5]: Copied! <pre>tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n</pre> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\") <p>Le tokenizer s'occupe de transformer la phrase en token mais il faut aussi adapter les labels. Chaque token doit avoir le bon label. Cette fonction permet d'associer les labels aux tokens.</p> In\u00a0[6]: Copied! <pre>def align_labels_with_tokens(labels, word_ids):\n  new_labels = []\n  current_word = None\n  for word_id in word_ids:\n    if word_id != current_word:\n      # D\u00e9but d'un nouveau mot\n      current_word = word_id\n      # -100 pour les tokens sp\u00e9ciaux\n      label = -100 if word_id is None else labels[word_id]\n      new_labels.append(label)\n    elif word_id is None:\n      # -100 pour les tokens sp\u00e9ciaux\n      new_labels.append(-100)\n    else:\n      # Les tokens du m\u00eame mot ont le m\u00eame label (sauf le premier)\n      label = labels[word_id]\n      # B pour le premier token du mot, I pour les suivants (cf itos)\n      if label % 2 == 1:\n        label += 1\n      new_labels.append(label)\n  return new_labels\n</pre> def align_labels_with_tokens(labels, word_ids):   new_labels = []   current_word = None   for word_id in word_ids:     if word_id != current_word:       # D\u00e9but d'un nouveau mot       current_word = word_id       # -100 pour les tokens sp\u00e9ciaux       label = -100 if word_id is None else labels[word_id]       new_labels.append(label)     elif word_id is None:       # -100 pour les tokens sp\u00e9ciaux       new_labels.append(-100)     else:       # Les tokens du m\u00eame mot ont le m\u00eame label (sauf le premier)       label = labels[word_id]       # B pour le premier token du mot, I pour les suivants (cf itos)       if label % 2 == 1:         label += 1       new_labels.append(label)   return new_labels <p>On peut maintenant transformer notre s\u00e9quence en tokens et avoir les labels correspondants :</p> In\u00a0[7]: Copied! <pre>def tokenize_and_align_labels(examples):\n  # On tokenise les phrases\n  tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True )\n  all_labels = examples[\"ner_tags\"]\n  new_labels = []\n  # On aligne les labels avec les tokens\n  for i, labels in enumerate(all_labels):\n    word_ids = tokenized_inputs.word_ids(i)\n    new_labels.append(align_labels_with_tokens(labels, word_ids))\n  tokenized_inputs[\"labels\"] = new_labels\n  return tokenized_inputs\n\n# On applique la fonction sur les donn\u00e9es de train et de validation\ntrain_tokenized_datasets = sub_train_dataset.map(\n  tokenize_and_align_labels,\n  batched=True,\n)\nval_tokenized_datasets = sub_val_dataset.map(\n  tokenize_and_align_labels,\n  batched=True,\n)\n</pre> def tokenize_and_align_labels(examples):   # On tokenise les phrases   tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True )   all_labels = examples[\"ner_tags\"]   new_labels = []   # On aligne les labels avec les tokens   for i, labels in enumerate(all_labels):     word_ids = tokenized_inputs.word_ids(i)     new_labels.append(align_labels_with_tokens(labels, word_ids))   tokenized_inputs[\"labels\"] = new_labels   return tokenized_inputs  # On applique la fonction sur les donn\u00e9es de train et de validation train_tokenized_datasets = sub_train_dataset.map(   tokenize_and_align_labels,   batched=True, ) val_tokenized_datasets = sub_val_dataset.map(   tokenize_and_align_labels,   batched=True, ) <pre>Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 12651.62 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:00&lt;00:00, 11565.07 examples/s]\n</pre> <p>Cr\u00e9ons notre mod\u00e8le BERT. HuggingFace permet de cr\u00e9er directement un mod\u00e8le pour la token-level classification avec AutoModelForTokenClassification.</p> In\u00a0[8]: Copied! <pre>model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-uncased\",id2label=itos, label2id=stoi) \n</pre> model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-uncased\",id2label=itos, label2id=stoi)  <pre>Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> <p>D\u00e9finissons aussi une fonction nous permettant de calculer l'accuracy et le f1-score sur nos donn\u00e9es de validation.</p> In\u00a0[9]: Copied! <pre>metric = evaluate.load(\"seqeval\")\n\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    # On supprime les labels -100\n    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"accuracy\": all_metrics[\"overall_accuracy\"],\n        \"f1\": all_metrics[\"overall_f1\"],\n    }\n</pre> metric = evaluate.load(\"seqeval\")  def compute_metrics(eval_preds):     logits, labels = eval_preds     predictions = np.argmax(logits, axis=-1)     # On supprime les labels -100     true_labels = [[label_names[l] for l in label if l != -100] for label in labels]     true_predictions = [         [label_names[p] for (p, l) in zip(prediction, label) if l != -100]         for prediction, label in zip(predictions, labels)     ]     all_metrics = metric.compute(predictions=true_predictions, references=true_labels)     return {         \"accuracy\": all_metrics[\"overall_accuracy\"],         \"f1\": all_metrics[\"overall_f1\"],     } <p>On est pr\u00eat \u00e0 entra\u00eener notre mod\u00e8le ! Pour cela on va utiliser le trainer de Hugging Face.</p> In\u00a0[10]: Copied! <pre># Pour param\u00e9trer l'entra\u00eenement, on peut changer tout un tas de param\u00e8tres mais ceux par d\u00e9faut sont souvent suffisants\nargs = TrainingArguments(\n    output_dir=\"./models\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"no\",\n    num_train_epochs=5,\n    weight_decay=0.01,\n)\n</pre> # Pour param\u00e9trer l'entra\u00eenement, on peut changer tout un tas de param\u00e8tres mais ceux par d\u00e9faut sont souvent suffisants args = TrainingArguments(     output_dir=\"./models\",     evaluation_strategy=\"no\",     save_strategy=\"no\",     num_train_epochs=5,     weight_decay=0.01, ) <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n</pre> In\u00a0[11]: Copied! <pre># la fonction DataCollatorForTokenClassification permet de rajouter du padding pour que les s\u00e9quences du batch aient la m\u00eame taille\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    data_collator=data_collator,\n    args=args,\n    train_dataset=train_tokenized_datasets, # Dataset d'entra\u00eenement\n    eval_dataset=val_tokenized_datasets, # Dataset d'\u00e9valuation\n    compute_metrics=compute_metrics, \n    tokenizer=tokenizer,\n)\ntrainer.train()\n</pre> # la fonction DataCollatorForTokenClassification permet de rajouter du padding pour que les s\u00e9quences du batch aient la m\u00eame taille data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)  trainer = Trainer(     model=model,     data_collator=data_collator,     args=args,     train_dataset=train_tokenized_datasets, # Dataset d'entra\u00eenement     eval_dataset=val_tokenized_datasets, # Dataset d'\u00e9valuation     compute_metrics=compute_metrics,      tokenizer=tokenizer, ) trainer.train() <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 501/625 [03:08&lt;00:46,  2.69it/s]</pre> <pre>{'loss': 0.1273, 'grad_norm': 11.809627532958984, 'learning_rate': 1e-05, 'epoch': 4.0}\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [03:55&lt;00:00,  2.66it/s]</pre> <pre>{'train_runtime': 235.0171, 'train_samples_per_second': 21.275, 'train_steps_per_second': 2.659, 'train_loss': 0.10341672458648682, 'epoch': 5.0}\n</pre> <pre>\n</pre> Out[11]: <pre>TrainOutput(global_step=625, training_loss=0.10341672458648682, metrics={'train_runtime': 235.0171, 'train_samples_per_second': 21.275, 'train_steps_per_second': 2.659, 'total_flos': 106538246287344.0, 'train_loss': 0.10341672458648682, 'epoch': 5.0})</pre> <p>L'entra\u00eenement est termin\u00e9, on peut \u00e9valuer notre mod\u00e8le sur les donn\u00e9es de validation :</p> In\u00a0[12]: Copied! <pre>trainer.evaluate()\n</pre> trainer.evaluate() <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63/63 [00:06&lt;00:00, 10.47it/s]\n</pre> Out[12]: <pre>{'eval_loss': 0.10586605966091156,\n 'eval_accuracy': 0.9793857803954564,\n 'eval_f1': 0.902547065337763,\n 'eval_runtime': 6.1292,\n 'eval_samples_per_second': 81.577,\n 'eval_steps_per_second': 10.279,\n 'epoch': 5.0}</pre> <p>On obtient de tr\u00e8s bon scores : accuracy de 0.98 et f1-score de 0.90.</p> <p>Maintenant, passons \u00e0 une t\u00e2che de type sentence-level classification : l'analyse de sentiment. Pour cela, nous utilisons le dataset imdb. C'est un dataset qui regroupe des critiques positives ou n\u00e9gatives de films. Le but du mod\u00e8le va \u00eatre de determiner sur la critique est positive ou n\u00e9gative.</p> In\u00a0[24]: Copied! <pre>dataset = load_dataset(\"stanfordnlp/imdb\",trust_remote_code=True)\n\n# 1000 \u00e9l\u00e9ments pour l'entra\u00eenement\nsub_train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))\n\n# 500 \u00e9l\u00e9ments pour l'\u00e9valuation\nsub_val_dataset = dataset['test'].shuffle(seed=42).select(range(500)) # 500 examples for evaluation\n\nprint(sub_train_dataset['text'][0])\nprint(sub_train_dataset['label'][0])\n\nitos={0: 'neg', 1:'pos'}\nstoi = {v: k for k, v in itos.items()}\n</pre> dataset = load_dataset(\"stanfordnlp/imdb\",trust_remote_code=True)  # 1000 \u00e9l\u00e9ments pour l'entra\u00eenement sub_train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))  # 500 \u00e9l\u00e9ments pour l'\u00e9valuation sub_val_dataset = dataset['test'].shuffle(seed=42).select(range(500)) # 500 examples for evaluation  print(sub_train_dataset['text'][0]) print(sub_train_dataset['label'][0])  itos={0: 'neg', 1:'pos'} stoi = {v: k for k, v in itos.items()} <pre>There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n1\n</pre> <p>On peut utiliser le m\u00eame tokenizer que pour la partie pr\u00e9c\u00e9dente pour extraire les tokens de notre texte. Ici, pas besoin de faire correspondre le label \u00e0 chaque token parce que le label concerne la phrase enti\u00e8re.</p> In\u00a0[21]: Copied! <pre>def preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True,is_split_into_words=False) \ntokenized_train_dataset = sub_train_dataset.map(preprocess_function, batched=True)\ntokenized_val_dataset = sub_val_dataset.map(preprocess_function, batched=True)\nprint(tokenized_train_dataset['input_ids'][0])\nprint(tokenized_train_dataset['label'][0])\n</pre>  def preprocess_function(examples):     return tokenizer(examples[\"text\"], truncation=True,is_split_into_words=False)  tokenized_train_dataset = sub_train_dataset.map(preprocess_function, batched=True) tokenized_val_dataset = sub_val_dataset.map(preprocess_function, batched=True) print(tokenized_train_dataset['input_ids'][0]) print(tokenized_train_dataset['label'][0]) <pre>Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 4040.25 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:00&lt;00:00, 5157.73 examples/s]\n</pre> <pre>[101, 2045, 2003, 2053, 7189, 2012, 2035, 2090, 3481, 3771, 1998, 6337, 2099, 2021, 1996, 2755, 2008, 2119, 2024, 2610, 2186, 2055, 6355, 6997, 1012, 6337, 2099, 3504, 15594, 2100, 1010, 3481, 3771, 3504, 4438, 1012, 6337, 2099, 14811, 2024, 3243, 3722, 1012, 3481, 3771, 1005, 1055, 5436, 2024, 2521, 2062, 8552, 1012, 1012, 1012, 3481, 3771, 3504, 2062, 2066, 3539, 8343, 1010, 2065, 2057, 2031, 2000, 3962, 12319, 1012, 1012, 1012, 1996, 2364, 2839, 2003, 5410, 1998, 6881, 2080, 1010, 2021, 2031, 1000, 17936, 6767, 7054, 3401, 1000, 1012, 2111, 2066, 2000, 12826, 1010, 2000, 3648, 1010, 2000, 16157, 1012, 2129, 2055, 2074, 9107, 1029, 6057, 2518, 2205, 1010, 2111, 3015, 3481, 3771, 3504, 2137, 2021, 1010, 2006, 1996, 2060, 2192, 1010, 9177, 2027, 9544, 2137, 2186, 1006, 999, 999, 999, 1007, 1012, 2672, 2009, 1005, 1055, 1996, 2653, 1010, 2030, 1996, 4382, 1010, 2021, 1045, 2228, 2023, 2186, 2003, 2062, 2394, 2084, 2137, 1012, 2011, 1996, 2126, 1010, 1996, 5889, 2024, 2428, 2204, 1998, 6057, 1012, 1996, 3772, 2003, 2025, 23105, 2012, 2035, 1012, 1012, 1012, 102]\n1\n</pre> <p>On peut cr\u00e9er notre mod\u00e8le.</p> In\u00a0[25]: Copied! <pre>model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2, id2label=itos, label2id=stoi)\n</pre> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2, id2label=itos, label2id=stoi) <pre>Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> <p>On peut maintenant cr\u00e9er notre fonction de calcul des performances :</p> In\u00a0[26]: Copied! <pre>accuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n    f1_score = f1.compute(predictions=predictions, references=labels,average=\"macro\")\n    return {\n        \"f1\": f1_score[\"f1\"],\n        \"accuracy\": accuracy_score[\"accuracy\"],\n    }\n</pre> accuracy = evaluate.load(\"accuracy\") f1 = evaluate.load(\"f1\")  def compute_metrics(eval_pred):     predictions, labels = eval_pred     predictions = np.argmax(predictions, axis=1)     accuracy_score = accuracy.compute(predictions=predictions, references=labels)     f1_score = f1.compute(predictions=predictions, references=labels,average=\"macro\")     return {         \"f1\": f1_score[\"f1\"],         \"accuracy\": accuracy_score[\"accuracy\"],     } <p>Et entrainer le mod\u00e8le :</p> In\u00a0[28]: Copied! <pre>training_args = TrainingArguments(\n    output_dir=\"models\",\n    num_train_epochs=5,\n    weight_decay=0.01,\n    eval_strategy=\"no\",\n    save_strategy=\"no\",\n)\n\n# Pad the inputs to the maximum length in the batch\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n</pre> training_args = TrainingArguments(     output_dir=\"models\",     num_train_epochs=5,     weight_decay=0.01,     eval_strategy=\"no\",     save_strategy=\"no\", )  # Pad the inputs to the maximum length in the batch data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  trainer = Trainer(     model=model,     args=training_args,     train_dataset=tokenized_train_dataset,     eval_dataset=tokenized_val_dataset,     tokenizer=tokenizer,     data_collator=data_collator,     compute_metrics=compute_metrics, ) trainer.train() <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 500/625 [12:05&lt;02:58,  1.43s/it]</pre> <pre>{'loss': 0.2295, 'grad_norm': 0.022515051066875458, 'learning_rate': 1e-05, 'epoch': 4.0}\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 625/625 [15:06&lt;00:00,  1.45s/it]</pre> <pre>{'train_runtime': 906.0393, 'train_samples_per_second': 5.519, 'train_steps_per_second': 0.69, 'train_loss': 0.1885655658721924, 'epoch': 5.0}\n</pre> <pre>\n</pre> Out[28]: <pre>TrainOutput(global_step=625, training_loss=0.1885655658721924, metrics={'train_runtime': 906.0393, 'train_samples_per_second': 5.519, 'train_steps_per_second': 0.69, 'total_flos': 613576571755968.0, 'train_loss': 0.1885655658721924, 'epoch': 5.0})</pre> <p>Evaluons notre mod\u00e8le :</p> In\u00a0[29]: Copied! <pre>trainer.evaluate()\n</pre> trainer.evaluate() <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63/63 [00:33&lt;00:00,  1.86it/s]\n</pre> Out[29]: <pre>{'eval_loss': 0.565979540348053,\n 'eval_f1': 0.8879354508196722,\n 'eval_accuracy': 0.888,\n 'eval_runtime': 34.4579,\n 'eval_samples_per_second': 14.51,\n 'eval_steps_per_second': 1.828,\n 'epoch': 5.0}</pre> <p>On obtient des bons scores : accuracy de 0.89 et f1-score de 0.89 \u00e9galement.</p>"},{"location":"10_TransferLearningEtDistillation/06_FineTuningBertHF.html#fine-tuning-bert-avec-hugging-face","title":"Fine tuning BERT avec Hugging Face\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/06_FineTuningBertHF.html#named-entity-recognition","title":"Named-entity recognition\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/06_FineTuningBertHF.html#analyse-de-sentiments","title":"Analyse de sentiments\u00b6","text":""},{"location":"10_TransferLearningEtDistillation/utils.html","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport torch\nfrom torch.utils.data import Dataset\nimport torch.nn.functional as F\nfrom torchvision import transforms as T\nimport cv2\nimport numpy as np\nimport glob\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.ndimage import gaussian_filter\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom skimage import morphology\nfrom skimage.segmentation import mark_boundaries\n</pre> import os import torch from torch.utils.data import Dataset import torch.nn.functional as F from torchvision import transforms as T import cv2 import numpy as np import glob from sklearn.metrics import roc_auc_score from scipy.ndimage import gaussian_filter import matplotlib.pyplot as plt import matplotlib from skimage import morphology from skimage.segmentation import mark_boundaries In\u00a0[\u00a0]: Copied! <pre>class MVTecDataset(Dataset):\n\n    def __init__(self, root_dir, resize_shape=None,crop_size=None,phase=\"train\"):\n        self.root_dir = root_dir\n        \n        image_extensions = ['png', 'tif', 'tiff', 'jpg', 'jpeg']\n        pattern = f\"{root_dir}/*/*\" + '/'.join(f\"*.{ext}\" for ext in image_extensions)\n\n        self.images = sorted(glob.glob(root_dir+\"/*/*.png\"))\n\n        self.image_paths = sorted(glob.glob(root_dir+\"/*.png\"))\n\n        self.resize_shape=resize_shape\n        if (crop_size==None):\n            crop_size=resize_shape[0]\n        self.transform=T.Compose([T.CenterCrop(crop_size)]) #,T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n        self.phase=phase\n        \n    def __len__(self):\n        if self.phase==\"test\":\n            return len(self.images)\n        else:\n            return len(self.image_paths)\n\n    def transform_image(self, image_path):\n        \n        \n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        \n        if self.resize_shape != None:\n            image = cv2.resize(image, dsize=(self.resize_shape[1], self.resize_shape[0]))\n            \n        image = np.array(image).reshape((image.shape[0], image.shape[1], 3)).astype(np.float32)/ 255.0\n        \n        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = np.transpose(image, (2, 0, 1))\n        image=np.asarray(self.transform(torch.from_numpy(image)))\n      \n        return image\n\n \n    def __getitem__(self, idx):\n        if self.phase==\"test\":\n            if torch.is_tensor(idx):\n                idx = idx.tolist()\n            img_path = self.images[idx]\n            dir_path, file_name = os.path.split(img_path)\n            base_dir = os.path.basename(dir_path)\n            image = self.transform_image(img_path)\n            if base_dir == 'good':\n                has_anomaly = np.array([0], dtype=np.int64)\n            else:\n                has_anomaly = np.array([1], dtype=np.int64)\n            sample = {'imageBase': image, 'has_anomaly': has_anomaly, 'idx': idx}\n        else:\n            idx = torch.randint(0, len(self.image_paths), (1,)).item()\n            image = self.transform_image(self.image_paths[idx])\n            sample = {'imageBase': image}\n        return sample\n</pre> class MVTecDataset(Dataset):      def __init__(self, root_dir, resize_shape=None,crop_size=None,phase=\"train\"):         self.root_dir = root_dir                  image_extensions = ['png', 'tif', 'tiff', 'jpg', 'jpeg']         pattern = f\"{root_dir}/*/*\" + '/'.join(f\"*.{ext}\" for ext in image_extensions)          self.images = sorted(glob.glob(root_dir+\"/*/*.png\"))          self.image_paths = sorted(glob.glob(root_dir+\"/*.png\"))          self.resize_shape=resize_shape         if (crop_size==None):             crop_size=resize_shape[0]         self.transform=T.Compose([T.CenterCrop(crop_size)]) #,T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])         self.phase=phase              def __len__(self):         if self.phase==\"test\":             return len(self.images)         else:             return len(self.image_paths)      def transform_image(self, image_path):                           image = cv2.imread(image_path, cv2.IMREAD_COLOR)                  if self.resize_shape != None:             image = cv2.resize(image, dsize=(self.resize_shape[1], self.resize_shape[0]))                      image = np.array(image).reshape((image.shape[0], image.shape[1], 3)).astype(np.float32)/ 255.0                  image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)         image = np.transpose(image, (2, 0, 1))         image=np.asarray(self.transform(torch.from_numpy(image)))                return image        def __getitem__(self, idx):         if self.phase==\"test\":             if torch.is_tensor(idx):                 idx = idx.tolist()             img_path = self.images[idx]             dir_path, file_name = os.path.split(img_path)             base_dir = os.path.basename(dir_path)             image = self.transform_image(img_path)             if base_dir == 'good':                 has_anomaly = np.array([0], dtype=np.int64)             else:                 has_anomaly = np.array([1], dtype=np.int64)             sample = {'imageBase': image, 'has_anomaly': has_anomaly, 'idx': idx}         else:             idx = torch.randint(0, len(self.image_paths), (1,)).item()             image = self.transform_image(self.image_paths[idx])             sample = {'imageBase': image}         return sample In\u00a0[\u00a0]: Copied! <pre>def computeAUROC(scores,gt_list,obj,name=\"base\"):\n  max_anomaly_score = scores.max()\n  min_anomaly_score = scores.min()\n  scores = (scores - min_anomaly_score) / (max_anomaly_score - min_anomaly_score)\n  img_scores = scores.reshape(scores.shape[0], -1).max(axis=1)\n  img_roc_auc = roc_auc_score(gt_list, img_scores)\n  print(obj + \" image\"+str(name)+\" ROCAUC: %.3f\" % (img_roc_auc))\n  return img_roc_auc,img_scores \n</pre> def computeAUROC(scores,gt_list,obj,name=\"base\"):   max_anomaly_score = scores.max()   min_anomaly_score = scores.min()   scores = (scores - min_anomaly_score) / (max_anomaly_score - min_anomaly_score)   img_scores = scores.reshape(scores.shape[0], -1).max(axis=1)   img_roc_auc = roc_auc_score(gt_list, img_scores)   print(obj + \" image\"+str(name)+\" ROCAUC: %.3f\" % (img_roc_auc))   return img_roc_auc,img_scores  In\u00a0[\u00a0]: Copied! <pre>@torch.no_grad()\ndef cal_anomaly_maps(fs_list, ft_list, out_size):\n  anomaly_map = 0\n  for i in range(len(ft_list)):\n    fs = fs_list[i]\n    ft = ft_list[i]\n    fs_norm = F.normalize(fs, p=2)\n    ft_norm = F.normalize(ft, p=2)\n\n    _, _, h, w = fs.shape\n\n    a_map = (0.5 * (ft_norm - fs_norm) ** 2) / (h * w)\n\n    a_map = a_map.sum(1, keepdim=True)\n\n    a_map = F.interpolate(\n        a_map, size=out_size, mode=\"bilinear\", align_corners=False\n    )\n    anomaly_map += a_map\n  anomaly_map = anomaly_map.squeeze().cpu().numpy()\n  for i in range(anomaly_map.shape[0]):\n    anomaly_map[i] = gaussian_filter(anomaly_map[i], sigma=4)\n\n  return anomaly_map\n</pre> @torch.no_grad() def cal_anomaly_maps(fs_list, ft_list, out_size):   anomaly_map = 0   for i in range(len(ft_list)):     fs = fs_list[i]     ft = ft_list[i]     fs_norm = F.normalize(fs, p=2)     ft_norm = F.normalize(ft, p=2)      _, _, h, w = fs.shape      a_map = (0.5 * (ft_norm - fs_norm) ** 2) / (h * w)      a_map = a_map.sum(1, keepdim=True)      a_map = F.interpolate(         a_map, size=out_size, mode=\"bilinear\", align_corners=False     )     anomaly_map += a_map   anomaly_map = anomaly_map.squeeze().cpu().numpy()   for i in range(anomaly_map.shape[0]):     anomaly_map[i] = gaussian_filter(anomaly_map[i], sigma=4)    return anomaly_map In\u00a0[\u00a0]: Copied! <pre>def plt_fig(\n    test_img, scores, img_scores, gts, threshold, cls_threshold, save_dir, class_name\n):\n    num = len(scores)\n    vmax = scores.max() * 255.0\n    vmin = scores.min() * 255.0\n    vmax = vmax * 0.5 + vmin * 0.5\n    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n    for i in range(num):\n        img = test_img[i]\n        gt = gts[i].squeeze()\n        heat_map = scores[i] * 255\n        mask = scores[i]\n        mask[mask &gt; threshold] = 1\n        mask[mask &lt;= threshold] = 0\n        kernel = morphology.disk(4)\n        mask = morphology.opening(mask, kernel)\n        mask *= 255\n        vis_img = mark_boundaries(img, mask, color=(1, 0, 0), mode=\"thick\")\n        fig_img, ax_img = plt.subplots(\n            1, 4, figsize=(9, 3), gridspec_kw={\"width_ratios\": [4, 4, 4, 3]}\n        )\n\n        fig_img.subplots_adjust(wspace=0.05, hspace=0)\n        for ax_i in ax_img:\n            ax_i.axes.xaxis.set_visible(False)\n            ax_i.axes.yaxis.set_visible(False)\n\n        ax_img[0].imshow(img)\n        ax_img[0].title.set_text(\"Input image\")\n        ax_img[1].imshow(gt, cmap=\"gray\")\n        ax_img[1].title.set_text(\"GroundTruth\")\n        ax_img[2].imshow(heat_map, cmap=\"jet\", norm=norm, interpolation=\"none\")\n        # ax_img[2].imshow(vis_img, cmap='gray', alpha=0.7, interpolation='none')\n        ax_img[2].imshow(img, cmap=\"gray\", alpha=0.7, interpolation=\"none\")\n        ax_img[2].title.set_text(\"Segmentation\")\n        black_mask = np.zeros((int(mask.shape[0]), int(3 * mask.shape[1] / 4)))\n        ax_img[3].imshow(black_mask, cmap=\"gray\")\n        ax = plt.gca()\n        if img_scores[i] &gt; cls_threshold:\n            cls_result = \"nok\"\n        else:\n            cls_result = \"ok\"\n\n        ax.text(\n            0.05,\n            0.89,\n            \"Detected anomalies\",\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.79,\n            \"------------------------\",\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.72,\n            \"Results\",\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.67,\n            \"------------------------\",\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.59,\n            \"'{}'\".format(cls_result),\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"r\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.47,\n            \"Anomaly scores: {:.2f}\".format(img_scores[i]),\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.37,\n            \"------------------------\",\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.30,\n            \"Thresholds\",\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.25,\n            \"------------------------\",\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.17,\n            \"Classification: {:.2f}\".format(cls_threshold),\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax.text(\n            0.05,\n            0.07,\n            \"Segmentation: {:.2f}\".format(threshold),\n            verticalalignment=\"bottom\",\n            horizontalalignment=\"left\",\n            transform=ax.transAxes,\n            fontdict=dict(\n                fontsize=8,\n                color=\"w\",\n                family=\"sans-serif\",\n            ),\n        )\n        ax_img[3].title.set_text(\"Classification\")\n        plt.show()\n        # fig_img.savefig(\n        #     os.path.join(save_dir, class_name + \"_{}\".format(i)),\n        #     dpi=300,\n        #     bbox_inches=\"tight\",\n        # )\n        plt.close()\n</pre> def plt_fig(     test_img, scores, img_scores, gts, threshold, cls_threshold, save_dir, class_name ):     num = len(scores)     vmax = scores.max() * 255.0     vmin = scores.min() * 255.0     vmax = vmax * 0.5 + vmin * 0.5     norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)     for i in range(num):         img = test_img[i]         gt = gts[i].squeeze()         heat_map = scores[i] * 255         mask = scores[i]         mask[mask &gt; threshold] = 1         mask[mask &lt;= threshold] = 0         kernel = morphology.disk(4)         mask = morphology.opening(mask, kernel)         mask *= 255         vis_img = mark_boundaries(img, mask, color=(1, 0, 0), mode=\"thick\")         fig_img, ax_img = plt.subplots(             1, 4, figsize=(9, 3), gridspec_kw={\"width_ratios\": [4, 4, 4, 3]}         )          fig_img.subplots_adjust(wspace=0.05, hspace=0)         for ax_i in ax_img:             ax_i.axes.xaxis.set_visible(False)             ax_i.axes.yaxis.set_visible(False)          ax_img[0].imshow(img)         ax_img[0].title.set_text(\"Input image\")         ax_img[1].imshow(gt, cmap=\"gray\")         ax_img[1].title.set_text(\"GroundTruth\")         ax_img[2].imshow(heat_map, cmap=\"jet\", norm=norm, interpolation=\"none\")         # ax_img[2].imshow(vis_img, cmap='gray', alpha=0.7, interpolation='none')         ax_img[2].imshow(img, cmap=\"gray\", alpha=0.7, interpolation=\"none\")         ax_img[2].title.set_text(\"Segmentation\")         black_mask = np.zeros((int(mask.shape[0]), int(3 * mask.shape[1] / 4)))         ax_img[3].imshow(black_mask, cmap=\"gray\")         ax = plt.gca()         if img_scores[i] &gt; cls_threshold:             cls_result = \"nok\"         else:             cls_result = \"ok\"          ax.text(             0.05,             0.89,             \"Detected anomalies\",             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.79,             \"------------------------\",             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.72,             \"Results\",             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.67,             \"------------------------\",             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.59,             \"'{}'\".format(cls_result),             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"r\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.47,             \"Anomaly scores: {:.2f}\".format(img_scores[i]),             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.37,             \"------------------------\",             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.30,             \"Thresholds\",             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.25,             \"------------------------\",             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.17,             \"Classification: {:.2f}\".format(cls_threshold),             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax.text(             0.05,             0.07,             \"Segmentation: {:.2f}\".format(threshold),             verticalalignment=\"bottom\",             horizontalalignment=\"left\",             transform=ax.transAxes,             fontdict=dict(                 fontsize=8,                 color=\"w\",                 family=\"sans-serif\",             ),         )         ax_img[3].title.set_text(\"Classification\")         plt.show()         # fig_img.savefig(         #     os.path.join(save_dir, class_name + \"_{}\".format(i)),         #     dpi=300,         #     bbox_inches=\"tight\",         # )         plt.close()"},{"location":"11_ModelesGeneratifs/index.html","title":"\ud83c\udf00 Mod\u00e8les g\u00e9n\u00e9ratifs \ud83c\udf00","text":"<p>Ce cours introduit le principe de mod\u00e8les g\u00e9n\u00e9ratifs par opposition aux mod\u00e8les discriminatifs. Les 4 grandes familles de mod\u00e8les g\u00e9n\u00e9ratifs sont pr\u00e9sent\u00e9es et impl\u00e9ment\u00e9es : les GAN, les VAE, les normalizing flow et les mod\u00e8les de diffusion. Les mod\u00e8les autoregressif ne sont pas abord\u00e9s car ceux-ci ont \u00e9t\u00e9 d\u00e9crits dans le cours NLP et Transformers.</p>"},{"location":"11_ModelesGeneratifs/index.html#notebook-1-introduction","title":"Notebook 1\ufe0f\u20e3 : Introduction","text":"<p>Ce notebook introduit la diff\u00e9rence entre mod\u00e8le discriminatifs et g\u00e9n\u00e9ratifs et propose un plan du cours.</p>"},{"location":"11_ModelesGeneratifs/index.html#notebook-2-gan","title":"Notebook 2\ufe0f\u20e3 : GAN","text":"<p>Ce notebook pr\u00e9sente l'architecture des generative adversarial networks(GAN), leurs avantages, faiblesses et utilisations possibles.</p>"},{"location":"11_ModelesGeneratifs/index.html#notebook-3-gan-implementation","title":"Notebook 3\ufe0f\u20e3 : Gan Implementation","text":"<p>Ce notebook propose une impl\u00e9mentation d'un GAN pour la g\u00e9n\u00e9ration de chiffres 5 \u00e0 partir du dataset MNIST.</p>"},{"location":"11_ModelesGeneratifs/index.html#notebook-4-vae","title":"Notebook 4\ufe0f\u20e3 : VAE","text":"<p>Ce notebook pr\u00e9sente l'architecture des variational autoencoders (VAE) en commen\u00e7ant par un rappel sur les autoencodeurs puis en d\u00e9crivant l'intuite et la m\u00e9thode derri\u00e8re l'architecture VAE.</p>"},{"location":"11_ModelesGeneratifs/index.html#notebook-5-vae-implementation","title":"Notebook 5\ufe0f\u20e3 : Vae Implementation","text":"<p>Ce notebook propose une impl\u00e9mentation d'un VAE pour la g\u00e9n\u00e9ration de chiffres \u00e0 partir du dataset MNIST.</p>"},{"location":"11_ModelesGeneratifs/index.html#notebook-6-normalizing-flows","title":"Notebook 6\ufe0f\u20e3 : Normalizing Flows","text":"<p>Ce notebook pr\u00e9sente l'architecture des normalizing flows.</p>"},{"location":"11_ModelesGeneratifs/index.html#notebook-7-diffusion-models","title":"Notebook 7\ufe0f\u20e3 : Diffusion Models","text":"<p>Ce notebook pr\u00e9sente l'architecture des diffusion models ainsi que leurs avantages et incov\u00e9nients.</p>"},{"location":"11_ModelesGeneratifs/index.html#notebook-8-diffusion-implementation","title":"Notebook 8\ufe0f\u20e3 : Diffusion Implementation","text":"<p>Ce notebook propose une impl\u00e9mentation simple d'un mod\u00e8le de diffusion pour g\u00e9n\u00e9rer des \u00e9l\u00e9ments sur le dataset MNIST.</p>"},{"location":"11_ModelesGeneratifs/01_Introduction.html","title":"Introductions aux mod\u00e8les g\u00e9n\u00e9ratifs","text":"<p>Dans les cours pr\u00e9c\u00e9dents, nous avons vu divers exemples de mod\u00e8les de deep learning. Parmi ces mod\u00e8les, certains \u00e9taient des mod\u00e8les discriminatifs et d'autres des mod\u00e8les g\u00e9n\u00e9ratifs. Ce cours introduit clairement la notion de mod\u00e8les g\u00e9n\u00e9ratifs et propose plusieurs examples et impl\u00e9mentations.</p> <p>D\u00e9finissons les termes de mod\u00e8les discriminatifs et g\u00e9n\u00e9ratifs :</p> <ul> <li>Mod\u00e8le discriminatif : Un mod\u00e8le discriminatif a pour objectif de faire la distinction entre diff\u00e9rents types de donn\u00e9es. Lors de l'entra\u00eenement de ce type de mod\u00e8le, on a des donn\u00e9es d'entr\u00e9es $X$ et les labels $Y$ correspondants. L'objectif du mod\u00e8le va \u00eatre de cat\u00e9goriser une nouvelle donn\u00e9es d'entr\u00e9e. Comme exemple de ce type de mod\u00e8les, on peut citer la classification (image, texte, son), la d\u00e9tection d'objets et la segmentation.</li> <li>Mod\u00e8le g\u00e9n\u00e9ratif : Un mod\u00e8le g\u00e9n\u00e9ratif a pour objectif d'apprendre la distribution des donn\u00e9es. On pourra ensuite utiliser ce mod\u00e8le pour g\u00e9n\u00e9rer des nouveaux \u00e9l\u00e9ments similaires aux \u00e9l\u00e9ments des donn\u00e9es d'entra\u00eenement. A l'inverse des mod\u00e8les discriminatifs, on ne dispose pas de labels pendant l'entra\u00eenement. Jusqu'ici, les seuls mod\u00e8les g\u00e9n\u00e9ratifs que nous avons vu sont ceux du cours 5 sur le NLP.</li> </ul> <p>Note :  Les autoencodeurs ne peuvent pas s'ins\u00e9rer dans une des deux cat\u00e9gories car ils ne pr\u00e9disent pas de labels et ils n'apprennent pas la distribution de probabilit\u00e9 des donn\u00e9es d'entr\u00e9e. Cependant, les autoencodeurs variationnels se base sur une architecture visuellement similaire et permettent d'apprendre la distribution de probabilit\u00e9 des donn\u00e9es d'entra\u00eenement.</p> <p>Plus formellement, les mod\u00e8les discriminatifs vont apprendre la probabilit\u00e9 conditionnelle $P(X \\mid Y)$ alors que les mod\u00e8les g\u00e9n\u00e9ratifs vont apprendre $P(X)$ ou $P(X,Y)$ si on a un label.</p> <p>Ce cours va pr\u00e9senter les grandes familles de mod\u00e8les g\u00e9n\u00e9ratifs et proposer des impl\u00e9mentations pour chacun d'entre eux :</p> <ul> <li>GAN : Les notebook 2 et 3 sont consacr\u00e9s aux Generative Adversarial Networks. Les GAN se basent sur un principe de d'entra\u00eenement de deux mod\u00e8les concurrents : un g\u00e9n\u00e9rateur qui tente de g\u00e9n\u00e9rer des donn\u00e9es similaires \u00e0 la distribution des donn\u00e9es d'entrainement et un discriminateur qui cherche \u00e0 faire diff\u00e9rencier les donn\u00e9es d'entrainement r\u00e9elles et les donn\u00e9es g\u00e9n\u00e9r\u00e9es par le g\u00e9n\u00e9rateur.</li> <li>VAE : Les notebooks 4 et 5 sont consacr\u00e9s aux Variational Autoencoders. Les Variational Autoencoders (VAE) reprennent l'id\u00e9e de base de l'autoencodeur, mais au lieu d'apprendre une repr\u00e9sentation d\u00e9terministe dans l'espace latent, ils apprennent une distribution de probabilit\u00e9.</li> <li>Normalizing Flows : Les notebooks 6 et 7 sont consacr\u00e9s aux Normalizing Flows. Les Normalizing Flows utilisent des transformations bijectives pour passer d'une distribution de probabilit\u00e9 simple, comme une distribution gaussienne, \u00e0 la distribution des donn\u00e9es d'entra\u00eenement.</li> <li>Diffusion models : Les notebooks 4 et 5 sont consacr\u00e9s aux Diffusion models. Les Diffusion models entra\u00eenent un r\u00e9seau \u00e0 supprimer un l\u00e9ger bruit d'une image et sont appliqu\u00e9s de mani\u00e8re r\u00e9p\u00e9t\u00e9e pour g\u00e9n\u00e9rer une image \u00e0 partir d'un bruit gaussien.</li> </ul> <p></p> <p>Note : Ce cours n'aborde pas les mod\u00e8les autoregressifs (type GPT) car ceux-ci ont \u00e9t\u00e9 d\u00e9crit et impl\u00e9ment\u00e9 en d\u00e9tail dans cours 5 sur le NLP.</p> <p>Note 2 : Ce cours vise \u00e0 pr\u00e9senter les mod\u00e8les g\u00e9n\u00e9ratifs sans entrer dans les d\u00e9tails. Si vous souhaitez en apprendre plus sur ce type de mod\u00e8les, je vous conseille le cours de stanford CS236 : site du cours et playlist youtube.</p>"},{"location":"11_ModelesGeneratifs/01_Introduction.html#introductions-aux-modeles-generatifs","title":"Introductions aux mod\u00e8les g\u00e9n\u00e9ratifs\u00b6","text":""},{"location":"11_ModelesGeneratifs/01_Introduction.html#discriminatif-ou-generatif","title":"Discriminatif ou g\u00e9n\u00e9ratif ?\u00b6","text":""},{"location":"11_ModelesGeneratifs/01_Introduction.html#contenu-du-cours","title":"Contenu du cours\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html","title":"Generative Adversarial Networks","text":"<p>Les mod\u00e8les antagonistes g\u00e9n\u00e9ratifs ou Generative Adversarial Networks (GAN) sont une famille de mod\u00e8les g\u00e9n\u00e9ratifs. De mani\u00e8re assez \u00e9vidente, les mod\u00e8les g\u00e9n\u00e9ratifs sont des mod\u00e8les non supervis\u00e9s, c'est-\u00e0-dire, que l'on ne dispose pas de labels pendant l'entra\u00eenement (on peut avoir des labels mais ceux-ci ne constitue pas notre objectif d'entra\u00eenement). Les GAN introduisent une astuce pour transformer ce probl\u00e8me non supervis\u00e9 en un probl\u00e8me supervis\u00e9.</p> <p>Le principe de base derri\u00e8re l'architecture GAN est d'utiliser deux r\u00e9seaux de neurones que l'on va entra\u00eener de mani\u00e8re concurrente. Les deux mod\u00e8les sont les suivants :</p> <ul> <li>Le mod\u00e8le g\u00e9n\u00e9rateur : Ce mod\u00e8le a pour objectif de g\u00e9n\u00e9rer des exemples qui sont similaires aux exemples des donn\u00e9es d'entra\u00eenement. Il prend en entr\u00e9e un vecteur al\u00e9atoire (issu d'une distribution gaussienne) de taille fixe et g\u00e9n\u00e9re un exemple \u00e0 partir de cette entr\u00e9e.</li> <li>Le mod\u00e8le discriminateur : Ce mod\u00e8le a pour objectif de classifier un exemple comme \u00e9tant r\u00e9el (issu des donn\u00e9es r\u00e9elles d'entrainement) ou faux (g\u00e9n\u00e9r\u00e9 par le mod\u00e8le g\u00e9n\u00e9rateur). C'est un simple classifier comme ceux que l'on a pu voir dans les cours pr\u00e9c\u00e9dents.</li> </ul> <p></p> <p>Figure extraite de blogpost.</p> <p>Ces mod\u00e8les sont entrain\u00e9s de mani\u00e8re concurrentes dans ce qu'on appelle un zero-sum game. Notre objectif d'entra\u00eenement est que le mod\u00e8le discriminatif soit incapables de d\u00e9terminer si un exemple est r\u00e9el ou g\u00e9n\u00e9r\u00e9 (il nous renvoie la probabilit\u00e9 $\\frac{1}{2}$ sur chaque \u00e9l\u00e9ment). Cela signifiera que notre g\u00e9n\u00e9rateur est capable de g\u00e9n\u00e9rer des exemples plausibles. Dans ce cas l\u00e0, le mod\u00e8le g\u00e9n\u00e9rateur a appris la distribution de probabilit\u00e9 des donn\u00e9es d'entra\u00eenement.</p> <p>Lors de l'entra\u00eenement, le g\u00e9n\u00e9rateur va g\u00e9n\u00e9rer un batch d'exemples que l'on va m\u00e9langer avec des exemples r\u00e9els (issus des donn\u00e9es d'entrainement). Ces exemples sont donn\u00e9s aux discriminateur qui va les classifier. Le discriminateur est ensuite mis \u00e0 jour par descente du gradient en fonction de sa performance sur la classification. Le g\u00e9n\u00e9rateur est ensuite mis \u00e0 jour en fonction de sa performance de tromperie du discriminateur.</p> <p>On peut faire une analogie o\u00f9 le g\u00e9n\u00e9rateur serait un faussaire et le discriminateur serait la police. Le but du faussaire est de tromper la police en g\u00e9n\u00e9rant des faux billets le plus parfait possible. A l'inverse, la police va cr\u00e9er de nouvelles techniques pour d\u00e9terminer si les billets sont vrais ou faux. De cette mani\u00e8re, le faussaire et la police vont progresser conjointement.</p> <p>Je trouve personnellement que l'id\u00e9e derri\u00e8re les GAN peut s'appliquer \u00e0 beaucoup de situations de la vie r\u00e9elle. On a tendance \u00e0 donner le meilleur de nous-m\u00eame dans l'adversit\u00e9.</p> <p>Note : En g\u00e9n\u00e9ral, dans l'entrainement d'un GAN, on est interess\u00e9 principalement par le g\u00e9n\u00e9rateur. Le discriminateur n'est alors utilis\u00e9 que pour l'entrainement. Cependant, comme la quantit\u00e9 d'images pour un entra\u00eenement non supervis\u00e9 est beaucoup plus importante que les images avec labels (les images non supervis\u00e9es sont en gros l'ensemble des images disponibles sur internet), il est parfois int\u00e9ressant d'entra\u00eener un GAN sur une grande quantit\u00e9 d'images et d'utiliser le discriminateur et le g\u00e9n\u00e9rateur comme mod\u00e8les pr\u00e9-entrain\u00e9 que l'on va adapter \u00e0 d'autres t\u00e2ches.</p> <p>La grande majorit\u00e9 des GAN sont des r\u00e9seaux de neurones convolutifs et ce pour plusieurs raisons. D'une part, il est assez facile pour un utilisateur humain d'\u00e9valuer la qualit\u00e9 d'une image g\u00e9n\u00e9r\u00e9e, ce qui permet une \u00e9valuation directe et intuitive des performances du mod\u00e8le. D'autre part, empiriquement, les GAN ont demontr\u00e9s des performances impressionnantes pour tout ce qui est g\u00e9n\u00e9ration d'image mais moins pour des t\u00e2ches diff\u00e9rentes comme la g\u00e9n\u00e9ration de texte.</p> <p>L'architecture de la plupart des GAN se base sur le papier DCGAN.</p> <p>Note : L'invention des GAN est ant\u00e9rieure \u00e0 l'invention de l'architecture transformer. R\u00e9cemment des architectures type GAN ont \u00e9t\u00e9 propos\u00e9es avec un transformer \u00e0 la place du CNN.</p> <p>Note 2 : Il y a un aspect th\u00e9orique important derri\u00e8re les GAN mais nous n'allons pas entrer dans les d\u00e9tails dans ce cours. Pour en apprendre plus, vous pouvez vous r\u00e9ferrer au cours CS236 de stanford et en particulier \u00e0 ce lien.</p> <p>L'architecture que nous avons vu permet de g\u00e9n\u00e9rer une image r\u00e9aliste \u00e0 partir d'un vecteur al\u00e9atoire sample depuis une distribution gaussienne. Cependant, nous n'avons aucun contr\u00f4le sur l'image g\u00e9n\u00e9r\u00e9e. Si on entrain\u00e9 le mod\u00e8le sur des images de personnes, le mod\u00e8le va g\u00e9n\u00e9rer une personne avec des attributs al\u00e9atoire (sexe, yeux, cheveux, peau etc...). Cela n'est pas tr\u00e8s pratique car dans de nombreux cas, on cherche \u00e0 g\u00e9n\u00e9rer une image sp\u00e9cifique et pas seulement une image r\u00e9aliste al\u00e9atoire.</p> <p>Pour palier \u00e0 ce probl\u00e8me, on peut utiliser ce qu'on appelle un conditional GAN qui reprend l'architecture classique du GAN mais dans lequel on donne en entr\u00e9e du g\u00e9n\u00e9rateur et du discriminateur une information sur la donn\u00e9es.</p> <p></p> <p>Figure extraite du blogpost</p> <p>De cette mani\u00e8re, on peut diriger la g\u00e9n\u00e9ration \u00e0 l'aide d'un label ce qui va nous permettre de g\u00e9n\u00e9rer des images contenant des attributs sp\u00e9cifiques.</p> <p>L'architecture GAN est une tr\u00e8s bonne id\u00e9e et fonctionne tr\u00e8s bien en pratique lorsque le mod\u00e8le est bien entrain\u00e9. Dans les cours pr\u00e9c\u00e9dents, nous avons insist\u00e9 sur la difficult\u00e9 \u00e0 entra\u00eener un mod\u00e8le de deep learning et nous avons pr\u00e9sent\u00e9 de nombreuses techniques pour faciliter l'entra\u00eenement. Ici, nous avons deux mod\u00e8les \u00e0 entra\u00eener en m\u00eame temps et de mani\u00e8re adversaire... C'est \u00e7a le principal probl\u00e8me des GAN : En pratique, ils sont tr\u00e8s compliqu\u00e9 \u00e0 entra\u00eener.</p> <p>Le probl\u00e8me principal qui fait frissonner tous les utilisateurs de GAN est le mode collapse. Cela se produit quand le g\u00e9n\u00e9rateur va apprendre \u00e0 produire une vari\u00e9t\u00e9 limit\u00e9e de r\u00e9sultats souvent tr\u00e8s similaires les uns aux autres. Dans ce cas l\u00e0, le g\u00e9n\u00e9rateur n'a pas r\u00e9ussi \u00e0 capturer la diversit\u00e9 des donn\u00e9es d'entra\u00eenement mais parvient quand m\u00eame \u00e0 duper le discriminateur syst\u00e9matiquement. On peut imaginer que le g\u00e9n\u00e9rateur a appris \u00e0 g\u00e9n\u00e9rer une image parfaite mais qu'il ne peut g\u00e9n\u00e9rer que cette image.</p> <p>Ce probl\u00e8me d\u00e9coule directement de l'objectif d'entrainement du GAN et il est tr\u00e8s difficile \u00e0 g\u00e9rer.</p> <p>Lors de l'entra\u00eenement, on souhaite que le g\u00e9n\u00e9rateur et le discriminateur progresse conjointement. Cependant, il peut arriver qu'un des deux mod\u00e8les progresse plus rapidement que l'autre ce qui peut causer des comportements chaotiques lors de l'entra\u00eenement.</p> <p>Il y a \u00e9galement d'autres probl\u00e8mes que l'on peut citer rapidement :</p> <ul> <li>Probl\u00e8me de convergence : Le mod\u00e8le peut du mal \u00e0 converger vers une solution stable m\u00eame apr\u00e8s un long entra\u00eenement.</li> <li>Choix de l'architecture des r\u00e9seaux : Il faut choisir une architecture coh\u00e9rente pour \u00e0 la fois le g\u00e9n\u00e9rateur et le discriminateur.</li> </ul> <p>Quelques strat\u00e9gies sont utilisables pour limiter ces probl\u00e8mes mais ce n'est rien de magique. Pour stabiliser l'entra\u00eenement, on peut utiliser un Wasserstein GAN ou/et un entra\u00eenement plus progressif.</p> <p>Il est possible d'utiliser les GAN pour faire de la super-r\u00e9solution, c'est-\u00e0-dire d'augmenter la r\u00e9solution d'une image. Le papier Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network propose une architecture de GAN permettant cela.</p> <p></p> <p>La data-augmentation consiste \u00e0 augmenter artificillement les donn\u00e9es d'entra\u00eenement via diverses techniques. Si ce n'est pas d\u00e9j\u00e0 fait, vous pouvez faire le cours bonus sur la data augmentation.</p> <p>En y r\u00e9fl\u00e9chissant, la data augmentation de base (crop, rotate etc ...) peut \u00eatre vue comme un sorte de generative modeling o\u00f9 l'on va g\u00e9n\u00e9rer des images d'entra\u00eenement proches de la distribution des images normales.</p> <p>En partant de cette consid\u00e9ration, il est assez \u00e9vident de voir comme un GAN peut nous aider pour la data augmentation. Si on veut classifier les chats, notre GAN va pouvoir nous g\u00e9n\u00e9rer des images de chats en quantit\u00e9.</p> <p>Une autre utilisation commune des GAN est le Image-to-Image translation introduit dans le papier Image-to-Image Translation with Conditional Adversarial Networks. Cela permet de convertir une image d'un domaine source vers un domaine cible tout en gardant les correspondances structurelles et contextuelles (en fonction de l'entra\u00eenement).</p> <p></p> <p>Note : On peut aussi utiliser le Image-to-Image translation pour la data augmentation. Supposons que l'on g\u00e9n\u00e9re des fausses donn\u00e9es \u00e0 l'aide d'un logiciel de jeux vid\u00e9os comme Unity et que l'on souhaite rendre ces images r\u00e9alistes. Alors il suffit d'entra\u00eener un GAN de transfert de style permettant de passer d'une image \u00e0 une autre.</p>"},{"location":"11_ModelesGeneratifs/02_GAN.html#generative-adversarial-networks","title":"Generative Adversarial Networks\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#principe-de-base","title":"Principe de base\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#deux-modeles-concurrents","title":"Deux mod\u00e8les concurrents\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#zero-sum-game","title":"Zero-sum game\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#architecture-du-gan","title":"Architecture du GAN\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#conditional-gan","title":"Conditional GAN\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#problemes-des-gan","title":"Probl\u00e8mes des GAN\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#mode-collapse","title":"Mode Collapse\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#equilibre-entre-generateur-et-discriminateur","title":"Equilibre entre g\u00e9n\u00e9rateur et discriminateur\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#exemples-dutilisation-des-gan","title":"Exemples d'utilisation des GAN\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#gan-pour-la-super-resolution","title":"GAN pour la super r\u00e9solution\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#gan-pour-la-data-augmentation","title":"GAN pour la data augmentation\u00b6","text":""},{"location":"11_ModelesGeneratifs/02_GAN.html#image-to-image-translation","title":"Image-to-Image translation\u00b6","text":""},{"location":"11_ModelesGeneratifs/03_GanImplementation.html","title":"Impl\u00e9mentation d'un GAN","text":"<p>Il est temps de passer \u00e0 l'impl\u00e9mentation d'un GAN. Pour cela, nous allons nous baser sur le papier Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks pour g\u00e9n\u00e9rer des images de chiffres 5 ressemblant \u00e0 ceux du dataset MNIST.</p> <p></p> <p>Architecture du generateur de DCGAN.</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader,Subset\nimport random\n</pre> import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt import torchvision.datasets as datasets import torchvision.transforms as transforms from torch.utils.data import DataLoader,Subset import random  <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Commen\u00e7ons par charger notre dataset MNIST :</p> In\u00a0[2]: Copied! <pre>transform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32,32)),\n])\ntrain_data = datasets.MNIST(root='./../data', train=True, transform=transform, download=True)\ntest_data = datasets.MNIST(root='./../data', train=False, transform=transform, download=True)\n\n\nindices = [i for i, label in enumerate(train_data.targets) if label == 5]\n# On cr\u00e9er un nouveau dataset avec uniquement les 5\ntrain_data = Subset(train_data, indices)\n\n# all_indices = list(range(len(train_data)))\n# random.shuffle(all_indices)\n# selected_indices = all_indices[:5000]\n# train_data = Subset(train_data, selected_indices)\n\n\nprint(\"taille du dataset d'entrainement : \",len(train_data))\nprint(\"taille d'une image : \",train_data[0][0].numpy().shape) \n\ntrain_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)\n</pre> transform=transforms.Compose([     transforms.ToTensor(),     transforms.Resize((32,32)), ]) train_data = datasets.MNIST(root='./../data', train=True, transform=transform, download=True) test_data = datasets.MNIST(root='./../data', train=False, transform=transform, download=True)   indices = [i for i, label in enumerate(train_data.targets) if label == 5] # On cr\u00e9er un nouveau dataset avec uniquement les 5 train_data = Subset(train_data, indices)  # all_indices = list(range(len(train_data))) # random.shuffle(all_indices) # selected_indices = all_indices[:5000] # train_data = Subset(train_data, selected_indices)   print(\"taille du dataset d'entrainement : \",len(train_data)) print(\"taille d'une image : \",train_data[0][0].numpy().shape)   train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True) test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False) <pre>taille du dataset d'entrainement :  5421\ntaille d'une image :  (1, 32, 32)\n</pre> In\u00a0[3]: Copied! <pre># Visualisons quelques images\nplt.figure(figsize=(10, 10))\nfor i in range(5):\n  plt.subplot(1, 5, i+1)\n  plt.imshow(train_data[i][0].squeeze(), cmap='gray')\n  plt.axis('off')\n  plt.title(train_data[i][1])\n</pre> # Visualisons quelques images plt.figure(figsize=(10, 10)) for i in range(5):   plt.subplot(1, 5, i+1)   plt.imshow(train_data[i][0].squeeze(), cmap='gray')   plt.axis('off')   plt.title(train_data[i][1]) <p>On peut maintenant impl\u00e9menter nos deux mod\u00e8les. Commen\u00e7ons par regarder les sp\u00e9cificit\u00e9s d'architecture d\u00e9crites dans la papier.</p> <p></p> <p>A partir de ces informations et de la figure du papier (voir plus haut dans le notebook), on peut construire notre mod\u00e8le g\u00e9n\u00e9rateur. Comme on travaille sur des images de taille $28 \\times 28$ au lieu de $64 \\times 64$ dans le papier, on va faire une architecture plus r\u00e9duite.</p> <p>Note : Dans le papier, les auteurs disent utiliser des fractional-strided convolutions. Il s'agit en fait de convolutions transpos\u00e9es et le terme fractional-strided convolutions n'est plus vraiment utilis\u00e9 de nos jours.</p> In\u00a0[4]: Copied! <pre>def convT_bn_relu(in_channels, out_channels, kernel_size, stride, padding):\n  return nn.Sequential(\n    nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding,bias=False),\n    nn.BatchNorm2d(out_channels),\n    nn.ReLU()\n  )\n\nclass generator(nn.Module):\n  def __init__(self, z_dim=100,features_g=64):\n    super(generator, self).__init__()\n    self.gen = nn.Sequential(\n      convT_bn_relu(z_dim, features_g*8, kernel_size=4, stride=1, padding=0),\n      convT_bn_relu(features_g*8, features_g*4, kernel_size=4, stride=2, padding=1),\n      convT_bn_relu(features_g*4, features_g*2, kernel_size=4, stride=2, padding=1),\n      nn.ConvTranspose2d(features_g*2, 1, kernel_size=4, stride=2, padding=1),\n      nn.Tanh()\n    )\n  def forward(self, x):\n    return self.gen(x)\n  \nz= torch.randn(64,100,1,1)\ngen = generator()\nimg = gen(z)\nprint(img.shape)\n</pre> def convT_bn_relu(in_channels, out_channels, kernel_size, stride, padding):   return nn.Sequential(     nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding,bias=False),     nn.BatchNorm2d(out_channels),     nn.ReLU()   )  class generator(nn.Module):   def __init__(self, z_dim=100,features_g=64):     super(generator, self).__init__()     self.gen = nn.Sequential(       convT_bn_relu(z_dim, features_g*8, kernel_size=4, stride=1, padding=0),       convT_bn_relu(features_g*8, features_g*4, kernel_size=4, stride=2, padding=1),       convT_bn_relu(features_g*4, features_g*2, kernel_size=4, stride=2, padding=1),       nn.ConvTranspose2d(features_g*2, 1, kernel_size=4, stride=2, padding=1),       nn.Tanh()     )   def forward(self, x):     return self.gen(x)    z= torch.randn(64,100,1,1) gen = generator() img = gen(z) print(img.shape) <pre>torch.Size([64, 1, 32, 32])\n</pre> <p>Le papier ne d\u00e9crit pas directement l'architecture du discriminateur. Nous allons globalement reprendre l'architecture du g\u00e9n\u00e9rateur mais dans l'autre sens.</p> In\u00a0[5]: Copied! <pre>def conv_bn_lrelu(in_channels, out_channels, kernel_size, stride, padding):\n  return nn.Sequential(\n    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,bias=False),\n    nn.BatchNorm2d(out_channels),\n    nn.LeakyReLU()\n  )\n\nclass discriminator(nn.Module):\n  def __init__(self, features_d=64) -&gt; None:\n    super().__init__()\n    self.discr = nn.Sequential(\n      conv_bn_lrelu(1, features_d, kernel_size=3, stride=2, padding=1),\n      conv_bn_lrelu(features_d, features_d*2, kernel_size=3, stride=2, padding=1),\n      conv_bn_lrelu(features_d*2, features_d*4, kernel_size=3, stride=2, padding=1),\n      nn.Conv2d(256, 1, kernel_size=3, stride=2, padding=0),\n      nn.Sigmoid()\n    )\n    \n  def forward(self, x):\n    return self.discr(x)\ndummy = torch.randn(64,1,32,32)\ndisc = discriminator()\nout = disc(dummy)\nprint(out.shape)\n</pre> def conv_bn_lrelu(in_channels, out_channels, kernel_size, stride, padding):   return nn.Sequential(     nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,bias=False),     nn.BatchNorm2d(out_channels),     nn.LeakyReLU()   )  class discriminator(nn.Module):   def __init__(self, features_d=64) -&gt; None:     super().__init__()     self.discr = nn.Sequential(       conv_bn_lrelu(1, features_d, kernel_size=3, stride=2, padding=1),       conv_bn_lrelu(features_d, features_d*2, kernel_size=3, stride=2, padding=1),       conv_bn_lrelu(features_d*2, features_d*4, kernel_size=3, stride=2, padding=1),       nn.Conv2d(256, 1, kernel_size=3, stride=2, padding=0),       nn.Sigmoid()     )        def forward(self, x):     return self.discr(x) dummy = torch.randn(64,1,32,32) disc = discriminator() out = disc(dummy) print(out.shape)  <pre>torch.Size([64, 1, 1, 1])\n</pre> <p>Il est temps de passer aux choses s\u00e9rieuses. La boucle d'entra\u00eenement d'un GAN est bien plus complexe que les boucles d'entra\u00eenements des mod\u00e8les que nous avons vu jusqu'\u00e0 pr\u00e9sent. Commen\u00e7ons par d\u00e9finir nos hyperparam\u00e8tres d'entra\u00eenement et par initialiser nos mod\u00e8les :</p> In\u00a0[6]: Copied! <pre>epochs = 50\nlr=0.001\nz_dim = 100\nfeatures_d = 64\nfeatures_g = 64\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ngen = generator(z_dim, features_g).to(device)\ndisc = discriminator(features_d).to(device)\n\nopt_gen = torch.optim.Adam(gen.parameters(), lr=lr)\nopt_disc = torch.optim.Adam(disc.parameters(), lr=lr*0.05)\ncriterion = nn.BCELoss()\n</pre> epochs = 50 lr=0.001 z_dim = 100 features_d = 64 features_g = 64  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') gen = generator(z_dim, features_g).to(device) disc = discriminator(features_d).to(device)  opt_gen = torch.optim.Adam(gen.parameters(), lr=lr) opt_disc = torch.optim.Adam(disc.parameters(), lr=lr*0.05) criterion = nn.BCELoss()  <p>Nous allons \u00e9galement cr\u00e9er un bruit fixed_noise pour \u00e9valuer visuellement notre mod\u00e8le \u00e0 chaque \u00e9tape d'entra\u00eenement.</p> In\u00a0[7]: Copied! <pre>fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)\n</pre> fixed_noise = torch.randn(64, z_dim, 1, 1, device=device) <p>Avant de consttuire notre boucle d'entra\u00eenement, r\u00e9capitulons les \u00e9tapes que nous devons faire :</p> <ul> <li>On commence par r\u00e9cuperer batch_size \u00e9l\u00e9ments du dataset d'entra\u00eenement et on pr\u00e9dit les labels avec notre discriminateur</li> <li>Ensuite, on g\u00e9n\u00e9re batch_size \u00e9l\u00e9ments avec notre g\u00e9n\u00e9rateur et on pr\u00e9dit les labels</li> <li>On va ensuite mettre \u00e0 jour les poids du mod\u00e8le discriminateur \u00e0 partir des deux loss</li> <li>On va ensuite \u00e0 nouveau pr\u00e9dire les labels des donn\u00e9es g\u00e9n\u00e9r\u00e9es (car on a mis \u00e0 jour le discriminateur entre temps)</li> <li>A partir de ces valeurs, on calcule le loss et on met \u00e0 jour notre g\u00e9n\u00e9rateur</li> </ul> In\u00a0[8]: Copied! <pre>all_fake_images = []\nfor epoch in range(epochs):\n  lossD_epoch = 0\n  lossG_epoch = 0\n  for real_images,_ in train_loader:\n    real_images=real_images.to(device)\n    pred_real = disc(real_images).view(-1)\n    lossD_real = criterion(pred_real, torch.ones_like(pred_real)) # Les labels sont 1 pour les vraies images\n    \n    batch_size = real_images.shape[0]\n    input_noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n    fake_images = gen(input_noise)\n    pred_fake = disc(fake_images.detach()).view(-1)\n    lossD_fake = criterion(pred_fake, torch.zeros_like(pred_fake)) # Les labels sont 0 pour les fausses images\n\n    lossD=lossD_real + lossD_fake\n    lossD_epoch += lossD.item()\n    disc.zero_grad()\n    lossD.backward()\n    opt_disc.step()\n    \n    # On refait l'inf\u00e9rence pour les images g\u00e9n\u00e9r\u00e9es (avec le discriminateur mis \u00e0 jour)\n    pred_fake = disc(fake_images).view(-1)\n    lossG=criterion(pred_fake, torch.ones_like(pred_fake)) # On veut que le g\u00e9n\u00e9rateur trompe le discriminateur donc on veut que les labels soient 1\n    lossG_epoch += lossG.item()\n    gen.zero_grad()\n    lossG.backward()\n    opt_gen.step()\n    \n  # On g\u00e9n\u00e8re des images avec le g\u00e9n\u00e9rateur\n  if epoch % 10 == 0 or epoch==0:\n    print(f\"Epoch [{epoch}/{epochs}] Loss D: {lossD_epoch/len(train_loader):.4f}, loss G: {lossG_epoch/len(train_loader):.4f}\")\n    gen.eval()\n    fake_images = gen(fixed_noise)\n    all_fake_images.append(fake_images)\n    #cv2.imwrite(f\"gen/image_base_gan_{epoch}.png\", fake_images[0].squeeze().detach().cpu().numpy()*255.0)\n    gen.train()\n    \n</pre> all_fake_images = [] for epoch in range(epochs):   lossD_epoch = 0   lossG_epoch = 0   for real_images,_ in train_loader:     real_images=real_images.to(device)     pred_real = disc(real_images).view(-1)     lossD_real = criterion(pred_real, torch.ones_like(pred_real)) # Les labels sont 1 pour les vraies images          batch_size = real_images.shape[0]     input_noise = torch.randn(batch_size, z_dim, 1, 1, device=device)     fake_images = gen(input_noise)     pred_fake = disc(fake_images.detach()).view(-1)     lossD_fake = criterion(pred_fake, torch.zeros_like(pred_fake)) # Les labels sont 0 pour les fausses images      lossD=lossD_real + lossD_fake     lossD_epoch += lossD.item()     disc.zero_grad()     lossD.backward()     opt_disc.step()          # On refait l'inf\u00e9rence pour les images g\u00e9n\u00e9r\u00e9es (avec le discriminateur mis \u00e0 jour)     pred_fake = disc(fake_images).view(-1)     lossG=criterion(pred_fake, torch.ones_like(pred_fake)) # On veut que le g\u00e9n\u00e9rateur trompe le discriminateur donc on veut que les labels soient 1     lossG_epoch += lossG.item()     gen.zero_grad()     lossG.backward()     opt_gen.step()        # On g\u00e9n\u00e8re des images avec le g\u00e9n\u00e9rateur   if epoch % 10 == 0 or epoch==0:     print(f\"Epoch [{epoch}/{epochs}] Loss D: {lossD_epoch/len(train_loader):.4f}, loss G: {lossG_epoch/len(train_loader):.4f}\")     gen.eval()     fake_images = gen(fixed_noise)     all_fake_images.append(fake_images)     #cv2.imwrite(f\"gen/image_base_gan_{epoch}.png\", fake_images[0].squeeze().detach().cpu().numpy()*255.0)     gen.train()      <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n  return F.conv_transpose2d(\n</pre> <pre>Epoch [0/50] Loss D: 0.4726, loss G: 2.0295\nEpoch [10/50] Loss D: 0.1126, loss G: 3.6336\nEpoch [20/50] Loss D: 0.0767, loss G: 4.0642\nEpoch [30/50] Loss D: 0.0571, loss G: 4.5766\nEpoch [40/50] Loss D: 0.0178, loss G: 5.3689\n</pre> <p>Visualizons les images g\u00e9n\u00e9r\u00e9es lors de l'entra\u00eenement.</p> In\u00a0[36]: Copied! <pre>index=0\nimage_begin = all_fake_images[0][index]\nimage_mid = all_fake_images[len(all_fake_images)//2][index]\nimage_end = all_fake_images[-1][index]\n\n# Cr\u00e9ation de la figure\nplt.figure(figsize=(10, 5))\n\n# Affichage de l'image du d\u00e9but de l'entra\u00eenement\nplt.subplot(1, 3, 1)\nplt.imshow(image_begin.squeeze().detach().cpu().numpy(), cmap='gray')\nplt.axis('off')\nplt.title(\"D\u00e9but de l'entrainement\")\n\n# Affichage de l'image du milieu de l'entra\u00eenement\nplt.subplot(1, 3, 2)\nplt.imshow(image_mid.squeeze().detach().cpu().numpy(), cmap='gray')\nplt.axis('off')\nplt.title(\"Milieu de l'entrainement\")\n\n# Affichage de l'image de la fin de l'entra\u00eenement\nplt.subplot(1, 3, 3)\nplt.imshow(image_end.squeeze().detach().cpu().numpy(), cmap='gray')\nplt.axis('off')\nplt.title(\"Fin de l'entrainement\")\n\n# Affichage de la figure\nplt.tight_layout()\nplt.show()\n</pre> index=0 image_begin = all_fake_images[0][index] image_mid = all_fake_images[len(all_fake_images)//2][index] image_end = all_fake_images[-1][index]  # Cr\u00e9ation de la figure plt.figure(figsize=(10, 5))  # Affichage de l'image du d\u00e9but de l'entra\u00eenement plt.subplot(1, 3, 1) plt.imshow(image_begin.squeeze().detach().cpu().numpy(), cmap='gray') plt.axis('off') plt.title(\"D\u00e9but de l'entrainement\")  # Affichage de l'image du milieu de l'entra\u00eenement plt.subplot(1, 3, 2) plt.imshow(image_mid.squeeze().detach().cpu().numpy(), cmap='gray') plt.axis('off') plt.title(\"Milieu de l'entrainement\")  # Affichage de l'image de la fin de l'entra\u00eenement plt.subplot(1, 3, 3) plt.imshow(image_end.squeeze().detach().cpu().numpy(), cmap='gray') plt.axis('off') plt.title(\"Fin de l'entrainement\")  # Affichage de la figure plt.tight_layout() plt.show() <p>On voit que notre g\u00e9n\u00e9rateur est maintenant capable de g\u00e9n\u00e9rer des chiffres ressemblant vaguement \u00e0 des 5. Si vous \u00eatre courageux, vous pouvez essayer d'am\u00e9liorer le mod\u00e8le et de l'entra\u00eener sur tous les chiffres de MNIST par exemple. Un bon exercice peut aussi \u00eatre d'impl\u00e9menter un conditional GAN.</p>"},{"location":"11_ModelesGeneratifs/03_GanImplementation.html#implementation-dun-gan","title":"Impl\u00e9mentation d'un GAN\u00b6","text":""},{"location":"11_ModelesGeneratifs/03_GanImplementation.html#dataset","title":"Dataset\u00b6","text":""},{"location":"11_ModelesGeneratifs/03_GanImplementation.html#creation-de-notre-modele","title":"Cr\u00e9ation de notre mod\u00e8le\u00b6","text":""},{"location":"11_ModelesGeneratifs/03_GanImplementation.html#entrainement-du-modele","title":"Entrainement du mod\u00e8le\u00b6","text":""},{"location":"11_ModelesGeneratifs/04_VAE.html","title":"Variational autoencoders","text":"<p>Dans ce cours, nous pr\u00e9sentons les autoencodeurs variationnels ou variational autoencoders (VAE). Le cours commence par un rappel rapide du cours 4 sur les autoencodeurs puis introduit l'utilisation des VAE en tant que mod\u00e8le g\u00e9n\u00e9ratif. Ce cours s'inspire du blogpost et n'entre pas dans les d\u00e9tails math\u00e9matiques du fonctionnement du VAE. Les figures utilis\u00e9es dans ce notebook sont \u00e9galement extraite du blogpost.</p> <p>Un autoencodeur est un r\u00e9seau de neurones en forme de sablier constitu\u00e9 d'en encodeur qui encode l'information dans un espace latent de dimension r\u00e9duite et d'un d\u00e9codeur qui reconstruit la donn\u00e9e initiale \u00e0 partir de la r\u00e9presentation latente.</p> <p></p> <p>Les autoencodeurs peuvent \u00eatre utilis\u00e9 pour de nombreuses choses mais leur r\u00f4le de base est la compression de donn\u00e9es. C'est une m\u00e9thode de compression de donn\u00e9es utilisant l'optimisation par descente du gradient.</p> <p>On peut imaginer que si l'espace latent de notre d\u00e9codeur est r\u00e9gulier (qu'il est represent\u00e9 par une distribution de probabilit\u00e9 connue), on pourrait sample un \u00e9l\u00e9ment al\u00e9atoire de cette distribution pour g\u00e9n\u00e9rer une nouvelle donn\u00e9e. En pratique, dans un autoencodeur classique, la repr\u00e9sentation latente n'est pas du tout r\u00e9guli\u00e8re et il est impossible de l'utiliser pour g\u00e9n\u00e9rer des donn\u00e9es.</p> <p>En y reflechissant, c'est tout \u00e0 fait logique, la fonction de loss de l'autoencodeur se base uniquement sur la qualit\u00e9 de la reconstruction et n'impose aucune contrainte sur la forme de l'espace latent.</p> <p>On voudrait pouvoir imposer la forme de l'espace latent de notre autoencodeur pour g\u00e9n\u00e9rer des donn\u00e9es nouvelles \u00e0 partir de cet espace latent.</p> <p>Un variational autoencoder (VAE) peut \u00eatre d\u00e9crit comme un autoencodeur contraint \u00e0 avoir un espace latent permettant la g\u00e9n\u00e9ration de donn\u00e9es et qui a un entra\u00eenement r\u00e9gularis\u00e9 dans cet objectif.</p> <p>L'id\u00e9e va \u00eatre d'encoder notre input en une distrubution de donn\u00e9es au lieu d'une simple valeur (comme dans l'AE). En pratique, notre encodeur va pr\u00e9dire deux valeurs r\u00e9presentant une distribution normale : la moyenne $\\mu$ et la variance $\\sigma^2$</p> <p>Le VAE fonctionne de la mani\u00e8re suivante lors de l'entra\u00eenement :</p> <ul> <li>L'encodeur encode l'input en une distribution de probabilit\u00e9 en pr\u00e9disant $\\mu$ et $\\sigma^2$.</li> <li>Une valeur est sample de la distribution gaussienne d\u00e9crite par $\\mu$ et $\\sigma^2$.</li> <li>Le d\u00e9codeur reconstruit la donn\u00e9es \u00e0 partir de la valeur sampled.</li> <li>On applique la backpropagation pour mettre \u00e0 jour les poids.</li> </ul> <p></p> <p>Pour garantir que l'entra\u00eenement fait bien ce que l'on veut, il est n\u00e9cessaire d'ajouter un terme \u00e0 la fonction de loss : la divergence de Kullback-Leibler. Ce terme va permettre de pousser la distribution \u00e0 \u00eatre une distribution centr\u00e9e r\u00e9duite.</p> <p>Pour avoir une g\u00e9n\u00e9ration de donn\u00e9es coh\u00e9rentes, il y a deux choses \u00e0 prendre en compte :</p> <ul> <li>La continuit\u00e9 : Des points proches dans l'espace latent vont produire des donn\u00e9es proches dans l'espace de sortie.</li> <li>La compl\u00e9tude : Les points d\u00e9cod\u00e9s doivent avoir du sens dans l'espace de sortie.</li> </ul> <p>La divergence de Kullback-Leibler va permettre de garantir ces deux propri\u00e9t\u00e9s. Si on se contentait du loss de reconstruction, le VAE pourrait se compoter comme un AE en pr\u00e9disant des variances presque nulles (ce qui serait presque \u00e9quivalent \u00e0 un point comme ce que l'on pr\u00e9dit avec un encoder d'un AE).</p> <p>La divergence de Kullback-Leibler va encourager les distributions de l'espace latent \u00e0 \u00eatre proches les unes des autres ce qui permet de g\u00e9n\u00e9rer des donn\u00e9es toujours coh\u00e9rentes lorsque l'on sample.</p> <p></p> <p>Note : Il y a un aspect th\u00e9orique important derri\u00e8re les Variational autoencoders mais nous n'allons pas entrer dans les d\u00e9tails dans ce cours. Pour en apprendre plus, vous pouvez vous r\u00e9ferrer au cours CS236 de stanford et en particulier \u00e0 ce lien.</p>"},{"location":"11_ModelesGeneratifs/04_VAE.html#variational-autoencoders","title":"Variational autoencoders\u00b6","text":""},{"location":"11_ModelesGeneratifs/04_VAE.html#rappel-autoencodeur","title":"Rappel autoencodeur\u00b6","text":""},{"location":"11_ModelesGeneratifs/04_VAE.html#intuition","title":"Intuition\u00b6","text":""},{"location":"11_ModelesGeneratifs/04_VAE.html#variational-autoencoder","title":"Variational autoencoder\u00b6","text":""},{"location":"11_ModelesGeneratifs/05_VaeImplementation.html","title":"Impl\u00e9mentation d'un VAE","text":"<p>Dans ce notebook, nous allons impl\u00e9menter un VAE pour la g\u00e9n\u00e9ration d'images sur le dataset MNIST. Le notebook commence par une impl\u00e9mentation d'une Autoencodeur classique pour montrer qu'une architecture d'AE classique ne permet pas de g\u00e9n\u00e9rer des \u00e9l\u00e9ments nouveaux.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import random import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as T from torchvision import datasets from torch.utils.data import DataLoader import matplotlib.pyplot as plt <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Commen\u00e7ons par charger notre dataset MNIST :</p> In\u00a0[2]: Copied! <pre>transform = T.Compose([\n    T.ToTensor(),\n    T.Normalize((0.5,), (0.5,))\n])\ndataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform)\ntest_dataset = datasets.MNIST(root='./../data', train=False,transform=transform)\n\nprint(\"taille du dataset d'entrainement : \",len(dataset))\nprint(\"taille d'une image : \",dataset[0][0].numpy().shape) \n\ntrain_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n</pre> transform = T.Compose([     T.ToTensor(),     T.Normalize((0.5,), (0.5,)) ]) dataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform) test_dataset = datasets.MNIST(root='./../data', train=False,transform=transform)  print(\"taille du dataset d'entrainement : \",len(dataset)) print(\"taille d'une image : \",dataset[0][0].numpy().shape)   train_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2]) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) <pre>taille du dataset d'entrainement :  60000\ntaille d'une image :  (1, 28, 28)\n</pre> In\u00a0[21]: Copied! <pre># Visualisons quelques images\nplt.figure(figsize=(10, 10))\nfor i in range(5):\n  plt.subplot(1, 5, i+1)\n  plt.imshow(dataset[i][0].squeeze(), cmap='gray')\n  plt.axis('off')\n  plt.title(dataset[i][1])\n</pre> # Visualisons quelques images plt.figure(figsize=(10, 10)) for i in range(5):   plt.subplot(1, 5, i+1)   plt.imshow(dataset[i][0].squeeze(), cmap='gray')   plt.axis('off')   plt.title(dataset[i][1]) <p>Construisons l'architecture de notre autoencodeur :</p> In\u00a0[22]: Copied! <pre>class AE(nn.Module):\n    def __init__(self):\n        super(AE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # -&gt; [16, 14, 14]\n            nn.ReLU(),\n            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # -&gt; [8, 7, 7]\n            nn.ReLU(),\n            nn.Conv2d(8, 8, 3, stride=2, padding=1)   # -&gt; [8, 4, 4]\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(8, 8, 3, stride=2, padding=1, output_padding=0),\n            nn.ReLU(),\n            nn.ConvTranspose2d(8, 16, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n            nn.Tanh()\n        )\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\ndummy_input = torch.randn(1, 1, 28, 28) \nmodel = AE()\noutput = model(dummy_input)\nprint(output.shape)\n</pre> class AE(nn.Module):     def __init__(self):         super(AE, self).__init__()         self.encoder = nn.Sequential(             nn.Conv2d(1, 16, 3, stride=2, padding=1),  # -&gt; [16, 14, 14]             nn.ReLU(),             nn.Conv2d(16, 8, 3, stride=2, padding=1),  # -&gt; [8, 7, 7]             nn.ReLU(),             nn.Conv2d(8, 8, 3, stride=2, padding=1)   # -&gt; [8, 4, 4]         )         self.decoder = nn.Sequential(             nn.ConvTranspose2d(8, 8, 3, stride=2, padding=1, output_padding=0),             nn.ReLU(),             nn.ConvTranspose2d(8, 16, 3, stride=2, padding=1, output_padding=1),             nn.ReLU(),             nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),             nn.Tanh()         )     def forward(self, x):         x = self.encoder(x)         x = self.decoder(x)         return x dummy_input = torch.randn(1, 1, 28, 28)  model = AE() output = model(dummy_input) print(output.shape) <pre>torch.Size([1, 1, 28, 28])\n</pre> <p>D\u00e9finissons nos hyperparam\u00e8tres d'entra\u00eenement :</p> In\u00a0[23]: Copied! <pre>epochs = 10\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> epochs = 10 criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) <p>Passons \u00e0 l'entra\u00eenement du mod\u00e8le :</p> In\u00a0[24]: Copied! <pre>for epoch in range(epochs): \n    for img,_ in train_loader:\n        optimizer.zero_grad()\n        recon = model(img)\n        loss = criterion(recon, img)\n        loss.backward()\n        optimizer.step()\n    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, epochs, loss.item()))\n</pre> for epoch in range(epochs):      for img,_ in train_loader:         optimizer.zero_grad()         recon = model(img)         loss = criterion(recon, img)         loss.backward()         optimizer.step()     print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, epochs, loss.item())) <pre>epoch [1/10], loss:0.0330\nepoch [2/10], loss:0.0220\nepoch [3/10], loss:0.0199\nepoch [4/10], loss:0.0186\nepoch [5/10], loss:0.0171\nepoch [6/10], loss:0.0172\nepoch [7/10], loss:0.0175\nepoch [8/10], loss:0.0168\nepoch [9/10], loss:0.0159\nepoch [10/10], loss:0.0148\n</pre> <p>V\u00e9rifions le comportement du mod\u00e8le sur les donn\u00e9es de test :</p> In\u00a0[25]: Copied! <pre>for data in test_loader:\n    img, _ = data\n    recon = model(img)\n    break\nplt.figure(figsize=(9, 2))\nplt.gray()\nplt.subplot(1, 2, 1)\nplt.imshow(img[0].detach().numpy().squeeze())\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(recon[0].detach().numpy().squeeze())\nplt.axis('off')\nplt.show()\n</pre> for data in test_loader:     img, _ = data     recon = model(img)     break plt.figure(figsize=(9, 2)) plt.gray() plt.subplot(1, 2, 1) plt.imshow(img[0].detach().numpy().squeeze()) plt.axis('off') plt.subplot(1, 2, 2) plt.imshow(recon[0].detach().numpy().squeeze()) plt.axis('off') plt.show() <p>On va maintenant visualiser l'espace latent et la r\u00e9partition de nos 10 classes dans cet espace.</p> In\u00a0[26]: Copied! <pre># On commence par extraire les repr\u00e9sentations latentes des donn\u00e9es de test\nlatents = []\nlabels = []\n\nwith torch.no_grad():\n    for data, target in test_loader:\n        latent = model.encoder(data)\n        latents.append(latent)\n        labels.append(target)\n\nlatents = torch.cat(latents)\nlabels = torch.cat(labels)\n</pre> # On commence par extraire les repr\u00e9sentations latentes des donn\u00e9es de test latents = [] labels = []  with torch.no_grad():     for data, target in test_loader:         latent = model.encoder(data)         latents.append(latent)         labels.append(target)  latents = torch.cat(latents) labels = torch.cat(labels) <p>On utilise la m\u00e9thode T-SNE pour extraire des r\u00e9presentations en 2D permettant de visualiser les donn\u00e9es.</p> In\u00a0[27]: Copied! <pre>from sklearn.manifold import TSNE\n\nlatents_flat = latents.view(latents.size(0), -1)\ntsne = TSNE(n_components=2, random_state=0)\nlatent_2d = tsne.fit_transform(latents_flat)\n</pre> from sklearn.manifold import TSNE  latents_flat = latents.view(latents.size(0), -1) tsne = TSNE(n_components=2, random_state=0) latent_2d = tsne.fit_transform(latents_flat) In\u00a0[28]: Copied! <pre>plt.figure(figsize=(10, 8))\nscatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='viridis', alpha=0.5)\nplt.colorbar(scatter)\nplt.title('Visualisation de l\\'espace latent de MNIST avec un autoencodeur CNN')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.show()\n</pre> plt.figure(figsize=(10, 8)) scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='viridis', alpha=0.5) plt.colorbar(scatter) plt.title('Visualisation de l\\'espace latent de MNIST avec un autoencodeur CNN') plt.xlabel('Dimension 1') plt.ylabel('Dimension 2') plt.show() <p>Comme on pouvait s'y attendre, il y a bien une s\u00e9paration entre les classes dans l'espace latent. Par contre, il y a \u00e9galement beaucoup d'espaces vides si bien que l'on peut difficilement sample un point quelconque de l'espace latent et esperer g\u00e9n\u00e9rer une donn\u00e9e r\u00e9elle coh\u00e9rente.</p> <p>On va regarder ce que l'on obtient si l'on g\u00e9n\u00e9re une image \u00e0 partir d'un point al\u00e9atoire de l'espace latent.</p> In\u00a0[29]: Copied! <pre>latent_dim = (8, 4, 4)\nsampled_latent = torch.randn(latent_dim).unsqueeze(0)\n\n# On g\u00e9n\u00e9rer l'image avec le d\u00e9codeur\nwith torch.no_grad():\n    generated_image = model.decoder(sampled_latent)\n\ngenerated_image = generated_image.squeeze().numpy()  # Supprimer la dimension batch et convertir en numpy\ngenerated_image = (generated_image + 1) / 2  # D\u00e9normaliser l'image (car Tanh est utilis\u00e9)\nplt.imshow(generated_image, cmap='gray')\nplt.title(\"Image g\u00e9n\u00e9r\u00e9e\")\nplt.axis('off')\nplt.show()\n</pre> latent_dim = (8, 4, 4) sampled_latent = torch.randn(latent_dim).unsqueeze(0)  # On g\u00e9n\u00e9rer l'image avec le d\u00e9codeur with torch.no_grad():     generated_image = model.decoder(sampled_latent)  generated_image = generated_image.squeeze().numpy()  # Supprimer la dimension batch et convertir en numpy generated_image = (generated_image + 1) / 2  # D\u00e9normaliser l'image (car Tanh est utilis\u00e9) plt.imshow(generated_image, cmap='gray') plt.title(\"Image g\u00e9n\u00e9r\u00e9e\") plt.axis('off') plt.show() <p>Comme on s'y attendait, cela ne g\u00e9n\u00e9re rien de coh\u00e9rent.</p> <p>Maintenant, reprenons la m\u00eame architecture (plus ou moins) mais avec l'architecture VAE pour voir si l'on est capable de g\u00e9n\u00e9rer des donn\u00e9es.</p> In\u00a0[70]: Copied! <pre>class VAE(nn.Module):\n    def __init__(self,latent_dim=8):\n        super(VAE, self).__init__()\n        # Encodeur\n        self.encoder_conv = nn.Sequential(\n            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # -&gt; [16, 14, 14]\n            nn.ReLU(),\n            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # -&gt; [8, 7, 7]\n            nn.ReLU(),\n            nn.Conv2d(8, 8, 3, stride=2, padding=1)   # -&gt; [8, 4, 4]\n        )\n        self.fc_mu = nn.Linear(8*4*4, latent_dim)\n        self.fc_logvar = nn.Linear(8*4*4, latent_dim)\n        \n        # D\u00e9codeur\n        self.decoder_fc = nn.Sequential(\n            nn.Linear(latent_dim, 8*4*4),\n            nn.ReLU()\n        )\n        self.decoder_conv = nn.Sequential(\n            nn.ConvTranspose2d(8, 8, 3, stride=2, padding=1, output_padding=0),\n            nn.ReLU(),\n            nn.ConvTranspose2d(8, 16, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n            nn.Tanh()\n        )\n    \n    def encode(self, x):\n        h = self.encoder_conv(x)\n        h = h.view(h.size(0), -1)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n    \n    def reparametrize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def decode(self, z):\n        h = self.decoder_fc(z)\n        h = h.view(h.size(0), 8, 4, 4)\n        return self.decoder_conv(h)\n    \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparametrize(mu, logvar)\n        return self.decode(z), mu, logvar\n    \ndummy_input = torch.randn(1, 1, 28, 28)\nmodel = VAE()\noutput,mu,logvar = model(dummy_input)  \nprint(output.shape, mu.shape, logvar.shape)\n</pre> class VAE(nn.Module):     def __init__(self,latent_dim=8):         super(VAE, self).__init__()         # Encodeur         self.encoder_conv = nn.Sequential(             nn.Conv2d(1, 16, 3, stride=2, padding=1),  # -&gt; [16, 14, 14]             nn.ReLU(),             nn.Conv2d(16, 8, 3, stride=2, padding=1),  # -&gt; [8, 7, 7]             nn.ReLU(),             nn.Conv2d(8, 8, 3, stride=2, padding=1)   # -&gt; [8, 4, 4]         )         self.fc_mu = nn.Linear(8*4*4, latent_dim)         self.fc_logvar = nn.Linear(8*4*4, latent_dim)                  # D\u00e9codeur         self.decoder_fc = nn.Sequential(             nn.Linear(latent_dim, 8*4*4),             nn.ReLU()         )         self.decoder_conv = nn.Sequential(             nn.ConvTranspose2d(8, 8, 3, stride=2, padding=1, output_padding=0),             nn.ReLU(),             nn.ConvTranspose2d(8, 16, 3, stride=2, padding=1, output_padding=1),             nn.ReLU(),             nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),             nn.Tanh()         )          def encode(self, x):         h = self.encoder_conv(x)         h = h.view(h.size(0), -1)         mu = self.fc_mu(h)         logvar = self.fc_logvar(h)         return mu, logvar          def reparametrize(self, mu, logvar):         std = torch.exp(0.5 * logvar)         eps = torch.randn_like(std)         return mu + eps * std          def decode(self, z):         h = self.decoder_fc(z)         h = h.view(h.size(0), 8, 4, 4)         return self.decoder_conv(h)          def forward(self, x):         mu, logvar = self.encode(x)         z = self.reparametrize(mu, logvar)         return self.decode(z), mu, logvar      dummy_input = torch.randn(1, 1, 28, 28) model = VAE() output,mu,logvar = model(dummy_input)   print(output.shape, mu.shape, logvar.shape) <pre>torch.Size([1, 1, 28, 28]) torch.Size([1, 8]) torch.Size([1, 8])\n</pre> In\u00a0[71]: Copied! <pre>epochs = 10\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> epochs = 10 criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[72]: Copied! <pre>def loss_function(recon_x, x, mu, logvar):\n    MSE = F.mse_loss(recon_x, x, reduction='sum')\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return MSE + KLD\n\nfor epoch in range(epochs): \n    for data,_ in train_loader:\n        optimizer.zero_grad()\n        recon, mu, logvar = model(data)\n        loss = loss_function(recon, data, mu, logvar)\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch {epoch}, Loss: {loss / len(train_loader.dataset)}')\n</pre> def loss_function(recon_x, x, mu, logvar):     MSE = F.mse_loss(recon_x, x, reduction='sum')     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())     return MSE + KLD  for epoch in range(epochs):      for data,_ in train_loader:         optimizer.zero_grad()         recon, mu, logvar = model(data)         loss = loss_function(recon, data, mu, logvar)         loss.backward()         optimizer.step()     print(f'Epoch {epoch}, Loss: {loss / len(train_loader.dataset)}') <pre>Epoch 0, Loss: 0.1811039298772812\nEpoch 1, Loss: 0.14575038850307465\nEpoch 2, Loss: 0.14808794856071472\nEpoch 3, Loss: 0.14365650713443756\nEpoch 4, Loss: 0.14496898651123047\nEpoch 5, Loss: 0.13169685006141663\nEpoch 6, Loss: 0.1442883014678955\nEpoch 7, Loss: 0.14070650935173035\nEpoch 8, Loss: 0.12996357679367065\nEpoch 9, Loss: 0.1352960765361786\n</pre> In\u00a0[73]: Copied! <pre>latents = []\nlabels = []\n\nwith torch.no_grad():\n    for data, target in test_loader:\n        mu, logvar = model.encode(data)\n        latents.append(mu)\n        labels.append(target)\n\nlatents = torch.cat(latents)\nlabels = torch.cat(labels)\n</pre> latents = [] labels = []  with torch.no_grad():     for data, target in test_loader:         mu, logvar = model.encode(data)         latents.append(mu)         labels.append(target)  latents = torch.cat(latents) labels = torch.cat(labels) In\u00a0[74]: Copied! <pre>from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=0)\nlatent_2d = tsne.fit_transform(latents)\n</pre> from sklearn.manifold import TSNE  tsne = TSNE(n_components=2, random_state=0) latent_2d = tsne.fit_transform(latents) In\u00a0[75]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='viridis', alpha=0.5)\nplt.colorbar(scatter)\nplt.title('Visualisation de l\\'espace latent de MNIST avec un VAE')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.figure(figsize=(10, 8)) scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='viridis', alpha=0.5) plt.colorbar(scatter) plt.title('Visualisation de l\\'espace latent de MNIST avec un VAE') plt.xlabel('Dimension 1') plt.ylabel('Dimension 2') plt.show() <p>On constate que l'espace latent est quand m\u00eame tr\u00e8s \u00e9parse. Cela s'explique par la diff\u00e9rence entre le loss de reconstruction et la divergence de Kullback-Leibler. Dans notre entra\u00eenement, le loss de reconstruction avait un valeur beaucoup plus importante que la divergence.</p> <p>On va maintenant pouvoir g\u00e9n\u00e9rer des images. Comme l'espace latent n'a pas les caract\u00e9ristiques de continuit\u00e9 et de compl\u00e9tude que l'on voulait, les \u00e9l\u00e9ments ne vont pas forc\u00e9ment ressembler \u00e0 des chiffres r\u00e9els.</p> In\u00a0[119]: Copied! <pre>latent_dim = 8\nnum_images = 10  \nimages_per_row = 5  \n\nsampled_latents = torch.randn(num_images, latent_dim)\n\nwith torch.no_grad():\n    generated_images = model.decode(sampled_latents)\n\ngenerated_images = generated_images.squeeze().numpy()  # Supprimer la dimension batch et convertir en numpy\ngenerated_images = (generated_images + 1) / 2  # D\u00e9normaliser les images (car Tanh est utilis\u00e9)\n\nfig, axes = plt.subplots(2, images_per_row, figsize=(15, 6))\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(generated_images[i], cmap='gray')\n    ax.axis('off')\n\nplt.suptitle(\"Images g\u00e9n\u00e9r\u00e9es \u00e0 partir de points \u00e9chantillonn\u00e9s dans l'espace latent d'un VAE\")\nplt.show()\n</pre> latent_dim = 8 num_images = 10   images_per_row = 5    sampled_latents = torch.randn(num_images, latent_dim)  with torch.no_grad():     generated_images = model.decode(sampled_latents)  generated_images = generated_images.squeeze().numpy()  # Supprimer la dimension batch et convertir en numpy generated_images = (generated_images + 1) / 2  # D\u00e9normaliser les images (car Tanh est utilis\u00e9)  fig, axes = plt.subplots(2, images_per_row, figsize=(15, 6))  for i, ax in enumerate(axes.flat):     ax.imshow(generated_images[i], cmap='gray')     ax.axis('off')  plt.suptitle(\"Images g\u00e9n\u00e9r\u00e9es \u00e0 partir de points \u00e9chantillonn\u00e9s dans l'espace latent d'un VAE\") plt.show() <p>Comme pr\u00e9vu, certaines images g\u00e9n\u00e9r\u00e9s n'ont pas vraiment de sens. Comme exercice, vous pouvez essayer d'am\u00e9liorer la representation latente pour g\u00e9n\u00e9rer des images coh\u00e9rentes \u00e0 tous les coups. Attention, il y toujours un compromis \u00e0 faire entre qualit\u00e9 de reconstruction et espace latent.</p>"},{"location":"11_ModelesGeneratifs/05_VaeImplementation.html#implementation-dun-vae","title":"Impl\u00e9mentation d'un VAE\u00b6","text":""},{"location":"11_ModelesGeneratifs/05_VaeImplementation.html#dataset","title":"Dataset\u00b6","text":""},{"location":"11_ModelesGeneratifs/05_VaeImplementation.html#autoencodeur-sur-mnist","title":"Autoencodeur sur MNIST\u00b6","text":""},{"location":"11_ModelesGeneratifs/05_VaeImplementation.html#variational-autoencoder","title":"Variational Autoencoder\u00b6","text":""},{"location":"11_ModelesGeneratifs/06_NormalizingFlows.html","title":"Normalizing Flows","text":"<p>Dans ce cours, nous pr\u00e9sentons les Normalizing Flows qui sont des mod\u00e8les g\u00e9n\u00e9ratifs de representation learning. Ils sont un peu moins connus que les VAE, GAN ou mod\u00e8les de diffusion mais ils ont quand m\u00eame de nombreux avantages non n\u00e9gligeables.</p> <p>Les GAN et les VAE ne peuvent pas \u00e9valuer avec pr\u00e9cision l'exactitude de la distribution de probabilit\u00e9 (un GAN ne le fait pas du tout et un VAE utilise la ELBO) et cela cause des probl\u00e8mes lors de l'entra\u00eenement : les VAE ont tendance \u00e0 produire des images floues tandis que les GAN sont sujets au mode collapse lors de l'entra\u00eenement.</p> <p>Les Normalizing Flows propose une solution \u00e0 ce probl\u00e8me.</p> <p>Un Normalizing Flow peut \u00eatre d\u00e9crit comme une s\u00e9rie de transformations bijectives. Ces transformations sont utilis\u00e9es pour mod\u00e9liser des distributions complexes de donn\u00e9es, telles que celles des images, en les transformant en une distribution simple, comme une distribution gaussienne centr\u00e9e r\u00e9duite.</p> <p>Les Normalizing Flows sont entra\u00een\u00e9s en maximisant la vraisemblance des donn\u00e9es, ce qui est g\u00e9n\u00e9ralement formul\u00e9 comme la minimisation du negative log-likelihood par rapport \u00e0 la densit\u00e9 de probabilit\u00e9 r\u00e9elle des donn\u00e9es. En d'autres termes, on ajuste les param\u00e8tres des transformations pour que la distribution g\u00e9n\u00e9r\u00e9e par le flow soit aussi proche que possible de la distribution cible des donn\u00e9es.</p> <p></p> <p>Figure extraire de blogpost.</p> <p>Les principaux avantages de Normalizing Flows sont les suivants :</p> <ul> <li>Leur entra\u00eenement est tr\u00e8s stable</li> <li>Ils convergent beaucoup plus simplement que les GAN ou les VAE (Yay)</li> <li>Il n'y a pas besoin de g\u00e9n\u00e9rer un bruit pour pouvoir g\u00e9n\u00e9rer des donn\u00e9es</li> </ul> <p>Cela semble assez g\u00e9nial mais il y a quand m\u00eame quelques d\u00e9savantages :</p> <ul> <li>Empiriquement, on constate qu'ils sont moins expressifs que les GAN ou les VAE</li> <li>Il y a une contrainte sur l'espace latent car on a besoin de fonctions bijectives et d'une pr\u00e9servation du volume. Cette contrainte fait que l'espace latent n'est pas vraiment utilisable car de grande dimension donc dur \u00e0 interpreter.</li> <li>Empiriquement, les \u00e9l\u00e9ments g\u00e9n\u00e9r\u00e9s sont souvents moins bons que ceux des GAN ou des VAE.</li> </ul> <p>Note : Il y a un aspect th\u00e9orique important derri\u00e8re les Normalizing Flows mais nous n'allons pas entrer dans les d\u00e9tails dans ce cours. Pour en apprendre plus, vous pouvez vous r\u00e9ferrer au cours CS236 de stanford et en particulier \u00e0 ce lien.</p>"},{"location":"11_ModelesGeneratifs/06_NormalizingFlows.html#normalizing-flows","title":"Normalizing Flows\u00b6","text":""},{"location":"11_ModelesGeneratifs/06_NormalizingFlows.html#comment-ca-marche","title":"Comment \u00e7a marche ?\u00b6","text":""},{"location":"11_ModelesGeneratifs/06_NormalizingFlows.html#avantages-et-desavantages","title":"Avantages et d\u00e9savantages\u00b6","text":""},{"location":"11_ModelesGeneratifs/07_DiffusionModels.html","title":"Diffusion Models","text":"<p>Dans ce cours, nous allons pr\u00e9senter les mod\u00e8les de diffusion (diffusion models) qui ont \u00e9t\u00e9 introduit dans le papier Denoising Diffusion Probabilistic Models en 2020. Depuis quelques ann\u00e9es, ce sont ces mod\u00e8les qui sont le plus utilis\u00e9s pour la g\u00e9n\u00e9ration d'images (et de loin). Ils sont tr\u00e8s performants et facilement guidables mais ils ont le d\u00e9faut d'\u00eatre tr\u00e8s lent. Ce cours s'inspire du CVPR 2022 Tutorial et du blogpost. Les figures utilis\u00e9es dans ce notebook sont \u00e9galement extraites de ces deux sources.</p> <p>Les diffusion models sont constitu\u00e9s de deux \u00e9tapes principales : une \u00e9tape de d'ajout de bruit appel\u00e9e diffusion process et une \u00e9tape pour enlever le bruit appel\u00e9e reverse process ou denoising process. Ces deux \u00e9tapes se font de mani\u00e8re it\u00e9rative, c'est \u00e0 dire que l'on ajoute du bruit petit \u00e0 petit et qu'on l'enl\u00e8ve petit \u00e0 petit \u00e9galement.</p> <p>Le premi\u00e8re \u00e9tape d'un mod\u00e8le de diffusion consiste \u00e0 prendre une image d'un dataset quelconque. Ce dataset est represent\u00e9 par une distribution de probabilit\u00e9 tr\u00e8s complexe. Le processus de diffusion consiste \u00e0 ajouter un bruit gaussien sur l'image de mani\u00e8re it\u00e9rative pour r\u00e9duire petit \u00e0 petit sa complexit\u00e9 jusqu'\u00e0 ce que l'on obtienne une simple distribution gaussienne. La figure suivante pr\u00e9sente le diffusion process :</p> <p></p> <p>Et on peut voir la modification de la distribution de cette mani\u00e8re (distribution de moins en moins complexe) :</p> <p></p> <p>Le diffusion process consiste en fait \u00e0 d\u00e9truire de mani\u00e8re it\u00e9rative la structure de la donn\u00e9e d'entr\u00e9e (souvent une image).</p> <p>Le diffusion process se fait en plusieurs \u00e9tapes appel\u00e9es diffusion step. Chaque \u00e9tape va ajouter une quantit\u00e9 pr\u00e9determin\u00e9e de bruit gaussien \u00e0 l'image et plus il y a d'\u00e9tapes, moins on va ajouter de bruit \u00e0 chaque \u00e9tape. En pratique, plus il y a d'\u00e9tapes, plus le mod\u00e8le est stable et les images produites de qualit\u00e9 mais plus on va avoir besoin de temps de calcul. Souvent, on va choisir un grand nombre d'\u00e9tapes (1000 dans le papier original).</p> <p>Bon c'est bien gentil mais \u00e0 quoi \u00e7a sert de bruiter une image ? En fait, le diffusion process va permettre de g\u00e9n\u00e9rer des donn\u00e9es d'entra\u00eenement pour le reverse process. L'id\u00e9e du reverse process va \u00eatre d'apprendre \u00e0 passer de la distribution gaussienne (que l'on a construite avec la diffusion) aux images de base. Donc on veut r\u00e9cuperer notre image \u00e0 partir du bruit gaussien de la derni\u00e8re \u00e9tape de notre diffusion process. De cette mani\u00e8re, on pourra aussi g\u00e9n\u00e9rer des images nouvelles \u00e0 partir d'un sample de la distribution gaussienne.</p> <p>Note : On peut noter une certain similarit\u00e9 avec les normalizing flow ou les variational autoencoders</p> <p></p> <p>Pour chaque \u00e9tape de denoising, on va utiliser un r\u00e9seau de neurones qui prend en entr\u00e9e l'image \u00e0 l'\u00e9tape $t$ et l'\u00e9tape de diffusion $t$ et qui a pour objectif de pr\u00e9dire le bruit gaussien ($\\mu$ et $\\sigma\u00b2$) qui a \u00e9t\u00e9 ajout\u00e9 \u00e0 l'image lors de l'\u00e9tape $t-1 \\Rightarrow t$.</p> <p>Note : Le fait de pr\u00e9dire le bruit permet en fait de pr\u00e9dire l'image \u00e0 l'\u00e9tape $t-1$.</p> <p>Note 2 : Le r\u00e9seau utilis\u00e9 est le m\u00eame \u00e0 chaque \u00e9tape, il n'y a pas un r\u00e9seau diff\u00e9rent par \u00e9tape de diffusion.</p> <p>Le mod\u00e8le utilis\u00e9 est g\u00e9n\u00e9ralement un mod\u00e8le de type U-Net. Pour en savoir plus sur l'architecture U-Net, vous pouvez vous r\u00e9f\u00e9rer au cours 3 sur les r\u00e9seaux convolutifs. En pratique, pour les mod\u00e8les tr\u00e8s puissants (stable diffusion), une variante du U-Net incorporant l'architecture des transformers est utilis\u00e9e.</p> <p></p> <p>Comme dit pr\u00e9cedemment, on peut voir les diffusion models comme des VAE ou des normalizing flows. Regardons un peu l'analogie possible avec un VAE et plus pr\u00e9cisement un hierarchical VAE. Un hierarchical VAE est un VAE qui a plusieurs \u00e9tapes de g\u00e9n\u00e9ration (decoder) d'images.</p> <p>Pour notre diffusion model, le diffusion process correspondrait \u00e0 l'encoder du VAE tandis que le reverse process correspondrait aux multiples \u00e9tapes hierarchique du VAE. En pratique, il y a quand m\u00eame quelques diff\u00e9rences notables :</p> <ul> <li>L'encoder est fix\u00e9 (non entra\u00eenable) pour le diffusion model, il s'agit d'un ajout de bruit.</li> <li>L'espace latent a la m\u00eame dimension que l'image d'entr\u00e9e (ce qui n'est pas le cas pour un VAE).</li> <li>Le mod\u00e8le utilis\u00e9 pour la diffusion est le m\u00eame \u00e0 chaque \u00e9tape de diffusion. Pour un hierarchical VAE, on a un mod\u00e8le diff\u00e9rent \u00e0 chaque \u00e9tape.</li> </ul> <p>Le m\u00eame genre d'analogie peut \u00eatre faite avec un normalizing flow, je vous invite \u00e0 regarder le CVPR 2022 Tutorial pour en apprendre plus.</p> <p>Comme expliqu\u00e9 pr\u00e9cedemment, les processus de diffusion et de denoising vont grandement b\u00e9n\u00e9ficier d'un nombre d'\u00e9tapes important. Chaque \u00e9tape de denoising va n\u00e9cessiter le forward du r\u00e9seau U-Net donc si on a 1000 \u00e9tapes de diffusion, on va devoir appeler le r\u00e9seau 1000 fois et \u00e7a pour g\u00e9n\u00e9rer une seule image.</p> <p>On voit tout de suite le probl\u00e8me des mod\u00e8les de diffusion : ils sont tr\u00e8s tr\u00e8s lents.</p> <p></p> <p>Figure extraite de l'article.</p> <p>Les diffusion models sont beaucoup plus puissants que les GANs, les VAEs et les normalizing Flows pour la g\u00e9n\u00e9ration d'images, c'est donc ces mod\u00e8les que l'on voudrait utiliser en priorit\u00e9.</p> <p>Une n\u00e9cessit\u00e9 est alors de trouver des m\u00e9thodes pour acc\u00e9lerer ces mod\u00e8les de diffusion via diverses techniques. Ce cours ne va pas entrer dans le d\u00e9tail sur les techniques existantes pour acc\u00e9lerer ces mod\u00e8les mais le CVPR 2022 Tutorial couvre tr\u00e8s bien le sujet (bien que depuis, des nouvelles techniques ont \u00e9t\u00e9 introduites). A titre informatif, on peut maintenant g\u00e9n\u00e9rer des images de qualit\u00e9 en une dizaine d'\u00e9tapes (contre 1000 auparavant).</p> <p>Les mod\u00e8les de diffusion sont un gros sujet de recherche en ce moment et il reste encore beaucoup de probl\u00e8mes \u00e0 r\u00e9soudre les concernant. En voici quelques-uns :</p> <ul> <li>Pourquoi les diffusion models sont si performant par rapport aux VAE et aux normalizing flows ? Et est ce qu'on devrait rediriger les efforts de recherche sur ces alternatives maintenant que l'on a appris beaucoup de choses sur la diffusion ?</li> <li>Est ce qu'il serait possible de passer \u00e0 une seule \u00e9tape pour g\u00e9n\u00e9rer des images ?</li> <li>Est ce que l'architecture de diffusion peut aider pour des applications discriminative ?</li> <li>Est ce que l'architecture U-Net est vraiment le meilleur choix pour la diffusion ?</li> <li>Est ce qu'on peut appliquer les diffusion models sur d'autres types de donn\u00e9es (comme du texte par exemple) ?</li> </ul>"},{"location":"11_ModelesGeneratifs/07_DiffusionModels.html#diffusion-models","title":"Diffusion Models\u00b6","text":""},{"location":"11_ModelesGeneratifs/07_DiffusionModels.html#comment-ca-marche","title":"Comment \u00e7a marche ?\u00b6","text":""},{"location":"11_ModelesGeneratifs/07_DiffusionModels.html#premiere-etape-diffusion-process","title":"Premi\u00e8re \u00e9tape : diffusion process\u00b6","text":""},{"location":"11_ModelesGeneratifs/07_DiffusionModels.html#deuxieme-etape-reverse-process","title":"Deuxi\u00e8me \u00e9tape : reverse process\u00b6","text":""},{"location":"11_ModelesGeneratifs/07_DiffusionModels.html#les-diffusion-models-sont-des-hierarchical-vae","title":"Les diffusion models sont des hierarchical VAE ?\u00b6","text":""},{"location":"11_ModelesGeneratifs/07_DiffusionModels.html#probleme-principal-des-diffusion-models","title":"Probl\u00e8me principal des diffusion models\u00b6","text":""},{"location":"11_ModelesGeneratifs/08_DiffusionImplementation.html","title":"Implementation Diffusion Model","text":"<p>Dans ce cours, nous allons impl\u00e9menter pas \u00e0 pas un mod\u00e8le de diffusion sur le dataset MNIST. Le cours est grandement inspir\u00e9 du github minDiffusion.</p> In\u00a0[1]: Copied! <pre>from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms as T\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n</pre> from typing import Tuple  import torch import torch.nn as nn from torch.utils.data import DataLoader  from torchvision.datasets import MNIST from torchvision import transforms as T from torchvision.utils import make_grid import matplotlib.pyplot as plt <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>transform = T.Compose([T.ToTensor(), T.Normalize((0.5,), (1.0))])\n\ndataset = MNIST(\"./../data\",train=True,download=True,transform=transform)\nprint('Taille du dataset :', len(dataset))\nprint('Taille d\\'une image :', dataset[0][0].numpy().shape)\n\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=8)\n</pre> transform = T.Compose([T.ToTensor(), T.Normalize((0.5,), (1.0))])  dataset = MNIST(\"./../data\",train=True,download=True,transform=transform) print('Taille du dataset :', len(dataset)) print('Taille d\\'une image :', dataset[0][0].numpy().shape)  dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=8) <pre>Taille du dataset : 60000\nTaille d'une image : (1, 28, 28)\n</pre> <p>Pour un mod\u00e8le de diffusion, il faut g\u00e9n\u00e9rer du bruit pour chaque \u00e9tape de diffusion. Pour cela, on va cr\u00e9er nos plages de valeurs de bruit comprises entre $\\beta_1$ et $\\beta_2$.</p> In\u00a0[3]: Copied! <pre>def ddpm_schedules(beta1: float, beta2: float, T: int):\n\n  # On v\u00e9rifie que beta1 et beta2 sont bien dans l'intervalle (0, 1) et que beta1 &lt; beta2\n  assert beta1 &lt; beta2 &lt; 1.0, \"beta1 et beta2 doivent \u00eatre dans l'intervalle (0, 1)\"\n\n  # On cr\u00e9e un vecteur de taille T+1 allant de beta1 \u00e0 beta2 qui \u00e9chantillonne lin\u00e9airement l'intervalle [beta1, beta2]\n  beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n  \n  # On calcule toutes les valeurs qui seront n\u00e9cessaires pour les calculs de l'optimisation\n  sqrt_beta_t = torch.sqrt(beta_t)\n  alpha_t = 1 - beta_t\n  log_alpha_t = torch.log(alpha_t)\n  alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n\n  sqrtab = torch.sqrt(alphabar_t)\n  oneover_sqrta = 1 / torch.sqrt(alpha_t)\n\n  sqrtmab = torch.sqrt(1 - alphabar_t)\n  mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n\n  return {\n    \"alpha_t\": alpha_t,  # \\alpha_t\n    \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n    \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n    \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n    \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n    \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n    \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n  }\n</pre> def ddpm_schedules(beta1: float, beta2: float, T: int):    # On v\u00e9rifie que beta1 et beta2 sont bien dans l'intervalle (0, 1) et que beta1 &lt; beta2   assert beta1 &lt; beta2 &lt; 1.0, \"beta1 et beta2 doivent \u00eatre dans l'intervalle (0, 1)\"    # On cr\u00e9e un vecteur de taille T+1 allant de beta1 \u00e0 beta2 qui \u00e9chantillonne lin\u00e9airement l'intervalle [beta1, beta2]   beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1      # On calcule toutes les valeurs qui seront n\u00e9cessaires pour les calculs de l'optimisation   sqrt_beta_t = torch.sqrt(beta_t)   alpha_t = 1 - beta_t   log_alpha_t = torch.log(alpha_t)   alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()    sqrtab = torch.sqrt(alphabar_t)   oneover_sqrta = 1 / torch.sqrt(alpha_t)    sqrtmab = torch.sqrt(1 - alphabar_t)   mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab    return {     \"alpha_t\": alpha_t,  # \\alpha_t     \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}     \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}     \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}     \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}     \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}     \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}   } <p>On peut maintenant cr\u00e9er notre mod\u00e8le ! En g\u00e9n\u00e9ral, on prend une architecture U-Net mais en pratique, on peut prendre un peu n'importe quoi. Pour plus de simplicit\u00e9, on va prendre un mod\u00e8le convolutif tout b\u00eate. Aussi, normalement le mod\u00e8le prend l'\u00e9tape $t$ en entr\u00e9e mais dans notre mod\u00e8le simplifi\u00e9, nous n'allons pas le faire.</p> In\u00a0[4]: Copied! <pre>def conv_bn_relu(in_channels, out_channels,kernel_size=7, stride=1, padding=3):\n  return nn.Sequential(\n    nn.Conv2d(in_channels, out_channels, kernel_size,stride, padding),\n    nn.BatchNorm2d(out_channels),\n    nn.LeakyReLU())\n\nclass model(nn.Module):\n\n  def __init__(self, channels: int) -&gt; None:\n    super(model, self).__init__()\n    # Tr\u00e8s petit mod\u00e8le\n    self.conv = nn.Sequential(  \n      conv_bn_relu(channels, 64),\n      conv_bn_relu(64, 128),\n      conv_bn_relu(128, 256),\n      conv_bn_relu(256, 512),\n      conv_bn_relu(512, 256),\n      conv_bn_relu(256, 128),\n      conv_bn_relu(128, 64),\n      nn.Conv2d(64, channels, 3, padding=1),\n    )\n\n  def forward(self, x):\n    return self.conv(x)\n</pre> def conv_bn_relu(in_channels, out_channels,kernel_size=7, stride=1, padding=3):   return nn.Sequential(     nn.Conv2d(in_channels, out_channels, kernel_size,stride, padding),     nn.BatchNorm2d(out_channels),     nn.LeakyReLU())  class model(nn.Module):    def __init__(self, channels: int) -&gt; None:     super(model, self).__init__()     # Tr\u00e8s petit mod\u00e8le     self.conv = nn.Sequential(         conv_bn_relu(channels, 64),       conv_bn_relu(64, 128),       conv_bn_relu(128, 256),       conv_bn_relu(256, 512),       conv_bn_relu(512, 256),       conv_bn_relu(256, 128),       conv_bn_relu(128, 64),       nn.Conv2d(64, channels, 3, padding=1),     )    def forward(self, x):     return self.conv(x) <p>Maintenant que l'on a notre mod\u00e8le pour passer d'une \u00e9tape \u00e0 une autre, construisons le mod\u00e8le global :</p> In\u00a0[5]: Copied! <pre>class DDPM(nn.Module):\n    def __init__(self,model: nn.Module,betas: Tuple[float, float],n_T: int,criterion: nn.Module = nn.MSELoss()) -&gt; None:\n        super(DDPM, self).__init__()\n        self.model = model\n\n        # Permet de stocker les ddpm schedules dans le mod\u00e8le pour acc\u00e9der aux valeurs plus facilement\n        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n            self.register_buffer(k, v)\n\n        self.n_T = n_T\n        self.criterion = criterion\n\n    # Etape d'entrainement\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Makes forward diffusion x_t, and tries to guess epsilon value from x_t using eps_model.\n        This implements Algorithm 1 in the paper.\n        \"\"\"\n        \n        # G\u00e9n\u00e8re un entier al\u00e9atoire entre 1 et n_T pour choisir un t al\u00e9atoire\n        _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(x.device) \n        # G\u00e9n\u00e8re un bruit al\u00e9atoire de la m\u00eame taille que x\n        eps = torch.randn_like(x)  # eps ~ N(0, 1)\n\n            # On applique le bruit gaussien \u00e0 x pour obtenir x_t\n        x_t = (self.sqrtab[_ts, None, None, None] * x+ self.sqrtmab[_ts, None, None, None] * eps)  \n        # On va essayer de pr\u00e9dire le bruit epsilon \u00e0 partir de x_t\n        pred_eps = self.model(x_t)\n        return self.criterion(eps, pred_eps)\n\n    # G\u00e9n\u00e9ration d'un \u00e9chantillon\n    def sample(self, n_sample: int, size, device) -&gt; torch.Tensor:\n        \n        # On g\u00e9n\u00e8re un \u00e9chantillon al\u00e9atoire de taille n_sample \u00e0 partir d'une distribution normale centr\u00e9e r\u00e9duite\n        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)\n\n        # On va appliquer le processus de diffusion inverse pour g\u00e9n\u00e9rer un \u00e9chantillon (\u00e7a prend du temps \n        # car on doit appliquer le processus de diffusion \u00e0 chaque \u00e9tape)\n        for i in range(self.n_T, 0, -1):\n            z = torch.randn(n_sample, *size).to(device) if i &gt; 1 else 0\n            eps = self.model(x_i)\n            x_i = (self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])+ self.sqrt_beta_t[i] * z)\n\n        return x_i\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nddpm = DDPM(model=model(channels=1), betas=(1e-4, 0.02), n_T=1000)\nddpm.to(device);\n</pre> class DDPM(nn.Module):     def __init__(self,model: nn.Module,betas: Tuple[float, float],n_T: int,criterion: nn.Module = nn.MSELoss()) -&gt; None:         super(DDPM, self).__init__()         self.model = model          # Permet de stocker les ddpm schedules dans le mod\u00e8le pour acc\u00e9der aux valeurs plus facilement         for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():             self.register_buffer(k, v)          self.n_T = n_T         self.criterion = criterion      # Etape d'entrainement     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Makes forward diffusion x_t, and tries to guess epsilon value from x_t using eps_model.         This implements Algorithm 1 in the paper.         \"\"\"                  # G\u00e9n\u00e8re un entier al\u00e9atoire entre 1 et n_T pour choisir un t al\u00e9atoire         _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(x.device)          # G\u00e9n\u00e8re un bruit al\u00e9atoire de la m\u00eame taille que x         eps = torch.randn_like(x)  # eps ~ N(0, 1)              # On applique le bruit gaussien \u00e0 x pour obtenir x_t         x_t = (self.sqrtab[_ts, None, None, None] * x+ self.sqrtmab[_ts, None, None, None] * eps)           # On va essayer de pr\u00e9dire le bruit epsilon \u00e0 partir de x_t         pred_eps = self.model(x_t)         return self.criterion(eps, pred_eps)      # G\u00e9n\u00e9ration d'un \u00e9chantillon     def sample(self, n_sample: int, size, device) -&gt; torch.Tensor:                  # On g\u00e9n\u00e8re un \u00e9chantillon al\u00e9atoire de taille n_sample \u00e0 partir d'une distribution normale centr\u00e9e r\u00e9duite         x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)          # On va appliquer le processus de diffusion inverse pour g\u00e9n\u00e9rer un \u00e9chantillon (\u00e7a prend du temps          # car on doit appliquer le processus de diffusion \u00e0 chaque \u00e9tape)         for i in range(self.n_T, 0, -1):             z = torch.randn(n_sample, *size).to(device) if i &gt; 1 else 0             eps = self.model(x_i)             x_i = (self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])+ self.sqrt_beta_t[i] * z)          return x_i  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ddpm = DDPM(model=model(channels=1), betas=(1e-4, 0.02), n_T=1000) ddpm.to(device); <p>Et voil\u00e0, notre impl\u00e9mentation simple est termin\u00e9e !</p> <p>Passons maintenant \u00e0 l'entrainement du mod\u00e8le. On va diminuer grandement la taille du dataset (seulement 1000 \u00e9l\u00e9ments) mais l'entra\u00eenement sera quand m\u00eame tr\u00e8s long. Je ne vous conseille pas d'essayer sauf si vous avez un tr\u00e8s bon GPU \u00e0 disposition.</p> In\u00a0[6]: Copied! <pre>epoch=100\nn_T=1000\noptimizer = torch.optim.Adam(ddpm.parameters(), lr=2e-4)\n</pre> epoch=100 n_T=1000 optimizer = torch.optim.Adam(ddpm.parameters(), lr=2e-4) In\u00a0[\u00a0]: Copied! <pre>generation=[]\nfor i in range(0,epoch+1):\n  ddpm.train()\n  loss_ema = None\n  for x, _ in dataloader:\n    optimizer.zero_grad()\n    x = x.to(device)\n    loss = ddpm(x)\n    loss.backward()\n    if loss_ema is None:\n      loss_ema = loss.item()\n    else:\n      loss_ema = 0.9 * loss_ema + 0.1 * loss.item()\n    optimizer.step()\n  print(f\"epoch {i}, loss: {loss_ema:.4f}\")\n  if i % 10 == 0:\n    ddpm.eval()\n    with torch.no_grad():\n      print('ici')\n      xh = ddpm.sample(4, (1, 28, 28), device)\n      grid = make_grid(xh, nrow=4)\n      generation.append(grid)      \n</pre> generation=[] for i in range(0,epoch+1):   ddpm.train()   loss_ema = None   for x, _ in dataloader:     optimizer.zero_grad()     x = x.to(device)     loss = ddpm(x)     loss.backward()     if loss_ema is None:       loss_ema = loss.item()     else:       loss_ema = 0.9 * loss_ema + 0.1 * loss.item()     optimizer.step()   print(f\"epoch {i}, loss: {loss_ema:.4f}\")   if i % 10 == 0:     ddpm.eval()     with torch.no_grad():       print('ici')       xh = ddpm.sample(4, (1, 28, 28), device)       grid = make_grid(xh, nrow=4)       generation.append(grid)       In\u00a0[\u00a0]: Copied! <pre>for grid in generation:\n  grid_image = grid.permute(1, 2, 0).cpu().numpy()\n  # Afficher l'image\n  plt.imshow(grid_image)\n  plt.axis('off')  # Pour masquer les axes\n  plt.show()\n</pre> for grid in generation:   grid_image = grid.permute(1, 2, 0).cpu().numpy()   # Afficher l'image   plt.imshow(grid_image)   plt.axis('off')  # Pour masquer les axes   plt.show()"},{"location":"11_ModelesGeneratifs/08_DiffusionImplementation.html#implementation-diffusion-model","title":"Implementation Diffusion Model\u00b6","text":""},{"location":"11_ModelesGeneratifs/08_DiffusionImplementation.html#dataset","title":"Dataset\u00b6","text":""},{"location":"11_ModelesGeneratifs/08_DiffusionImplementation.html#noise-scheduling","title":"Noise scheduling\u00b6","text":""},{"location":"11_ModelesGeneratifs/08_DiffusionImplementation.html#modele","title":"Mod\u00e8le\u00b6","text":""},{"location":"11_ModelesGeneratifs/08_DiffusionImplementation.html#entrainement","title":"Entrainement\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html","title":"\ud83c\udf1f Cours sp\u00e9cifiques \ud83c\udf1f","text":"<p>Ce cours pr\u00e9sente des concepts tr\u00e8s int\u00e9ressant \u00e0 comprendre mais non essentiels dans une pratique courante du deep learning. Si vous \u00eates int\u00e9ress\u00e9 par comprendre le fonctionnement d'un r\u00e9seau de neurones de mani\u00e8re plus approfondie et de d\u00e9couvrir la raison de l'utilisation de techniques comme la BatchNorm, les connexions r\u00e9siduelles, les optimizers, le dropout, la data augmentation etc ..., ce cours est fait pour vous !</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-1-activation-et-initialisation","title":"Notebook 1\ufe0f\u20e3 : Activation et initialisation","text":"<p>Ce notebook pr\u00e9sente les consid\u00e9rentions importantes \u00e0 prendre en compte lors de l'initialisation d'un r\u00e9seau de neurones.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-2-batchnorm","title":"Notebook 2\ufe0f\u20e3 : BatchNorm","text":"<p>Ce notebook introduit un d\u00e9tail la batch normalization en pr\u00e9sentant ses int\u00earets et son impl\u00e9mentation.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-3-data-augmentation","title":"Notebook 3\ufe0f\u20e3 : Data augmentation","text":"<p>Ce notebook pr\u00e9sente la data augmentation et son utilit\u00e9 pour l'entra\u00eenement des r\u00e9seaux de neurones.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-4-broadcasting","title":"Notebook 4\ufe0f\u20e3 : Broadcasting","text":"<p>Ce notebook pr\u00e9sente les broadcasting rules de pytorch qui sont des r\u00e8gles sur la manipulation des tenseurs torch. Ces r\u00e8gles sont tr\u00e8s importantes \u00e0 ma\u00eetriser.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-5-optimizer","title":"Notebook 5\ufe0f\u20e3 : Optimizer","text":"<p>Ce notebook d\u00e9crit les diff\u00e9rents optimizer utilisables pour entra\u00eener un r\u00e9seau de neurones.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-6-regularisation","title":"Notebook 6\ufe0f\u20e3 : R\u00e9gularisation","text":"<p>Ce notebook pr\u00e9sente en d\u00e9tails deux m\u00e9thodes de r\u00e9gularisation : le r\u00e9gularisation L2 et le dropout.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-7-connexions-residuelles","title":"Notebook 7\ufe0f\u20e3 : Connexions R\u00e9siduelles","text":"<p>Ce notebook introduit les connexions r\u00e9siduelles et leurs utilit\u00e9s pour l'entra\u00eenement de r\u00e9seaux de neurones profonds.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-8-crossvalidation","title":"Notebook 8\ufe0f\u20e3 : CrossValidation","text":"<p>Ce notebook introduit la m\u00e9thode de cross validation pour \u00e9valuer les mod\u00e8les et d\u00e9tecter l'overfitting.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-9-metriques-evaluation","title":"Notebook 9\ufe0f\u20e3 : Metriques Evaluation","text":"<p>Ce notebook introduit diff\u00e9rents m\u00e9triques que l'on peut utiliser pour \u00e9valuer son mod\u00e8le.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-10-tokenization","title":"Notebook 1\ufe0f\u20e30\ufe0f\u20e3 : Tokenization","text":"<p>Ce notebook introduit la tokenization en expliquant le fonctionnement, les limitations et en proposant une impl\u00e9mentation du byte-pair encoding.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/index.html#notebook-11-quantization","title":"Notebook 1\ufe0f\u20e31\ufe0f\u20e3 : Quantization","text":"<p>Ce notebook introduit la quantization et les m\u00e9thodes de fine-tuning de mod\u00e8les LLM. La premi\u00e8re partie d\u00e9crit les diff\u00e9rentes forme de quantization et la th\u00e9orie derri\u00e8re. La derni\u00e8re partie introduit les m\u00e9thodes LoRA et QLoRA pour le fine-tuning.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html","title":"Activations et Initialisations","text":"<p>Dans ce cours, nous allons reprendre le mod\u00e8le Fully Connected introduit dans le cours 5 sur les NLP et nous allons regarder le comportement des activations tout au long du r\u00e9seau \u00e0 l'initialisation de celui-ci. Ce cours est inspir\u00e9 du cours d'Andrej Karpathy Building makemore Part 3: Activations &amp; Gradients, BatchNorm.</p> <p>On a pu voir que les r\u00e9seaux de neurones ont des nombreux avantages :</p> <ul> <li>Ils sont tr\u00e8s flexibles et sont capables de r\u00e9soudre de nombreux probl\u00e8mes.</li> <li>Ils sont assez simple \u00e0 impl\u00e9menter.</li> </ul> <p>Cependant (\u00e7a serait trop facile sinon), il est souvent assez complexe des les optimiser et en particulier si il s'agit de r\u00e9seaux profonds.</p> <p>On reprend le code du notebook 3 du cours 5 sur les NLP.</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre>words = open('../05_NLP/prenoms.txt', 'r').read().splitlines()\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\n</pre> words = open('../05_NLP/prenoms.txt', 'r').read().splitlines() chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} <p>Pour des fins p\u00e9dagogiques, nous l'allons pas utiliser le dataset et dataloader de pytorch. On veut \u00e9valuer le loss au d\u00e9but de notre entra\u00eenement apr\u00e8s le premier batch. Dans l'ensemble c'est \u00e0 peu pr\u00e8s la m\u00eame chose sauf que l'on prend un batch au hasard \u00e0 chaque it\u00e9ration au lieu de parcourir l'ensemble du dataset \u00e0 chaque epoch.</p> In\u00a0[3]: Copied! <pre>block_size = 3 # Contexte\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] \n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> block_size = 3 # Contexte  def build_dataset(words):     X, Y = [], []      for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix]     X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([180834, 3]) torch.Size([180834])\ntorch.Size([22852, 3]) torch.Size([22852])\ntorch.Size([22639, 3]) torch.Size([22639])\n</pre> In\u00a0[4]: Copied! <pre>embed_dim=10 # Dimension de l'embedding de C\nhidden_dim=200 # Dimension de la couche cach\u00e9e\n\nC = torch.randn((46, embed_dim))\nW1 = torch.randn((block_size*embed_dim, hidden_dim))\nb1 = torch.randn(hidden_dim)\nW2 = torch.randn((hidden_dim, 46))\nb2 = torch.randn(46)\nparameters = [C, W1, b1, W2, b2]\nfor p in parameters:\n  p.requires_grad = True\n</pre> embed_dim=10 # Dimension de l'embedding de C hidden_dim=200 # Dimension de la couche cach\u00e9e  C = torch.randn((46, embed_dim)) W1 = torch.randn((block_size*embed_dim, hidden_dim)) b1 = torch.randn(hidden_dim) W2 = torch.randn((hidden_dim, 46)) b2 = torch.randn(46) parameters = [C, W1, b1, W2, b2] for p in parameters:   p.requires_grad = True In\u00a0[5]: Copied! <pre>max_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  # Permet de construire un mini-batch\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  \n  # Forward\n  Xb, Yb = Xtr[ix], Ytr[ix] \n  emb = C[Xb] \n  embcat = emb.view(emb.shape[0], -1)\n  hpreact = embcat @ W1 + b1 \n\n  h = torch.tanh(hpreact) \n  logits = h @ W2 + b2 \n  loss = F.cross_entropy(logits, Yb)\n  \n  # Retropropagation\n  for p in parameters:\n    p.grad = None\n  \n  loss.backward()\n  # Mise \u00e0 jour des param\u00e8tres\n  lr = 0.1 if i &lt; 100000 else 0.01 # On descend le learning rate d'un facteur 10 apr\u00e8s 100000 it\u00e9rations\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  if i % 10000 == 0:\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n</pre> max_steps = 200000 batch_size = 32 lossi = []  for i in range(max_steps):   # Permet de construire un mini-batch   ix = torch.randint(0, Xtr.shape[0], (batch_size,))      # Forward   Xb, Yb = Xtr[ix], Ytr[ix]    emb = C[Xb]    embcat = emb.view(emb.shape[0], -1)   hpreact = embcat @ W1 + b1     h = torch.tanh(hpreact)    logits = h @ W2 + b2    loss = F.cross_entropy(logits, Yb)      # Retropropagation   for p in parameters:     p.grad = None      loss.backward()   # Mise \u00e0 jour des param\u00e8tres   lr = 0.1 if i &lt; 100000 else 0.01 # On descend le learning rate d'un facteur 10 apr\u00e8s 100000 it\u00e9rations   for p in parameters:     p.data += -lr * p.grad    if i % 10000 == 0:     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item()) <pre>      0/ 200000: 21.9772\n  10000/ 200000: 2.9991\n  20000/ 200000: 2.5258\n  30000/ 200000: 1.9657\n  40000/ 200000: 2.4326\n  50000/ 200000: 1.7670\n  60000/ 200000: 2.1324\n  70000/ 200000: 2.4160\n  80000/ 200000: 2.2237\n  90000/ 200000: 2.3905\n 100000/ 200000: 1.9304\n 110000/ 200000: 2.1710\n 120000/ 200000: 2.3444\n 130000/ 200000: 2.0970\n 140000/ 200000: 1.8623\n 150000/ 200000: 1.9792\n 160000/ 200000: 2.4602\n 170000/ 200000: 2.0968\n 180000/ 200000: 2.0466\n 190000/ 200000: 2.3746\n</pre> In\u00a0[6]: Copied! <pre>plt.plot(lossi)\n</pre> plt.plot(lossi) Out[6]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f028467b990&gt;]</pre> <p>Il y a beaucoup de \"bruit\" car on calcule \u00e0 chaque fois le loss sur des petits batchs par rapport \u00e0 l'ensemble de nos donn\u00e9es d'entra\u00eenement.</p> <p>L'entra\u00eenement se passe correctement. On peut cependant remarquer quelque chose de bizarre. Le loss au d\u00e9but de l'entra\u00eenement est anormalement \u00e9l\u00e9v\u00e9. On s'attendrait \u00e0 avoir une valeur correspondant \u00e0 un cas o\u00f9 chaque lettre a une probabilit\u00e9 uniforme d'apparation (donc $\\frac{1}{46}$).</p> <p>Dans ce cas, on pourrait calculer le negative log likelihood et on obtiendrait : $-ln(\\frac{1}{46})=3.83$</p> <p>Il serait donc logique de tomber sur une valeur de cet ordre l\u00e0 lors du premier calcul de notre loss.</p> <p>Pour voir ce qu'il se passe, utilisons un petit exemple et regardons les valeurs de loss en fonction de l'initialisation. Imaginons que tous les poids dans logits sont initialis\u00e9 \u00e0 0. Alors on aurait des probabilit\u00e9s uniformes.</p> In\u00a0[7]: Copied! <pre>logits=torch.tensor([0.0,0.0,0.0,0.0])\nprobs=torch.softmax(logits,dim=0)\nloss= -probs[1].log()\nprobs,loss\n</pre> logits=torch.tensor([0.0,0.0,0.0,0.0]) probs=torch.softmax(logits,dim=0) loss= -probs[1].log() probs,loss Out[7]: <pre>(tensor([0.2500, 0.2500, 0.2500, 0.2500]), tensor(1.3863))</pre> <p>En revanche, il n'est pas conseill\u00e9 d'initialiser les poids d'un r\u00e9seau de neurones \u00e0 0. Ce que l'on avait fait, c'est une initialisation random bas\u00e9e sur une gaussienne centr\u00e9e r\u00e9duite.</p> In\u00a0[8]: Copied! <pre>logits=torch.randn(4)\nprobs=torch.softmax(logits,dim=0)\nloss= -probs[1].log()\nprobs,loss\n</pre> logits=torch.randn(4) probs=torch.softmax(logits,dim=0) loss= -probs[1].log() probs,loss Out[8]: <pre>(tensor([0.3143, 0.0607, 0.3071, 0.3178]), tensor(2.8012))</pre> <p>On voit assez rapidement le probl\u00e8me, l'al\u00e9atoire de la gaussienne va faire pencher la balance d'un c\u00f4t\u00e9 ou de l'autre (vous pouvez lancer plusieurs fois le code pr\u00e9c\u00e9dent pour vous en assurer). Mais alors, que peut-on faire ? Il suffit en fait de multiplier notre vecteur logit par une petite valeur pour diminuer la valeur initiale des poids et rendre le softmax plus uniforme.</p> In\u00a0[9]: Copied! <pre>logits=torch.randn(4)*0.01\nprobs=torch.softmax(logits,dim=0)\nloss= -probs[1].log()\nprobs,loss\n</pre> logits=torch.randn(4)*0.01 probs=torch.softmax(logits,dim=0) loss= -probs[1].log() probs,loss Out[9]: <pre>(tensor([0.2489, 0.2523, 0.2495, 0.2493]), tensor(1.3772))</pre> <p>On obtient, \u00e0 peu de choses pr\u00e8s, le m\u00eame loss que pour des probabilit\u00e9s uniformes.</p> <p>Notes : Par contre, on peut initialiser la valeur du biais \u00e0 z\u00e9ro car cela n'a pas de sens d'avoir un biais positif ou n\u00e9gatif \u00e0 l'initialisation.</p> <p>Reprenons le code pr\u00e9c\u00e9dent mais avec les nouvelles valeurs d'initialisation.</p> In\u00a0[10]: Copied! <pre>C = torch.randn((46, embed_dim))\nW1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids \u00e0 une petite valeur\nb1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0\nW2 = torch.randn((hidden_dim, 46))*0.01\nb2 = torch.randn(46)*0 \nparameters = [C, W1, b1, W2, b2]\nfor p in parameters:\n  p.requires_grad = True\n</pre> C = torch.randn((46, embed_dim)) W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids \u00e0 une petite valeur b1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0 W2 = torch.randn((hidden_dim, 46))*0.01 b2 = torch.randn(46)*0  parameters = [C, W1, b1, W2, b2] for p in parameters:   p.requires_grad = True In\u00a0[11]: Copied! <pre>lossi = []\n\nfor i in range(max_steps):\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  Xb, Yb = Xtr[ix], Ytr[ix] \n  emb = C[Xb] \n  embcat = emb.view(emb.shape[0], -1)\n  hpreact = embcat @ W1 + b1 \n  h = torch.tanh(hpreact) \n  logits = h @ W2 + b2 \n  loss = F.cross_entropy(logits, Yb)\n  \n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  lr = 0.1 if i &lt; 100000 else 0.01 \n  for p in parameters:\n    p.data += -lr * p.grad\n  if i % 10000 == 0:\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n</pre> lossi = []  for i in range(max_steps):   ix = torch.randint(0, Xtr.shape[0], (batch_size,))   Xb, Yb = Xtr[ix], Ytr[ix]    emb = C[Xb]    embcat = emb.view(emb.shape[0], -1)   hpreact = embcat @ W1 + b1    h = torch.tanh(hpreact)    logits = h @ W2 + b2    loss = F.cross_entropy(logits, Yb)      for p in parameters:     p.grad = None   loss.backward()   lr = 0.1 if i &lt; 100000 else 0.01    for p in parameters:     p.data += -lr * p.grad   if i % 10000 == 0:     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item()) <pre>      0/ 200000: 3.8304\n  10000/ 200000: 2.4283\n  20000/ 200000: 2.0651\n  30000/ 200000: 2.1124\n  40000/ 200000: 2.3158\n  50000/ 200000: 2.2752\n  60000/ 200000: 2.1887\n  70000/ 200000: 2.1783\n  80000/ 200000: 1.8120\n  90000/ 200000: 2.3178\n 100000/ 200000: 2.0973\n 110000/ 200000: 1.8992\n 120000/ 200000: 1.6917\n 130000/ 200000: 2.2747\n 140000/ 200000: 1.8054\n 150000/ 200000: 2.3569\n 160000/ 200000: 2.4231\n 170000/ 200000: 2.0711\n 180000/ 200000: 2.1379\n 190000/ 200000: 1.8419\n</pre> In\u00a0[12]: Copied! <pre>plt.plot(lossi)\n</pre> plt.plot(lossi) Out[12]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f0278310110&gt;]</pre> <p>On a maintenant une courbe de loss qui ne commence pas \u00e0 une valeur aberrante, ce qui a pour effet d'acc\u00e9lerer l'optimisation.</p> <p>On peut se dire qu'un loss \u00e9l\u00e9v\u00e9 n'est pas forc\u00e9ment un probl\u00e8me. Mais une mauvaise initialisation des poids peut poser d'autres probl\u00e8mes. Consid\u00e9rons la premi\u00e8re it\u00e9ration de l'entra\u00eenement avec des valeurs initialis\u00e9e sans le facteur 0.01</p> In\u00a0[13]: Copied! <pre>C = torch.randn((46, embed_dim))\nW1 = torch.randn((block_size*embed_dim, hidden_dim)) \nb1 = torch.randn(hidden_dim) \nW2 = torch.randn((hidden_dim, 46))\nb2 = torch.randn(46)\nparameters = [C, W1, b1, W2, b2]\nfor p in parameters:\n  p.requires_grad = True\n</pre> C = torch.randn((46, embed_dim)) W1 = torch.randn((block_size*embed_dim, hidden_dim))  b1 = torch.randn(hidden_dim)  W2 = torch.randn((hidden_dim, 46)) b2 = torch.randn(46) parameters = [C, W1, b1, W2, b2] for p in parameters:   p.requires_grad = True In\u00a0[14]: Copied! <pre>ix = torch.randint(0, Xtr.shape[0], (batch_size,))\nXb, Yb = Xtr[ix], Ytr[ix] \nemb = C[Xb] \nembcat = emb.view(emb.shape[0], -1)\nhpreact = embcat @ W1 + b1 \nh = torch.tanh(hpreact) \nlogits = h @ W2 + b2 \nloss = F.cross_entropy(logits, Yb)\n  \nfor p in parameters:\n  p.grad = None\nloss.backward()\n</pre> ix = torch.randint(0, Xtr.shape[0], (batch_size,)) Xb, Yb = Xtr[ix], Ytr[ix]  emb = C[Xb]  embcat = emb.view(emb.shape[0], -1) hpreact = embcat @ W1 + b1  h = torch.tanh(hpreact)  logits = h @ W2 + b2  loss = F.cross_entropy(logits, Yb)    for p in parameters:   p.grad = None loss.backward() <p>On regarde l'histogramme des valeurs apr\u00e8s la fonction d'activation tanh.</p> In\u00a0[15]: Copied! <pre>plt.hist(h.view(-1).tolist(),50);\n</pre> plt.hist(h.view(-1).tolist(),50); <p>On voit que la majorit\u00e9 des valeurs sont autour de 1 ou -1.</p> <p>En quoi cela pose t-il un probl\u00e8me ? Lors du calcul du gradient, avec la r\u00e8gle de la cha\u00eene, on va multiplier les gradients des diff\u00e9rentes \u00e9tapes de calcul. La d\u00e9riv\u00e9e de la fonction tanh est : $tanh'(t)= 1 - t^2$ Si les valeurs de t sont \u00e0 1 ou -1 alors le gradient va \u00eatre extremement faible (jamais nul car c'est une asymptote) ce qui veut dire que le gradient ne se propage pas et donc l'optimization ne peut pas fonctionner de mani\u00e8re optimale au d\u00e9but de l'entra\u00eenement.</p> <p>On peut visualiser les valeurs de chaque neurone.</p> In\u00a0[16]: Copied! <pre>plt.figure(figsize=(20,10))\nplt.imshow(h.abs()&gt;0.99,cmap='gray',interpolation='nearest')\n</pre> plt.figure(figsize=(20,10)) plt.imshow(h.abs()&gt;0.99,cmap='gray',interpolation='nearest') Out[16]: <pre>&lt;matplotlib.image.AxesImage at 0x7f02780ae550&gt;</pre> <p>Chaque point blanc correspond \u00e0 un neurone dont le gradient est \u00e0 peu pr\u00e8s \u00e9gal \u00e0 0.</p> <p>Neurone mort : Si une de ces colonnes \u00e9tait enti\u00e8rement blanche, cela voudrait dire que le neurone ne s'active sur aucun \u00e9l\u00e9ment (du batch), ce qui signifie que c'est un neurone inutile, qui n'aura aucun impact sur le r\u00e9sultat et qu'on ne peut pas optimiser (sur les valeurs pr\u00e9sentes dans ce batch).</p> <p>Notes :</p> <ul> <li>Ce type de comportement n'est pas exclusif \u00e0 la tanh : la sigmoid et la ReLU peuvent avoir le m\u00eame probl\u00e8me.</li> <li>Le probl\u00e8me ne nous a pas empech\u00e9 d'entra\u00eener notre r\u00e9seau correctement car il s'agit d'un petit mod\u00e8le. Sur des r\u00e9seaux plus profonds, c'est un gros probl\u00e8me et il est conseill\u00e9 de v\u00e9rifier les activations de votre r\u00e9seau aux diff\u00e9rentes \u00e9tapes.</li> <li>Les neurones morts peuvent arriver \u00e0 l'initialisation mais aussi pendant l'entra\u00eenement si le learning rate est trop \u00e9l\u00e9v\u00e9 par exemple.</li> </ul> <p>Par chance, ce probl\u00e8me peut se r\u00e9soudre exactement de la m\u00eame mani\u00e8re que le probl\u00e8me du loss trop \u00e9l\u00e9v\u00e9. Pour nous en assurer, regardons les valeurs des activations et les neurones inactifs \u00e0 l'initialisation avec nos nouvelles valeurs.</p> In\u00a0[17]: Copied! <pre>C = torch.randn((46, embed_dim))\nW1 = torch.randn((block_size*embed_dim, hidden_dim)) *0.01# On initialise les poids \u00e0 une petite valeur\nb1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0\nW2 = torch.randn((hidden_dim, 46)) *0.01\nb2 = torch.randn(46)*0 \nparameters = [C, W1, b1, W2, b2]\nfor p in parameters:\n  p.requires_grad = True\n</pre> C = torch.randn((46, embed_dim)) W1 = torch.randn((block_size*embed_dim, hidden_dim)) *0.01# On initialise les poids \u00e0 une petite valeur b1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0 W2 = torch.randn((hidden_dim, 46)) *0.01 b2 = torch.randn(46)*0  parameters = [C, W1, b1, W2, b2] for p in parameters:   p.requires_grad = True In\u00a0[18]: Copied! <pre>ix = torch.randint(0, Xtr.shape[0], (batch_size,))\nXb, Yb = Xtr[ix], Ytr[ix] \nemb = C[Xb] \nembcat = emb.view(emb.shape[0], -1)\nhpreact = embcat @ W1 + b1 \nh = torch.tanh(hpreact) \nlogits = h @ W2 + b2 \nloss = F.cross_entropy(logits, Yb)\n  \nfor p in parameters:\n  p.grad = None\nloss.backward()\n</pre> ix = torch.randint(0, Xtr.shape[0], (batch_size,)) Xb, Yb = Xtr[ix], Ytr[ix]  emb = C[Xb]  embcat = emb.view(emb.shape[0], -1) hpreact = embcat @ W1 + b1  h = torch.tanh(hpreact)  logits = h @ W2 + b2  loss = F.cross_entropy(logits, Yb)    for p in parameters:   p.grad = None loss.backward() In\u00a0[19]: Copied! <pre>plt.hist(h.view(-1).tolist(),50);\n</pre> plt.hist(h.view(-1).tolist(),50); In\u00a0[20]: Copied! <pre>plt.figure(figsize=(20,10))\nplt.imshow(h.abs()&gt;0.99,cmap='gray',interpolation='nearest')\n</pre> plt.figure(figsize=(20,10)) plt.imshow(h.abs()&gt;0.99,cmap='gray',interpolation='nearest') Out[20]: <pre>&lt;matplotlib.image.AxesImage at 0x7f025c538190&gt;</pre> <p>Tout va pour le mieux !</p> <p>Ce probl\u00e8me \u00e9tant tr\u00e8s important, de nombreuses recherches se sont dirig\u00e9es sur ce sujet. Une publication notable est Delving Deep into Rectifiers qui introduit la Kaiming initialization. Le papier propose des valeurs d'initialization pour chaque fonction d'activation qui font garantir une distribution centr\u00e9e r\u00e9duite sur l'ensemble du r\u00e9seau. Cette m\u00e9thode est implement\u00e9e en pytorch et les couches que l'on va cr\u00e9er en pytorch sont directement initialis\u00e9e de cette mani\u00e8re.</p> <p>Ce probl\u00e8me est en effet un probl\u00e8me majeur. Cependant, lorsque l'on utilise pytorch, tout est d\u00e9j\u00e0 initialis\u00e9 de mani\u00e8re correcte et ce n'est en g\u00e9n\u00e9ral pas n\u00e9cessaire de modifier ces valeurs.</p> <p>De plus, de nombreuses m\u00e9thodes ont \u00e9t\u00e9 propos\u00e9es pour diminuer ce probl\u00e8me, principalement :</p> <ul> <li>La batch norm, que nous verrons dans le notebook suivant, qui consiste \u00e0 normaliser les valeurs avant l'activation tout le long du r\u00e9seau.</li> <li>Les connexions r\u00e9siduelles qui permettent de transmettre le gradient dans l'int\u00e9gralit\u00e9 du r\u00e9seau sans que celui-ci ne soit trop impact\u00e9 par les fonctions d'activations.</li> </ul> <p>Malgr\u00e9 l'importance de ces consid\u00e9rations, en pratique, il n'est pas forc\u00e9ment n\u00e9cessaire d'\u00eatre au courant pour entra\u00eener un r\u00e9seau de neurones.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html#activations-et-initialisations","title":"Activations et Initialisations\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html#reprise-du-code","title":"Reprise du code\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html#loss-anormalement-eleve-a-linitialisation","title":"Loss anormalement \u00e9l\u00e9v\u00e9 \u00e0 l'initialisation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html#petit-exemple-illustrant-le-probleme","title":"Petit exemple illustrant le probl\u00e8me\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html#entrainement-avec-lajustement-de-linitialisation","title":"Entra\u00eenement avec l'ajustement de l'initialisation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html#autre-probleme","title":"Autre probl\u00e8me\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html#comment-resoudre-ce-probleme","title":"Comment r\u00e9soudre ce probl\u00e8me ?\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html#valeurs-optimales-a-linitialisation","title":"Valeurs optimales \u00e0 l'initialisation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/01_ActivationEtInitialisation.html#pourquoi-ce-cours-est-dans-les-bonus-alors-quil-semble-tres-important","title":"Pourquoi ce cours est dans les bonus alors qu'il semble tr\u00e8s important ?\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html","title":"Batch Normalization","text":"<p>La Batch Normalization (ou normalisation par lot) a \u00e9t\u00e9 introduite en 2015 dans l'article Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift et a eu un impact majeur dans le milieu du Deep Learning. Aujourd'hui, la normalisation est utilis\u00e9e presque syst\u00e9matiquement qu'il s'agisse de BatchNorm, LayerNorm ou GroupNorm (et d'autres).</p> <p>L'id\u00e9e de la BatchNorm est assez simple et est en rapport direct avec le notebook pr\u00e9c\u00e9dent. On veut avoir des preactivations suivant une distribution \u00e0 peu pr\u00e8s gaussienne \u00e0 chaque couche de notre r\u00e9seau. On va vu qu'avec une bonne initialisation, cela permettait d'avoir ce comportement mais une bonne initialisation n'est pas toujours \u00e9vidente surtout quand on utilise beaucoup de couches diff\u00e9rentes.</p> <p>La BatchNorm consiste \u00e0 normaliser les preactivations par rapport \u00e0 la dimension du batch avant de les passer dans les fonctions d'activations. Cela va garantir qu'on a une distribution environ gausienne \u00e0 chaque \u00e9tape.</p> <p>Cette normalisation n'a pas d'impact pour l'optimisation car il s'agit d'une fonction d\u00e9rivable.</p> <p>On va \u00e0 nouveau reprendre le code du notebook pr\u00e9c\u00e9dent pour cr\u00e9er notre batch normalization.</p> In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn.functional as F\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F %matplotlib inline In\u00a0[3]: Copied! <pre>words = open('../05_NLP/prenoms.txt', 'r').read().splitlines()\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\n</pre> words = open('../05_NLP/prenoms.txt', 'r').read().splitlines() chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} In\u00a0[4]: Copied! <pre>block_size = 3 # Contexte\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] \n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> block_size = 3 # Contexte  def build_dataset(words):     X, Y = [], []      for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix]     X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([180834, 3]) torch.Size([180834])\ntorch.Size([22852, 3]) torch.Size([22852])\ntorch.Size([22639, 3]) torch.Size([22639])\n</pre> In\u00a0[5]: Copied! <pre>embed_dim=10 # Dimension de l'embedding de C\nhidden_dim=200 # Dimension de la couche cach\u00e9e\n\nC = torch.randn((46, embed_dim))\nW1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids \u00e0 une petite valeur\nb1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0\nW2 = torch.randn((hidden_dim, 46))*0.01\nb2 = torch.randn(46)*0 \nparameters = [C, W1, b1, W2, b2]\nfor p in parameters:\n  p.requires_grad = True\n</pre> embed_dim=10 # Dimension de l'embedding de C hidden_dim=200 # Dimension de la couche cach\u00e9e  C = torch.randn((46, embed_dim)) W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids \u00e0 une petite valeur b1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0 W2 = torch.randn((hidden_dim, 46))*0.01 b2 = torch.randn(46)*0  parameters = [C, W1, b1, W2, b2] for p in parameters:   p.requires_grad = True <p>Voici notre code de propagation avant :</p> In\u00a0[6]: Copied! <pre>batch_size = 32\n\nix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  \n# Forward\nXb, Yb = Xtr[ix], Ytr[ix] \nemb = C[Xb] \nembcat = emb.view(emb.shape[0], -1)\nhpreact = embcat @ W1 + b1 \n\nh = torch.tanh(hpreact) \nlogits = h @ W2 + b2 \nloss = F.cross_entropy(logits, Yb)\n</pre> batch_size = 32  ix = torch.randint(0, Xtr.shape[0], (batch_size,))    # Forward Xb, Yb = Xtr[ix], Ytr[ix]  emb = C[Xb]  embcat = emb.view(emb.shape[0], -1) hpreact = embcat @ W1 + b1   h = torch.tanh(hpreact)  logits = h @ W2 + b2  loss = F.cross_entropy(logits, Yb) <p>Si on reprend l'article, on a les informations suivantes :</p> <p></p> <p>Dans un premier temps, il s'agit simplement de normaliser.</p> <p>Pour cela, on va calculer la moyenne et l'\u00e9cart type de hpreact puis normaliser gr\u00e2ce \u00e0 ces valeurs :</p> In\u00a0[7]: Copied! <pre>epsilon=1e-6\nhpreact_mean = hpreact.mean(dim=0, keepdim=True)\nhpreact_std= hpreact.std(dim=0, keepdim=True)\nhpreact_norm = (hpreact - hpreact_mean) / (hpreact_std+epsilon)\n</pre> epsilon=1e-6 hpreact_mean = hpreact.mean(dim=0, keepdim=True) hpreact_std= hpreact.std(dim=0, keepdim=True) hpreact_norm = (hpreact - hpreact_mean) / (hpreact_std+epsilon) <p>On va maintenant pouvoir l'int\u00e9grer \u00e0 notre propagation avant.</p> <p>Mais avant cela, on peut remarquer que l'on a pas implement\u00e9 la partie scale and shift :</p> <p></p> <p>A quoi \u00e7a sert ? : En appliquant la normalisation, on confine les poids \u00e0 ne prendre que des valeurs d'une gaussienne centr\u00e9e r\u00e9duite. Cela va r\u00e9duire les capacit\u00e9s d'expression de notre mod\u00e8le. Les param\u00e8tres apprenables $\\gamma$ et $\\beta$ permettent de contourner ce probl\u00e8me en ajoutant d'un part un shift avec $\\beta$ et un scale avec $\\gamma$.</p> <p>Comme il s'agit de param\u00e8tres apprenables, on doit aussi les ajouter dans les param\u00e8tres du mod\u00e8le :</p> In\u00a0[\u00a0]: Copied! <pre>C = torch.randn((46, embed_dim))\nW1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids \u00e0 une petite valeur\nb1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0\nW2 = torch.randn((hidden_dim, 46))*0.01\nb2 = torch.randn(46)*0 \n# Param\u00e8tres de batch normalization\nbngain = torch.ones((1, hidden_dim))\nbnbias = torch.zeros((1, hidden_dim))\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nfor p in parameters:\n  p.requires_grad = True\n</pre> C = torch.randn((46, embed_dim)) W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids \u00e0 une petite valeur b1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0 W2 = torch.randn((hidden_dim, 46))*0.01 b2 = torch.randn(46)*0  # Param\u00e8tres de batch normalization bngain = torch.ones((1, hidden_dim)) bnbias = torch.zeros((1, hidden_dim))  parameters = [C, W1, b1, W2, b2, bngain, bnbias] for p in parameters:   p.requires_grad = True <p>Et en propagation avant, on aura donc :</p> In\u00a0[\u00a0]: Copied! <pre>batch_size = 32\n\nix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  \n# Forward\nXb, Yb = Xtr[ix], Ytr[ix] \nemb = C[Xb] \nembcat = emb.view(emb.shape[0], -1)\nhpreact = embcat @ W1 + b1 \n\n# Batch normalization\nbnmean = hpreact.mean(0, keepdim=True)\nbnstd = hpreact.std(0, keepdim=True)\nhpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n\nh = torch.tanh(hpreact) \nlogits = h @ W2 + b2 \nloss = F.cross_entropy(logits, Yb)\n</pre> batch_size = 32  ix = torch.randint(0, Xtr.shape[0], (batch_size,))    # Forward Xb, Yb = Xtr[ix], Ytr[ix]  emb = C[Xb]  embcat = emb.view(emb.shape[0], -1) hpreact = embcat @ W1 + b1   # Batch normalization bnmean = hpreact.mean(0, keepdim=True) bnstd = hpreact.std(0, keepdim=True) hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias  h = torch.tanh(hpreact)  logits = h @ W2 + b2  loss = F.cross_entropy(logits, Yb) <p>En y refl\u00e9chissant un peu, on peut vite trouver des probl\u00e8mes potentiels d\u00fb \u00e0 la BatchNorm :</p> <p>**Un exemple est impact\u00e9 par les autres \u00e9l\u00e9ments du batch*** :  Le fait de normaliser selon la dimension du *batch fait que les valeurs de chaque exemple au sein du batch sont impact\u00e9es par les autres exemples du batch. Cela pourrait sembler catastrophique mais en pratique, c'est plut\u00f4t une bonne chose. Le fait d'avoir des batchs al\u00e9atoires \u00e0 chaque epoch permet une sorte de r\u00e9gularisation ce qui va permettre au mod\u00e8le d'\u00eatre moins enclin \u00e0 *overfit sur les donn\u00e9es. N\u00e9anmoins, si on veut \u00e9viter ce probl\u00e8me, on peut utiser d'autres m\u00e9thodes de normalisation qui ne normalisent pas selon la dimension du batch. En pratique, la BatchNorm est encore \u00e9normement utilis\u00e9e car elle fonctionne tr\u00e8s bien empiriquement.</p> <p>Phase de test sur un seul \u00e9l\u00e9ment : Pendant l'entra\u00eenement, chaque \u00e9l\u00e9ment est impact\u00e9 par les \u00e9l\u00e9ments de son batch mais lorsqu'on est en phase d'inf\u00e9rence et que l'on veut utiliser notre mod\u00e8le sur un seul \u00e9l\u00e9ment, on ne peut plus faire la BatchNorm. C'est un probl\u00e8me car on ne veut pas avoir un comportement diff\u00e9rent pendant l'entra\u00eenement et pendant l'inf\u00e9rence.</p> <p>Pour palier \u00e0 ce probl\u00e8me, on a deux solutions :</p> <ul> <li>On peut calculer la moyenne et la variance sur l'ensemble des \u00e9l\u00e9ments  \u00e0 la fin de l'entra\u00eenement et utiliser ces valeurs. En pratique, on ne veut pas faire une it\u00e9ration suppl\u00e9mentaire sur l'ensemble du dataset juste pour \u00e7a donc personne ne fait comme \u00e7a.</li> <li>Une autre solution consiste \u00e0 mettre \u00e0 jour notre moyenne et vaiance tout au long de l'entra\u00eenement gr\u00e2ce \u00e0 un EMA (exponential moving average). A la fin de l'entra\u00eenement, on aura une bonne approximation de la moyenne et de la variance de l'ensemble des \u00e9l\u00e9ments d'entra\u00eenement.</li> </ul> <p>En pratique, on peut l'implementer comme \u00e7a en python :</p> In\u00a0[9]: Copied! <pre>C = torch.randn((46, embed_dim))\nW1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids \u00e0 une petite valeur\nb1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0\nW2 = torch.randn((hidden_dim, 46))*0.01\nb2 = torch.randn(46)*0 \n# Param\u00e8tres de batch normalization\nbngain = torch.ones((1, hidden_dim))\nbnbias = torch.zeros((1, hidden_dim))\nbnmean_running = torch.zeros((1, hidden_dim))\nbnstd_running = torch.ones((1, hidden_dim))\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nfor p in parameters:\n  p.requires_grad = True\n</pre> C = torch.randn((46, embed_dim)) W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids \u00e0 une petite valeur b1 = torch.randn(hidden_dim) *0 # On initialise les biais \u00e0 0 W2 = torch.randn((hidden_dim, 46))*0.01 b2 = torch.randn(46)*0  # Param\u00e8tres de batch normalization bngain = torch.ones((1, hidden_dim)) bnbias = torch.zeros((1, hidden_dim)) bnmean_running = torch.zeros((1, hidden_dim)) bnstd_running = torch.ones((1, hidden_dim))  parameters = [C, W1, b1, W2, b2, bngain, bnbias] for p in parameters:   p.requires_grad = True In\u00a0[11]: Copied! <pre>batch_size = 32\n\nix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  \n# Forward\nXb, Yb = Xtr[ix], Ytr[ix] \nemb = C[Xb] \nembcat = emb.view(emb.shape[0], -1)\nhpreact = embcat @ W1 + b1 \n\n# Batch normalization\nbnmeani = hpreact.mean(0, keepdim=True)\nbnstdi = hpreact.std(0, keepdim=True)\nhpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\nwith torch.no_grad(): # On ne veut pas calculer de gradient pour ces op\u00e9rations\n    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n\nh = torch.tanh(hpreact) \nlogits = h @ W2 + b2 \nloss = F.cross_entropy(logits, Yb)\n</pre> batch_size = 32  ix = torch.randint(0, Xtr.shape[0], (batch_size,))    # Forward Xb, Yb = Xtr[ix], Ytr[ix]  emb = C[Xb]  embcat = emb.view(emb.shape[0], -1) hpreact = embcat @ W1 + b1   # Batch normalization bnmeani = hpreact.mean(0, keepdim=True) bnstdi = hpreact.std(0, keepdim=True) hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias with torch.no_grad(): # On ne veut pas calculer de gradient pour ces op\u00e9rations     bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani     bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi  h = torch.tanh(hpreact)  logits = h @ W2 + b2  loss = F.cross_entropy(logits, Yb) <p>Note : Dans notre impl\u00e9mentation, on a choisi de prendre 0.001 pour notre EMA. Dans la couche BatchNorm de pytorch, ce param\u00e8tre est d\u00e9fini par momentum et sa valeur par d\u00e9faut est 0.1. En pratique, le choix de cette valeur va d\u00e9pendre de la taille de votre batch par rapport \u00e0 la taille du jeu de donn\u00e9es d'entra\u00eenement. Pour un gros batch avec un petit jeu de donn\u00e9es, on va prendre 0.1 par exemple et pour un petit batch avec un gros jeu de donn\u00e9es, on prendre plut\u00f4t une plus petite valeur.</p> <p>Testons maintenant l'entra\u00eenement de notre mod\u00e8le pour v\u00e9rifier que le couche fonctionne. Pour ce petit mod\u00e8le, on aura pas de diff\u00e9rence de performance.</p> In\u00a0[12]: Copied! <pre>lossi = []\nmax_steps = 200000\n\nfor i in range(max_steps):\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  Xb, Yb = Xtr[ix], Ytr[ix] \n  emb = C[Xb] \n  embcat = emb.view(emb.shape[0], -1)\n  hpreact = embcat @ W1 + b1 \n  \n  # Batch normalization\n  bnmeani = hpreact.mean(0, keepdim=True)\n  bnstdi = hpreact.std(0, keepdim=True)\n  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n  with torch.no_grad(): # On ne veut pas calculer de gradient pour ces op\u00e9rations\n      bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n      bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n    \n  h = torch.tanh(hpreact) \n  logits = h @ W2 + b2 \n  loss = F.cross_entropy(logits, Yb)\n  \n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  lr = 0.1 if i &lt; 100000 else 0.01 \n  for p in parameters:\n    p.data += -lr * p.grad\n  if i % 10000 == 0:\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n</pre> lossi = [] max_steps = 200000  for i in range(max_steps):   ix = torch.randint(0, Xtr.shape[0], (batch_size,))   Xb, Yb = Xtr[ix], Ytr[ix]    emb = C[Xb]    embcat = emb.view(emb.shape[0], -1)   hpreact = embcat @ W1 + b1       # Batch normalization   bnmeani = hpreact.mean(0, keepdim=True)   bnstdi = hpreact.std(0, keepdim=True)   hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias   with torch.no_grad(): # On ne veut pas calculer de gradient pour ces op\u00e9rations       bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani       bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi        h = torch.tanh(hpreact)    logits = h @ W2 + b2    loss = F.cross_entropy(logits, Yb)      for p in parameters:     p.grad = None   loss.backward()   lr = 0.1 if i &lt; 100000 else 0.01    for p in parameters:     p.data += -lr * p.grad   if i % 10000 == 0:     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item()) <pre>      0/ 200000: 3.8241\n  10000/ 200000: 1.9756\n  20000/ 200000: 2.7151\n  30000/ 200000: 2.3287\n  40000/ 200000: 2.1411\n  50000/ 200000: 2.3207\n  60000/ 200000: 2.3250\n  70000/ 200000: 2.0320\n  80000/ 200000: 2.0615\n  90000/ 200000: 2.2468\n 100000/ 200000: 2.2081\n 110000/ 200000: 2.1418\n 120000/ 200000: 1.9665\n 130000/ 200000: 1.8572\n 140000/ 200000: 2.0577\n 150000/ 200000: 2.1804\n 160000/ 200000: 1.8604\n 170000/ 200000: 1.9810\n 180000/ 200000: 1.8228\n 190000/ 200000: 1.9977\n</pre> <p>Biais : La batch norm a pour effet de normaliser les preactivations des poids. Cette normalisation va nullifier le biais (car celui-ci d\u00e9cale la distribution et nous on la recentre). Lorsqu'on utilise la BatchNorm, on peut se passer du biais. En pratique, si on laisse un biais \u00e7a ne pose pas de probl\u00e8me mais c'est un param\u00e8tre du r\u00e9seau qui sera inutile.</p> <p>Placement de la BatchNorm : D'apr\u00e8s ce qu'on a vu, il est logique de placer la BatchNorm avant la fonction d'activation. En pratique, certains pr\u00e9ferent la placer apr\u00e8s la couche d'activation donc ne soyez pas \u00e9tonn\u00e9 si vous tombez sur \u00e7a dans la litt\u00e9rature ou dans un code.</p> <p>Nous allons faire un tour rapide des autres normalisation qui sont utilis\u00e9 pour l'entra\u00eenement des r\u00e9seaux de neurones.</p> <p></p> <p>Figure extraite de l'article</p> <p>Layer Normalization : Cette couche de normalisation est \u00e9galement tr\u00e8s fr\u00e9quemment utilis\u00e9e notamment dans les mod\u00e8les de langages (GPT, Llama). Il s'agit simplement de normaliser sur l'ensemble des activations de la couche plut\u00f4t que sur l'axe du batch. Dans notre impl\u00e9mentation, cela reviendrait simplement \u00e0 changer :</p> In\u00a0[\u00a0]: Copied! <pre># Batch normalization\nbnmeani = hpreact.mean(0, keepdim=True)  \nbnstdi = hpreact.std(0, keepdim=True)   \n# Layer normalization\nbnmeani = hpreact.mean(1, keepdim=True)  \nbnstdi = hpreact.std(1, keepdim=True)  \n</pre> # Batch normalization bnmeani = hpreact.mean(0, keepdim=True)   bnstdi = hpreact.std(0, keepdim=True)    # Layer normalization bnmeani = hpreact.mean(1, keepdim=True)   bnstdi = hpreact.std(1, keepdim=True)   <p>Instance Normalization : Cette couche va normaliser les activations sur chaque canal de chaque \u00e9l\u00e9ment ind\u00e9pendamment.</p> <p>Group Normalization : Cette couche est une sorte de fusion entre la LayerNorm et l'InstanceNorm puisqu'on va calculer la normalisation sur des groupes de canaux (si la taille d'un groupe vaut 1, c'est l'InstanceNorm et si la taille d'un groupe vaut C, c'est la LayerNorm)</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html#batch-normalization","title":"Batch Normalization\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html#reprise-du-code","title":"Reprise du code\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html#implementation-de-la-batchnorm","title":"Impl\u00e9mentation de la BatchNorm\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html#le-probleme-de-la-batch-normalization","title":"Le probl\u00e8me de la Batch Normalization\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html#considerations-supplementaires","title":"Consid\u00e9rations suppl\u00e9mentaires\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/02_BatchNorm.html#autres-normalisation","title":"Autres normalisation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html","title":"Data Augmentation","text":"<p>Vous le savez, la potentiel des mod\u00e8les de deep learning est limit\u00e9 par le quantit\u00e9 de donn\u00e9es dont on dispose pour entra\u00eener notre mod\u00e8le. De mani\u00e8re g\u00e9n\u00e9rale, on a besoin d'une \u00e9norme quantit\u00e9 de donn\u00e9es : les mod\u00e8les de vision les plus performants sont entra\u00een\u00e9s sur des milliards d'images et les mod\u00e8les de NLP (LLM) sur des trillions de tokens.</p> <p>Bien souvent, obtenir des donn\u00e9es de qualit\u00e9 labelis\u00e9es est une t\u00e2che complexe et surtout couteuse.</p> <p>Est ce qu'il ne serait pas possible d'augmenter artificiellement nos donn\u00e9es gr\u00e2ce \u00e0 des transformations ing\u00e9nieuses ?</p> <p>OUI ! C'est possible et c'est ce qu'on appelle la data augmentation. Dans cette partie, nous allons regarder diff\u00e9rentes m\u00e9thodes de data augmentation pour les images et pr\u00e9senter rapidement les possibilit\u00e9s de data augmentation pour le NLP et l'audio.</p> <p>Les techniques de data augmentation pr\u00e9sent\u00e9es dans cette partie ont montr\u00e9 des int\u00earets pour l'entra\u00eenement de mod\u00e8les de deep learning. En revanche, il faut \u00eatre prudent car parfois certains types de data augmentation ne sont pas en accord avec notre objectif d'entra\u00eenement (mettons que l'on veut d\u00e9tecter les personnes allong\u00e9es, on va \u00e9viter de tourner l'image de 90 degr\u00e9s).</p> <p>Pour introduire les diff\u00e9rentes m\u00e9thodes de data augmentation, nous utilisons pytorch et plus particuli\u00e8rement torchvision qui propose un large choix de techniques de data augmentation.</p> <p>Commen\u00e7ons avec notre image de base :</p> In\u00a0[1]: Copied! <pre>from PIL import Image\nimage_pil=Image.open(\"images/tigrou.png\")\nimage_pil\n</pre> from PIL import Image image_pil=Image.open(\"images/tigrou.png\") image_pil Out[1]: <p>Transformons notre image en tensor torch.</p> In\u00a0[2]: Copied! <pre>import torchvision.transforms as T \ntransform=T.Compose([T.ToTensor(),T.Resize((360,360))])\nimage=transform(image_pil)[0:3,:,:]\n</pre> import torchvision.transforms as T  transform=T.Compose([T.ToTensor(),T.Resize((360,360))]) image=transform(image_pil)[0:3,:,:] <pre>/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Une premi\u00e8re id\u00e9e pour la data augmentation consiste \u00e0 appliquer des transformations tel que l'inversion de l'image (horizontalement ou verticalement) ou la rotation de l'image. En effet, un chat \u00e0 l'envers reste un chat. A consid\u00e9rer : Si on voulait diff\u00e9rencier la classe \"chat\" et la classe \"chat \u00e0 l'envers\", on ne pourrait pas utiliser cette technique. Il faut toujours bien \u00eatre s\u00fbr de ce dont on a besoin.</p> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\n\nhoriz_flip=T.Compose([T.RandomHorizontalFlip(p=1)])\nimage_horiz_flip=horiz_flip(image)\nvert_flip=T.Compose([T.RandomVerticalFlip(p=1)])\nimage_vert_flip=vert_flip(image)\nrot=T.Compose([T.RandomRotation(degrees=90)])\nimage_rot=rot(image)\n\nplt.figure(figsize=(5,5))\nplt.subplot(221)\nplt.imshow(image.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(222)\nplt.imshow(image_horiz_flip.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(223)\nplt.imshow(image_vert_flip.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(224)\nplt.imshow(image_rot.permute(1,2,0))\nplt.axis(\"off\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  horiz_flip=T.Compose([T.RandomHorizontalFlip(p=1)]) image_horiz_flip=horiz_flip(image) vert_flip=T.Compose([T.RandomVerticalFlip(p=1)]) image_vert_flip=vert_flip(image) rot=T.Compose([T.RandomRotation(degrees=90)]) image_rot=rot(image)  plt.figure(figsize=(5,5)) plt.subplot(221) plt.imshow(image.permute(1,2,0)) plt.axis(\"off\") plt.subplot(222) plt.imshow(image_horiz_flip.permute(1,2,0)) plt.axis(\"off\") plt.subplot(223) plt.imshow(image_vert_flip.permute(1,2,0)) plt.axis(\"off\") plt.subplot(224) plt.imshow(image_rot.permute(1,2,0)) plt.axis(\"off\") plt.show() <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> <p>Une autre technique consiste \u00e0 crop une partie de l'image et utiliser ce crop comme image d'entr\u00e9e. Il y a la possibilit\u00e9 de CenterCrop (un crop au centre) ou RandomCrop (crop al\u00e9atoire dans l'image). A consid\u00e9rer : Avec cette m\u00e9thode, il faut faire attention \u00e0 ce que l'objet soit bien pr\u00e9sent dans notre crop. Si on choisit une valeur de crop trop petite ou que l'objet n'occupe pas une place assez importante dans l'image, ce type de data augmentation peut \u00eatre d\u00e9trimentaire (voir derni\u00e8re image).</p> In\u00a0[4]: Copied! <pre>crop=T.Compose([T.RandomCrop(200)])\nimage_crop=crop(image)\ncenter_crop=T.Compose([T.CenterCrop(150)])\nimage_center_crop=center_crop(image)\n\ncrop_small=T.Compose([T.RandomCrop(100)])\nimage_crop_small=crop_small(image)\n\nplt.figure(figsize=(5,5))\nplt.subplot(221)\nplt.imshow(image.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(222)\nplt.imshow(image_crop.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(223)\nplt.imshow(image_center_crop.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(224)\nplt.imshow(image_crop_small.permute(1,2,0))\nplt.axis(\"off\")\nplt.show()\n</pre> crop=T.Compose([T.RandomCrop(200)]) image_crop=crop(image) center_crop=T.Compose([T.CenterCrop(150)]) image_center_crop=center_crop(image)  crop_small=T.Compose([T.RandomCrop(100)]) image_crop_small=crop_small(image)  plt.figure(figsize=(5,5)) plt.subplot(221) plt.imshow(image.permute(1,2,0)) plt.axis(\"off\") plt.subplot(222) plt.imshow(image_crop.permute(1,2,0)) plt.axis(\"off\") plt.subplot(223) plt.imshow(image_center_crop.permute(1,2,0)) plt.axis(\"off\") plt.subplot(224) plt.imshow(image_crop_small.permute(1,2,0)) plt.axis(\"off\") plt.show() <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> <p>On peut \u00e9galement choisir de modifier les valeurs de luminosit\u00e9 (brightness), contraste (contrast), saturation (saturation) et teinte (hue) gr\u00e2ce \u00e0 la transformation ColorJitter.</p> In\u00a0[87]: Copied! <pre>bright=T.Compose([T.ColorJitter(brightness=0.8)])\nimage_bright=bright(image)\ncontr=T.Compose([T.ColorJitter(contrast=0.8)])\nimage_contr=contr(image)\n\nsatur=T.Compose([T.ColorJitter(saturation=0.8)])\nimage_satur=satur(image)\n\nplt.figure(figsize=(5,5))\nplt.subplot(221)\nplt.imshow(image.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(222)\nplt.imshow(image_bright.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(223)\nplt.imshow(image_contr.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(224)\nplt.imshow(image_satur.permute(1,2,0))\nplt.axis(\"off\")\nplt.show()\n</pre> bright=T.Compose([T.ColorJitter(brightness=0.8)]) image_bright=bright(image) contr=T.Compose([T.ColorJitter(contrast=0.8)]) image_contr=contr(image)  satur=T.Compose([T.ColorJitter(saturation=0.8)]) image_satur=satur(image)  plt.figure(figsize=(5,5)) plt.subplot(221) plt.imshow(image.permute(1,2,0)) plt.axis(\"off\") plt.subplot(222) plt.imshow(image_bright.permute(1,2,0)) plt.axis(\"off\") plt.subplot(223) plt.imshow(image_contr.permute(1,2,0)) plt.axis(\"off\") plt.subplot(224) plt.imshow(image_satur.permute(1,2,0)) plt.axis(\"off\") plt.show() <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> <p>Beaucoup d'autres transformations sont possible. On peut supprimer une partie de l'image. On peut \u00e9galement ajouter un padding autour de l'image ou solarize l'image. Il est aussi possible de d\u00e9finir une transformation affine pr\u00e9cise de ce que l'on va appliquer comme transformation \u00e0 l'image</p> In\u00a0[85]: Copied! <pre>erase=T.Compose([T.RandomErasing(p=1)])\nimage_erase=erase(image)\nsolar=T.Compose([T.Pad(50),T.RandomSolarize(0.5,p=1)])\nimage_solar=solar(image)\n\naffin=T.Compose([T.RandomAffine(degrees=30,scale=(0.8,1.2),shear=30)])\nimage_affin=affin(image)\n\nplt.figure(figsize=(5,5))\nplt.subplot(221)\nplt.imshow(image.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(222)\nplt.imshow(image_erase.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(223)\nplt.imshow(image_solar.permute(1,2,0))\nplt.axis(\"off\")\nplt.subplot(224)\nplt.imshow(image_affin.permute(1,2,0))\nplt.axis(\"off\")\nplt.show()\n</pre> erase=T.Compose([T.RandomErasing(p=1)]) image_erase=erase(image) solar=T.Compose([T.Pad(50),T.RandomSolarize(0.5,p=1)]) image_solar=solar(image)  affin=T.Compose([T.RandomAffine(degrees=30,scale=(0.8,1.2),shear=30)]) image_affin=affin(image)  plt.figure(figsize=(5,5)) plt.subplot(221) plt.imshow(image.permute(1,2,0)) plt.axis(\"off\") plt.subplot(222) plt.imshow(image_erase.permute(1,2,0)) plt.axis(\"off\") plt.subplot(223) plt.imshow(image_solar.permute(1,2,0)) plt.axis(\"off\") plt.subplot(224) plt.imshow(image_affin.permute(1,2,0)) plt.axis(\"off\") plt.show() <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> <p>La data augmentation est une technique tr\u00e8s int\u00e9ressante permettant d'augmenter articifiellement les donn\u00e9es d'entra\u00eenement. Cela conduit \u00e0 la possibilit\u00e9 d'entra\u00eener de plus gros mod\u00e8les sans overfitting. En pratique, il est tr\u00e8s souvent int\u00e9ressant d'en inclure dans l'entra\u00eenement de son r\u00e9seau de neurones mais il faut quand m\u00eame faire attention \u00e0 ne pas faire n'importe quoi. Je vous invite \u00e0 tester votre data augmentation sur quelques \u00e9l\u00e9ments de votre dataset pour voir si rien ne vous choque.</p> <p>Note : Il existe d'autres m\u00e9thodes de data augmentation notamment l'ajout de bruit sur l'image. Vous pouvez retrouver la liste des transformations possible dans la documentation pytorch.</p> <p>On peut \u00e9galement faire de la data augmentation en NLP. Voici une liste de ce qu'il est possible de faire :</p> <ul> <li>On peut changer al\u00e9atoirement la position de certains mots dans une phrase (rend le mod\u00e8le robuste mais potentiellement dangereux \u00e0 faire)</li> <li>On peut remplacer certains mots par un de leurs synonymes</li> <li>On peut paraphraser</li> <li>On peut ajouter ou supprimer des mots au hasard dans la phrase</li> </ul> <p>Ces techniques ne sont pas adapt\u00e9es \u00e0 tous les probl\u00e8mes de NLP et il faut faire bien attention en les utilisant.</p> <p>Note : Avec l'arriv\u00e9e des LLM, il est souvent possible de fine-tune notre mod\u00e8le de mani\u00e8re efficace, m\u00eame avec tr\u00e8s peu de donn\u00e9es, ce qui diminue le recours \u00e0 la data augmentation en NLP.</p> <p>Dans le domaine de l'audio, il est aussi parfois utile d'utiliser de la data augmentation. Voici une liste de certaines techniques pour augmenter vos donn\u00e9es artificiellement en audio :</p> <ul> <li>Ajout de bruit dans l'audio (gaussien ou al\u00e9atoire) pour augmenter la performance du mod\u00e8le dans des situations complexes</li> <li>D\u00e9calage de l'enregistrement</li> <li>Changement de la vitesse de l'enregistrement</li> <li>Changer la hauteur du son (plus aigu ou plus grave)</li> </ul>"},{"location":"Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html#data-augmentation","title":"Data Augmentation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html#data-augmentation-pour-les-images","title":"Data augmentation pour les images\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html#inversion-horizontaleverticale-et-rotation","title":"Inversion horizontale/verticale et rotation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html#image-cropping","title":"Image cropping\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html#contraste-luminosite-saturation-et-teinte","title":"Contraste, luminosit\u00e9, saturation et teinte\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html#autres-transformations","title":"Autres transformations\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html#data-augmentation-pour-le-texte","title":"Data augmentation pour le texte\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/03_DataAugmentation.html#data-augmentation-pour-laudio","title":"Data augmentation pour l'audio\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html","title":"Broadcasting","text":"<p>Le broadcasting est la fa\u00e7on qu'\u00e0 pytorch de tra\u00eeter les tenseurs lors d'op\u00e9rations arithm\u00e9tiques non \u00e9videntes.</p> <p>Par exemple, il est \u00e9vident qu'on ne va pas pouvoir ajouter une matrice $3 \\times 3$ \u00e0 une matrice $4 \\times 2$ et ce cas-l\u00e0 nous causera une erreur dans pytorch. Par contre, si on veut ajouter un scalaire \u00e0 une matrice $3 \\times 3$ ou un vecteur de taille $3$ \u00e0 une matrice $3 \\times 3$, il est possible de trouver des op\u00e9rations logique m\u00eame si la fa\u00e7on de proc\u00e9der n'est pas forc\u00e9ment \u00e9vidente.</p> <p>Le broadcasting de pytorch est bas\u00e9 sur des r\u00e8gles simples qu'il faut garder en t\u00eate lorsque l'on manipule des tenseurs.</p> <p>Pour que deux tenseurs soient broadcastable, ils doivent satisfaire les r\u00e8gles suivantes :</p> <ul> <li>Chaque tenseur a au moins une dimension</li> <li>Lors de l'it\u00e9ration sur les tailles des dimensions, en commen\u00e7ant par la dimension de fin, les tailles des dimensions doivent soit \u00eatre \u00e9gales, soit l'une d'entre elles doit \u00eatre 1, soit l'une d'entre elles ne doit pas exister.</li> </ul> <p>Utilisons des examples pour que \u00e7a soit plus clair :</p> In\u00a0[2]: Copied! <pre>import torch\n\n# Deux tenseurs de la m\u00eame taille sont toujours broadcastables\nx=torch.zeros(5,7,3)\ny=torch.zeros(5,7,3)\n\n# Les deux tenseurs suivants ne sont pas broadcastables car x n'a pas au moins une dimension\nx=torch.zeros((0,))\ny=torch.zeros(2,2)\n\n# On aligne les dimensions visuellement pour voir si les tenseurs sont broadcastables\n# En partant de la droite,\n# 1. x et y ont la m\u00eame taille et sont de taille 1\n# 2. y est de taille 1\n# 3. x et y ont la m\u00eame taille\n# 4. la dimension de y n'existe pas\n# Les deux tenseurs sont donc broadcastables\nx=torch.zeros(5,3,4,1)\ny=torch.zeros(  3,1,1)\n\n# A l'inverse, ces deux tenseurs ne sont pas broadcastables car 3. x et y n'ont pas la m\u00eame taille\nx=torch.zeros(5,2,4,1)\ny=torch.zeros(  3,1,1)\n</pre> import torch  # Deux tenseurs de la m\u00eame taille sont toujours broadcastables x=torch.zeros(5,7,3) y=torch.zeros(5,7,3)  # Les deux tenseurs suivants ne sont pas broadcastables car x n'a pas au moins une dimension x=torch.zeros((0,)) y=torch.zeros(2,2)  # On aligne les dimensions visuellement pour voir si les tenseurs sont broadcastables # En partant de la droite, # 1. x et y ont la m\u00eame taille et sont de taille 1 # 2. y est de taille 1 # 3. x et y ont la m\u00eame taille # 4. la dimension de y n'existe pas # Les deux tenseurs sont donc broadcastables x=torch.zeros(5,3,4,1) y=torch.zeros(  3,1,1)  # A l'inverse, ces deux tenseurs ne sont pas broadcastables car 3. x et y n'ont pas la m\u00eame taille x=torch.zeros(5,2,4,1) y=torch.zeros(  3,1,1) <p>Maintenant que l'on sait comment reconna\u00eetre deux tenseurs broadcastables, on doit d\u00e9finir les r\u00e8gles qui s'applique lors de l'op\u00e9ration entre les deux. Les r\u00e8gles sont les suivantes :</p> <ul> <li>R\u00e8gle 1 : Si le nombre de dimensions de x et y n'est pas \u00e9gal, ajoutez 1 au d\u00e9but des dimensions du tenseur ayant le moins de dimensions pour les rendre de m\u00eame longueur.</li> <li>R\u00e8gle 2 : Ensuite, pour chaque taille de dimension, la taille de la dimension r\u00e9sultante est le maximum des tailles de x et y le long de cette dimension.</li> </ul> <p>Le tenseur dont la taille est modifi\u00e9 va \u00eatre dupliqu\u00e9 le nombre de fois n\u00e9cessaire pour faire coincider les tailles.</p> <p>Note : Si deux tenseurs ne sont pas broadcastables et qu'on tente de les ajouter, il y aura une erreur. Par contre, dans de nombreux cas, l'op\u00e9ration de broadcasting va fonctionner mais ne va pas faire l'op\u00e9ration que l'on souhaite \u00e0 cause des r\u00e8gles de broadcating. C'est pour ce cas qu'il est important de ma\u00eetriser ces r\u00e8gles.</p> <p>Reprenons, dans un premier temps, nos deux exemples :</p> <p>Ajouter un scalaire \u00e0 une matrice $3 \\times 3$ :</p> In\u00a0[12]: Copied! <pre>x=torch.randn(3,3)\ny=torch.tensor(1)\nprint(\"x : \" ,x)\nprint(\"y : \" ,y)\nprint(\"x+y : \" ,x+y)\nprint(\"x+y shape : \",(x+y).shape)\n# Le tenseur y est broadcast\u00e9 pour avoir la m\u00eame taille que x, il se transforme en tenseur de 1 de taille 3x3\n</pre> x=torch.randn(3,3) y=torch.tensor(1) print(\"x : \" ,x) print(\"y : \" ,y) print(\"x+y : \" ,x+y) print(\"x+y shape : \",(x+y).shape) # Le tenseur y est broadcast\u00e9 pour avoir la m\u00eame taille que x, il se transforme en tenseur de 1 de taille 3x3 <pre>x :  tensor([[ 0.6092, -0.6887,  0.3060],\n        [ 1.3496,  1.7739, -0.4011],\n        [-0.8876,  0.7196, -0.3810]])\ny :  tensor(1)\nx+y :  tensor([[1.6092, 0.3113, 1.3060],\n        [2.3496, 2.7739, 0.5989],\n        [0.1124, 1.7196, 0.6190]])\nx+y shape :  torch.Size([3, 3])\n</pre> <p>Ajouter un vecteur de taille $3$ \u00e0 une matrice $3 \\times 3$ :</p> In\u00a0[23]: Copied! <pre>x=torch.randn(3,3)\ny=torch.tensor([1,2,3]) # tenseur de taille 3\nprint(\"x : \" ,x)\nprint(\"y : \" ,y)\nprint(\"x+y : \" ,x+y)\nprint(\"x+y shape : \",(x+y).shape)\n# Le tenseur y est broadcast\u00e9 pour avoir la m\u00eame taille que x, il se transforme en tenseur de 1 de taille 3x3\n</pre> x=torch.randn(3,3) y=torch.tensor([1,2,3]) # tenseur de taille 3 print(\"x : \" ,x) print(\"y : \" ,y) print(\"x+y : \" ,x+y) print(\"x+y shape : \",(x+y).shape) # Le tenseur y est broadcast\u00e9 pour avoir la m\u00eame taille que x, il se transforme en tenseur de 1 de taille 3x3 <pre>x :  tensor([[ 0.9929, -0.1435,  1.5740],\n        [ 1.2143,  1.3366,  0.6415],\n        [-0.2718,  0.3497, -0.2650]])\ny :  tensor([1, 2, 3])\nx+y :  tensor([[1.9929, 1.8565, 4.5740],\n        [2.2143, 3.3366, 3.6415],\n        [0.7282, 2.3497, 2.7350]])\nx+y shape :  torch.Size([3, 3])\n</pre> <p>Consid\u00e9rons maintenant d'autres exemples plus compliqu\u00e9s :</p> In\u00a0[14]: Copied! <pre>x=torch.zeros(5,3,4,1)\ny=torch.zeros(  3,1,1)\nprint(\"x+y shape : \",(x+y).shape)\n# Le tenseur y a \u00e9t\u00e9 \u00e9tendu en taille 1x3x1x1 (r\u00e8gle 1) puis dupliqu\u00e9 en taille 5x3x4x1 (r\u00e8gle 2)\n</pre> x=torch.zeros(5,3,4,1) y=torch.zeros(  3,1,1) print(\"x+y shape : \",(x+y).shape) # Le tenseur y a \u00e9t\u00e9 \u00e9tendu en taille 1x3x1x1 (r\u00e8gle 1) puis dupliqu\u00e9 en taille 5x3x4x1 (r\u00e8gle 2) <pre>x+y shape :  torch.Size([5, 3, 4, 1])\n</pre> In\u00a0[16]: Copied! <pre>x=torch.empty(1)\ny=torch.empty(3,1,7)\nprint(\"x+y shape : \",(x+y).shape)\n# Le tenseur y a \u00e9t\u00e9 \u00e9tendu en taille 1x1x1 (r\u00e8gle 1) puis dupliqu\u00e9 en taille 3x1x7 (r\u00e8gle 2)\n</pre> x=torch.empty(1) y=torch.empty(3,1,7) print(\"x+y shape : \",(x+y).shape) # Le tenseur y a \u00e9t\u00e9 \u00e9tendu en taille 1x1x1 (r\u00e8gle 1) puis dupliqu\u00e9 en taille 3x1x7 (r\u00e8gle 2) <pre>x+y shape :  torch.Size([3, 1, 7])\n</pre> In\u00a0[18]: Copied! <pre>x=torch.empty(5,2,4,1)\ny=torch.empty(3,1,1)\nprint(\"x+y shape : \",(x+y).shape)\n# L'op\u00e9ration n'est pas possible car les tenseurs ne sont pas broadcastables (dimension 3 en partant de la fin ne correspond pas)\n</pre> x=torch.empty(5,2,4,1) y=torch.empty(3,1,1) print(\"x+y shape : \",(x+y).shape) # L'op\u00e9ration n'est pas possible car les tenseurs ne sont pas broadcastables (dimension 3 en partant de la fin ne correspond pas) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[18], line 3\n      1 x=torch.empty(5,2,4,1)\n      2 y=torch.empty(3,1,1)\n----&gt; 3 print(\"x+y shape : \",(x+y).shape)\n      4 # L'op\u00e9ration n'est pas possible car les tenseurs ne sont pas broadcastables (dimension 3 en partant de la fin ne correspond pas)\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1</pre> <p>On n'y pense pas forc\u00e9ment mais cela va nous permettre de faire des op\u00e9rations de comparaisons de mani\u00e8re simple.</p> In\u00a0[22]: Copied! <pre>a = torch.tensor([10., 0, -4])\nprint(a &gt; 0)\nprint(a==0)\n</pre> a = torch.tensor([10., 0, -4]) print(a &gt; 0) print(a==0) <pre>tensor([ True, False, False])\ntensor([False,  True, False])\n</pre> <p>On peut aussi comparer deux tenseurs entre eux :</p> In\u00a0[47]: Copied! <pre>a=torch.tensor([1,2,3])\nb=torch.tensor([4,2,6])\n# Comparaison \u00e9l\u00e9ment par \u00e9l\u00e9ment\nprint(a==b)\n# Comparaison \u00e9l\u00e9ment par \u00e9l\u00e9ment et \u00e9galit\u00e9 pour tous les \u00e9l\u00e9ments\nprint((a==b).all())\n# Comparaison \u00e9l\u00e9ment par \u00e9l\u00e9ment et \u00e9galit\u00e9 pour au moins un \u00e9l\u00e9ment\nprint((a==b).any())\n# Comparaison avec sup\u00e9rieur ou \u00e9gal\nprint(a&gt;=b)\n</pre> a=torch.tensor([1,2,3]) b=torch.tensor([4,2,6]) # Comparaison \u00e9l\u00e9ment par \u00e9l\u00e9ment print(a==b) # Comparaison \u00e9l\u00e9ment par \u00e9l\u00e9ment et \u00e9galit\u00e9 pour tous les \u00e9l\u00e9ments print((a==b).all()) # Comparaison \u00e9l\u00e9ment par \u00e9l\u00e9ment et \u00e9galit\u00e9 pour au moins un \u00e9l\u00e9ment print((a==b).any()) # Comparaison avec sup\u00e9rieur ou \u00e9gal print(a&gt;=b) <pre>tensor([False,  True, False])\ntensor(False)\ntensor(True)\ntensor([False,  True, False])\n</pre> <p>Cela peut \u00eatre tr\u00e8s utile pour cr\u00e9er des masques \u00e0 partir d'un seuil par exemple ou v\u00e9rifier que deux op\u00e9rations sont \u00e9quivalentes.</p> <p>On va vu pr\u00e9cedemment qu'il est possible de broadcast un tenseur de taille $3$ \u00e0 une matrice de taille $3 \\times 3$.  Le broadcasting de pytorch va automatiquement le transformer en taille $1 \\times 3$ pour r\u00e9aliser l'op\u00e9ration. Cependant, on peut vouloir r\u00e9aliser l'op\u00e9ration dans l'autre sens, c'est-\u00e0-dire, ajouter un tenseur $3 \\times 1$ \u00e0 une matrice de taille $3 \\times 3$.</p> <p>Dans ce cas l\u00e0, on va devoir remplacer la r\u00e8gle 1 manuellement \u00e0 l'aide de la fonction unsqueeze() qui permet de rajouter une dimension.</p> In\u00a0[29]: Copied! <pre>x=torch.tensor([1,2,3])\ny=torch.randn(3,3)\nprint(\"y : \",y )\nprint(\"x+y : \",x+y) \n\nx=x.unsqueeze(1)\nprint(\"x shape : \",x.shape)\nprint(\"x+y : \",x+y)\n</pre> x=torch.tensor([1,2,3]) y=torch.randn(3,3) print(\"y : \",y ) print(\"x+y : \",x+y)   x=x.unsqueeze(1) print(\"x shape : \",x.shape) print(\"x+y : \",x+y) <pre>y :  tensor([[ 1.3517,  1.1880,  0.4483],\n        [ 0.5137, -0.5406, -0.1412],\n        [-0.0108,  1.3757,  0.6112]])\nx+y :  tensor([[2.3517, 3.1880, 3.4483],\n        [1.5137, 1.4594, 2.8588],\n        [0.9892, 3.3757, 3.6112]])\nx shape :  torch.Size([3, 1])\nx+y :  tensor([[2.3517, 2.1880, 1.4483],\n        [2.5137, 1.4594, 1.8588],\n        [2.9892, 4.3757, 3.6112]])\n</pre> <p>Comme vous le voyez, on a pu contourner les r\u00e8gles de pytorch pour obtenir le r\u00e9sultat souhait\u00e9.</p> <p>Note :</p> <ul> <li>La r\u00e8gle 1 de pytorch correspond \u00e0 faire x.unsqueeze(0) jusqu'\u00e0 ce que le nombre de dimensions soit le m\u00eame</li> <li>C'est possible de remplacer unsqueeze() avec None de la mani\u00e8re suivante :</li> </ul> In\u00a0[32]: Copied! <pre>x=torch.tensor([1,2,3])\n# La premi\u00e8re op\u00e9ration est l'\u00e9quivalent de unsqueeze(0) et la seconde de unsqueeze(1)\nx[None].shape,x[...,None].shape\n</pre> x=torch.tensor([1,2,3]) # La premi\u00e8re op\u00e9ration est l'\u00e9quivalent de unsqueeze(0) et la seconde de unsqueeze(1) x[None].shape,x[...,None].shape Out[32]: <pre>(torch.Size([1, 3]), torch.Size([3, 1]))</pre> <p>Les fonctions de pytorch qui r\u00e9duisent la taille d'un tenseur selon une dimension (torch.sum pour sommer selon une dimension, torch.mean pour calculer la moyenne et bien d'autres) ont un param\u00e8tre int\u00e9ressant \u00e0 utiliser dans certains cas.</p> <p>Ces op\u00e9rations vont changer la dimension du tenseur et automatiquement supprimer la dimension sur laquelle on a r\u00e9alis\u00e9 l'op\u00e9ration.</p> In\u00a0[36]: Copied! <pre>x=torch.randn(3,4,5)\nprint(x.shape)\nx=x.sum(dim=1) # somme sur la dimension 1\nprint(x.shape)\n</pre> x=torch.randn(3,4,5) print(x.shape) x=x.sum(dim=1) # somme sur la dimension 1 print(x.shape) <pre>torch.Size([3, 4, 5])\ntorch.Size([3, 5])\n</pre> <p>Si vous souhaitez conserver la dimension sur laquelle on fait la somme, vous pouvez utiliser l'argument keepdim=True.</p> In\u00a0[37]: Copied! <pre>x=torch.randn(3,4,5)\nprint(x.shape)\nx=x.sum(dim=1,keepdim=True) # somme sur la dimension 1\nprint(x.shape)\n</pre> x=torch.randn(3,4,5) print(x.shape) x=x.sum(dim=1,keepdim=True) # somme sur la dimension 1 print(x.shape) <pre>torch.Size([3, 4, 5])\ntorch.Size([3, 1, 5])\n</pre> <p>C'est parfois tr\u00e8s utile pour ne pas faire n'importe quoi avec les dimensions. Regardons un cas o\u00f9 cela va impacter le broadcasting.</p> In\u00a0[42]: Copied! <pre>x=torch.randn(3,4,5)\ny=torch.randn(1,1,1)\nx_sum=x.sum(dim=1)\nx_sum_keepdim=x.sum(dim=1,keepdim=True)\nprint(\"Les deux op\u00e9rations sont elles \u00e9quivalentes ? :\",(x_sum+y==x_sum_keepdim+y).all().item())\n</pre> x=torch.randn(3,4,5) y=torch.randn(1,1,1) x_sum=x.sum(dim=1) x_sum_keepdim=x.sum(dim=1,keepdim=True) print(\"Les deux op\u00e9rations sont elles \u00e9quivalentes ? :\",(x_sum+y==x_sum_keepdim+y).all().item()) <pre>Les deux op\u00e9rations sont elles \u00e9quivalentes ? : False\n</pre> <p>Ce qu'il s'est pass\u00e9 :</p> <ul> <li>Dans le premier cas, on obtient x_sum de taille $3 \\times 5$, la r\u00e8gle 1 le transforme en taille $1 \\times 3 \\times 5$ et la r\u00e8gle 2 transforme y en $1 \\times 3 \\times 5$.</li> <li>Dans le second cas, on obtient x_sum_keepdim de taille $3 \\times 1 \\times 5$ et la r\u00e8gle 2 transforme y en $1 \\times 3 \\times 5$.</li> </ul> <p>Cette partie n'est pas directement en rapport avec le broadcasting mais il s'agit d'une information importante \u00e0 conna\u00eetre.</p> <p>Pour multiplier les matrices dans pytorch, nous avons utilis\u00e9 l'op\u00e9rateur @ (ou torch.matmul) jusqu'ici. Il existe une autre mani\u00e8re de faire des multiplications matricielles avec la Einstein Summation (torch.einsum).</p> <p>Il s'agit d'une repr\u00e9sentation compacte pour \u00e9crire les produits et les sommes par exemple : ik,kj -&gt; ij Le c\u00f4t\u00e9 gauche repr\u00e9sente les dimensions des entr\u00e9es, s\u00e9par\u00e9es par des virgules. Ici, nous avons deux tenseurs qui ont chacun deux dimensions (i,k et k,j). Le c\u00f4t\u00e9 droit repr\u00e9sente les dimensions du r\u00e9sultat, donc ici nous avons un tenseur avec deux dimensions i,j.</p> <p>Les r\u00e8gles de la notation de sommation d'Einstein sont les suivantes :</p> <ul> <li>Les indices r\u00e9p\u00e9t\u00e9s sur le c\u00f4t\u00e9 gauche sont implicitement somm\u00e9s s'ils ne se trouvent pas sur le c\u00f4t\u00e9 droit.</li> <li>Chaque indice peut appara\u00eetre au maximum deux fois sur le c\u00f4t\u00e9 gauche.</li> <li>Les indices non r\u00e9p\u00e9t\u00e9s sur le c\u00f4t\u00e9 gauche doivent appara\u00eetre sur le c\u00f4t\u00e9 droit.</li> </ul> <p>On peut l'utiliser pour plusieurs choses :</p> <pre><code>torch.einsum('ij-&gt;ji', a)\n</code></pre> <p>renvoie la transpos\u00e9e de la matrice a.</p> <p>Alors que</p> <pre><code>torch.einsum('bi,ij,bj-&gt;b', a, b, c)\n</code></pre> <p>renverra un vecteur de taille b o\u00f9 la k-i\u00e8me coordonn\u00e9e est la somme de $a[k,i]\u22c5b[i,j]\u22c5c[k,j]$. Cette notation est particuli\u00e8rement pratique lorsque vous avez plus de dimensions lors de la manipulation de batchs. Par exemple, si vous avez deux lots de matrices et que vous voulez calculer le produit matriciel par batch, vous pourriez utiliser ceci :</p> <pre><code>torch.einsum('bik,bkj-&gt;bij', a, b)\n</code></pre> <p>C'est une fa\u00e7on pratique d'effectuer des multiplications matricielles dans pytorch. De plus, c'est tr\u00e8s rapide et c'est souvent la mani\u00e8re la plus rapide d'effectuer des op\u00e9rations customs dans pytorch.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html#regle-de-broadcasting","title":"R\u00e8gle de broadcasting\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html#autres-points-a-considerer","title":"Autres points \u00e0 consid\u00e9rer\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html#comparaison-a-des-scalaires","title":"Comparaison \u00e0 des scalaires\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html#utilisation-de-unsqueeze","title":"Utilisation de unsqueeze()\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html#utilisation-de-keepdim","title":"Utilisation de keepdim\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/04_Broadcasting.html#einstein-summation","title":"Einstein Summation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html","title":"Comprendre les diff\u00e9rents optimizers","text":"<p>Dans le tout premier cours, nous avons introduit la descente du gradient. Il s'agit de l'optimizer le plus simple pour entra\u00eener les mod\u00e8les de deep learning. Dans les cours suivants, nous avons utilis\u00e9 la descente du gradient stochastique par mini-batch ou Adam en fonction des sc\u00e9narios.</p> <p>L'optimizer permet d'ajuster les param\u00e8tres du mod\u00e8le pendant l'entra\u00eenement dans un objectif de minimisation du loss. Le choix d'un bon optimizer est tr\u00e8s important pour la performance du mod\u00e8le ainsi que la vitesse de convergence de celui-ci.</p> <p>Ce cours va vous pr\u00e9senter les diff\u00e9rents optimizers existants (non exhaustif) ainsi que leurs avantages et inconv\u00e9nients. En pratique, le meilleur optimizer est souvent Adam mais vous pouvez exp\u00e9rimenter avec d'autres optimizers et diff\u00e9rentes valeurs de learning rate (la valeur optimale de learning rate n'est pas la m\u00eame en fonction de l'optimizer choisi).</p> <p>Ce cours s'inspire de ce blogpost pour les explications et reprend les figures.</p> <p>Commen\u00e7ons par un rappel sur l'algorithme de descente du gradient. On peut simplement la d\u00e9finir de cette mani\u00e8re : $\\theta = \\theta - \\alpha \\cdot \\nabla L(\\theta)$ o\u00f9 $\\theta$ correspond aux param\u00e8tres du mod\u00e8le, $L(\\theta)$ est le loss sur l'ensemble des donn\u00e9es et $\\alpha$ est le learning rate.</p> <p>On peut r\u00e9sumer cette approche en une phrase simple : A chaque \u00e9tape d'entra\u00eenement, on ajuste les param\u00e8tres du mod\u00e8les pour minimiser le loss sur l'ensemble des donn\u00e9es.</p> <p>Voici une figure montrant l'id\u00e9e derri\u00e8re la descente du gradient :</p> <p></p> <p>C'est une approche tr\u00e8s simple \u00e0 impl\u00e9menter et qui fonctionne tr\u00e8s bien sur une petite quantit\u00e9 de donn\u00e9es si on a bien choisi notre learning rate. Cependant, comme on calcule le loss sur l'ensemble de nos \u00e9l\u00e9ments avant d'ajuster les poids, cette approche est extremement lente pour les gros datasets et inutilisable en pratique. De plus, le choix du learning rate est tr\u00e8s important et il faut \u00eatre pr\u00e9cis ce qui n'est pas toujours \u00e9vident.</p> <p>Cette m\u00e9thode adapte la descente du gradient pour des datasets cons\u00e9quents. Au lieu de changer les param\u00e8tres apr\u00e8s avoir vu toutes les donn\u00e9es, on va calculer le loss sur une partie des donn\u00e9es (un mini-batch) et ajuster les poids par rapport au loss calcul\u00e9 sur ce mini-batch. C'est de l\u00e0 que vient le terme stochastique : comme on ajuste les param\u00e8tres sur une partie des donn\u00e9es, il est possible que \u00e7a ne diminue pas le loss sur l'ensemble des donn\u00e9es. En pratique, apr\u00e8s plusieurs it\u00e9rations, le loss global diminue bien et le mod\u00e8le converge beaucoup plus rapidement.</p> <p>La formule est presque identique : $\\theta = \\theta - \\alpha \\cdot \\nabla L(\\theta;x^{(i)};y^{(i)})$ o\u00f9 $(x^{(i)},y^{(i)})$ est un mini-batch de donn\u00e9es.</p> <p></p> <p>Comme on le voit sur la figure, la pente n'est pas une ligne droite mais le mod\u00e8le finit par converger au minimum global.</p> <p>Cette m\u00e9thode est beaucoup plus rapide que la descente du gradient classique surtout sur les gros datasets. Cela a aussi l'avantage de pouvoir \u00e9chapper aux minimums locaux plus facilement (d\u00fb \u00e0 l'instabilit\u00e9 du processus). Ce n'est cependant pas parfait \u00e0 cause justement de l'instabilit\u00e9 et a besoin \u00e9galement d'un learning rate tr\u00e8s bien choisi pour fonctionner de mani\u00e8re optimale.</p> <p>Cet optimizer reprend l'id\u00e9e de la descente du gradient stochastique mais en ajoutant un terme momentum. Comme son nom l'indique, c'est un terme qui permet de garder en m\u00e9moire la direction de l'optimisation pr\u00e9cedente et de pousser l'optimisation actuelle \u00e0 continuer dans la m\u00eame direction. Ce terme est calcul\u00e9 et mis \u00e0 jour via un exponentially decaying average. C'est tr\u00e8s b\u00e9n\u00e9fique pour palier au probl\u00e8me de gradient qui serait faible (une r\u00e9gion plate). La formule est la suivante : $v = \\beta \\cdot v + (1 - \\beta) \\cdot \\nabla_\\theta L(\\theta; x^{(i)}; y^{(i)})$ $\\theta = \\theta - \\alpha \\cdot v$ o\u00f9 $v$ est appel\u00e9 le vecteur de momentum et $\\beta$ est un hyperparam\u00e8tre (calcul\u00e9 par exponentially decaying average) ajustant l'impact de la valeur actuelle.</p> <p></p> <p>En pratique, cela aide \u00e0 passer les r\u00e9gions plates de la fonction de loss plus efficacement tout en augmentant la vitesse de convergence. Par contre, le choix du param\u00e8tre momentum est important car un momentum trop grand pourrait nous faire sauter la solution optimale. En pratique ce terme est choisi en fonction de la taille de votre mini-batch (batch size).</p> <p>Exponentially decaying average :  M\u00e9thode de lissage des donn\u00e9es o\u00f9 chaque nouvelle valeur a un poids d\u00e9croissant exponentiellement, donnant plus d'importance aux valeurs r\u00e9centes tout en diminuant progressivement l'impact des valeurs pass\u00e9es.</p> <p>Adagrad est un optimizer qui ajuste le learning rate par param\u00e8tre pendant l'entra\u00eenement. Le learning rate de chaque param\u00e8tre est bas\u00e9 sur l'historique des gradients. L'id\u00e9e est que les param\u00e8tres que l'on va ajuster souvent vont avoir un plus petit learning rate que les param\u00e8tres que l'on ajuste rarement. La formule s'\u00e9crit de cette mani\u00e8re : $g = \\nabla_\\theta L(\\theta; x^{(i)}; y^{(i)})$ $G = G + g \\odot g$ $\\theta = \\theta - \\frac{\\alpha}{\\sqrt{G + \\epsilon}} \\odot g$ o\u00f9 $G$ est la matrice qui accumule le carr\u00e9 des gradients (le carr\u00e9 pour \u00e9viter les valeurs n\u00e9gatives) et $\\epsilon$ est une petite valeur pour \u00e9viter de diviser par 0.</p> <p>C'est une m\u00e9thode qui s'av\u00e8re performante quand les donn\u00e9es d'entra\u00eenement sont tr\u00e8s diff\u00e9rentes les unes des autres car cela va garantir un ajustement des param\u00e8tres en fonction de l'occurence. Cependant, on constate que le learning rate va baisser sans arr\u00eat ce qui peut conduire \u00e0 une convergence tr\u00e8s lente et m\u00eame une non-convergence si le learning rate devient trop petit avant la fin de l'optimisation.</p> <p>RMSProp se base sur la m\u00eame id\u00e9e que Adagrad mais corrige une partie des inconv\u00e9nients. Au lieu de diminuer le learning rate de mani\u00e8re au f\u00fbr et \u00e0 mesure de l'entra\u00eenement, RMSProp utilise un exponentially decaying average sur les carr\u00e9s des gradients au lieu de prendre la somme comme le faisait Adagrad. Cela va corriger le probl\u00e8me principal d'adagrad qui est la diminution du learning rate sans retour en arri\u00e8re possible. La formule de RMSProp est la suivante : $g = \\nabla_\\theta L(\\theta; x^{(i)}; y^{(i)})$ $G = \\beta \\cdot G + (1 - \\beta) \\cdot g \\odot g$ $\\theta = \\theta - \\frac{\\alpha}{\\sqrt{G + \\epsilon}} \\odot g$ o\u00f9 $\\beta$ est le param\u00e8tre pour la gestion de l'exponentially decaying average.</p> <p>Les avantages sont les m\u00eame qu'adagrad sauf que la convergence est plus rapide. Cependant, parfois ce n'est pas suffisant et cela converge trop lentement.</p> <p>AdaDelta est un optimizer assez similaire \u00e0 RMSProp sauf que celui-ci n'a pas besoin d'un learning rate en hyperparam\u00e8tre. Pour compenser cela, adaDelta utilise un exponentially decaying average sur les gradients et les carr\u00e9s des gradients pour determiner une valeur d'ajustement coh\u00e9rente. La formule d'adaDelta est la suivante : $g = \\nabla_\\theta L(\\theta; x^{(i)}; y^{(i)})$ $G = \\beta \\cdot G + (1 - \\beta) \\cdot g \\odot g$ $\\Delta\\theta = - \\frac{\\sqrt{S + \\epsilon}}{\\sqrt{G + \\epsilon} } \\odot g$ $S = \\beta \\cdot S + (1 - \\beta) \\cdot \\Delta\\theta \\odot \\Delta\\theta$ $\\theta = \\theta + \\Delta\\theta$ o\u00f9 $G$ est la matrice qui accumule les gradients et $S$ est la matrice qui accumule le carr\u00e9 des ajustements.</p> <p>C'est une m\u00e9thode assez int\u00e9ressant car elle ne n\u00e9cessite pas de choix de learning rate. Cependant, elle peut converger assez lentement et le learning rate calcul\u00e9 peut devenir trop petit ce qui causerait un arr\u00eat de l'entra\u00eenement.</p> <p>Adam est probablement l'optimizer le plus utilis\u00e9 aujourd'hui. Adam combine les id\u00e9es de la descente du gradient avec momentum et de RMSProp. Adam utilise un exponentially decaying average sur les gradients et sur les carr\u00e9s des gradients pour modifier le learning rate comme RMSProp. Adam calcule aussi un momentum pour permettre une optimisation plus rapide. La formule est la suivante : $g = \\nabla_\\theta L(\\theta; x^{(i)}; y^{(i)})$ $m = \\beta_1 \\cdot m + (1 - \\beta_1) \\cdot g$ $v = \\beta_2 \\cdot v + (1 - \\beta_2) \\cdot g \\odot g$ $\\hat{m} = \\frac{m}{1 - \\beta_1^t}$ $\\hat{v} = \\frac{v}{1 - \\beta_2^t}$ $\\theta = \\theta - \\frac{\\alpha}{\\sqrt{\\hat{v}} + \\epsilon} \\odot \\hat{m}$ o\u00f9 $m$ est le vecteur de momentum, $v$ le vecteur de velocity, $\\beta_1$ le decay pour le momentum et  $\\beta_2$ le decay pour la velocity.</p> <p>Il s'agit de l'optimizer le plus rapide pour la convergence et il fonctionne bien sur des donn\u00e9es bruit\u00e9es. Par contre, il y a 3 hyperparam\u00e8tres \u00e0 d\u00e9finir ce qui peut \u00eatre un peu lourd.</p> <p>En pratique Adam fonctionne extremement bien et il est souvent n\u00e9cessaire de ne modifier que le param\u00e8tre learning rate. Les valeurs par d\u00e9fauts de $\\beta_1$ et $\\beta_2$ ont rarement besoin d'\u00eatre modifi\u00e9es (sur pytorch elles sont par d\u00e9faut \u00e0 $\\beta_1=0.9$ et $\\beta_2=0.999$). De plus, contrairement \u00e0 la descente de gradient stochastique, un choix pr\u00e9cis de learning rate n'est pas forc\u00e9ment indispensable pour obtenir une bonne optimisation.</p> <p>Note : De mani\u00e8re g\u00e9n\u00e9rale, je vous conseillerais d'utiliser Adam ou AdamW (version am\u00e9lior\u00e9e de Adam) syst\u00e9matiquent comme optimizer par d\u00e9faut. En fonction de votre probl\u00e8me, vous pouvez \u00eatre amen\u00e9 \u00e0 tester d'autres optimizers.</p> <p>Autre note : En fonction de l'optimizer choisi, l'espace m\u00e9moire n\u00e9cessaire lors de l'entra\u00eenement du mod\u00e8le est diff\u00e9rent. A l'heure o\u00f9 les gros mod\u00e8le type LLM sont l\u00e9gions, c'est une information \u00e0 garder en t\u00eate. Voici quelques indications sur le co\u00fbt en m\u00e9moire en fonction de l'optimizer ($n$ est le nombre de param\u00e8tres du mod\u00e8les):</p> <ul> <li>Co\u00fbt en m\u00e9moire n : Descente du gradient stochastique (SGD)</li> <li>Co\u00fbt en m\u00e9moire 2n : SGD avec momentum, Adagrad, RMSProp</li> <li>Co\u00fbt en m\u00e9moire 3n : Adam et ses variantes (AdamW, AdaMax, Nadam)</li> </ul> <p>Le choix du learning rate est tr\u00e8s li\u00e9 avec le choix de l'optimizer. Voici les situations possibles sur lesquelles on peut tomber en fonction du choix du learning rate :</p> <p></p> <ul> <li>Dans le premier cas, le learning rate est trop faible et la convergence du mod\u00e8le va \u00eatre lente. C'est couteux en terme de temps et d'argent.</li> <li>Dans le second cas, le learning rate est bien choisi et il d\u00e9croit assez vite pour atteindre le minimum de la fonction de loss. C'est cette valeur l\u00e0 que l'on cherche \u00e0 trouver.</li> <li>Dans le dernier cas, le learning rate est trop important et les ajustements des param\u00e8tres sont beaucoup trop important. Cela peut conduire \u00e0 une non-convergence ou m\u00eame une divergence du mod\u00e8le.</li> </ul> <p>Ces consid\u00e9rations sont tr\u00e8s importantes pour le choix du learning rate lorsque l'on utilise la descente du gradient stochastique. Pour Adam, c'est un hyperparam\u00e8tre important \u00e9galement mais la tol\u00e9rance \u00e0 l'erreur est plus importante car le learning rate est adapt\u00e9 dans l'optimizer.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html#comprendre-les-differents-optimizers","title":"Comprendre les diff\u00e9rents optimizers\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html#descente-du-gradient","title":"Descente du gradient\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html#descente-du-gradient-stochastique","title":"Descente du gradient stochastique\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html#descente-du-gradient-stochastique-avec-momentum","title":"Descente du gradient stochastique avec momentum\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html#adagrad","title":"Adagrad\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html#rmsprop","title":"RMSProp\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html#adadelta","title":"AdaDelta\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html#adam","title":"Adam\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/05_Optimizer.html#point-sur-le-choix-du-learning-rate","title":"Point sur le choix du learning rate\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html","title":"R\u00e9gularisation","text":"<p>Nous avons vu, tout au long de ce cours sur le deep learning, que les r\u00e9seaux de neurones sont tr\u00e8s performants pour r\u00e9soudre tout type de probl\u00e8mes. Il y a cependant quelques probl\u00e8mes majeurs qui limitent le potentiel de nos r\u00e9seaux. Le probl\u00e8me que nous allons adresser ici est l'overfitting.</p> <p>L'overfitting, c'est quand le mod\u00e8le s'adapte trop pr\u00e9cisement aux donn\u00e9es d'entrainement en capturant les moindres anomalies et bruits ce qui va entra\u00eener une mauvaise performance sur les donn\u00e9es de test.</p> <p></p> <p>Ce probl\u00e8me est courant lors de l'entra\u00eenement des r\u00e9seaux de neurones. C'est pour surveiller ce ph\u00e9nom\u00e8ne au cours de l'entra\u00eenement que l'on utilise un dataset de validation que l'on va \u00e9valuer r\u00e9guli\u00e8rement pour v\u00e9rifier que le loss d\u00e9croit bien. Dans un cas d'overfitting, nos courbes de training et validation vont ressembler \u00e0 cela :</p> <p></p> <p>Le loss de training continue de diminuer tandis que le loss de validation augmente : cela traduit un apprentissage des sp\u00e9cificit\u00e9s des donn\u00e9es de training ce qui est l'inverse de ce que l'on veut en entra\u00eenant notre r\u00e9seau.</p> <p>Si on y r\u00e9flechit bien, c'est un comportement tout \u00e0 fait logique de l'optimisation : le mod\u00e8le va tout faire pour diminuer le loss sur les donn\u00e9es d'entra\u00eenement.</p> <p>Il s'agit d'un \u00e9l\u00e9ment tr\u00e8s important \u00e0 consid\u00e9rer lors de l'entra\u00eenement d'un r\u00e9seau de neurones. Pour lutter contre ce probl\u00e8me, plusieurs techniques ont \u00e9t\u00e9 propos\u00e9es et nous allons en pr\u00e9senter deux ici en essayant de comprendre intuitivement pourquoi ces m\u00e9thodes fonctionnent.</p> <p>Ces deux m\u00e9thodes sont la r\u00e9gularisation-L2 et le dropout. Ce notebook s'inspire du cours de fastai.</p> <p>La r\u00e9gularisation L2 est une technique tr\u00e8s simple qui consiste \u00e0 ajouter la somme des poids au carr\u00e9 \u00e0 notre fonction de loss (avec un facteur $wd$ appel\u00e9 weight_decay). Ce terme dans la fonction de loss va encourager les poids \u00e0 \u00eatre le plus petit possible.</p> <p>Intuitivement, on peut imaginer que des poids plus importants vont conduire \u00e0 des pente plus abruptes dans la fonction de loss. Pour confirmer notre intuition, on peut regarder ce qu'il se passe sur la fonction parabole : $y=a \\times x\u00b2$</p> In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(-2,2,100)\na_s = [1,2,5,10,50] \nys = [a * x**2 for a in a_s]\n_,ax = plt.subplots(figsize=(6,4))\nfor a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')\nax.set_ylim([0,5])\nax.legend();\n</pre> import numpy as np import matplotlib.pyplot as plt x = np.linspace(-2,2,100) a_s = [1,2,5,10,50]  ys = [a * x**2 for a in a_s] _,ax = plt.subplots(figsize=(6,4)) for a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}') ax.set_ylim([0,5]) ax.legend(); <p>Plus $a$ augmente, plus la parabole devient abrupte. Si on fait l'analogie entre $a$ et nos poids, on peut imaginer que des poids importants vont conduire \u00e0 une fonction represent\u00e9 par un r\u00e9seau ayant des changements abruptes ce qui peut correspondre \u00e0 de l'overfitting.</p> <p>Cette justification peut sembler un peu \"magique\" mais en pratique, la r\u00e9gularisation-L2 va vraiment avoir un impact positif pour la pr\u00e9vention de l'overfitting.</p> <p>Il faut aussi ajouter que limiter la valeur des poids va \u00e9galement limiter la capacit\u00e9 d'apprentissage du r\u00e9seau globalement mais cette limitation permet d'arriver \u00e0 une meilleure g\u00e9n\u00e9ralisation et donc d'\u00e9viter l'overfitting.</p> <p>Pour impl\u00e9menter la r\u00e9gularisation-l2, on peut le faire manuellement en ajoutant le terme \u00e0 notre loss : $L(w) = L_0(w) + wd \\sum_{i=1}^{n} w_i^2$ o\u00f9 $L(w)$ est la perte r\u00e9gularis\u00e9e, $L_0(w)$ est la fonction de perte classique, $wd$ (weight_decay) est le coefficient de r\u00e9gularisation et $w_i$ est un poids du mod\u00e8le. ou  <code>loss_regu = loss + wd * (parameters**2).sum()</code> en python.</p> <p>Plus simplement, il est possible de l'utiliser en pytorch en ajoutant la param\u00e8tre weight_decay dans l'initialisaton de l'optimizer (par d\u00e9faut fix\u00e9 \u00e0 0). Par exemple :</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nparams=torch.tensor([1.0], requires_grad=True)\n# Pour SGD\ntorch.nn.optim.SGD(params, lr=0.1, weight_decay=0.1)\n# Pour Adam\ntorch.nn.optim.Adam(params, lr=0.1, weight_decay=0.1)\n</pre> import torch params=torch.tensor([1.0], requires_grad=True) # Pour SGD torch.nn.optim.SGD(params, lr=0.1, weight_decay=0.1) # Pour Adam torch.nn.optim.Adam(params, lr=0.1, weight_decay=0.1) <p>Si votre mod\u00e8le a des probl\u00e8mes d'overfitting, c'est une m\u00e9thode \u00e0 tester !</p> <p>Le dropout est une m\u00e9thode de r\u00e9gularisation qui a \u00e9t\u00e9 introduite dans le papier Improving neural networks by preventing co-adaptation of feature detectors. L'id\u00e9e est simple et consiste \u00e0 changer al\u00e9atoirement une partie des activations du r\u00e9seau \u00e0 z\u00e9ro \u00e0 chaque \u00e9tape de l'entra\u00eenement. De cette mani\u00e8re, chaque neurone devient n\u00e9cessaire pour sortir un output coh\u00e9rent en fonction de l'input.</p> <p></p> <p>Pour comprendre comment fonctionne le dropout, on peut reprendre une m\u00e9taphore de l'auteur du papier (le fameux Geoffrey Hinton) :</p> <p>Je suis all\u00e9 \u00e0 ma banque. Les guichetiers changeaient constamment et j'ai demand\u00e9 \u00e0 l'un d'eux pourquoi. Il m'a dit qu'il ne savait pas, mais qu'ils \u00e9taient souvent d\u00e9plac\u00e9s. J'ai pens\u00e9 que cela devait \u00eatre parce que cela n\u00e9cessiterait la coop\u00e9ration entre les employ\u00e9s mettre en place une fraude bancaire et donc que cela limite \u00e9normement les possibilit\u00e9s de le faire. Cela m'a fait r\u00e9aliser que retirer al\u00e9atoirement un sous-ensemble diff\u00e9rent de neurones \u00e0 chaque exemple emp\u00eacherait les conspirations et ainsi r\u00e9duirait le surapprentissage.</p> <p>Cela va emp\u00eacher les neurones de frauder, c'est \u00e0 dire de trouver un raccourci frauduleux pour une pr\u00e9diction (on pourrait supposer que chaque neurone se base sur un d\u00e9tail de l'entr\u00e9e uniquement pour faire sa pr\u00e9diction).</p> <p>Cette m\u00e9thode va pousser les neurones \u00e0 coop\u00e9rer et va \u00e9galement augmenter le bruit dans les activations ce qui conduit \u00e0 un mod\u00e8le plus robuste. On peut aussi voir le dropout comme une mixture d'un grand nombre de mod\u00e8les plus petits qui vont coop\u00e9rer lors de la phase de test.</p> <p>On peut impl\u00e9menter le dropout de cette mani\u00e8re en python :</p> In\u00a0[4]: Copied! <pre>import torch.nn as nn\nclass Dropout(nn.Module):\n  def __init__(self, p): \n    self.p = p\n  def forward(self, x):\n    # Le droupout n'est appliqu\u00e9 que pendant l'entrainement\n    if not self.training: \n      return x\n    # On cr\u00e9e un masque de dropout \u00e0 partir d'une distribution de Bernoulli\n    mask = torch.zeros_like(x)\n    mask.bernoulli_(1 - self.p) # chaque \u00e9l\u00e9ment a une probabilit\u00e9 de 1-p d'\u00eatre mis \u00e0 0 sinon il est mis \u00e0 1\n    # On applique le masque et on divise par 1-p pour garder une moyenne coh\u00e9rente\n    return x * mask/(1-self.p)\n</pre> import torch.nn as nn class Dropout(nn.Module):   def __init__(self, p):      self.p = p   def forward(self, x):     # Le droupout n'est appliqu\u00e9 que pendant l'entrainement     if not self.training:        return x     # On cr\u00e9e un masque de dropout \u00e0 partir d'une distribution de Bernoulli     mask = torch.zeros_like(x)     mask.bernoulli_(1 - self.p) # chaque \u00e9l\u00e9ment a une probabilit\u00e9 de 1-p d'\u00eatre mis \u00e0 0 sinon il est mis \u00e0 1     # On applique le masque et on divise par 1-p pour garder une moyenne coh\u00e9rente     return x * mask/(1-self.p) <p>La couche est \u00e9galement d\u00e9j\u00e0 impl\u00e9ment\u00e9 en pytorch et on peut l'utiliser avec nn.Dropout() qui prend en param\u00e8tre $p$ la probabilit\u00e9 qu'une activation soit mise \u00e0 z\u00e9ro.</p> <p>Note :  Lors de l'utilisation du dropout de pytorch, il faut bien faire attention \u00e0 passer le mod\u00e8le en mode train pendant l'entrainement et eval pendant la validation/test (comme avec la batchnorm). On peut le faire avec <code>model.train()</code> et <code>model.eval()</code>.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html#regularisation","title":"R\u00e9gularisation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html#regularisation-l2","title":"R\u00e9gularisation-L2\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html#pourquoi-ca-reduit-loverfitting","title":"Pourquoi \u00e7a r\u00e9duit l'overfitting ?\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html#implementation-de-la-methode","title":"Impl\u00e9mentation de la m\u00e9thode\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html#dropout","title":"Dropout\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html#intuition","title":"Intuition\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/06_Regularisation.html#implementation","title":"Impl\u00e9mentation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/07_ConnexionsResiduelles.html","title":"Connexions r\u00e9siduelles","text":"<p>Les connexions r\u00e9siduelles (ou skip connections) ont \u00e9t\u00e9 introduite dans l'article Deep Residual Learning for Image Recognition. C'est une technique qui a permis l'utilisation de r\u00e9seaux profonds ce qui n'\u00e9tait pas vraiment possible avant.</p> <p>Depuis, les connexions r\u00e9siduelles sont partout :</p> <p></p> <p>Dans ce cours, nous allons voir pourquoi ces connexions r\u00e9siduelles sont si importantes et nous allons comprendre leur inter\u00eat de mani\u00e8re intuitive. Ce notebook s'inspire du cours de fastai.</p> <p>L'article sur les connexions r\u00e9siduelles part d'un constat : M\u00eame en utilisant la batchnorm, un r\u00e9seau profond avec un nombre sup\u00e9rieur de couches est moins performant qu'un r\u00e9seau moins profond avec moins de couches (en supposant les autres points identiques et pour un r\u00e9seau peu profond d\u00e9j\u00e0 relativement profond ie 20 couches) et ce que ce soit sur les donn\u00e9es de training ou de validation (donc il ne s'agit pas d'un probl\u00e8me d'overfitting).</p> <p></p> <p>Figure extraite de l'article original.</p> <p>De mani\u00e8re intuitive, cela para\u00eet assez abberant. Imaginons que l'on remplace nos 36 couches suppl\u00e9mentaires par des fonctions identit\u00e9 (ne font aucune transformation sur l'entr\u00e9e) alors le r\u00e9seau de 56 couches devrait \u00eatre aussi bon que le r\u00e9seau de 20 couches. Mais en pratique, ce n'est pas le cas et l'optimisation n'arrive m\u00eame pas \u00e0 transformer ces 36 couches en identit\u00e9.</p> <p>Une mani\u00e8re de comprendre les connexions r\u00e9siduelles est de consid\u00e9rer qu'elles ajoutent directement l'identit\u00e9 \u00e0 la transformation. Au lieu du classique <code>x=layer(x)</code>, on utilise <code>x=x+layer(x)</code>. En pratique, l'ajout de cette skip connections permet une bien meilleure optimisation.</p> <p>Une autre fa\u00e7on de voir les choses et qui explique le terme \"r\u00e9siduel\" est de voir la transformations comme \u00e9tant <code>y=x+layer(x)</code> ce qui \u00e9quivaut \u00e0 <code>y-x=layer(x)</code>. Le mod\u00e8le n'a plus pour objectif de pr\u00e9dire $y$ mais plut\u00f4t de minimiser la diff\u00e9rence entre la sortie voulue et l'entr\u00e9e. C'est de l\u00e0 que vient le terme \"r\u00e9siduel\" qui signifie \"le reste de la soustraction\".</p> <p>Le universal approximation theorem stipule qu'un r\u00e9seau de neurones suffisament large peut apprendre n'importe quelle fonction. Mais entre ce qu'il est possible de faire th\u00e9oriquement et ce qu'on arrive \u00e0 faire en pratique, il y a un \u00e9norme gouffre. Une bonne partie de la recherche en deep learning a pour objectif de r\u00e9duire ce gouffre et les connexions r\u00e9siduelles sont un \u00e9norme pas en avant dans cette direction.</p> <p>Pour entrer un peu plus dans les d\u00e9tails, prenons l'exemple du block resnet qui est la premi\u00e8re version des connexions r\u00e9siduelles et qui s'attaque aux r\u00e9seaux de neurones convolutifs. Au lieu d'avoir <code>x=x+conv(x)</code> \u00e0 chaque \u00e9tape, on a <code>x=x+conv2(conv1(x))</code> ce qui correspond \u00e0 cette figure :</p> <p></p> <p>Le truc avec les r\u00e9seaux convolutifs, c'est qu'on souhaite diminuer le r\u00e9solution et augmenter le nombre de filtres avec la profondeur du r\u00e9seau. Les connexions r\u00e9siduelles ne permettent pas \u00e7a car on ne peut pas sommer des tenseurs de tailles diff\u00e9rentes. En pratique, on peut modifier le tenseur provenant de la connexion r\u00e9siduelle :</p> <ul> <li>Pour diminuer la r\u00e9solution, il suffit d'appliquer une op\u00e9ration de pooling (Max ou Average).</li> <li>Pour augmenter le nombre de filtres, on utilise une convolution avec des filtres de taille $1 \\times 1$ ce qui correspond \u00e0 un simple produit scalaire.</li> </ul> <p>Convolution $1 \\times 1$ : Par rapport \u00e0 une convolution classique, la convolution 1x1 simplifie la transformation des canaux de l'image sans m\u00e9langer les informations spatiales, fonctionnant principalement comme une op\u00e9ration de r\u00e9duction de dimensionnalit\u00e9 ou de r\u00e9ajustement des canaux.</p> <p>Voil\u00e0 comment on pourrait impl\u00e9menter le block de resnet en pytorch :</p> In\u00a0[\u00a0]: Copied! <pre>import torch.nn as nn\nimport torch.nn.functional as F\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, stride=1):\n        self.convs = nn.Sequential(\n            nn.Conv2d(ni, nf, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(nf, nf, kernel_size=3, stride=1, padding=1)\n            )\n        # Si le nombre de filtre de l'entr\u00e9e et de la sortie ne sont pas les m\u00eames\n        self.idconv = nn.Identity() if ni==nf else nn.Conv2d(ni, nf, kernel_size=1, stride=1)\n        # Si le stride est diff\u00e9rent de 1, on utilise une couche de pooling (average)\n        self.pool =nn.Identity() if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x):\n        return F.relu(self.convs(x) + self.idconv(self.pool(x)))\n</pre> import torch.nn as nn import torch.nn.functional as F class ResBlock(nn.Module):     def __init__(self, ni, nf, stride=1):         self.convs = nn.Sequential(             nn.Conv2d(ni, nf, kernel_size=3, stride=1, padding=1),             nn.ReLU(),             nn.Conv2d(nf, nf, kernel_size=3, stride=1, padding=1)             )         # Si le nombre de filtre de l'entr\u00e9e et de la sortie ne sont pas les m\u00eames         self.idconv = nn.Identity() if ni==nf else nn.Conv2d(ni, nf, kernel_size=1, stride=1)         # Si le stride est diff\u00e9rent de 1, on utilise une couche de pooling (average)         self.pool =nn.Identity() if stride==1 else nn.AvgPool2d(2, ceil_mode=True)      def forward(self, x):         return F.relu(self.convs(x) + self.idconv(self.pool(x))) <p>Note : On utilise la fonction d'activation apr\u00e8s l'ajout de la partie r\u00e9siduelle car on consid\u00e8re le ResBlock comme une couche \u00e0 part enti\u00e8re.</p> <p>L'exemple des connexions r\u00e9siduelles illustre combien il est crucial pour les chercheurs de pratiquer et d'exp\u00e9rimenter avec les r\u00e9seaux de neurones (au lieu de faire uniquement de la th\u00e9orie).</p> <p>En pratique, il a \u00e9t\u00e9 d\u00e9montr\u00e9 dans le papier Visualizing the Loss Landscape of Neural Nets que les connexions r\u00e9siduelles ont pour effet de lisser la fonction de loss ce qui explique que l'optimisation se passe beaucoup mieux.</p> <p></p> <p>Un autre type de block a \u00e9t\u00e9 introduit dans le papier Deep Residual Learning for Image Recognition. Il s'agit du block bottleneck qui ressemble \u00e0 \u00e7a :</p> <p></p> <p>A gauche le block resnet de base et \u00e0 droite le block bottleneck.</p> <p>Ce block contient plus de convolutions mais est en fait plus rapide que le block resnet de base et c'est gr\u00e2ce aux convolutions $1 \\times 1$ qui sont tr\u00e8s rapides. Le gros avantage de ce block est qu'il permet d'augmenter le nombre de filtres sans augmenter le temps de traitement (et m\u00eame en le r\u00e9duisant). C'est ce block l\u00e0 qui est utilis\u00e9 pour les versions les plus profondes de resnet (50, 101 et 152 couches) alors que le block de base est utilis\u00e9 pour les versions moins profondes (18 et 34 couches).</p> <p>Note : En pratique, si l'on utilise les couches bottleneck sur les architectures moins profondes (18 et 34 couches), on a en g\u00e9n\u00e9ral des meilleurs r\u00e9sultats qu'avec les resnet block classique. Pourtant, dans la litt\u00e9rature, la plupart des gens continuent d'utiliser la version avec le resnet block. Parfois les habitudes restent ancr\u00e9es mais cela montre l'importance de se questionner sur les \"choses que tout le monde sait\".</p> <p>Pour conclure, les couches r\u00e9siduelles sont une avanc\u00e9e majeure dans le deep learning et il est conseill\u00e9 de les utiliser d\u00e8s que votre r\u00e9seau commence \u00e0 devenir profond.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/07_ConnexionsResiduelles.html#connexions-residuelles","title":"Connexions r\u00e9siduelles\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/07_ConnexionsResiduelles.html#intuition","title":"Intuition\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/07_ConnexionsResiduelles.html#le-block-resnet","title":"Le block Resnet\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/07_ConnexionsResiduelles.html#point-sur-le-block-bottleneck","title":"Point sur le block bottleneck\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html","title":"Introduction \u00e0 la cross validation","text":"<p>La validation crois\u00e9e ou cross-validation est une m\u00e9thode permettant d'\u00e9valuer pr\u00e9cisement un mod\u00e8le de deep learning sur l'ensemble des donn\u00e9es disponibles pour l'entrainement. Ce cours est inspir\u00e9 du blogpost et les figures utilis\u00e9es sont \u00e9galement tir\u00e9es de ce blogpost.</p> <p>Comme on l'a vu pr\u00e9cedemment, un des probl\u00e8mes des mod\u00e8les de deep learning est l'overfitting. Pour en apprendre plus sur l'overfitting, vous pouvez vous r\u00e9ferer au cours bonus sur la r\u00e9gularisation. En effet, il ne suffit pas d'avoir un mod\u00e8le performant sur les donn\u00e9es d'entrainement, il faut surtout \u00eatre performant sur les donn\u00e9es de test.</p> <p></p> <p>La cross-validation est une technique permettant de d\u00e9tecter plus facilement l'overfitting et d'ajuster plus pr\u00e9cisement les hyperparam\u00e8tres pour lutter contre l'overfitting.</p> <p>La technique de cross-validation peut \u00eatre d\u00e9comp\u00e9e en 3 phases :</p> <ul> <li>On va partionner notre dataset en un nombre de subsets choisi.</li> <li>On va \u00e9carter un des subsets et entra\u00eener le mod\u00e8le sur le reste des subsets.</li> <li>On va finalement tester notre mod\u00e8le sur le subset que l'on avait \u00e9cart\u00e9 pour l'entra\u00eenement.</li> </ul> <p>On r\u00e9pete les deux derniers points jusqu'\u00e0 ce que tous les subsets aient \u00e9t\u00e9 \u00e9valu\u00e9s. Si on s\u00e9pare notre dataset en 10 subsets, alors il faudra entra\u00eener le mod\u00e8le 10 fois. Une fois que tous les entra\u00eenements sont termin\u00e9s, on peut \u00e9valuer notre mod\u00e8le en prenant la moyenne de ses performances sur les diff\u00e9rents entrainements.</p> <p></p> <p>Il existe 3 type de cross-validation qui sont assez proches les uns des autres : le k-fold cross validation, le stratified k-fold cross validation et le leave one out cross validation (LOOCV).</p> <p>Le k-fold cross validation est la version la plus classique. On divise notre dataset en k subsets. On entra\u00eene k mod\u00e8les \u00e0 chaque fois avec un diff\u00e9rent set de validation et on fait la moyenne des scores pour \u00e9valuer le mod\u00e8le de mani\u00e8re g\u00e9n\u00e9rale.</p> <p></p> <p>Comment choisir le param\u00e8tre k : En g\u00e9n\u00e9ral, on va choisir k de sorte \u00e0 ce que les subset soit suffisamment important pour representer statistiquement le dataset original. La choix de k va aussi d\u00e9pendre du temps et des ressources dont l'on dispose car plus k est grand, plus on doit faire d'entrainements.</p> <p>En g\u00e9n\u00e9ral, k=10 est une bonne valeur.</p> <p>Cette m\u00e9thode est presque identique \u00e0 la k-fold cross validation de base mais on rajoute une contrainte. On sp\u00e9cifie que chaque subset doit la m\u00eame distribution de classes. Cela permet de juger chaque subset sur un pied d'\u00e9galit\u00e9 en terme performance relative \u00e0 chaque classe.</p> <p></p> <p>Encore une fois, cette m\u00e9thode est tr\u00e8s proche de la classique k-fold cross validation puisqu'il s'agit simplement de cette m\u00e9thode avec k=n (n \u00e9tant le taille du dataset). A chaque fois, on va entra\u00eener le mod\u00e8le sur toutes les donn\u00e9es sauf une. Cela va revenir \u00e0 entra\u00eener notre mod\u00e8le n fois ce qui peut vite \u00eatre couteux en temps et en ressources. L'avantage de cette m\u00e9thode est que l'on peut entra\u00eener le mod\u00e8le sur presque toutes les donn\u00e9es du dataset. En pratique, cette m\u00e9thode ne s'utilise pas trop sauf dans des cas o\u00f9 l'on fait un finetuning sur peu de donn\u00e9es (et \u00e0 ce moment l\u00e0, elle est tr\u00e8s interessante).</p> <p>La cross validation a plusieurs avantages :</p> <ul> <li>On va d\u00e9tecter l'overfitting plus facilement et on va pouvoir r\u00e9gler les hyperparam\u00e8tres en cons\u00e9quence.</li> <li>Dans un cadre scientifique, \u00e9valuer les mod\u00e8les avec la cross validation permet une \u00e9valuation plus fiable et supprime en partie la chance (que l'on pourrait avoir lorsque l'on s\u00e9pare al\u00e9atoirement nos donn\u00e9es de train et de validation).</li> </ul> <p>Si vous pouvez vous le permettre (suffisament de temps et de ressources de calcul), je vous inviterai \u00e0 utiliser la cross validation syst\u00e9matiquement.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html#introduction-a-la-cross-validation","title":"Introduction \u00e0 la cross validation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html#probleme-des-modeles-de-deep-learning","title":"Probl\u00e8me des mod\u00e8les de Deep Learning\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html#la-cross-validation-comment-ca-marche","title":"La cross-validation, comment \u00e7a marche ?\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html#k-fold-cross-validation","title":"K-fold cross validation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html#stratified-k-fold-cross-validation","title":"Stratified K-fold cross validation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html#leave-one-out-cross-validation","title":"Leave one out cross validation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/08_CrossValidation.html#avantages-et-interets-de-la-cross-validation","title":"Avantages et int\u00earets de la cross-validation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html","title":"M\u00e9triques d'\u00e9valuation de mod\u00e8les","text":"<p>Une partie essentielle de l'entra\u00eenement d'un mod\u00e8le est son \u00e9valuation. Jusqu'\u00e0 pr\u00e9sent, nous avons uniquement utilis\u00e9 le loss de test ou des m\u00e9triques simples comme la pr\u00e9cision pour \u00e9valuer notre mod\u00e8le.</p> <p>En fonction du probl\u00e8me \u00e0 r\u00e9soudre, il existe diff\u00e9rents types de m\u00e9triques permettant d'\u00e9valuer diff\u00e9rents aspects de notre mod\u00e8le. Ce cours pr\u00e9sente plusieurs m\u00e9triques que l'on peut choisir ou non d'utiliser pour \u00e9valuer notre mod\u00e8le.</p> <p>Dans un cadre de classification binaire, on peut se representer les pr\u00e9dictions du mod\u00e8le de la mani\u00e8re suivante :</p> <p></p> <p>Figure extraite de wikipedia</p> <p>D\u00e9finissons d'abord les termes de base :</p> <ul> <li>Faux positif (fp) : L'\u00e9l\u00e9ment a \u00e9t\u00e9 class\u00e9 comme positif alors qu'il est n\u00e9gatif.</li> <li>Vrai positif (vp) : L'\u00e9l\u00e9ment a \u00e9t\u00e9 class\u00e9 comme positif et il est r\u00e9ellement positif.</li> <li>Faux n\u00e9gatif (fn) : L'\u00e9l\u00e9ment a \u00e9t\u00e9 class\u00e9 comme n\u00e9gatif alors qu'il est positif.</li> <li>Vrai n\u00e9gatif (vn) : L'\u00e9l\u00e9ment a \u00e9t\u00e9 class\u00e9 comme n\u00e9gatif et il est r\u00e9ellement n\u00e9gatif.</li> </ul> <p>Pour se representer la figure plus haut, on peut cr\u00e9er une matrice de confusion :</p> <p></p> <p>Note : Pour une classification avec plus de deux classes, on peut aussi construire une matrice de confusion. Cela nous donnera des informations sur les classes que le mod\u00e8le confond.</p> <p>A partir de la matrice de confusion et des termes de base (fn,vp etc ...), on peut calculer plusieurs m\u00e9triques int\u00e9ressants :</p> <ul> <li>Pr\u00e9cision : La pr\u00e9cision est d\u00e9finie par l'\u00e9quation suivante : $Pr\u00e9cision=\\frac{vp}{vp+fp}$. C'est la mesure qui indique combien d'\u00e9l\u00e9ments positifs ont \u00e9t\u00e9 correctement classifi\u00e9s sur l'ensemble des \u00e9l\u00e9ments classifi\u00e9s positifs par le mod\u00e8le.</li> <li>Rappel ou sensitivit\u00e9 : Le rappel est d\u00e9fini par l'\u00e9quation suivante : $Rappel=\\frac{vp}{vp+fn}$. C'est la mesure qui indique combien d'\u00e9l\u00e9ments positifs ont \u00e9t\u00e9 correctement classifi\u00e9s sur l'ensemble des \u00e9l\u00e9ments positifs.</li> <li>Sp\u00e9cificit\u00e9 ou s\u00e9lectivit\u00e9 : La sp\u00e9cificit\u00e9 est d\u00e9finie par l'\u00e9quation suivante : $Sp\u00e9cificit\u00e9=\\frac{vn}{vn+fp}$. C'est le nombre d'\u00e9l\u00e9ments n\u00e9gatifs correctement classifi\u00e9es sur l'ensemble des \u00e9l\u00e9ments n\u00e9gatifs.</li> </ul> <p>La m\u00e9trique accuracy (nom un peu trompeur car la traduction de accuracy en fran\u00e7ais c'est pr\u00e9cision mais que ce ne sont pas les m\u00eames m\u00e9triques) est d\u00e9finie comme le nombre de pr\u00e9dictions correctes sur le nombre total de pr\u00e9dictions. Son \u00e9quation est la suivante : $Accuracy=\\frac{vp+vn}{vp+vn+fp+fn}$</p> <p>Note : Il faut faire attention avec cette m\u00e9trique en cas de class imbalance car celui ci peut \u00eatre trompeur.</p> <p>Le F1-score est une mesure tr\u00e8s souvent utilis\u00e9e pour \u00e9valuer la performance du mod\u00e8le : il s'agit en fait de la moyenne harmonique de la pr\u00e9cision et du rappel. Son \u00e9quation est la suivante : $F1=2 \\times \\frac{pr\u00e9cision \\times rappel}{pr\u00e9cision + rappel}$</p> <p>Si les valeurs de pr\u00e9cision et de rappel sont proches, le F1-score correpond \u00e0 peu pr\u00e8s \u00e0 la moyenne.</p> <p>La courbe ROC (Receiver Operating Characteristic) est un graphique qui illustre les performances d'un mod\u00e8le de classification binaire \u00e0 diff\u00e9rents seuils de d\u00e9cision. La courbe ROC contient les \u00e9l\u00e9ments suivants :</p> <ul> <li>Axe des X est de le taux de faux positifs ($1-sp\u00e9cificit\u00e9$). C'est le nombre d'\u00e9l\u00e9ments n\u00e9gatifs incorrectement class\u00e9s comme positifs, divis\u00e9 par le nombre total d'\u00e9l\u00e9ments n\u00e9gatifs.</li> <li>Axe des Y est le taux de vrais positifs ($rappel$). C'est le nombre d'\u00e9l\u00e9ments positifs correctement class\u00e9s comme positifs, divis\u00e9 par le nombre total d'\u00e9l\u00e9ments positifs.</li> </ul> <p>Chaque point de la courbe va repr\u00e9senter un seuil de d\u00e9cision diff\u00e9rent pour la classification des \u00e9l\u00e9ments.</p> <p></p> <p>Figure extraite de blogpost.</p> <p>Pour juger la qualit\u00e9 d'un mod\u00e8le, on peut calculer l'aire sous la courbe ROC (AUROC). Dans le cas du random classifier on aura un AUROC de 0.5 alors que si il s'agit du classifieur parfait, l'AUROC sera de 1.</p> <p>Comme m\u00e9trique, on peut simplement regarder la valeur du loss sur les donn\u00e9es de test. Comme le loss est cens\u00e9 repr\u00e9senter notre objectif, regarder uniquement le loss peut \u00eatre suffisant dans de nombreux cas.</p> <p>Pour les mod\u00e8les de r\u00e9gression ou les autoencodeurs, on a plut\u00f4t comparer les valeurs pr\u00e9dites par rapport aux valeurs r\u00e9elles en calculant une distance. On peut le faire avec la MAE qui est la moyenne des valeurs absolues des erreurs entre les pr\u00e9dictions et les valeurs r\u00e9elles. La formule est la suivante : $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|$</p> <p>Souvent, on pr\u00e9fere utiliser MSE qui est la moyenne des carr\u00e9s des erreurs entre les pr\u00e9dictions et les valeurs r\u00e9elles. La formule est la suivante : $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2$</p> <p>Dans un probl\u00e8me de d\u00e9tection, on ne peut pas simplement \u00e9valuer la pr\u00e9cision et il faut tenir compte des diff\u00e9rents seuils de rappels pour avoir une \u00e9valuation sens\u00e9e. On la d\u00e9finit average precision (AP) de la mani\u00e8re suivante :</p> <p>$\\text{AP} = \\int_{0}^{1} \\text{Precision}(r) \\, \\text{d}r$</p> <p>ou de mani\u00e8re discr\u00e8te :</p> <p>$\\text{AP} = \\sum_{k=1}^{K} \\text{Precision}(r_k) \\cdot (r_k - r_{k-1})$</p> <p>o\u00f9 :</p> <ul> <li>$\\text{Precision}(r)$ est la pr\u00e9cision au rappel $r$.</li> <li>$K$ est le nombre de points d'\u00e9valuation du rappel.</li> <li>$r_k $ et $r_{k-1}$ sont les rappels aux points $k$ et $k-1$, respectivement.</li> </ul> <p>La mean average precision (mAP) est la moyenne des AP pour toutes les classes dans un probl\u00e8me de d\u00e9tection multi-classes. Elle donne une \u00e9valuation globale de la performance du mod\u00e8le en prenant en compte toutes les classes.</p> <p>$\\text{mAP} = \\frac{1}{C} \\sum_{c=1}^{C} \\text{AP}_c$</p> <p>Dans la cadre de la d\u00e9tection et de la segmentation, la mesure d'IoU est tr\u00e8s importante. Pour la d\u00e9tection, on va d\u00e9finir un seuil d'IoU en dessous duquel la d\u00e9tection sera consid\u00e9r\u00e9e comme invalide (donc ne sera pas prise en compte pour le calcul du mAP). Pour la segmentation, l'IoU est une m\u00e9trique \u00e0 part enti\u00e8re et permet de juger de la qualit\u00e9 de la segmentation. La formule est la suivante : $\\text{IoU} = \\frac{|\\text{Intersection}|}{|\\text{Union}|} = \\frac{|\\text{Pr\u00e9diction} \\cap \\text{V\u00e9rit\u00e9 terrain}|}{|\\text{Pr\u00e9diction} \\cup \\text{V\u00e9rit\u00e9 terrain}|}$</p> <p>Note : Un d\u00e9faut de l'IoU pour \u00e9valuer la segmentation est que cette mesure va p\u00e9naliser les petits objets et les classes rares. Pour r\u00e9duire ce biais, on peut utiliser le dice coefficient.</p> <p>Pour une t\u00e2che de segmentation, on va plus souvent utilis\u00e9 le dice coefficient plut\u00f4t que l'IoU. Sa formule est la suivante : $\\text{Dice} = \\frac{2 \\times |\\text{Pr\u00e9diction} \\cap \\text{V\u00e9rit\u00e9 terrain}|}{|\\text{Pr\u00e9diction}| + |\\text{V\u00e9rit\u00e9 terrain}|}$</p> <p>Le dice coefficient met l'accent sur l'importance de l'intersection entre la pr\u00e9diction et la v\u00e9rit\u00e9 terrain, en donnant plus de poids \u00e0 la pr\u00e9sence d'\u00e9l\u00e9ments communs.</p> <p>L'\u00e9valuation des mod\u00e8les de langages est une t\u00e2che tr\u00e8s compliqu\u00e9e. De mani\u00e8re \u00e9vidente, on peut se baser sur le loss de test pour avoir une id\u00e9e des performances du mod\u00e8le mais cela ne nous donne pas une r\u00e9elle information sur les capacit\u00e9s du mod\u00e8le en pratique. De nombreuses m\u00e9thodes et benchmarks ont \u00e9t\u00e9 propos\u00e9s pour essayer d'\u00e9valuer les mod\u00e8les de langages selon diff\u00e9rents crit\u00e8res. Ce blogpost pr\u00e9sente les diff\u00e9rentes m\u00e9thodes d'\u00e9valuation des LLMs. Sur hugging face, un leaderboard des LLMs est aussi disponible o\u00f9 chaque mod\u00e8le open-source est \u00e9valu\u00e9 selon diff\u00e9rents benchmarks.</p> <p>Pour les mod\u00e8les de g\u00e9n\u00e9ration d'images, c'est aussi assez compliqu\u00e9 d'\u00e9valuer les mod\u00e8les en se basant sur de simples calculs. En g\u00e9n\u00e9ral, on va avoir besoin d'une \u00e9valuation humaine pour juger de la qualit\u00e9 de la g\u00e9n\u00e9ration d'images. Ce blogpost permet d'avoir une vision d'ensemble de l'\u00e9valuation de ce type de mod\u00e8les.</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#metriques-devaluation-de-modeles","title":"M\u00e9triques d'\u00e9valuation de mod\u00e8les\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#metriques-de-classification","title":"M\u00e9triques de classification\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#matrice-de-confusion","title":"Matrice de confusion\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#precision-rappel-et-specificite","title":"Pr\u00e9cision, rappel et sp\u00e9cificit\u00e9\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#accuracy","title":"Accuracy\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#f1-score","title":"F1-score\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#courbe-roc","title":"Courbe ROC\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#log-loss","title":"Log loss\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#metriques-de-regressionautoencoder","title":"M\u00e9triques de regression/autoencoder\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#mean-squared-error-mse","title":"Mean Squared Error (MSE)\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#metriques-pour-la-detection-et-la-segmentation","title":"M\u00e9triques pour la d\u00e9tection et la segmentation\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#ap-et-map","title":"AP et mAP\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#intersection-over-union-iou","title":"Intersection Over Union (IoU)\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#dice-coefficient","title":"Dice Coefficient\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#evaluation-des-modele-de-langages","title":"Evaluation des mod\u00e8le de langages\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/09_MetriquesEvaluation.html#evaluation-des-modele-de-generation-dimages","title":"Evaluation des mod\u00e8le de g\u00e9n\u00e9ration d'images\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html","title":"Introduction \u00e0 la tokenization","text":"<p>Un \u00e9l\u00e9ment cl\u00e9 des mod\u00e8les de langages (LLM) est la tokenization. Il s'agit de la toute premi\u00e8re \u00e9tape d'un r\u00e9seau transformer et cela consiste \u00e0 transformer un texte en une s\u00e9quence d'entiers. Ce cours est grandement inspir\u00e9 de la vid\u00e9o de Andrej Karpathy Let's build the GPT Tokenizer.</p> <p>Quand nous avons implement\u00e9 notre GPT, nous avons utilis\u00e9 un tokenizer tr\u00e8s simple qui consiste \u00e0 encoder chaque caract\u00e8re avec un entier diff\u00e9rent. En pratique, on pr\u00e9fere encoder des chunks de caract\u00e8res c'est \u00e0 dire des groupements de caract\u00e8res.</p> <p>Il est important de comprendre comment fonctionne un tokenizer pour comprendre compl\u00e9tement le fonctionnement d'un mod\u00e8le de langage.</p> <p>A la fin du cours, nous serons en mesure de r\u00e9pondre \u00e0 ces questions :</p> <ul> <li>Pourquoi les LLM ont du mal a \u00e9peler les mots ?</li> <li>Pourquoi les LLM ont du mal \u00e0 faire des op\u00e9rations simples sur les string (comme inverser un string) ?</li> <li>Pourquoi les LLM sont meilleurs sur l'anglais ?</li> <li>Pourquoi les LLM sont mauvais en arithm\u00e9tique ?</li> <li>Pourquoi GPT-2 n'est pas tr\u00e8s bon en python ?</li> <li>Pourquoi mon LLM s'arr\u00eate directement si je lui envoie le string \"\" ?</li> <li>Pourquoi le LLM se casse quand je lui parle de SolidGoldMagiKarp ?</li> <li>Pourquoi il faut mieux utiliser YAML plut\u00f4t que JSON avec les LLM ?</li> </ul> <p>Note : Le tokenizer est une partie compl\u00e9tement s\u00e9par\u00e9e du LLM avec son propre dataset d'entrainement et qui est entrain\u00e9 diff\u00e9remment :</p> <p></p> <p>Commen\u00e7ons par analyser la tokenization de GPT-2 via le site internet Tiktokenizer et essayons de comprendre ce qui peut poser probl\u00e8me. Le tokenizer de GPT-2 a un vocabulaire d'environ 50 000 mots (donc 50 000 tokens distincts).</p> <p>Dans un premier temps, si on regarde pour la partie arithm\u00e9tique, on se rend vite compte que les nombres peuvent \u00eatre s\u00e9parer en token d'une mani\u00e8re qui semble assez arbitraire. Par exemple :</p> <p></p> <p>998 est un token \u00e0 part enti\u00e8re mais 9988 est s\u00e9par\u00e9 en deux tokens 99 et 88. On peut facilement imaginer que pour le LLM, \u00e7a va \u00eatre compliqu\u00e9 de compter.</p> <p>Pour des mots identiques et en fonction de la mani\u00e8re dont ils sont \u00e9crit, on a des tokens diff\u00e9rents. Par exemple : </p> <p>Les 4 mots identiques sont represent\u00e9es par des tokens diff\u00e9rents (le token 198 correspond au retour \u00e0 la ligne). Le mod\u00e8le va donc devoir apprendre que ces tokens sont presques les m\u00eames.</p> <p>Pour la m\u00eame phrase dans diff\u00e9rentes langues, le nombre de tokens utilis\u00e9s n'est pas le m\u00eame :</p> <p></p> <p>Cela est du au fait que le tokenizer de GPT-2 est entrain\u00e9 majoritairement sur des donn\u00e9es en langue anglaise. En pratique, cela diminue les capacit\u00e9s du mod\u00e8le dans les autres langues car le contexte n'est plus le m\u00eame en terme d'information. On peut mettre un texte beaucoup plus long en anglais qu'en japonais.</p> <p>On peut regarder comment le tokenizer se comporte pour le code python :</p> <p></p> <p>Chaque espace de l'indentation est compt\u00e9 comme un token... Si on a un code avec de nombreuses conditions ou boucles, le contexte va augmenter de mani\u00e8re tr\u00e8s rapide ce qui rendra le mod\u00e8le assez peu performant.</p> <p>Note : Ce d\u00e9faut a \u00e9t\u00e9 corrig\u00e9 dans les versions suivantes de GPT (3 et 4), une indentation de 4 tab sera un unique token par exemple.</p> <p></p> <p>Note 2 : La fa\u00e7on dont est configur\u00e9 notre \u00e9diteur de code (2 ou 4 espaces pour l'indentation en python) peut \u00e9galement avoir un impact sur la tokenization.</p> <p>Note 3 : Un LLM sp\u00e9cialis\u00e9 dans le code aura aussi un tokenizer sp\u00e9cialis\u00e9 dans le code ce qui n'est pas n\u00e9gligeable en terme de gain de performance.</p> <p>Pour construire notre propre tokenizer, commen\u00e7ons par voir comment on peut convertir nos string en entier.</p> <p>Une m\u00e9thode possible de d'utiliser le unicode. Cela va permettre de convertir chaque caract\u00e8re en un entier.</p> In\u00a0[2]: Copied! <pre>sentence=\"Ce cours de deep learning est g\u00e9nial !\"\n# ord() permet de r\u00e9cup\u00e9rer le code unicode d'un caract\u00e8re\nunicode=[ord(char) for char in sentence]\nprint(unicode)\n</pre> sentence=\"Ce cours de deep learning est g\u00e9nial !\" # ord() permet de r\u00e9cup\u00e9rer le code unicode d'un caract\u00e8re unicode=[ord(char) for char in sentence] print(unicode) <pre>[67, 101, 32, 99, 111, 117, 114, 115, 32, 100, 101, 32, 100, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 101, 115, 116, 32, 103, 233, 110, 105, 97, 108]\n</pre> <p>En pratique, on ne peut pas utiliser \u00e7a pour plusieurs raisons :</p> <ul> <li>A ce jour, il y a presque 150 000 caract\u00e8res ce qui est trop important comme taille de vocabulaire.</li> <li>Il y a r\u00e9gulierement des mise \u00e0 jours (une par an) ce qui fait qu'un tokenizer bas\u00e9 sur \u00e7a deviendrait obsol\u00e8te au bout d'un an.</li> </ul> <p>Une autre possibilit\u00e9 est d'utiliser l'encoding utf-8 (16 ou 32 serait aussi possible mais moins pratique) qui permet d'encoder l'unicode en 4 \u00e0 8 bits. En faisant cela, notre taille de vocabulaire de base sera de 256.</p> <p>On va garder l'id\u00e9e d'utf-8 mais on souhaiterait augmenter la taille du vocabulaire car 256 c'est tr\u00e8s petits et \u00e7a demanderait aux LLMs d'avoir des tailles de contexte \u00e9normes.</p> In\u00a0[6]: Copied! <pre>sentence=\"Bonjour\"\nlist(sentence.encode('utf-8'))\n</pre> sentence=\"Bonjour\" list(sentence.encode('utf-8')) Out[6]: <pre>[66, 111, 110, 106, 111, 117, 114]</pre> <p>Pour augmenter notre taille de vocabulaire, on va utiliser l'algorithme byte-pair encoding. Le fonctionnement de cet algorithme est tr\u00e8s simple : On va, de mani\u00e8re it\u00e9rative, trouver la paire de byte la plus commune et la remplacer avec un nouveau token (ce qui augmente le vocabulaire de 1). Par exemple, prenons la s\u00e9quence :</p> <pre><code>aaabdaaabac\n</code></pre> <p>A la premi\u00e8re it\u00e9ration, on voit que la paire \"aa\" est la plus fr\u00e9quente, on va donc la remplacer par le token Z :</p> <pre><code>ZabdZabac\nZ=aa\n</code></pre> <p>Puis \u00e0 la seconde it\u00e9ration, c'est la paire \"ab\" que l'on va remplacer avec Y :</p> <pre><code>ZYdZYac\nY=ab\nZ=aa\n</code></pre> <p>Et enfin, \u00e0 la troisi\u00e8me it\u00e9ration, on peut remplacer ZY par X :</p> <pre><code>XdXac\nX=ZY\nY=ab\nZ=aa\n</code></pre> <p>On aura donc augment\u00e9 le vocabulaire tout en r\u00e9duisant la taille de la s\u00e9quence (et donc le contexte n\u00e9cessaire pour traiter la s\u00e9quence).</p> <p>L'avantage de cet algorithme, c'est qu'on peut le faire autant de fois que l'on veut jusqu'\u00e0 ce qu'on obtienne une taille de contexte qui nous satisfait.</p> <p>Note : Le choix des donn\u00e9es d'entra\u00eenement a un impact crucial sur le tokenizer, il faut veiller \u00e0 les choisir en fonction de nos objectifs.</p> <p>Pour montrer l'utilisation du byte-pair encoding, prenons un gros morceau de texte et comptons les paires. Pour cela, utilisons le premier chapitre du premier volume de la com\u00e9die humaine de Balzac. Le texte a \u00e9t\u00e9 r\u00e9cuper\u00e9 sur Gutenberg.</p> In\u00a0[25]: Copied! <pre>with open('balzac.txt', 'r', encoding='utf-8') as f:\n  text = f.read()\nprint(text[:1000])\n\ntokens = list(map(int, text.encode('utf-8')))\nprint(list(tokens[:1000]))\n</pre> with open('balzac.txt', 'r', encoding='utf-8') as f:   text = f.read() print(text[:1000])  tokens = list(map(int, text.encode('utf-8'))) print(list(tokens[:1000])) <pre>Au milieu de la rue Saint-Denis, presque au coin de la rue du\nPetit-Lion, existait nagu\u00e8re une de ces maisons pr\u00e9cieuses qui donnent\naux historiens la facilit\u00e9 de reconstruire par analogie l'ancien Paris.\nLes murs mena\u00e7ants de cette bicoque semblaient avoir \u00e9t\u00e9 bariol\u00e9s\nd'hi\u00e9roglyphes. Quel autre nom le fl\u00e2neur pouvait-il donner aux X et aux\nV que tra\u00e7aient sur la fa\u00e7ade les pi\u00e8ces de bois transversales ou\ndiagonales dessin\u00e9es dans le badigeon par de petites l\u00e9zardes\nparall\u00e8les? \u00c9videmment, au passage de toutes les voitures, chacune de\nces solives s'agitait dans sa mortaise. Ce v\u00e9n\u00e9rable \u00e9difice \u00e9tait\nsurmont\u00e9 d'un toit triangulaire dont aucun mod\u00e8le ne se verra bient\u00f4t\nplus \u00e0 Paris. Cette couverture, tordue par les intemp\u00e9ries du climat\nparisien, s'avan\u00e7ait de trois pieds sur la rue, autant pour garantir des\neaux pluviales le seuil de la porte, que pour abriter le mur d'un\ngrenier et sa lucarne sans appui. Ce dernier \u00e9tage \u00e9tait construit en\nplanches clou\u00e9es l'une sur l'autre comme de\n[65, 117, 32, 109, 105, 108, 105, 101, 117, 32, 100, 101, 32, 108, 97, 32, 114, 117, 101, 32, 83, 97, 105, 110, 116, 45, 68, 101, 110, 105, 115, 44, 32, 112, 114, 101, 115, 113, 117, 101, 32, 97, 117, 32, 99, 111, 105, 110, 32, 100, 101, 32, 108, 97, 32, 114, 117, 101, 32, 100, 117, 10, 80, 101, 116, 105, 116, 45, 76, 105, 111, 110, 44, 32, 101, 120, 105, 115, 116, 97, 105, 116, 32, 110, 97, 103, 117, 195, 168, 114, 101, 32, 117, 110, 101, 32, 100, 101, 32, 99, 101, 115, 32, 109, 97, 105, 115, 111, 110, 115, 32, 112, 114, 195, 169, 99, 105, 101, 117, 115, 101, 115, 32, 113, 117, 105, 32, 100, 111, 110, 110, 101, 110, 116, 10, 97, 117, 120, 32, 104, 105, 115, 116, 111, 114, 105, 101, 110, 115, 32, 108, 97, 32, 102, 97, 99, 105, 108, 105, 116, 195, 169, 32, 100, 101, 32, 114, 101, 99, 111, 110, 115, 116, 114, 117, 105, 114, 101, 32, 112, 97, 114, 32, 97, 110, 97, 108, 111, 103, 105, 101, 32, 108, 39, 97, 110, 99, 105, 101, 110, 32, 80, 97, 114, 105, 115, 46, 10, 76, 101, 115, 32, 109, 117, 114, 115, 32, 109, 101, 110, 97, 195, 167, 97, 110, 116, 115, 32, 100, 101, 32, 99, 101, 116, 116, 101, 32, 98, 105, 99, 111, 113, 117, 101, 32, 115, 101, 109, 98, 108, 97, 105, 101, 110, 116, 32, 97, 118, 111, 105, 114, 32, 195, 169, 116, 195, 169, 32, 98, 97, 114, 105, 111, 108, 195, 169, 115, 10, 100, 39, 104, 105, 195, 169, 114, 111, 103, 108, 121, 112, 104, 101, 115, 46, 32, 81, 117, 101, 108, 32, 97, 117, 116, 114, 101, 32, 110, 111, 109, 32, 108, 101, 32, 102, 108, 195, 162, 110, 101, 117, 114, 32, 112, 111, 117, 118, 97, 105, 116, 45, 105, 108, 32, 100, 111, 110, 110, 101, 114, 32, 97, 117, 120, 32, 88, 32, 101, 116, 32, 97, 117, 120, 10, 86, 32, 113, 117, 101, 32, 116, 114, 97, 195, 167, 97, 105, 101, 110, 116, 32, 115, 117, 114, 32, 108, 97, 32, 102, 97, 195, 167, 97, 100, 101, 32, 108, 101, 115, 32, 112, 105, 195, 168, 99, 101, 115, 32, 100, 101, 32, 98, 111, 105, 115, 32, 116, 114, 97, 110, 115, 118, 101, 114, 115, 97, 108, 101, 115, 32, 111, 117, 10, 100, 105, 97, 103, 111, 110, 97, 108, 101, 115, 32, 100, 101, 115, 115, 105, 110, 195, 169, 101, 115, 32, 100, 97, 110, 115, 32, 108, 101, 32, 98, 97, 100, 105, 103, 101, 111, 110, 32, 112, 97, 114, 32, 100, 101, 32, 112, 101, 116, 105, 116, 101, 115, 32, 108, 195, 169, 122, 97, 114, 100, 101, 115, 10, 112, 97, 114, 97, 108, 108, 195, 168, 108, 101, 115, 63, 32, 195, 137, 118, 105, 100, 101, 109, 109, 101, 110, 116, 44, 32, 97, 117, 32, 112, 97, 115, 115, 97, 103, 101, 32, 100, 101, 32, 116, 111, 117, 116, 101, 115, 32, 108, 101, 115, 32, 118, 111, 105, 116, 117, 114, 101, 115, 44, 32, 99, 104, 97, 99, 117, 110, 101, 32, 100, 101, 10, 99, 101, 115, 32, 115, 111, 108, 105, 118, 101, 115, 32, 115, 39, 97, 103, 105, 116, 97, 105, 116, 32, 100, 97, 110, 115, 32, 115, 97, 32, 109, 111, 114, 116, 97, 105, 115, 101, 46, 32, 67, 101, 32, 118, 195, 169, 110, 195, 169, 114, 97, 98, 108, 101, 32, 195, 169, 100, 105, 102, 105, 99, 101, 32, 195, 169, 116, 97, 105, 116, 10, 115, 117, 114, 109, 111, 110, 116, 195, 169, 32, 100, 39, 117, 110, 32, 116, 111, 105, 116, 32, 116, 114, 105, 97, 110, 103, 117, 108, 97, 105, 114, 101, 32, 100, 111, 110, 116, 32, 97, 117, 99, 117, 110, 32, 109, 111, 100, 195, 168, 108, 101, 32, 110, 101, 32, 115, 101, 32, 118, 101, 114, 114, 97, 32, 98, 105, 101, 110, 116, 195, 180, 116, 10, 112, 108, 117, 115, 32, 195, 160, 32, 80, 97, 114, 105, 115, 46, 32, 67, 101, 116, 116, 101, 32, 99, 111, 117, 118, 101, 114, 116, 117, 114, 101, 44, 32, 116, 111, 114, 100, 117, 101, 32, 112, 97, 114, 32, 108, 101, 115, 32, 105, 110, 116, 101, 109, 112, 195, 169, 114, 105, 101, 115, 32, 100, 117, 32, 99, 108, 105, 109, 97, 116, 10, 112, 97, 114, 105, 115, 105, 101, 110, 44, 32, 115, 39, 97, 118, 97, 110, 195, 167, 97, 105, 116, 32, 100, 101, 32, 116, 114, 111, 105, 115, 32, 112, 105, 101, 100, 115, 32, 115, 117, 114, 32, 108, 97, 32, 114, 117, 101, 44, 32, 97, 117, 116, 97, 110, 116, 32, 112, 111, 117, 114, 32, 103, 97, 114, 97, 110, 116, 105, 114, 32, 100, 101, 115, 10, 101, 97, 117, 120, 32, 112, 108, 117, 118, 105, 97, 108, 101, 115, 32, 108, 101, 32, 115, 101, 117, 105, 108, 32, 100, 101, 32, 108, 97, 32, 112, 111, 114, 116, 101, 44, 32, 113, 117, 101, 32, 112, 111, 117, 114, 32, 97, 98, 114, 105, 116, 101, 114, 32, 108, 101, 32, 109, 117, 114, 32, 100, 39, 117, 110, 10, 103, 114, 101, 110, 105, 101, 114, 32, 101, 116, 32, 115, 97, 32, 108, 117, 99, 97, 114, 110, 101, 32, 115, 97, 110, 115, 32, 97, 112, 112, 117, 105, 46, 32, 67, 101, 32, 100, 101, 114, 110, 105, 101, 114, 32, 195, 169, 116, 97, 103, 101, 32, 195, 169, 116, 97, 105, 116, 32, 99, 111, 110, 115, 116, 114, 117, 105, 116, 32, 101, 110, 10, 112, 108, 97, 110, 99, 104, 101, 115, 32, 99, 108, 111, 117, 195, 169]\n</pre> <p>Comptons maintenant les paires :</p> In\u00a0[30]: Copied! <pre>def get_stats(ids):\n    counts = {}\n    for pair in zip(ids, ids[1:]): \n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\n\nstats = get_stats(tokens)\nprint(\"Les 5 paires les plus fr\u00e9quentes : \",sorted(((v,k) for k,v in stats.items()), reverse=True)[:5])\n\ntop_pair = max(stats, key=stats.get)\nprint(\"La paire la plus fr\u00e9quente est : \", top_pair)\n</pre> def get_stats(ids):     counts = {}     for pair in zip(ids, ids[1:]):          counts[pair] = counts.get(pair, 0) + 1     return counts  stats = get_stats(tokens) print(\"Les 5 paires les plus fr\u00e9quentes : \",sorted(((v,k) for k,v in stats.items()), reverse=True)[:5])  top_pair = max(stats, key=stats.get) print(\"La paire la plus fr\u00e9quente est : \", top_pair) <pre>Les 5 paires les plus fr\u00e9quentes :  [(5025, (101, 32)), (2954, (115, 32)), (2429, (32, 100)), (2332, (116, 32)), (2192, (101, 115))]\nLa paire la plus fr\u00e9quente est :  (101, 32)\n</pre> <p>D\u00e9finissons maintenant une fonction pour merge les paires les plus fr\u00e9quentes :</p> In\u00a0[34]: Copied! <pre># Fonction pour fusionner les paires les plus fr\u00e9quentes, on donne en entr\u00e9e la liste des tokens, la paire \u00e0 fusionner et le nouvel index\ndef merge(ids, pair, idx):\n  newids = []\n  i = 0\n  while i &lt; len(ids):\n    # Si on est pas \u00e0 la derni\u00e8re position et que la paire correspond, on la remplace\n    if i &lt; len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n      newids.append(idx)\n      i += 2\n    else:\n      newids.append(ids[i])\n      i += 1\n  return newids\n\n# Test de la fonction merge\nprint(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n\n\nprint(\"taille du texte avant :\", len(tokens))\n# On fusionne la paire la plus fr\u00e9quente et on lui donne un nouvel index (256 car on a d\u00e9j\u00e0 les caract\u00e8res de 0 \u00e0 255)\ntokens2 = merge(tokens, top_pair, 256)\nprint(tokens2[:100])\nprint(\"taille du texte apr\u00e8s :\", len(tokens2))\n</pre> # Fonction pour fusionner les paires les plus fr\u00e9quentes, on donne en entr\u00e9e la liste des tokens, la paire \u00e0 fusionner et le nouvel index def merge(ids, pair, idx):   newids = []   i = 0   while i &lt; len(ids):     # Si on est pas \u00e0 la derni\u00e8re position et que la paire correspond, on la remplace     if i &lt; len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:       newids.append(idx)       i += 2     else:       newids.append(ids[i])       i += 1   return newids  # Test de la fonction merge print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))   print(\"taille du texte avant :\", len(tokens)) # On fusionne la paire la plus fr\u00e9quente et on lui donne un nouvel index (256 car on a d\u00e9j\u00e0 les caract\u00e8res de 0 \u00e0 255) tokens2 = merge(tokens, top_pair, 256) print(tokens2[:100]) print(\"taille du texte apr\u00e8s :\", len(tokens2)) <pre>[5, 6, 99, 9, 1]\ntaille du texte avant : 128987\n[65, 117, 32, 109, 105, 108, 105, 101, 117, 32, 100, 256, 108, 97, 32, 114, 117, 256, 83, 97, 105, 110, 116, 45, 68, 101, 110, 105, 115, 44, 32, 112, 114, 101, 115, 113, 117, 256, 97, 117, 32, 99, 111, 105, 110, 32, 100, 256, 108, 97, 32, 114, 117, 256, 100, 117, 10, 80, 101, 116, 105, 116, 45, 76, 105, 111, 110, 44, 32, 101, 120, 105, 115, 116, 97, 105, 116, 32, 110, 97, 103, 117, 195, 168, 114, 256, 117, 110, 256, 100, 256, 99, 101, 115, 32, 109, 97, 105, 115, 111]\ntaille du texte apr\u00e8s : 123962\n</pre> <p>Avec une seule merge, on a d\u00e9j\u00e0 bien r\u00e9duit la taille de l'encoding du texte. Maintenant, on va d\u00e9finir la taille de vocabulaire que l'on veut et on va merger autant de fois que n\u00e9cessaire !</p> In\u00a0[38]: Copied! <pre>vocab_size = 276 # La taille du vocabulaire que l'on souhaite\nnum_merges = vocab_size - 256\ntokens_merged=tokens\n\n\nmerges = {} # (int, int) -&gt; int\nfor i in range(num_merges):\n  stats = get_stats(tokens_merged)\n  pair = max(stats, key=stats.get)\n  idx = 256 + i\n  print(f\"merging {pair} into a new token {idx}\")\n  tokens_merged = merge(tokens_merged, pair, idx)\n  merges[pair] = idx\n</pre> vocab_size = 276 # La taille du vocabulaire que l'on souhaite num_merges = vocab_size - 256 tokens_merged=tokens   merges = {} # (int, int) -&gt; int for i in range(num_merges):   stats = get_stats(tokens_merged)   pair = max(stats, key=stats.get)   idx = 256 + i   print(f\"merging {pair} into a new token {idx}\")   tokens_merged = merge(tokens_merged, pair, idx)   merges[pair] = idx <pre>merging (101, 32) into a new token 256\nmerging (115, 32) into a new token 257\nmerging (116, 32) into a new token 258\nmerging (195, 169) into a new token 259\nmerging (101, 110) into a new token 260\nmerging (97, 105) into a new token 261\nmerging (44, 32) into a new token 262\nmerging (111, 110) into a new token 263\nmerging (101, 257) into a new token 264\nmerging (111, 117) into a new token 265\nmerging (114, 32) into a new token 266\nmerging (97, 110) into a new token 267\nmerging (113, 117) into a new token 268\nmerging (100, 256) into a new token 269\nmerging (97, 32) into a new token 270\nmerging (101, 117) into a new token 271\nmerging (101, 115) into a new token 272\nmerging (108, 256) into a new token 273\nmerging (105, 110) into a new token 274\nmerging (46, 32) into a new token 275\n</pre> <p>On peut maintenant voir la diff\u00e9rence entre les deux s\u00e9quences de tokens :</p> In\u00a0[39]: Copied! <pre>print(\"Taille de base:\", len(tokens))\nprint(\"Taille apr\u00e8s merge:\", len(tokens_merged))\nprint(f\"compression ratio: {len(tokens) / len(tokens_merged):.2f}X\")\n</pre> print(\"Taille de base:\", len(tokens)) print(\"Taille apr\u00e8s merge:\", len(tokens_merged)) print(f\"compression ratio: {len(tokens) / len(tokens_merged):.2f}X\") <pre>Taille de base: 128987\nTaille apr\u00e8s merge: 98587\ncompression ratio: 1.31X\n</pre> <p>On a bien compress\u00e9 la taille de la s\u00e9quence alors que l'on a augment\u00e9 le vocabulaire de seulement 20. GPT-2 augmente le vocabulaire de 50 000 donc vous imaginez bien que \u00e7a r\u00e9duit drastiquement la taille des s\u00e9quences.</p> <p>Maintenant que l'on a construit notre tokenizer, on veut pouvoir passer des entiers (tokens) \u00e0 notre texte et inversement.</p> <p>Pour cela, construisons d'abord la fonction de decoding :</p> In\u00a0[45]: Copied! <pre>vocab = {idx: bytes([idx]) for idx in range(256)}\nfor (p0, p1), idx in merges.items():\n    vocab[idx] = vocab[p0] + vocab[p1]\n\n# Fonction pour d\u00e9coder les ids en texte, prend en entr\u00e9e une liste d'entiers et retourne une chaine de caract\u00e8res\ndef decode(ids):\n  tokens = b\"\".join(vocab[idx] for idx in ids)\n  text = tokens.decode(\"utf-8\", errors=\"replace\") # errors=\"replace\" permet de remplacer les caract\u00e8res non reconnus par le caract\u00e9re sp\u00e9cial \ufffd\n  return text\n\nprint(decode([87]))\n</pre> vocab = {idx: bytes([idx]) for idx in range(256)} for (p0, p1), idx in merges.items():     vocab[idx] = vocab[p0] + vocab[p1]  # Fonction pour d\u00e9coder les ids en texte, prend en entr\u00e9e une liste d'entiers et retourne une chaine de caract\u00e8res def decode(ids):   tokens = b\"\".join(vocab[idx] for idx in ids)   text = tokens.decode(\"utf-8\", errors=\"replace\") # errors=\"replace\" permet de remplacer les caract\u00e8res non reconnus par le caract\u00e9re sp\u00e9cial \ufffd   return text  print(decode([87])) <pre>W\n</pre> <p>Et la fonction d'encoding :</p> In\u00a0[52]: Copied! <pre># Fonction pour encoder le texte en ids, prend en entr\u00e9e une chaine de caract\u00e8res et retourne une liste d'entiers \ndef encode(text):\n  tokens = list(text.encode(\"utf-8\"))\n  while len(tokens) &gt;= 2:\n    stats = get_stats(tokens)\n    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n    if pair not in merges:\n      break \n    idx = merges[pair]\n    tokens = merge(tokens, pair, idx)\n  return tokens\n\nprint(encode(\"Bonjour\"))\n\n# On eut v\u00e9ifier que l'encodage et le d\u00e9codage fonctionne correctement\nprint(decode(encode(\"Bonjour\")))\n\n# Et sur le text en entier\ntext2 = decode(encode(text))\nprint(text2 == text)\n</pre> # Fonction pour encoder le texte en ids, prend en entr\u00e9e une chaine de caract\u00e8res et retourne une liste d'entiers  def encode(text):   tokens = list(text.encode(\"utf-8\"))   while len(tokens) &gt;= 2:     stats = get_stats(tokens)     pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))     if pair not in merges:       break      idx = merges[pair]     tokens = merge(tokens, pair, idx)   return tokens  print(encode(\"Bonjour\"))  # On eut v\u00e9ifier que l'encodage et le d\u00e9codage fonctionne correctement print(decode(encode(\"Bonjour\")))  # Et sur le text en entier text2 = decode(encode(text)) print(text2 == text) <pre>[66, 263, 106, 265, 114]\nBonjour\nTrue\n</pre> <p>La s\u00e9rie des GPT utilise des regex patterns pour s\u00e9parer le texte avant de cr\u00e9er le vocabulaire. Cela va permettre d'avoir plus de controle sur le type de tokens qui vont \u00eatre g\u00e9n\u00e9r\u00e9s (on va par exemple \u00e9viter d'avoir diff\u00e9rents tokens pour \"chien\", \"chien!\" et \"chien?\"). Dans le code source de Tiktoken (tokenizer de GPT), on peut retrouver le pattern suivant 's|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+. La syntaxe est assez compliqu\u00e9e mais nous allons le d\u00e9composer petit \u00e0 petit pour comprendre ce que \u00e7a fait :</p> <ul> <li>'s|'t|'re|'ve|'m|'ll|'d  : Correspond aux contractions anglaises comme \"is\", \"it\", \"are\", \"have\", \"am\", \"will\" et \"had\". Ces tokens sont souvent importants \u00e0 isoler dans le traitement du langage naturel.</li> <li>?\\p{L}+ : Correspond aux mots constitu\u00e9s de lettres. Le \"?\" en d\u00e9but signifie que le mot peut \u00eatre pr\u00e9c\u00e9d\u00e9 d'un espace, ce qui permet de capturer des mots avec ou sans espace initial.</li> <li>?\\p{N}+ : Correspond aux s\u00e9quences de chiffres (nombres). De la m\u00eame mani\u00e8re que pr\u00e9c\u00e9demment, un espace optionnel peut pr\u00e9c\u00e9der la s\u00e9quence de chiffres.</li> <li>?[^\\s\\p{L}\\p{N}]+ : Correspond \u00e0 un ou plusieurs caract\u00e8res qui ne sont ni des espaces, ni des lettres, ni des chiffres. Cela capture des symboles et des ponctuations, avec un espace optionnel au d\u00e9but.</li> <li>\\s+(?!\\S) : Correspond \u00e0 un ou plusieurs espaces suivies uniquement d'espaces (donc une s\u00e9quence d'espaces en fin de cha\u00eene ou avant une rupture de ligne).</li> <li>\\s+ : Correspond \u00e0 une ou plusieurs espaces. C'est une correspondance g\u00e9n\u00e9rique pour les espaces multiples entre les mots.</li> </ul> In\u00a0[53]: Copied! <pre>import regex as re\ngpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n\nprint(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))\n</pre> import regex as re gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")  print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\")) <pre>['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n</pre> <p>Le texte a \u00e9t\u00e9 s\u00e9par\u00e9 avec les conditions d\u00e9crites dans le regex pattern.</p> <p>Des tokens sp\u00e9ciaux sont \u00e9galement ajout\u00e9 pour l'entra\u00eenement et pour le finetuning :</p> <ul> <li>&lt;|endoftext|&gt; : Ce token est utilis\u00e9 pour d\u00e9limiter la s\u00e9paration entre des documents diff\u00e9rents dans les donn\u00e9es d'entrainement.</li> <li>&lt;|im_start|&gt; et &lt;|im_end|&gt; : Ces tokens d\u00e9limitent le d\u00e9but et la fin d'un message de l'utilisateur pour un chatbot par exemple.</li> </ul> <p>Note : Lors du finetuning, il est possible d'ajouter des tokens au tokenizer (comme &lt;|im_start|&gt; et &lt;|im_end|&gt; par exemple) sp\u00e9cifiques \u00e0 la t\u00e2che que l'on souhaite r\u00e9aliser. Bien sur, cela demandera de modifier la matrice d'embedding et de la r\u00e9entrainer.</p> <p>Le tokenizer que nous avons implement\u00e9 se calque sur le tokenizer tiktoken de OpenAI qui est utilis\u00e9 sur les mod\u00e8les GPT. Un autre tokenizer r\u00e9pandu est sentencepiece qui est utilis\u00e9 sur les mod\u00e8les de google et de meta par exemple.</p> <p>Note : Sentencepiece est bien plus complexe que tiktoken et dispose d'\u00e9normement de param\u00e8tres \u00e0 r\u00e9gler. En pratique, il est utilis\u00e9 sans doute parce que le code est open source (alors que le code d'entrainement de tiktoken n'est pas open-source, on a juste acc\u00e8s au code pour encoder et d\u00e9coder).</p> <p>Lorsqu'on va vouloir faire du traitement multimodal (c'est \u00e0 la mode en ce moment), il va falloir produire des tokens \u00e0 partir de modalit\u00e9s diff\u00e9rentes du texte comme du son ou des images. Ce qu'on voudrait id\u00e9alement c'est transformer notre son ou image en tokens et ensuite les donner au transformer comme s'il s'agissait de texte.</p> <p>Pour les images, on peut faire \u00e7a avec un VQVAE ou un VQGAN. L'id\u00e9e est d'utiliser un VAE ou GAN pour g\u00e9n\u00e9rer des valeurs discr\u00e8tes dans un espace latent. Ces valeurs discr\u00e8tes vont alors \u00eatre utilis\u00e9es comme tokens.</p> <p></p> <p>Figure extraite de l'article.</p> <p>Le mod\u00e8le SORA de OpenAi fait un peu la m\u00eame chose mais sur des videos :</p> <p></p> <p>Figure extraite de l'article</p> <p>Nous allons maintenant pouvoir r\u00e9pondre aux questions du d\u00e9but du cours \u00e0 l'aide de ce que nous avons appris :</p> <ul> <li><p>Pourquoi les LLM ont du mal a \u00e9peler les mots ? La s\u00e9paration en tokens fait que chaque mot n'est pas s\u00e9par\u00e9 en tous ses caract\u00e8res mais plut\u00f4t en chunks de caract\u00e8re. Il est alors compliqu\u00e9 pour le mod\u00e8le de les d\u00e9composer.</p> </li> <li><p>Pourquoi les LLM ont du mal \u00e0 faire des op\u00e9rations simples sur les string (comme inverser un string) ? C'est \u00e0 peu pr\u00e8s pour la m\u00eame raison que la question pr\u00e9c\u00e9dente. Pour inverser un mot, il ne suffit pas d'inverser les tokens representant ce mot.</p> </li> <li><p>Pourquoi les LLM sont meilleurs sur l'anglais ? Il y a plusieurs raisons \u00e0 cela : les donn\u00e9es d'entra\u00eenement du transformer et les donn\u00e9es d'entra\u00eenement du tokenizer. Pour le transformer, plus de donn\u00e9es en anglais va lui permettre d'apprendre mieux la langue et ses subtilit\u00e9s. Pour le tokenizer, si on l'entraine sur des donn\u00e9es en anglais, les tokens g\u00e9n\u00e9r\u00e9s vont principalement \u00eatre adapt\u00e9 pour des mots anglais donc on aura besoin de moins de contexte que pour les autres langues.</p> </li> <li><p>Pourquoi les LLM sont mauvais en arithm\u00e9tique ? Les nombres sont represent\u00e9s assez arbitrairement en fonction des donn\u00e9es d'entra\u00eenement. R\u00e9aliser des op\u00e9rations sur ces tokens n'est pas une chose ais\u00e9e pour le LLM.</p> </li> <li><p>Pourquoi GPT-2 n'est pas tr\u00e8s bon en python ? Comme nous l'avons vu dans ce cours, le tokenizer de GPT-2 transforme un simple espace en un token. En python, avec l'indentation et de multiples conditions/boucles, il y a rapidement beaucoup d'espace et cela impact fortement le contexte.</p> </li> <li><p>Pourquoi mon LLM s'arr\u00eate directement si je lui envoie le string \"\" ? Il s'agit d'un token sp\u00e9cial ajout\u00e9 dans les donn\u00e9es d'entra\u00eenement pour s\u00e9parer le texte. Lorsque le LLM le rencontre, il doit arr\u00eater sa g\u00e9n\u00e9ration.</p> </li> <li><p>Pourquoi le LLM se casse quand je lui parle de SolidGoldMagiKarp ? Celle-ci est un peu moins \u00e9vidente et je vous conseille de lire l'excellent blogpost. En expliquant simplement, si des mots sont pr\u00e9sents dans les donn\u00e9es d'entrainement du tokenizer mais pas dans les donn\u00e9es d'entrainement du LLM, alors l'embedding de ce token ne sera pas du tout entrain\u00e9 et le LLM va se comporter de mani\u00e8re al\u00e9atoire lorsqu'il rencontre ce token. SolidGoldMagiKarp est un utilisateur reddit qui devait apparaitre r\u00e9guli\u00e8rement dans les donn\u00e9es d'entrainement du tokenizer mais pas dans les donn\u00e9es d'entrainement du transformer.</p> </li> <li><p>Pourquoi il faut mieux utiliser YAML plut\u00f4t que JSON avec les LLM ? C'est un peu la m\u00eame id\u00e9e que pour python. Le tokenizer de GPT-2 (et de la plupart des mod\u00e8les \u00e0 vrai dire) transforme un document json en plus de tokens que son \u00e9quivalent yaml. Passer de json \u00e0 yaml va donc r\u00e9duire le contexte n\u00e9cessaire pour traiter le document.</p> </li> </ul>"},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#introduction-a-la-tokenization","title":"Introduction \u00e0 la tokenization\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#tokenizer-de-gpt-2","title":"Tokenizer de GPT-2\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#arithmetique","title":"Arithm\u00e9tique\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#meme-mots-differents-tokens","title":"M\u00eame mots, diff\u00e9rents tokens\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#autres-langues","title":"Autres langues\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#python","title":"Python\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#construisons-notre-propre-tokenizer","title":"Construisons notre propre tokenizer\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#unicode","title":"Unicode\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#utf-8","title":"UTF-8\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#byte-pair-encoding-algorithm","title":"Byte-pair encoding algorithm\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#byte-pair-encoding-application","title":"Byte-pair encoding application\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#decodingencoding","title":"Decoding/Encoding\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#regex-patterns","title":"Regex patterns\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#special-tokens","title":"Special tokens\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#autres-types-de-tokenizer","title":"Autres types de tokenizer\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#tokenization-sur-dautres-modalites","title":"Tokenization sur d'autres modalit\u00e9s ?\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/10_Tokenization.html#reponses-aux-questions-du-debut","title":"R\u00e9ponses aux questions du d\u00e9but\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html","title":"Quantization","text":"<p>Les mod\u00e8les de Deep Learning sont de plus en plus puissants et \u00e9galement de plus en plus gros. Si l'on regarde le cas des LLM, les meilleurs LLM ont maintenant plusieurs centaines de milliards de param\u00e8tres Llama 3.1 (pour les LLMs Open-Source). Avec un simple GPU, c'est impossible de charger un mod\u00e8le aussi gros. M\u00eame avec le plus gros GPU du march\u00e9 (H100 qui poss\u00e8de 80 giga de VRAM), il faut plusieurs GPU pour l'inf\u00e9rence et encore plus pour l'entra\u00eenement.</p> <p>En pratique, on sait qu'un nombre sup\u00e9rieur de param\u00e8tres est correl\u00e9 avec une meilleure performance. On ne veut donc pas diminuer la taille des mod\u00e8les. Ce que l'on voudrait, c'est r\u00e9duire l'espace m\u00e9moire que le mod\u00e8le occupe.</p> <p>Ce cours s'inspire fortement du blogpost et du blogpost. Les images utilis\u00e9es sont \u00e9galement extraites de ces deux blogposts.</p> <p>Pour r\u00e9presenter les nombres flottants sur un ordinateur, on utilise un certain nombre de bits. La norme IEEE_754 d\u00e9crit comment les bits peuvent repr\u00e9senter un nombre. Cela se fait via 3 parties : le signe, l'exposant et la mantisse.</p> <p>Voici un exemple de la r\u00e9presentation FP16 (16 bits) :</p> <p></p> <p>Le signe permet de d\u00e9terminer * roulement de tambour * le signe du nombre, l'exposant va donner les chiffres avant la virgule et la mantisse les chiffres apr\u00e8s la virgule. Voici un exemple en image de la mani\u00e8re de convertir la representation FP16 en chiffre.</p> <p></p> <p>De mani\u00e8re g\u00e9n\u00e9rale, plus on utilise de bits pour representer une valeur, plus cette valeur peut \u00eatre pr\u00e9cise ou sur une grande plage de valeur. Par exemple, on peut comparer la pr\u00e9cision FP16 et FP32 :</p> <p></p> <p>Une derni\u00e8re chose importante \u00e0 savoir. Il y a deux fa\u00e7on de juger une r\u00e9pr\u00e9sentation. D'une part, la dynamic range qui donne la plage des valeurs que l'on peut repr\u00e9senter et la precision qui d\u00e9crit l'\u00e9cart entre deux valeurs voisines.</p> <p>Plus l'exposant est important, plus la dynamic range est grande et plus la mantisse est importante, plus la precision est grande (donc 2 valeurs voisines sont proches).</p> <p>En deep learning, on pr\u00e9fere souvent utiliser la r\u00e9presentation BF16 au lieu de FP16. La repr\u00e9sentation BF16 a un exposant plus important mais une pr\u00e9cision plus faible.</p> <p>La figure suivante illustre les diff\u00e9rences :</p> <p></p> <p>Maitenant que l'on comprend les notions de pr\u00e9cision des nombres flottants, on peut calculer la place que prend un mod\u00e8le en m\u00e9moire en fonction de la pr\u00e9cision. En FP32, un nombre est repr\u00e9sent\u00e9 par 32 bits ce qui correspond \u00e0 4 octets (un octet vaut 8 bits pour rappel). Pour obtenir l'utilisation m\u00e9moire d'un mod\u00e8le, on peut faire le calcul suivant : $memory= \\frac{n_{bits}}{8}*n_{params}$</p> <p>Prenons l'exemple d'un mod\u00e8le de 70 milliards de param\u00e8tres \u00e0 plusieurs niveau de pr\u00e9cision : double(FP64), full-precision(FP32) et half-precision(FP16). Pour FP64 : $\\frac{64}{8} \\times 70B = 560GB$ Pour FP32 : $\\frac{32}{8} \\times 70B = 280GB$ Pour FP16 : $\\frac{16}{8} \\times 70B = 140GB$</p> <p>On se rend bien compte que c'est n\u00e9cessaire de trouver une mani\u00e8re de r\u00e9duire la taille des mod\u00e8les. Ici, m\u00eame le mod\u00e8le en half-precision occupe 140GB ce qui correspond \u00e0 2 GPU H100.</p> <p>Note : Ici, on parle de la pr\u00e9cision pour faire l'inf\u00e9rence. Pour l'entra\u00eenement, comme il faut garder les activations en m\u00e9moire pour la descente du gradient, on se retrouve avec beaucoup plus de param\u00e8tres (voir partie sur QLoRA plus loin dans le cours).</p> <p>Le but de la quantization est de r\u00e9duire la pr\u00e9cision d'un mod\u00e8le en passant d'une pr\u00e9cision riche comme FP32 \u00e0 une pr\u00e9cision plus faible comme INT8.</p> <p>Note : INT8 est la fa\u00e7on de repr\u00e9senter les entiers de -127 \u00e0 127 sur 8 bits.</p> <p></p> <p>Bien s\u00fbr, en diminuant le nombre de bits pour repr\u00e9senter les valeurs, on a une perte de pr\u00e9cision. Pour illustrer cela, on peut regarder sur une image :</p> <p></p> <p>On remarque un \"grain\" dans l'image ce qui est d\u00fb \u00e0 un manque de couleurs disponibles pour repr\u00e9senter l'image. Ce que l'on voudrait, c'est r\u00e9duire le nombre de bits pour repr\u00e9senter l'image en gardant au maximum la pr\u00e9cision de l'image de base.</p> <p>Il existe plusieurs mani\u00e8re de faire de la quantization : la quantization sym\u00e9trique et la quantization asym\u00e9trique.</p> <p>FP16 : La precision et la dynamic range diminuent par rapport \u00e0 FP32.</p> <p></p> <p>BF16 : La precision diminue fortement mais la dynamic range reste la m\u00eame par rapport \u00e0 FP32.</p> <p></p> <p>INT8 : On passe \u00e0 une repr\u00e9sentation en entier.</p> <p></p> <p>Dans le cas de la quantization sym\u00e9trique, la plage de valeurs de nos flottants d'origine est mapp\u00e9 de mani\u00e8re sym\u00e9trique sur la plage de valeur de quantization. C'est \u00e0 dire que le 0 dans les flottants est mapp\u00e9 sur le 0 dans la pr\u00e9cision de quantization.</p> <p></p> <p>Une des mani\u00e8res la plus commune et \u00e9galement la plus simple de r\u00e9aliser cette op\u00e9ration est d'utiliser la m\u00e9thode absmax (absolute maximum quantization). On prend la valeur maximale (en valeur absolue) et on r\u00e9alise le mapping par rapport \u00e0 cette valeur :</p> <p></p> <p>La formule est assez basique : Consid\u00e9rons $b$ le nombre d'octets que l'on veut quantize, $\\alpha$ la plus grande valeur absolue. Alors on peut calculer le scale factor de la mani\u00e8re suivante : $s=\\frac{2^{b-1}-1}{\\alpha}$ On peut alors effectuer la quantization de $x$ comme ceci : $x_{quantized}=round(s \\times x)$ Pour d\u00e9quantizer et retrouver une valeur FP32, on peut faire comme cela : $x_{dequantized}=\\frac{x_{quantized}}{s}$</p> <p>Bien entendu, la valeur dequantiz\u00e9 ne sera pas \u00e9quivalente \u00e0 la valeur avant quantization :</p> <p></p> <p>et on peut quantifier les erreurs de quantization :</p> <p></p> <p>A l'inverse de la quantization sym\u00e9trique, la quantization asym\u00e9trique n'est pas sym\u00e9trique autour de 0. Au lieu de \u00e7a, on map le minimum $\\beta$ et le maximum $\\alpha$ de la range des flottants d'origine sur le minimum et le maximum de la range quantiz\u00e9. La m\u00e9thode la plus commune pour cela est appel\u00e9 zero-point quantization.</p> <p></p> <p>Avec cette m\u00e9thode, le 0 a chang\u00e9 de position et c'est pourquoi cette m\u00e9thode est appel\u00e9 asym\u00e9trique.</p> <p>Comme le 0 a \u00e9t\u00e9 deplac\u00e9, on a besoin de calculer la position du 0 (zero-point) pour effectuer le mapping lin\u00e9aire.</p> <p>On peut quantizer de la mani\u00e8re suivante : $s=\\frac{128 - - 127}{\\alpha- \\beta}$ On calcule le zero-point : $z=round(-s \\times \\beta)-2^{b-1}$ et : $x_{quantized}=round(s \\times x + z)$ Pour d\u00e9quantizer, on peut alors appliquer la formule suivante : $x_{dequantized}=\\frac{x_{quantized}-z}{s}$</p> <p>Les deux m\u00e9thodes ont leurs avantages et inconv\u00e9nients, on peut les comparer en regardant le comportement sur un $x$ quelconque :</p> <p></p> <p>Les m\u00e9thodes que nous avons pr\u00e9sent\u00e9 pr\u00e9sentent un d\u00e9faut majeur. Ces m\u00e9thodes ne sont pas du tout robustes aux outliers. Imaginons que notre vecteur $x$ contient les valeurs suivante : [-0.59, -0.21, -0.07, 0.13, 0.28, 0.57, 256]. Si l'on fait notre mapping habituel, on va obtenir des valeurs identiques pour tous les \u00e9l\u00e9ments sauf l'outlier (256) :</p> <p></p> <p>C'est tr\u00e8s probl\u00e9matique car la perte d'information est colossale.</p> <p>En pratique, on peut d\u00e9cider de clip certaines valeurs pour diminuer la range dans l'espace des flottants (avant d'appliquer la quantization). Par exemple, on pourrait d\u00e9cider de limiter les valeurs dans la plage [-5,5] et toutes les valeurs en dehors de cette plage seront mapp\u00e9 aux valeurs maximales ou minimales de quantization (127 ou -127 pour INT8) :</p> <p></p> <p>En faisant cela, on diminue grandement l'erreur sur les non-outliers mais on l'augmente pour les outliers (ce qui peut \u00e9galement \u00eatre probl\u00e9matique).</p> <p>Dans la partie pr\u00e9c\u00e9dente, on a utilis\u00e9 arbitrairement une plage de valeur de [-5,5]. La s\u00e9lection de cette plage de valeur n'est pas al\u00e9atoire et est determin\u00e9e par une m\u00e9thode que l'on appelle calibration. L'id\u00e9e est de trouver une plage de valeur qui minimise l'erreur l'erreur de quantization pour l'ensemble des valeurs. Les m\u00e9thodes de calibration utilis\u00e9es sont diff\u00e9rentes selon le type de param\u00e8tres que l'on cherche \u00e0 quantizer.</p> <p>Calibration pour les poids et les biais  : Les poids et les biais sont des valeurs statiques (fixes apr\u00e8s l'entra\u00eenement du mod\u00e8le). Ce sont des valeurs que l'on connait avant de faire l'inf\u00e9rence. Souvent, comme il y a beaucoup plus de poids que de biais, on va conserver la pr\u00e9cision de base sur les biais et effectuer la quantization uniquement sur les poids.</p> <p>Pour les poids, il y a plusieurs m\u00e9thodes de calibration possibles :</p> <ul> <li>On peut choisir manuellement un pour\u00e7entage de la plage d'entr\u00e9e</li> <li>On peut optimiser la distance MSE entre les poids de base et les poids quantiz\u00e9s</li> <li>On peut minimiser l'entropie (avec le KL-divergence) entre les poids de base et les poids quantiz\u00e9s</li> </ul> <p>La m\u00e9thode avec pour\u00e7entage est similaire \u00e0 la m\u00e9thode que nous avons utilis\u00e9 pr\u00e9cedemment. Les deux autres m\u00e9thodes sont plus rigoureuses et efficaces.</p> <p>Calibration pour les activations : A l'inverse des poids et des biais, les activations sont d\u00e9pendantes de la valeur d'entr\u00e9e du mod\u00e8le. Il est donc tr\u00e8s compliqu\u00e9 de les quantizer efficacement. Ces valeurs sont mises \u00e0 jour apr\u00e8s chaque couche et on peut conna\u00eetre leurs valeurs uniquement pendant l'inf\u00e9rence lorsque la couche du mod\u00e8le traite les valeurs. Cela nous am\u00e8ne \u00e0 la partie suivante qui traite de deux m\u00e9thodes diff\u00e9rentes pour la quantization des activations (et \u00e9galement des poids). Ces m\u00e9thodes sont :</p> <ul> <li>La post-training quantization (PTQ) : la quantization intervient apr\u00e8s l'entra\u00eenement du mod\u00e8le</li> <li>La quantization aware training (QAT) : la quantization se fait pendant l'entra\u00eenement ou le fine-tuning du mod\u00e8le.</li> </ul> <p>Une des mani\u00e8res les plus fr\u00e9quentes de faire de la quantization est de le faire apr\u00e8s l'entra\u00eenement du mod\u00e8le. D'un point de vue pratique, c'est assez logique car cela ne n\u00e9cessite pas d'entra\u00eener ou de fine-tune le mod\u00e8le.</p> <p>La quantization des poids est effectu\u00e9e en utilisant soit la quantization sym\u00e9trique ou asym\u00e9trique.</p> <p>Pour les activations, ce n'est pas pareil puisqu'on ne connait pas la plage de valeurs prises par la distribution des activations. On a deux formes de quantization pour les activations :</p> <ul> <li>La dynamic quantization</li> <li>La static quantization</li> </ul> <p>Dans la quantization dynamique, on collecte les activations apr\u00e8s que la donn\u00e9e soit pass\u00e9e dans une couche. La distribution de la couche est ensuite quantiz\u00e9 en calculant le zeropoint et le scale factor.</p> <p></p> <p>Dans ce processus, chaque couche  ses propres valeurs de zeropoint et de scale factor et donc la quantization n'est pas la m\u00eame.</p> <p></p> <p>Note : Ce processus de quantization a lieu pendant l'inf\u00e9rence.</p> <p>A l'inverse de la dynamic quantization, la static quantization ne calcule pas le zeropoint et le scale factor pendant l'inf\u00e9rence. En effet, dans la m\u00e9thode de static quantization, les valeurs de zeropoint et scale factor sont calcul\u00e9s avant l'inf\u00e9rence \u00e0 l'aide d'un dataset de calibration. Ce dataset est suppos\u00e9 \u00eatre representatif des donn\u00e9es et permet de calculer les distributions potentiels prises par les activations.</p> <p></p> <p>Apr\u00e8s avoir collect\u00e9 les valeurs des activations sur l'ensemble du dataset de calibration, on peut les utiliser pour calculer le scale factor et le zeropoint qui seront ensuite utilis\u00e9 pour toutes les activations.</p> <p>En g\u00e9n\u00e9ral, la dynamic quantization est un peu plus pr\u00e9cise car elle calcule les valeurs de scale factor et de zeropoint pour chaque couche mais ce processus a \u00e9galement tendance \u00e0 ralentir le temps d'inf\u00e9rence.</p> <p>A l'inverse, la static quantization est moins pr\u00e9cise mais plus rapide.</p> <p>Dans l'id\u00e9al, on aimerait pousser la quantization au maximum, c'est-\u00e0-dire 4 bits au lieu de 8 bits. En pratique, ce n'est pas facile car cela augmente drastiquement l'erreur si l'on emploie simplement les m\u00e9thodes que l'on a vu jusqu'\u00e0 pr\u00e9sent.</p> <p>Il y a cependant quelques m\u00e9thodes permettant de r\u00e9duire le nombre de bits jusqu'\u00e0 2 bits (il est recommand\u00e9 de rester \u00e0 4 bits).</p> <p>Parmi ces m\u00e9thodes, on en retrouve deux principales :</p> <ul> <li>GPTQ (utilise seulement le GPU)</li> <li>GGUF (peut \u00e9galement utilis\u00e9 le CPU en partie)</li> </ul> <p>GPTQ est probablement la m\u00e9thode la plus utilis\u00e9e pour la quantization 4-bits. L'id\u00e9e est d'utiliser la quantization asym\u00e9trique sur chaque couche ind\u00e9pendamment :</p> <p></p> <p>Pendant le processus de quantization, les poids sont convertis en l'inverse de la matrice Hessian (d\u00e9riv\u00e9e seconde de la fonction de loss) ce qui nous permet de savoir si la sortie du mod\u00e8le est sensible aux changements de chaque poids. De mani\u00e8re simplifi\u00e9, cela permet de calculer l'importance de chaque poids dans une couche. Les poids associ\u00e9s \u00e0 de petites valeurs dans la Hessian sont les plus importants car un changement de ces poids va affecter le mod\u00e8le significativement.</p> <p></p> <p>On va ensuite quantizer puis dequantizer les poids pour obtenir notre quantization error. Cette erreur nous permet pond\u00e9rer l'erreur de quantization par rapport \u00e0 la vraie erreur et \u00e0 la matrice Hessian.</p> <p></p> <p>L'erreur pond\u00e9r\u00e9e est calcul\u00e9e comme ceci : $q=\\frac{x_1-y_1}{h_1}$ o\u00f9 $x_1$ est la valeur avant quantization, $y_1$ est la valeur apr\u00e8s quantization/dequantization et $h_1$ est la valeur correspondante dans la matrice Hessian.</p> <p>Ensuite, nous redistribuons cette erreur de quantification pond\u00e9r\u00e9e sur les autres poids de la ligne. Cela permet de maintenir la fonction globale et la sortie du r\u00e9seau. Par exemple, pour $x_2$: $x_2=x_2 + q \\times h_2$</p> <p></p> <p>On fait ce process jusqu'\u00e0 ce que toutes les valeurs soient quantiz\u00e9s. En pratique, cette m\u00e9thode marche bien car tous les poids sont corr\u00e9l\u00e9s les uns avec les autres donc si un poids a une grosse erreur de quantization, les autres poids sont chang\u00e9s pour compenser l'erreur (en se basant sur la Hessian).</p> <p>GPTQ est une tr\u00e8s bonne m\u00e9thode pour faire tourner un LLM sur un GPU. Cependant, m\u00eame avec cette quantization, on a parfois pas assez de m\u00e9moire GPU pour faire tourner un mod\u00e8le LLM profond. La m\u00e9thode GGUF permet de d\u00e9placer n'importe quelle couche du LLM sur le CPU.</p> <p>De cette mani\u00e8re, on peut utiliser la m\u00e9moire vive et la m\u00e9moire vid\u00e9o (vram) en m\u00eame temps.</p> <p>Cette m\u00e9thode de quantization est chang\u00e9e fr\u00e9quemment et d\u00e9pend du niveau de bit quantization que l'on souhaite.</p> <p>De mani\u00e8re g\u00e9n\u00e9rale, la m\u00e9thode fonctionne de la mani\u00e8re suivante :</p> <p>D'abord, les poids d'une couche sont divis\u00e9s en super block o\u00f9 chaque super block est \u00e0 nouveau divis\u00e9 en sub blocks. On va ensuite extraire les valeurs $s$ et $\\alpha$ (absmax) pour chaque block (le super et les sub).</p> <p></p> <p>Les scales factor $s$ des sub block sont ensuite quantiz\u00e9 \u00e0 nouveau en utilisant l'information du super block (qui a son propre scale factor). Cette m\u00e9thode est appel\u00e9e block-wise quantization.</p> <p>Note : De mani\u00e8re g\u00e9n\u00e9rale, le niveau de quantization est diff\u00e9rent entre les sub block et le super block : le super block a une pr\u00e9cision sup\u00e9rieure aux sub block le plus souvent.</p> <p>Au lieu d'effectuer la quantization apr\u00e8s l'entra\u00eenement, on peut le faire pendant l'entra\u00eenement. En effet, faire la quantization apr\u00e8s l'entra\u00eenement ne tient pas compte du proc\u00e9d\u00e9 d'entra\u00eenement ce qui peut poser des probl\u00e8mes.</p> <p>La quantization aware training est une m\u00e9thode permettant d'effectuer la quantization pendant l'entra\u00eenement et d'apprendre les diff\u00e9rents param\u00e8tres de quantization pendant la r\u00e9tropropagation :</p> <p></p> <p>En pratique, cette m\u00e9thode est souvent plus pr\u00e9cise que la PTQ parce que la quantization est d\u00e9j\u00e0 pr\u00e9vue lors de l'entrainement et on peut donc adapter le mod\u00e8le sp\u00e9cifiquement dans un objectif futur de quantization.</p> <p>Cette approche fonctionne de la mani\u00e8re suivante : Pendant l'entra\u00eenement, un processus de quantization/dequantization (fake quantization) est introduit (quantize de 32 bits \u00e0 4 bits puis dequantize de 4 bits \u00e0 32 bits par exemple).</p> <p></p> <p>Cette approche permet au mod\u00e8le de consid\u00e9rer la quantization pendant l'entra\u00eenement et donc d'adapter la mise \u00e0 jours de poids pour favoriser des bons r\u00e9sultats du mod\u00e8le quantiz\u00e9.</p> <p>Une fa\u00e7on de voir les choses est d'imaginer que le mod\u00e8le va converger vers des minimums larges qui minimize l'erreur de quantization plut\u00f4t que des minimuns \u00e9troits qui pourraient provoquer des erreurs lors de la quantization. Pour un mod\u00e8le entra\u00een\u00e9 sans fake quantization, il n'y aurait pas de pr\u00e9f\u00e9rences sur le minimum choisi pour la convergence :</p> <p></p> <p>En pratique, les mod\u00e8les entrain\u00e9 de mani\u00e8re classique ont un loss plus faible que les mod\u00e8le entra\u00een\u00e9 en QAT lorsque la pr\u00e9cision est grande (FP32) mais d\u00e8s lors que l'on quantize le mod\u00e8le, le mod\u00e8le QAT sera bien plus performant qu'un mod\u00e8le quantiz\u00e9 via une m\u00e9thode PTQ.</p> <p>L'id\u00e9al pour r\u00e9duire la taille d'un mod\u00e8le serait de quantit\u00e9 en 1 seul bit. Cela parait fou, comment peut-on imaginer repr\u00e9senter un r\u00e9seau de neurones avec uniquement est 0 et des 1 pour chaque poids.</p> <p>BitNet propose de representer les poids d'un mod\u00e8le avec un seul bit en utilisant la valeur -1 ou 1 pour un poids. Il faut imaginer que l'on remplace les couches lin\u00e9aires de l'architecture transformers par des couches BitLinear :</p> <p></p> <p>La couche BitLinear marche exactement comme une couche lin\u00e9aire de base sauf que les poids sont repr\u00e9sent\u00e9 avec un unique bit et les activations en INT8.</p> <p>Comme expliqu\u00e9 pr\u00e9cedemment, il y a une forme de fake quantization permettant d'apprendre au mod\u00e8le l'effet de la quantization pour le forcer \u00e0 s'adapter \u00e0 cette nouvelle contrainte :</p> <p></p> <p>Analysons cette couche \u00e9tape par etape :</p> <p>Premi\u00e8re Etape : Quantization des poids Pendant l'entra\u00eenement, les poids sont stock\u00e9s en INT8 et quantiz\u00e9 en 1-bit en utilisant la fonction signum. Cette fonction permet simplement de centrer la distribution des poids en 0 et convertit tout ce qui est inf\u00e9rieur \u00e0 0 en -1 et tout ce qui est sup\u00e9rieur \u00e0 0 en 1.</p> <p></p> <p>Une valeur $\\beta$ (valeur moyenne absolue) est \u00e9galement extraite pour le processus de d\u00e9quantization.</p> <p>Deuxi\u00e8me Etape : Quantization des activation Pour les activations, la couche BitLinear utilise la quantization absmax pour convertir de FP16 \u00e0 INT8 et une valeur $\\alpha$ (valeur maximum absolue) est stock\u00e9e pour la d\u00e9quantization.</p> <p>Troisi\u00e8me Etape : Dequantization A partir des $\\alpha$ et $\\beta$ que l'on a gard\u00e9, on peut utiliser ces valeurs pour d\u00e9quantizer et repasser en pr\u00e9cision FP16.</p> <p>Et c'est tout, la proc\u00e9dure est assez simple et permet au mod\u00e8le d'\u00eatre repr\u00e9sent\u00e9 avec uniquement des -1 et des 1.</p> <p>Les auteurs du papier ont remarqu\u00e9 que, en utilisant cette technique, on obtient des bons r\u00e9sultats sur des mod\u00e8les assez profonds (plus de 30B) mais les r\u00e9sultats sont assez moyens pour des mod\u00e8les plus petits.</p> <p>La m\u00e9thode BitNet1.58 a \u00e9t\u00e9 introduite pour am\u00e9liorer le mod\u00e8le pr\u00e9c\u00e9dent notamment pour le cas des mod\u00e8les plus petits. Dans cette m\u00e9thode, les auteurs proposent d'ajouter la valeur 0 en plus de -1 et 1. Cela ne parait pas \u00eatre un gros changement mais cette m\u00e9thode permet d'am\u00e9liorer grandement le mod\u00e8le BitNet original.</p> <p>Note : Le mod\u00e8le est surnomm\u00e9 1.58 bits car $log_2(3)=1.58$ donc th\u00e9oriquement, une repr\u00e9sentation de 3 valeurs utilise 1.58 bits.</p> <p>Mais alors pourquoi 0 est si utile ? En fait, il faut simplement revenir aux bases et regarder la multiplication matricielle. Une multiplication matricielle peut \u00eatre d\u00e9compos\u00e9e en deux op\u00e9rations : la multiplication des poids deux par deux et la somme de l'ensemble des ces poids. Avec -1 et 1, lors de la somme, on pouvait d\u00e9cider uniquement d'ajouter la valeur ou de la soustraire. Avec l'ajout du 0, on peut maintenant ignorer la valeur :</p> <ul> <li>1 : Je veux ajouter cette valeur</li> <li>0 : Je veux ignorer cette valeur</li> <li>-1 : Je veux soustraire cette valeur</li> </ul> <p>De cette mani\u00e8re, on peut filtrer efficacement les valeurs ce qui permet une bien meilleure repr\u00e9sentation.</p> <p>Pour r\u00e9aliser le quantization en 1.58b, on utilise la quantization absmean qui est une variante de absmax. Au lieu de se baser sur le maximum, on se base sur la moyenne en valeur absolue $\\alpha$ et on arrondit ensuite les valeurs \u00e0 -1, 0 ou 1 :</p> <p></p> <p>Et voil\u00e0, c'est simplement ces deux techniques (representation ternaire et absmean quantization) qui permettent d'am\u00e9liorer drastiquement la m\u00e9thode BitNet classique et de proposer des mod\u00e8les extr\u00e9mement quantiz\u00e9s et encore performants.</p> <p>Lorsque nous avons calcul\u00e9 la VRAM n\u00e9cessaire pour un mod\u00e8le, on a regard\u00e9 uniquement pour l'inf\u00e9rence. Si l'on souhaite entra\u00eener le mod\u00e8le, la VRAM n\u00e9cessaire est beaucoup plus importante et va d\u00e9pendre de l'optimizer que l'on utilise (voir cours sur les optimizers). On peut alors imaginer que les LLMs ont besoin d'une quantit\u00e9 \u00e9norme de m\u00e9moire pour \u00eatre entra\u00een\u00e9 ou fine-tune.</p> <p>Pour r\u00e9duire cette n\u00e9cessit\u00e9 en m\u00e9moire, des m\u00e9thodes de parameter efficient fine-tuning(PEFT) ont \u00e9t\u00e9 propos\u00e9es et permettent de ne r\u00e9entrainer qu'une partie du mod\u00e8le. En plus de permettre de fine-tuner les mod\u00e8les, cela a \u00e9galement pour effet d'\u00e9viter le catastrophic forgetting car on entra\u00eene uniquement une petite partie des param\u00e8tres totaux du mod\u00e8le.</p> <p>Il existe de nombreuses m\u00e9thodes pour le PEFT : LoRA, Adapter, Prefix Tuning, Prompt Tuning, QLoRA etc ...</p> <p>L'id\u00e9e avec les m\u00e9thodes type Adapter, LoRA et QLora est d'ajouter une couche entra\u00eenable permettant d'adapter la valeur des poids (sans avoir besoin de r\u00e9-entra\u00eener les couches de base du mod\u00e8le).</p> <p>La m\u00e9thode LoRA (low-rank adaptation of large language models) est une technique de fine-tuning permettant d'adapter un LLM \u00e0 une t\u00e2che ou un domaine sp\u00e9cifique. Cette m\u00e9thode introduit des matrices entra\u00eenables de d\u00e9composition en rang \u00e0 chaque couche du transformer ce qui r\u00e9duit les param\u00e8tres entra\u00eenables du mod\u00e8le car les couches de bases sont frozen. La m\u00e9thode peut potentiellement diminuer le nombre de param\u00e8tres entra\u00eenables d'un facteur 10 000 tout en r\u00e9duisant la VRAM n\u00e9cessaire pour l'entra\u00eenement d'un facteur allant jusqu'\u00e0 3. Les performances des mod\u00e8les fine-tune avec cette m\u00e9thode sont \u00e9quivalent ou mieux que les mod\u00e8les fine-tune de mani\u00e8re classique sur de nombreuses t\u00e2ches.</p> <p></p> <p>Au lieu de modifier la matrice $W$ d'une couche, la m\u00e9thode LoRA ajoute deux nouvelles matrices $A$ et $B$ dont le produit representent les modifications \u00e0 apporter \u00e0 la matrice $W$. $Y=W+AB$ Si $W$ est de taille $m \\times n$ alors $A$ est de taille $m \\times r$ et $B$ de taille $r \\times n$ o\u00f9 $r$ est le rang qui est bien plus petit que $m$ ou $n$ (ce qui explique la diminution du nombre de param\u00e8tres). Pendant l'entra\u00eenement, seulement $A$ et $B$ sont modifi\u00e9 ce qui permet au mod\u00e8le d'apprendre la t\u00e2che sp\u00e9cifique.</p> <p>QLoRA est une version am\u00e9lior\u00e9e de LoRA qui permet d'ajouter la quantization 4-bit pour les param\u00e8tres du mod\u00e8le pr\u00e9-entrain\u00e9. Comme nous l'avons vu pr\u00e9c\u00e9demment, la quantization permet de r\u00e9duire drastiquement la m\u00e9moire n\u00e9cessaire pour faire tourner le mod\u00e8le. En combinant LoRA et la quantization, on peut maintenant imaginer faire entra\u00eener un LLM sur un simple GPU grand public ce qui paraissait impossible il y encore quelques ann\u00e9es.</p> <p>Note : QLoRA quantize les poids en Normal Float 4 (NF4) qui est une m\u00e9thode de quantization sp\u00e9cifique aux mod\u00e8les de deep learning. Pour en savoir plus, vous pouvez consulter cette vid\u00e9o au temps indiqu\u00e9. Le NF4 est con\u00e7u sp\u00e9cifiquement pour repr\u00e9senter des distributions gaussiennes (et les r\u00e9seaux de neurones sont suppos\u00e9s avoir des poids suivants une distribution gaussienne).</p>"},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#quantization","title":"Quantization\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#comment-representer-les-nombres-sur-un-ordinateur","title":"Comment repr\u00e9senter les nombres sur un ordinateur ?\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#introduction-a-la-quantization","title":"Introduction \u00e0 la quantization\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#point-rapide-sur-les-precisions-communes","title":"Point rapide sur les pr\u00e9cisions communes\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#quantization-symetrique","title":"Quantization sym\u00e9trique\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#quantization-asymetrique","title":"Quantization asym\u00e9trique\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#clipping-et-modification-de-range","title":"Clipping et modification de range\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#calibration","title":"Calibration\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#post-training-quantization-ptq","title":"Post-Training Quantization (PTQ)\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#dynamic-quantization","title":"Dynamic quantization\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#static-quantization","title":"Static quantization\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#difference-entre-dynamic-et-static-quantization","title":"Diff\u00e9rence entre dynamic et static quantization\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#ptq-la-quantization-en-4-bit","title":"PTQ : la quantization en 4-bit\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#gptq","title":"GPTQ\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#gguf","title":"GGUF\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#quantization-aware-training-qat","title":"Quantization Aware Training (QAT)\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#bitnet-quantization-1-bit","title":"BitNet : quantization 1-bit\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#bitnet-158-on-a-besoin-du-zero","title":"BitNet 1.58 : On a besoin du z\u00e9ro !\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#fine-tuning-des-modele-de-langages","title":"Fine-Tuning des mod\u00e8le de langages\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#lora","title":"LoRA\u00b6","text":""},{"location":"Bonus_CoursSp%C3%A9cifiques/11_Quantization.html#qlora","title":"QLoRA\u00b6","text":""}]}