{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, nous présentons les *Normalizing Flows* qui sont des modèles génératifs de *representation learning*. Ils sont un peu moins connus que les VAE, GAN ou modèles de diffusion mais ils ont quand même de nombreux avantages non négligeables.  \n",
    "\n",
    "Les GAN et les VAE ne peuvent pas évaluer avec précision l'exactitude de la distribution de probabilité (un GAN ne le fait pas du tout et un VAE utilise la [ELBO](https://deepgenerativemodels.github.io/notes/vae/)) et cela cause des problèmes lors de l'entraînement : les VAE ont tendance à produire des images floues tandis que les GAN sont sujets au *mode collapse* lors de l'entraînement.\n",
    "\n",
    "Les *Normalizing Flows* propose une solution à ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment ça marche ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *Normalizing Flow* peut être décrit comme une série de transformations bijectives. Ces transformations sont utilisées pour modéliser des distributions complexes de données, telles que celles des images, en les transformant en une distribution simple, comme une distribution gaussienne centrée réduite.\n",
    "\n",
    "Les *Normalizing Flows* sont entraînés en maximisant la vraisemblance des données, ce qui est généralement formulé comme la minimisation du *negative log-likelihood* par rapport à la densité de probabilité réelle des données. En d'autres termes, on ajuste les paramètres des transformations pour que la distribution générée par le flow soit aussi proche que possible de la distribution cible des données.\n",
    "\n",
    "<img src=\"images/NFlow.png\" alt=\"NFlow\" width=\"800\"/>\n",
    "\n",
    "Figure extraire de [blogpost](https://towardsdatascience.com/introduction-to-normalizing-flows-d002af262a4b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avantages et désavantages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les principaux avantages de *Normalizing Flows* sont les suivants : \n",
    "- Leur entraînement est très stable\n",
    "- Ils convergent beaucoup plus simplement que les GAN ou les VAE (Yay)\n",
    "- Il n'y a pas besoin de générer un bruit pour pouvoir générer des données\n",
    "\n",
    "Cela semble assez génial mais il y a quand même quelques désavantages : \n",
    "- Empiriquement, on constate qu'ils sont moins expressifs que les GAN ou les VAE\n",
    "- Il y a une contrainte sur l'espace latent car on a besoin de fonctions bijectives et d'une préservation du volume. Cette contrainte fait que l'espace latent n'est pas vraiment utilisable car de grande dimension donc dur à interpreter.\n",
    "- Empiriquement, les éléments générés sont souvents moins bons que ceux des GAN ou des VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Il y a un aspect théorique important derrière les *Normalizing Flows* mais nous n'allons pas entrer dans les détails dans ce cours. Pour en apprendre plus, vous pouvez vous réferrer au cours CS236 de stanford et en particulier à ce [lien](https://deepgenerativemodels.github.io/notes/flow/)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
