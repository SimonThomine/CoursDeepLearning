{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation d'un GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps de passer à l'implémentation d'un GAN. Pour cela, nous allons nous baser sur le papier [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434) pour générer des images de chiffres 5 ressemblant à ceux du dataset MNIST.\n",
    "\n",
    "<img src=\"images/dcgan.png\" alt=\"dcgan\" width=\"800\"/>\n",
    "\n",
    "Architecture du generateur de DCGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par charger notre dataset MNIST :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du dataset d'entrainement :  5421\n",
      "taille d'une image :  (1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32,32)),\n",
    "])\n",
    "train_data = datasets.MNIST(root='./../data', train=True, transform=transform, download=True)\n",
    "test_data = datasets.MNIST(root='./../data', train=False, transform=transform, download=True)\n",
    "\n",
    "\n",
    "indices = [i for i, label in enumerate(train_data.targets) if label == 5]\n",
    "# On créer un nouveau dataset avec uniquement les 5\n",
    "train_data = torch.utils.data.Subset(train_data, indices)\n",
    "\n",
    "# all_indices = list(range(len(train_data)))\n",
    "# random.shuffle(all_indices)\n",
    "# selected_indices = all_indices[:5000]\n",
    "# train_data = Subset(train_data, selected_indices)\n",
    "\n",
    "\n",
    "print(\"taille du dataset d'entrainement : \",len(train_data))\n",
    "print(\"taille d'une image : \",train_data[0][0].numpy().shape) \n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACvCAYAAACVbcM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYoUlEQVR4nO3de2zV9f3H8U+BlraUItJSQLlUKIJSyiVcFaGKd5wbIsaJKGbOhalzS2ayxUwTE/8yxkuimXOLmuAFsqmI2mUTFISCVi7CCpQCchcZt0KpVqD745df9P36fO1poR++Pec8H4l/vE7P+Zwv8On3ez6ez/v7zmhqampyAAAAANDGOsR9AAAAAABSE4sNAAAAAEGw2AAAAAAQBIsNAAAAAEGw2AAAAAAQBIsNAAAAAEGw2AAAAAAQBIsNAAAAAEGw2AAAAAAQBIsNAAAAAEGw2GiBjz76yGVkZET+t3LlyrgPDymO+Ye4MQcRJ+Yf4sT8O3ud4j6AZPLggw+6MWPGmMcGDRoU09Eg3TD/EDfmIOLE/EOcmH9njsVGK0yaNMnNmDEj7sNAmmL+IW7MQcSJ+Yc4Mf/OHNuoWunYsWPu5MmTcR8G0hTzD3FjDiJOzD/Eifl3ZlhstMKcOXNcfn6+y87OduXl5a6qqiruQ0IaYf4hbsxBxIn5hzgx/84c26haICsry91yyy3uhhtucAUFBa66uto9+eSTbtKkSW7FihVu5MiRcR8iUhjzD3FjDiJOzD/Eifl39jKampqa4j6IZFRbW+uGDx/urrjiCldRURH34SDNMP8QN+Yg4sT8Q5yYf63DNqozNGjQIHfzzTe7JUuWuFOnTsV9OEgzzD/EjTmIODH/ECfmX+uw2DgLffv2dY2Nja6+vj7uQ0EaYv4hbsxBxIn5hzgx/1qOxcZZ2LZtm8vOznZ5eXlxHwrSEPMPcWMOIk7MP8SJ+ddyLDZa4MCBA95j69atcwsXLnTXXHON69CBv0aEw/xD3JiDiBPzD3Fi/p09CsRb4Morr3Q5OTlu4sSJrmfPnq66utq9+OKLLjMz01VWVrqhQ4fGfYhIYcw/xI05iDgx/xAn5t/ZY7HRAs8++6ybN2+eq62tdXV1da6wsNBdddVV7tFHH6VVPYJj/iFuzEHEifmHODH/zh6LDQAAAABBsNEMAAAAQBAsNgAAAAAEwWIDAAAAQBAsNgAAAAAEwWIDAAAAQBAsNgAAAAAE0amlT8zIyAh5HEhS5+rOycw/RDmXd+5mDiIK50DEifmHOLV0/vHNBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACKJT3AfQHmVkZJjcqVPr/pp69erlPdalSxeTs7OzTf7mm29M/uqrr0wePHhwwvdQe/fuNXnr1q0mHz58OOEYAAAAwJnimw0AAAAAQbDYAAAAABAEiw0AAAAAQSR9zYbWU3ToYNdPnTt3NjknJ8dkrZ2Iek5RUVGrjmnSpEneY8XFxSZ369bNZK2v+PDDD01+8MEHTZ48ebL3Hg0NDSa/+eabJr/wwgsmU7MBoL3S2rnMzEyTe/To4b0mNzfXZD2Xnz592uQDBw40m5G6dK7k5+ebrJ8tCgoK2vwYjhw5YrLWajrn13MCyYhvNgAAAAAEwWIDAAAAQBAsNgAAAAAEkVQ1G1qP4Zxz/fv3N7lr164ma63EyJEjTR40aJA3Zp8+fUyOqo84W7pXs7a21uSSkhKTx40bZ/KhQ4e8MWtqakxetWqVyVH7QYGW0P3zmp3z99Rr1t9f3T/f2NjojRn1GJJTx44dm826R17r6QYMGGDyrFmzvPcYPXq0yWVlZSZrndrTTz9t8jPPPOONieSj5x7n/PrN8ePHmzx16lSTe/bsafKcOXPa6Oi+t3DhQpMff/xx7zlVVVVt/r5IT4mu401NTd5roh47E3yzAQAAACAIFhsAAAAAgmCxAQAAACCIdl2zoXu8hw4d6j3n+eefN3nYsGEmZ2Vlmaz7gnXfsHPR+9HPlu57032Yr732msl6b239+ddff+29h9ZxbNu2zWT6aqCldM9zr169TNb9z845d/XVV5s8c+ZMk7W+aseOHSa/8cYb3ph//vOfEx8skoKevy+//HKThw8fbrLWW+jP9dzuXOK6EJ1zdXV1zRwxktW0adO8x+bOnWuy1mzoOS/E5wB1/fXXm3zixAnvObfffnvw40Bq0vOf9ibSGuf6+npvjLaq9eWbDQAAAABBsNgAAAAAEASLDQAAAABBsNgAAAAAEES7LhDXouoDBw54z9GmXzk5Oc3mM6HNxzZt2mTysWPHTL7ooou8MfLz802urq42+a233jI5USOV7777LuFx6t9NWzVnQfLTgu8xY8aYPGnSJJPLy8tNzsvL88bUOd69e3eTtaA3NzfX5IsvvriZI0actFi2sLDQ5BtvvNHkK6+80htDb96hTdP0XK3zQ+fPxo0bvffQJmlLliwxWQvEo64pSD6XXHKJybNnz/aeM3HiRJN1foWwdetWk7/44guT9UYva9euDX1IaKf0hkhdunQxWW+wcumll3pjaJPqgQMHmqzNorVg/MMPP/TGfOCBB37kiFuHbzYAAAAABMFiAwAAAEAQLDYAAAAABJFUNRtRTekWLFhg8sGDB03WvcZ9+/Y1ecKECd6YWutQW1tr8h//+EeT//vf/5pcXFzsjan7RbVm48iRI95rgDOhjaKc82sytD5CG64VFRU1m0+ePOm9x2effWay/t7ofmX9ue6nR/uh9RJz5swxecaMGSbr3mHn/PNkZWWlydu3bzdZG0ytWbPG5Khzps4hbUilzVKRGn7/+9+bfMUVV3jP0QaOer5KNFc2b95sstZuOud/Zjl69KjJ+hlG30PrP5E6CgoKTB4yZIjJo0ePNlmbmGr9hdZEOufXTWrTPq211Pl26tQpb8y2wjcbAAAAAIJgsQEAAAAgCBYbAAAAAIJo1zUbKqq3xPvvv29yTU2Nybq3XPeu632GnfP3wr399tsmL1682GTd96Z706MeY+8wzpTeH37y5Mkm33vvvd5rSktLTdY9+FrrpL9X2o/gxIkT3nusW7fO5D179pi8c+fOZt+TPjDth+791TqgadOmmdypk72ULFq0yBtzxYoVJut80Z4DWjun8wfJqVu3biYPGDDAe472GNC5o7Se4qmnnvKeo+ejffv2maz1FTr/tKZDX+8c57BUodfHfv36mZydnW2yfq7UfhbO+XWPffr0aTZr3yH9LKv1yM4519DQYPLu3btN3rZtm8n6uXT58uXemG2FbzYAAAAABMFiAwAAAEAQLDYAAAAABJFUNRtRdE/a/v37TT59+rTJmZmZJm/dutUbU+93/O2335oc1WPgh6LulR1VxwG0hO6Hv+yyy0y+7777TNb9pc75PQo2btxosv4eVFVVmaw1G1G/A3of+5D37Ebb0f3xzvnnwLlz55qstW7a7+iDDz7wxtTeQjpfkJrKyspMLi8vb/bnzvk1YYlqNubNm2fyoUOHvOdo3xakp44dO5ocVTN03XXXmax9qbRucuzYsSZH9RnSOl39TPjRRx+ZrH2J9LNtVM2G1gzp74HWbGgO2e+NbzYAAAAABMFiAwAAAEAQLDYAAAAABJH0NRsqqhfHD+m93Ddt2uQ9Z+TIkSbfcMMNJuv+0B07dpicqKYD6Uv3WXbt2tV7ju4hveCCC0weMWKEybp/tKKiwhtz/vz5Jm/YsMFk5mz60vnlnHPTp083WeuEtNdQZWWlybt27fLG1No3pCY9p/30pz81+Ve/+pXJeXl53hj//ve/W/WeUfMNcM65Xr16mTxw4ECTr732Wu81v/71r03WOa3nssOHD5usNZHOObd+/XqT33rrLZP/9a9/mayfZRN9tm3v+GYDAAAAQBAsNgAAAAAEwWIDAAAAQBAsNgAAAAAEkXIF4oloQbgWzjrnXElJiclTpkwxWQvGtVhSm7E45zewamhoSHisSD1aaDZhwgTvObNnzzZ5yJAhJv/pT38yeeXKlSZHFafpjRG02SXSR35+vsl6QwznnLvmmmtM1kal//nPf0wuLS01uXv37t6Yn332mcnbt283mSaQqWHcuHEmX3755SZrQ8jVq1d7Y+hNWICWysnJMVnPZXPmzDF51KhR3hjZ2dkm6w0ItEGpNujTa7JzfoH4vn37TE71G2jwzQYAAACAIFhsAAAAAAiCxQYAAACAINKuZkP3BeteO+ece+6550weO3asyQ8//LDJuicwasx3333X5M8//9xkbarW1NTkjYHkpw35tHmQc/4e5wULFpi8YsUKk48cOWIycye9dehg/x9SVlaWyXq+euihh7wxCgoKTNY9yNrI9Pbbbzf5xIkT3pidOtnLjda26TxGcrr//vtNnjp1qsl79+41eenSpd4YH3zwgcmdO3c2Wa/jNCXF/+vXr5/J119/vcmTJ082WRvtOufXU7zwwgsmv/TSSyYfPXrUZK7BPr7ZAAAAABAEiw0AAAAAQbDYAAAAABBE2tVsqKi9xVpP8cgjj5j8+OOPm3z11VebrHtUnXNu8ODBJv/tb38zubKy0mT2L6emsrIyk7Vni3PO7d692+T333/f5OPHj5vM/lD80EUXXWTyrFmzTL7ttttMjtrv/uijj5pcUVFh8p49e0zWe9vfc8893pjFxcU/csRIJ7179zb5jjvu8J6j++6rqqqazdrDpb6+/mwOEUlMr49aG6Y9g7TvkHN+LxjtzaE1HYsWLTJZazjANxsAAAAAAmGxAQAAACAIFhsAAAAAgkj7mo0ohw8fNvkf//iHyfv37zf5F7/4hcnaJ8E5/972uk+wsLDQZN0DeOjQoWaOGMlCa3F076dzzvXs2dNk7YNQV1dn8qpVq0yOqkNC+hg4cKDJ48ePN1n3E7/88sveGPPnzzdZ9zl/9913Jjc2NpqsvT5+7DGkHq3v0Rqi0tJSk4uKirwxEvVG0JqMV199tdnsnHNffvll9AEjpejnM+2R8fXXX5usPYKcc66kpMRkrTe7++67TdaaW2o2fJz9AQAAAATBYgMAAABAECw2AAAAAARBzUaE06dPm3zw4EGTlyxZ0uzPdf+ec87deuutJus+6tzcXJPPP/98k59++ukfP2DEpn///iZrvY/udV+2bJnJTzzxhDfmT37yE5O1Buixxx4z+Q9/+IPJq1ev9sb85ptvvMeQmtatW2dyonPH2rVrvcdaWyM2YsQIk/X8hfTx7rvvmlxdXW2yXvtGjRrljXHeeeeZfO211zb7ntoHYdeuXd5zFi5caLJet5EatG/Qli1bTH7llVdM1vOlc/58u++++0y+8MILTc7MzGz1caYbvtkAAAAAEASLDQAAAABBsNgAAAAAEASLDQAAAABBUCB+BrTo99NPPzU5qjjt4osvNlkLKkePHm2yNsnS4raoBkVa2I7W0aZjWuT685//3HvNkCFDTP7444+bzTo33nnnHW9M/Xe87rrrTJ44caLJ/fr1M1kLMp2jQDydfPXVVybrDS3Ut99+m3DMTp3spUKLfMvKykzesWOHN4YWajY0NCR8XySfPXv2NJtrampMXrp0qTdG9+7dTdbrnTb508aB9957rzemNnN77733vOcg9WgD0p07d5ocdR7q3bu3yXpN1jmt7wEf32wAAAAACILFBgAAAIAgWGwAAAAACIKajRbIyMgwWffya2O3oqIib4xETa60XiArK6s1h4g20KNHD5N/+ctfmnzXXXd5r9EajBUrVpisez1PnTplctReT51vTU1NJuue/KNHj5qsTY3QfnXp0sVknYO6v/hMtKQmQ+Xk5JisNRnaRE2P+/XXX/fG/Pzzz8/6uJD89Pyl2Tn/+qfnWa0h0mvu4MGDvTF79erVquPEuafnQz3vRDUb3b59u8mtPa9069bNe+ySSy4xWa/jWn/GuSwxvtkAAAAAEASLDQAAAABBsNgAAAAAEETa12x07NjRe0z38A0YMMDkUaNGmax9D0pKSrwxtc+G7ss/ceKEyXpfcd2XqPv4cfb03/3mm282+YILLvBes379epN1z6nODa3R6NOnjzfm9OnTTa6rqzN50aJFJut96+lfkDz03KL/9tojI6p/he55T3TPd62vGDRokPcc7R9z1VVXmVxaWmryqlWrTI7qYRDVGwiIorVtR44caTbr86PqQPQ1aH8KCwtN/u1vf2tybW2t95q//vWvJut5RmsY9TNfcXGxN+bUqVNN1vml11z6WCXGNxsAAAAAgmCxAQAAACAIFhsAAAAAgkj5mg3tX5Gbm2ty1L23x40bZ7Luo77ssstMLigoMDmqDkT3UR84cMBk3YtYXV1tMjUa4ek9vF955RWT77//fu81U6ZMMXnatGkm6/3gjx07ZnJ9fb03pr5m+fLlJr/00ksm796922TdX4r2a+DAgSY//PDDJmvd0IIFC7wxKioqTNb9w5mZmSaPGTPG5FmzZnljjh492mStFVm8eLHJum9a70PvHPeiT0ZaW6h76p3zr6l6jtOaM70WRl0ve/fubfJNN91k8oQJE0zWWrmoPfSJapkQv7y8PJO1dmL8+PHea/bv32+y9ro6fvy4yV27dk04Zr9+/Uzet2+fyZWVlSZHXcdh8c0GAAAAgCBYbAAAAAAIgsUGAAAAgCCSvmZD95RqjYbeU173Ims9hnP+vvuouo4fOn36tMlR+0W1T4beh/711183efXq1c2+J9qe1mz85S9/MVnv1+2cc8OGDTN57NixJms9jzp69Kj32LJly0x+5plnTN65c2fC40Jy0P46OgdHjBhhclSvF+2BUVRUZHLfvn1NzsnJMVnrx5xz7u9//7vJjzzyiMnax4CastSg11PtPTR37lzvNbrn/Z///Gezee/evSaff/753pg33nijyc8+++yPHPH/0To1rXmMel+0P1rfo3NH54Vzzj355JMmaw2H1urk5+ebrPU+zvk1Gi+++KLJa9asMZl6tMT4ZgMAAABAECw2AAAAAATBYgMAAABAEBlNLdxsq3s524vu3bub3L9/f5PvvPNOk3/2s5+ZHLUHWu9Ln+jPrvtD58+f7z3nnXfeMbmmpsZk3fOXLL0SztVe7fYw/7KysrzH9B7x2iMj0XFH/f1pDYbODa0RSmfnslYgxBzUeoqZM2eafMcdd5g8fPjwhGN++umnJuu5Zt26dSYvXbrUG2Pjxo0mNzQ0mEyNxvdS6RzYs2dPk7VG46GHHvJeo3vedY+8Zv37ivpz6TVY64zU2rVrTf7d737nPUfnebJcYxNJpfnXuXNnk4cOHWryb37zG+81l156qcklJSUm6/Xziy++MFnPh845t3nzZpMXLlxosta5pfP5sKV/dr7ZAAAAABAEiw0AAAAAQbDYAAAAABAEiw0AAAAAQbSrAnFtyKcN0R544AHvNaWlpSYXFxebrAXg2qRIC3qd85vyrV+/3mRtePXxxx+b/OWXX3pjarOuxsZG7znJKJWK05B8kr1AXAth9fykDc+iCmX1uI4fP26yFkhqI0F9ftRr8ONS6RyoxbaLFy82ubCw0HvNuTguvV7OmzfPZG26ptds55yrr69v+wNrB1Jp/ul76E1ZouZfXl6eyb179zZZb1Cgxd1RjXX1/KfP4SYt36NAHAAAAECsWGwAAAAACILFBgAAAIAgzmnNxnnnnWdyWVmZyVOmTDF5yJAhJk+cONEbU5v65ebmmqxN13Sv3e7du70xKyoqTH7jjTdM3rJli8m6BzCd9jun0n5RJJ9kr9lA8kulc6A2xX3qqadMvummm7zXaN3RJ598YnKi5nlVVVXeY4nG2Lp1a7OZa3Dba6/nPz0ubQyon/kSNZlE61CzAQAAACBWLDYAAAAABMFiAwAAAEAQ57RmY9iwYSbPnj3b5JkzZ5rco0cPk/V+ys75/St076beH3nHjh0mL1++3BtzzZo1Jm/YsMHkkydPeq9JV+m+XxTxomYDcUulc6DWPI4cObLZ7Jzfq2rz5s0mJ+pJsH37du+xmpqaZl+D76XS/EPyoWYDAAAAQKxYbAAAAAAIgsUGAAAAgCDOac3GgAEDTC4vLzdZ+260xMGDB02ura01ua6uzuRdu3aZvGnTJm/MxsbGVh9HumK/KOJEzQbixjkQcWL+IU7UbAAAAACIFYsNAAAAAEGw2AAAAAAQxDmt2UDqYb8o4kTNBuLGORBxYv4hTtRsAAAAAIgViw0AAAAAQbDYAAAAABAEiw0AAAAAQbDYAAAAABAEiw0AAAAAQbDYAAAAABAEiw0AAAAAQbDYAAAAABAEiw0AAAAAQbDYAAAAABAEiw0AAAAAQWQ0NTU1xX0QAAAAAFIP32wAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACILFBgAAAIAgWGwAAAAACOJ/IJIBG7b7UugAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisons quelques images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "  plt.subplot(1, 5, i+1)\n",
    "  plt.imshow(train_data[i][0].squeeze(), cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.title(train_data[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de notre modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant implémenter nos deux modèles. Commençons par regarder les spécificités d'architecture décrites dans la papier. \n",
    "\n",
    "<img src=\"images/dcgan_arch.png\" alt=\"dcgan_arch\" width=\"800\"/>\n",
    "\n",
    "A partir de ces informations et de la figure du papier (voir plus haut dans le notebook), on peut construire notre modèle générateur. Comme on travaille sur des images de taille $28 \\times 28$ au lieu de $64 \\times 64$ dans le papier, on va faire une architecture plus réduite.\n",
    "\n",
    "**Note** : Dans le papier, les auteurs disent utiliser des *fractional-strided convolutions*. Il s'agit en fait de convolutions transposées et le terme *fractional-strided convolutions* n'est plus vraiment utilisé de nos jours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "def convT_bn_relu(in_channels, out_channels, kernel_size, stride, padding):\n",
    "  return nn.Sequential(\n",
    "    nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding,bias=False),\n",
    "    nn.BatchNorm2d(out_channels),\n",
    "    nn.ReLU()\n",
    "  )\n",
    "\n",
    "class generator(nn.Module):\n",
    "  def __init__(self, z_dim=100,features_g=64):\n",
    "    super(generator, self).__init__()\n",
    "    self.gen = nn.Sequential(\n",
    "      convT_bn_relu(z_dim, features_g*8, kernel_size=4, stride=1, padding=0),\n",
    "      convT_bn_relu(features_g*8, features_g*4, kernel_size=4, stride=2, padding=1),\n",
    "      convT_bn_relu(features_g*4, features_g*2, kernel_size=4, stride=2, padding=1),\n",
    "      nn.ConvTranspose2d(features_g*2, 1, kernel_size=4, stride=2, padding=1),\n",
    "      nn.Tanh()\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.gen(x)\n",
    "  \n",
    "z= torch.randn(64,100,1,1)\n",
    "gen = generator()\n",
    "img = gen(z)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le papier ne décrit pas directement l'architecture du discriminateur. Nous allons globalement reprendre l'architecture du générateur mais dans l'autre sens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "def conv_bn_lrelu(in_channels, out_channels, kernel_size, stride, padding):\n",
    "  return nn.Sequential(\n",
    "    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,bias=False),\n",
    "    nn.BatchNorm2d(out_channels),\n",
    "    nn.LeakyReLU()\n",
    "  )\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "  def __init__(self, features_d=64) -> None:\n",
    "    super().__init__()\n",
    "    self.discr = nn.Sequential(\n",
    "      conv_bn_lrelu(1, features_d, kernel_size=3, stride=2, padding=1),\n",
    "      conv_bn_lrelu(features_d, features_d*2, kernel_size=3, stride=2, padding=1),\n",
    "      conv_bn_lrelu(features_d*2, features_d*4, kernel_size=3, stride=2, padding=1),\n",
    "      nn.Conv2d(256, 1, kernel_size=3, stride=2, padding=0),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.discr(x)\n",
    "dummy = torch.randn(64,1,32,32)\n",
    "disc = discriminator()\n",
    "out = disc(dummy)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps de passer aux choses sérieuses. La boucle d'entraînement d'un GAN est bien plus complexe que les boucles d'entraînements des modèles que nous avons vu jusqu'à présent.   \n",
    "Commençons par définir nos hyperparamètres d'entraînement et par initialiser nos modèles :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr=0.001\n",
    "z_dim = 100\n",
    "features_d = 64\n",
    "features_g = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gen = generator(z_dim, features_g).to(device)\n",
    "disc = discriminator(features_d).to(device)\n",
    "\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "opt_disc = torch.optim.Adam(disc.parameters(), lr=lr*0.05)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons également créer un bruit *fixed_noise* pour évaluer visuellement notre modèle à chaque étape d'entraînement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de consttuire notre boucle d'entraînement, récapitulons les étapes que nous devons faire : \n",
    "- On commence par récuperer *batch_size* éléments du dataset d'entraînement et on prédit les labels avec notre discriminateur\n",
    "- Ensuite, on génére *batch_size* éléments avec notre générateur et on prédit les labels\n",
    "- On va ensuite mettre à jour les poids du modèle discriminateur à partir des deux *loss*\n",
    "- On va ensuite à nouveau prédire les labels des données générées (car on a mis à jour le discriminateur entre temps)\n",
    "- A partir de ces valeurs, on calcule le *loss* et on met à jour notre générateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose2d(\n"
     ]
    }
   ],
   "source": [
    "all_fake_images = []\n",
    "for epoch in range(epochs):\n",
    "  lossD_epoch = 0\n",
    "  lossG_epoch = 0\n",
    "  for real_images,_ in train_loader:\n",
    "    real_images=real_images.to(device)\n",
    "    pred_real = disc(real_images).view(-1)\n",
    "    lossD_real = criterion(pred_real, torch.ones_like(pred_real)) # Les labels sont 1 pour les vraies images\n",
    "    \n",
    "    batch_size = real_images.shape[0]\n",
    "    input_noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "    fake_images = gen(input_noise)\n",
    "    pred_fake = disc(fake_images.detach()).view(-1)\n",
    "    lossD_fake = criterion(pred_fake, torch.zeros_like(pred_fake)) # Les labels sont 0 pour les fausses images\n",
    "\n",
    "    lossD=lossD_real + lossD_fake\n",
    "    lossD_epoch += lossD.item()\n",
    "    disc.zero_grad()\n",
    "    lossD.backward()\n",
    "    opt_disc.step()\n",
    "    \n",
    "    # On refait l'inférence pour les images générées (avec le discriminateur mis à jour)\n",
    "    pred_fake = disc(fake_images).view(-1)\n",
    "    lossG=criterion(pred_fake, torch.ones_like(pred_fake)) # On veut que le générateur trompe le discriminateur donc on veut que les labels soient 1\n",
    "    lossG_epoch += lossG.item()\n",
    "    gen.zero_grad()\n",
    "    lossG.backward()\n",
    "    opt_gen.step()\n",
    "    \n",
    "  # On génère des images avec le générateur\n",
    "  if epoch % 10 == 0 or epoch==0:\n",
    "    print(f\"Epoch [{epoch}/{epochs}] Loss D: {lossD_epoch/len(train_loader):.4f}, loss G: {lossG_epoch/len(train_loader):.4f}\")\n",
    "    gen.eval()\n",
    "    fake_images = gen(fixed_noise)\n",
    "    all_fake_images.append(fake_images)\n",
    "    #cv2.imwrite(f\"gen/image_base_gan_{epoch}.png\", fake_images[0].squeeze().detach().cpu().numpy()*255.0)\n",
    "    gen.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizons les images générées lors de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des images générées\n",
    "images_begin = all_fake_images[0]\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "  plt.subplot(1, 5, i+1)\n",
    "  plt.imshow(images_begin[i].squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.title(\"Début de l'entrainement\")\n",
    "  \n",
    "images_mid = all_fake_images[len(all_fake_images)//2]\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "  plt.subplot(1, 5, i+1)\n",
    "  plt.imshow(images_mid[i].squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.title(\"Milieu de l'entrainement\")\n",
    "  \n",
    "images_end = all_fake_images[-1]\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "  plt.subplot(1, 5, i+1)\n",
    "  plt.imshow(images_end[i].squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.title(\"Fin de l'entrainement\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
