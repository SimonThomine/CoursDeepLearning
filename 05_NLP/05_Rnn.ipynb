{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de neurones récurrents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, nous allons introduire les réseaux de neurones récurrents (RNN) dans le cadre de la prédiction du prochain caractère. Pour ce faire, nous allons nous baser sur l'architecture décrite dans le papier [Recurrent neural network based language model](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) qui présente une version basique de RNN pour la prédiction du prochain caractère.  \n",
    "\n",
    "La motivation derrière l'utilisation d'un RNN pour cette tâche est de ne pas avoir à spécifier une taille de contexte pour l'entraînement du modèle contrairement aux deux modèles basées sur des réseaux fully connected que nous avons vu dans les notebooks précédents. \n",
    "\n",
    "Les RNN ont pour motivation de garder une information de contexte peu importe la longueur de la séquence. C'est une idée très intéressante sur le papier mais nous verrons, à la fin du cours, qu'il y a de grosses limitations.  \n",
    "\n",
    "<img src=\"images/rnn.png\" alt=\"rnn\" width=\"250\"/>    \n",
    "\n",
    "Figure extraite de l'article original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctionnement de l'architecture RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture de réseau de neurones récurrents se base sur une approche séquentielle. Les caractères vont être passés un par un dans le modèle et la valeur du caractère suivant dépend du *state* gardé en mémoire et de l'élément actuel. Le *state* contient les informations de contexte de tous les caractères précédents.  \n",
    "\n",
    "Posons le problème mathématiquement :    \n",
    "Un RNN est constitué de 3 éléments : l'*input* $x$, le *state* (hidden layer) $s$ et l'*output* $y$. On introduit également le temps $t$ qui rajoute la composante temporelle pour le traitement séquentiel.   \n",
    "$x$ au temps $t$ est alors défini comme :    \n",
    "$x(t)=w(t) + s(t-1)$ où $w()$ est l'opération de *one_hot encoding* et $s(t-1)$ est le *state* au temps $t-1$.   \n",
    "Et ensuite, on estime $s(t)$ et $y(t)$ :    \n",
    "$s(t)=sigmoid(x(t))$   \n",
    "$y(t)=softmax(s(t))$    \n",
    "\n",
    "On peut constater que ce modèle n'a en fait qu'un seul paramètre à ajuster : la dimension de la couche cachée $s$. \n",
    "\n",
    "Pour l'initialisation $s(0)$ peut être initialisée en un vecteur de petite valeurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser un RNN pour générer des prénoms n'est pas très intéressant car les prénoms ne sont jamais très longs et la taille de contexte est donc limitée. Pour ce type de tâches, il est intéressant d'utiliser un dataset avec un contexte conséquent.   \n",
    "Pour cela, nous utilisons un fichier texte moliere.txt qui regroupe l'intégralité des dialogues des pièces de Molière.   \n",
    "Ce dataset a été crée à partir des oeuvres complètes de Molière disponibles sur le site [Gutenberg.org](https://www.gutenberg.org/). J'ai nettoyé un peu les données afin de ne garder que les dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est un gros dataset donc pour avoir un temps de traitement raisonnables nous prenons uniquement une partie de ce dataset (par exemple les 50 000 premiers caractères)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  50000\n"
     ]
    }
   ],
   "source": [
    "text=text[:50000]\n",
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons les 250 premiers caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALÈRE.\n",
      "\n",
      "Eh bien, Sabine, quel conseil me donnes-tu?\n",
      "\n",
      "SABINE.\n",
      "\n",
      "Vraiment, il y a bien des nouvelles. Mon oncle veut résolûment que ma\n",
      "cousine épouse Villebrequin, et les affaires sont tellement avancées,\n",
      "que je crois qu'ils eussent été mariés dès aujo\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons le nombre de caractères différents : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJLMNOPQRSTUVYabcdefghijlmnopqrstuvxyzÇÈÉàâæçèéêîïôùû\n",
      "Nombre de caractères différents :  73\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un mapping de caractère à entiers et inversement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encode : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodons notre dataset en convertissant les *string* en *int* puis en le transformant en tenseur pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 12, 22, 59, 28, 16,  8,  0,  0, 16, 41,  1, 35, 42, 38, 46,  6,  1,\n",
      "        29, 34, 35, 42, 46, 38,  6,  1, 49, 53, 38, 44,  1, 36, 47, 46, 51, 38,\n",
      "        42, 44,  1, 45, 38,  1, 37, 47, 46, 46, 38, 51,  7, 52, 53, 11,  0,  0,\n",
      "        29, 12, 13, 20, 24, 16,  8,  0,  0, 32, 50, 34, 42, 45, 38, 46, 52,  6,\n",
      "         1, 42, 44,  1, 56,  1, 34,  1, 35, 42, 38, 46,  1, 37, 38, 51,  1, 46,\n",
      "        47, 53, 54, 38, 44, 44, 38, 51,  8,  1, 23, 47, 46,  1, 47, 46, 36, 44,\n",
      "        38,  1, 54, 38, 53, 52,  1, 50, 66, 51, 47, 44, 72, 45, 38, 46, 52,  1,\n",
      "        49, 53, 38,  1, 45, 34,  0, 36, 47, 53, 51, 42, 46, 38,  1, 66, 48, 47,\n",
      "        53, 51, 38,  1, 32, 42, 44, 44, 38, 35, 50, 38, 49, 53, 42, 46,  6,  1,\n",
      "        38, 52,  1, 44, 38, 51,  1, 34, 39, 39, 34, 42, 50, 38, 51,  1, 51, 47,\n",
      "        46, 52,  1, 52, 38, 44, 44, 38, 45, 38, 46, 52,  1, 34, 54, 34, 46, 36,\n",
      "        66, 38, 51,  6,  0, 49, 53, 38,  1, 43, 38,  1, 36, 50, 47, 42, 51,  1,\n",
      "        49, 53,  3, 42, 44, 51,  1, 38, 53, 51, 51, 38, 46, 52,  1, 66, 52, 66,\n",
      "         1, 45, 34, 50, 42, 66, 51,  1, 37, 65, 51,  1, 34, 53, 43, 47])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:250]) # Les 250 premiers caractères encodé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sépare training et test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour le test\n",
    "train_data = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Chaque itération de l'entraînement correspondra à un passage dans l'intégralité du dataset de manière séquentielle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant temps de créer notre modèle !  \n",
    "\n",
    "Dans l'article, il est indiqué que l'entrée du modèle (le caractère) est encodé en *one hot* et qu'il est ensuite sommé avec le *state* à $t-1$. On va donc avoir besoin de deux couches fully connected, la première pour transformer l'entrée $x(t)$ en *state* au temps $t$, $s(t)$ et la seconde pour transformer $s(t)$ en $y(t)$, notre prédiction. \n",
    "\n",
    "<img src=\"images/rnn_math.png\" alt=\"rnn_math\" width=\"250\"/>    \n",
    "\n",
    "Equation extraite de l'article original. $f$ est la fonction *sigmoid* et $g$ la *softmax*.\n",
    "\n",
    "**Note** : L'[article](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) est très accessible et concis, je vous invite à le lire. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(nn.Module): \n",
    "  def __init__(self,hidden_dim,vocab_size) -> None:\n",
    "    super(rnn, self).__init__()\n",
    "    self.hidden_to_hidden=nn.Linear(hidden_dim+vocab_size, hidden_dim)\n",
    "    self.hidden_to_output=nn.Linear(hidden_dim, vocab_size)\n",
    "    self.vocab_size=vocab_size\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.sigmoid=nn.Sigmoid() \n",
    "    \n",
    "  # Le réseau prend en entrée le caractère actuel et le state précédent\n",
    "  def forward(self, x,state):\n",
    "    # On one-hot encode le caractère\n",
    "    x = torch.nn.functional.one_hot(x, self.vocab_size).float()\n",
    "    if state is None:\n",
    "      # Si on a pas de state (début de la séquence), on initialise le state avec des petites valeurs aléatoires\n",
    "      state = torch.randn(self.hidden_dim) * 0.1\n",
    "    x = torch.cat((x, state), dim=-1)  # Concaténation de x et du state\n",
    "    state = self.sigmoid(self.hidden_to_hidden(x)) # Calcul du nouveau state\n",
    "    output = self.hidden_to_output(state) # Calcul de l'output\n",
    "    # On renvoie l'output et le state pour le prochain pas de temps\n",
    "    return output, state.detach() # detach() pour éviter de propager le gradient dans le state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons nos paramètres d'entraînement : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr=0.1\n",
    "hidden_dim=128\n",
    "model=rnn(hidden_dim,vocab_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps d'entraîner le modèle !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 2.63949568\n",
      "Epoch: 1 \t Loss: 2.16456994\n",
      "Epoch: 2 \t Loss: 2.00850788\n",
      "Epoch: 3 \t Loss: 1.91673251\n",
      "Epoch: 4 \t Loss: 1.84440742\n",
      "Epoch: 5 \t Loss: 1.78986003\n",
      "Epoch: 6 \t Loss: 1.74923073\n",
      "Epoch: 7 \t Loss: 1.71709289\n",
      "Epoch: 8 \t Loss: 1.68791167\n",
      "Epoch: 9 \t Loss: 1.66215199\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    state=None\n",
    "    running_loss = 0\n",
    "    n=0\n",
    "    for i in range(len(train_data)-1):\n",
    "        x = train_data[i]\n",
    "        y = train_data[i+1]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons maintenant le dataset sur nos données de test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.77312289\n"
     ]
    }
   ],
   "source": [
    "state=None\n",
    "running_loss = 0\n",
    "n=0\n",
    "for i in range(len(train_data)-1):\n",
    "    with torch.no_grad():\n",
    "        x = train_data[i]\n",
    "        y = train_data[i+1]\n",
    "        y_pred,state = model.forward(x,state)\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "        n+=1\n",
    "print(\"Loss: {0:.8f}\".format(running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *loss* sur nos données de test est légerement plus élevé que sur notre dataset d'entraînement. Le modèle a légérement *overfit*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que le modèle est entraîné, on va pouvoir générer du Molière !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "VARDILE.\n",
      "\n",
      "Vout on est nt, jes l'un ouint; sabhil.\n",
      "\n",
      "LE DOCTE.\n",
      "\n",
      "Si vous dicefalassîntes\n",
      "GIRGIB.\n",
      "\n",
      "MARGRIILÉ.\n",
      "\n",
      "LE DOCTE. Jort; et\n",
      "; bieu,\n",
      "et je mu tu d'ais d'ai coupce!\n",
      "\n",
      "SGÉLLÉ.\n",
      "\n",
      "Il Sgnous elli massit que\n",
      "Suis pluagil dés.\n",
      "Cais téscompas: y totte demes\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "moliere='.'\n",
    "sequence_length=250\n",
    "state=None\n",
    "for i in range(sequence_length):\n",
    "    x = torch.tensor(encode(moliere[-1]), dtype=torch.long).squeeze()\n",
    "    y_pred,state = model.forward(x,state)\n",
    "    probs=F.softmax(torch.squeeze(y_pred), dim=0)\n",
    "    sample=torch.multinomial(probs, 1)\n",
    "    moliere+=itos[sample.item()]\n",
    "print(moliere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce n'est pas très convaincant ... Mais on reconnaît quand même quelques mots et un agencement des phrases similaire au fichier \"moliere.txt\". Ce n'est finalement pas si mal pour un réseau récurrent d'une seule couche. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment améliorer nos résultats ?** : Pour améliorer les résultats, il y a plusieurs options possibles :\n",
    "- On peut augmenter le nombre de couche récurrente ou augmenter la dimension de la couche cachée.\n",
    "- On peut utiliser un *embedding* plutôt qu'un *one hot encoding*.\n",
    "- On peut utiliser d'autres variantes de RNN comme [LSTM](https://arxiv.org/pdf/1308.0850) ou [GRU](https://arxiv.org/abs/1409.1259).\n",
    "- ~~On peut utiliser une architecture *transformer*~~ (oups spoiler)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le problème des RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendant longtemps, les RNN étaient au centre de la recherche en NLP et également utilisés dans d'autres domaines du deep learning. Cependant, il y certains problèmes qui font que les RNN sont difficilement utilisables en pratique et pour des gros modèles : \n",
    "- Leur architecture permet d'avoir un contexte théoriquement infini mais leur structure séquentielle où chaque état dépend du précédent rend difficile la propagation de l'information sur des longues séquences. \n",
    "- Le problème de *vanishing gradient* sur des séquences longues ne rend pas la chose facile également. Plus la séquence est longue, plus le gradient peut avoir tendance à se dissiper.\n",
    "- L'archicture séquentielle rend la parallélisation compliquée et peu efficace alors que les GPU sont justement bons pour les calculs en parallèles. L'entraînement est donc beaucoup plus long que pour un modèle qu'on peut paralleliser efficacement.\n",
    "- La structure séquentielle fixe n'est pas forcément adaptée pour capturer les relations complexes entre les données.   \n",
    "\n",
    "Aujourd'hui et depuis l'arrivée des [transformers](https://arxiv.org/pdf/1706.03762), les RNN sont de moins en moins utilisés dans l'ensemble des domaines du deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
