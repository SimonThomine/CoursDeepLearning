{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP, Qu'est ce que c'est ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le NLP ou natural language processing (traitement du langage naturel) est un problème important dans le domaine du machine learning. Ce domaine regroupe plusieurs tâches liées au texte comme la traduction, la compréhension de texte, les questions/réponses et bien d'autres.   \n",
    "\n",
    "<img src=\"images/applicationsNLP.png\" alt=\"nlp\" width=\"600\"/>\n",
    "\n",
    "Figure extraite du [blogpost](https://eastgate-software.com/top-8-applications-of-natural-language-processing-nlp/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est une tâche un peu à part dans le domaine du deep learning car il s'agit de traiter des données discrètes qui se lisent en général de gauche à droite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qu'est ce que ce cours aborde ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce cours va aborder principalement l'aspect \"prédiction du prochain token\" en prenant l'exemple de la prédiction du prochain caractère dans un premier temps pour plus de simplicité.  \n",
    "Ce problème est le fondement des modèles de langages (type GPT, Llama, gemini etc ...).   \n",
    "L'idée est de prédire le mot qui va suivre en fonction des mots précédents (avec plus ou moins de contexte en fonction de la méthode et de la puissance du modèle).   \n",
    "Le contexte est défini par le nombre de token (ou mots) qui vont être utilisés pour prédire le prochain mot.   \n",
    "\n",
    "**Un token, qu'est ce que c'est ?** : Un token correspond à un élément d'entrée du modèle, il peut s'agir d'un caractère, d'un groupe de caractères ou d'un mot qui est converti en vecteur avant d'être donné en entrée au modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspiration pour le cours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce cours s'inspire fortement de la série de [vidéos de Andrej Karpathy](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) ([lien du repository github](https://github.com/karpathy/nn-zero-to-hero)) et en particulier des cours \"building makemore\". Ce cours va consister à apporter une version écrite et en français des enseignements de cette série de vidéos. Je vous invite à regarder cette série de vidéos en plus, il s'agit d'un des meilleurs cours sur les modèles de langages à ce jour (et c'est gratuit). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous subdiviserons ce cours en plusieurs notebooks avec des modèles de difficulté croissante. L'intêret est de voir les limitations de chaque modèle avant de passer à un modèle plus complexe.  \n",
    "Voici un plan du cours :\n",
    "- Cours 1 : Bigramme : Méthode classique et réseau de neurones \n",
    "- Cours 2 : Prédiction du prochain mot avec Réseau Fully Connected\n",
    "- Cours 3 : WaveNet : architecture hierarchique\n",
    "- Cours 4 : RNN : réseau de neurones récurrents avec architecture séquentielle\n",
    "- Cours 5 : LSTM : réseau récurrent \"amélioré\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes** : Le [cours 7](https://github.com/SimonThomine/CoursDeepLearning/tree/main/07_Transformers) sur les transformers s'attaque au même problème de génération du prochain caractère mais avec une architecture transformer et sur un dataset plus complexe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récuperation du dataset prenom.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, nous travaillons sur un dataset de prénoms qui contient environ 30 000 prénoms les plus courant en France depuis 1900 (données de l'INSEE).  Le fichier prenoms.txt existe déjà dans le dossier, il n'est pas donc pas nécessaire d'executer le code ci-dessous. Si vous souhaitez le faire il faut d'abord télécharger le fichier 'nat2022.csv' sur le [site de l'INSEE](https://www.insee.fr/fr/statistiques/7633685?sommaire=7635552)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chargement du fichier CSV\n",
    "df = pd.read_csv('nat2022.csv', sep=';')\n",
    "\n",
    "# On enlève la catégorie '_PRENOMS_RARES' qui regroupe les prénoms peu fréquents\n",
    "df_filtered = df[df['preusuel'] != '_PRENOMS_RARES']\n",
    "\n",
    "# Pour compter, on fait la somme des nombres de naissances pour chaque prénom\n",
    "df_grouped = df_filtered.groupby('preusuel', as_index=False)['nombre'].sum()\n",
    "\n",
    "# On va trier les prénoms par popularité\n",
    "df_sorted = df_grouped.sort_values(by='nombre', ascending=False)\n",
    "\n",
    "# On extrait les 30 000 prénoms les plus populaires\n",
    "top_prenoms = df_sorted['preusuel'].head(30000).values\n",
    "\n",
    "with open('prenoms.txt', 'w', encoding='utf-8') as file:\n",
    "  for prenom in top_prenoms:\n",
    "    file.write(f\"{prenom}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le notebook suivant, nous commencerons par analyser le dataset (caractères distincts etc ...)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
