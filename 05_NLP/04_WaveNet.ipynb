{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch et WaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, nous allons nous nous inspire de l'architecture du modèle [wavenet](https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/)  proposé par google deepmind pour le traitement de l'audio.  \n",
    "Notre but dans ce cours est prendre plus de caractères pour le contexte de notre prédicteur du prochain mot.  \n",
    "Le cours commence par la transformation du cours précédent avec les fonctions de pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture d'un WaveNet est une architecture hierarchique qui accorde plus de poids aux éléments de contexte les plus proches.   \n",
    "Voici à quoi ressemble l'architecture  :   \n",
    "<img src=\"images/wavenet.png\" alt=\"bengio\" width=\"600\"/>    \n",
    "Figure extraite de l'article original.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation du modèle fully connected avec pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprenons le code du notebook précédent pour la génération du dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MARIE', 'JEAN', 'PIERRE', 'MICHEL', 'ANDRÉ', 'JEANNE', 'PHILIPPE', 'LOUIS']\n"
     ]
    }
   ],
   "source": [
    "# Lecture du dataset\n",
    "words = open('prenoms.txt', 'r').read().splitlines()\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('prenoms.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour changer, augmentons le contexte en le passant de 3 à 8. Cela nous donnera un indicateur de performance car nous utiliserons aussi 8 pour notre WaveNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # La longueur du contexte, combien de caractères pour prédire le suivant ?\n",
    "X, Y = [], []\n",
    "for k,w in enumerate(words):\n",
    "  \n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    context = context[1:] + [ix] \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=TensorDataset(X, Y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(TensorDataset(X, Y),[train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle et entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer, nous allons réimplementer le modèle du notebook précédent mais avec pytorch. Cela nous permettra également d'ajouter d'autres couches utiles comme la batch norm et d'autres fonctions d'activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcn(nn.Module):\n",
    "  def __init__(self,embed_dim=10,context_len=8,hidden_dim=300, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.embed_dim=embed_dim\n",
    "    self.context_len=context_len\n",
    "    \n",
    "    #La fonction nn.Embedding de pytorch est l'équivalent de la matrice C \n",
    "    self.embedding=nn.Embedding(46,embed_dim)\n",
    "    self.layer1=nn.Linear(embed_dim*context_len,hidden_dim)\n",
    "    self.layer2=nn.Linear(hidden_dim,46)\n",
    "\n",
    "  def forward(self,x):       \n",
    "    embed=self.embedding(x) # Remplace la matrice C\n",
    "    embed=embed.view(-1,self.embed_dim*self.context_len)\n",
    "    hidden=F.tanh(self.layer1(embed))\n",
    "    logits=self.layer2(hidden)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=fcn(context_len=8)\n",
    "epochs=50\n",
    "lr=0.2\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes** Pour reproduire à l'identique le code du notebook précédent, il faudra réduire le learning rate par un facteur 10 à la moitié de l'entraînement.   \n",
    "En pytorch, on peut faire ça à l'aide du [scheduler](https://pytorch.org/docs/stable/optim.html). Il existe plusieurs types de scheduler : LambdaLR (pour changer le lr par rappport à une fonction), StepLR (pour diminuer le lr toutes les n epochs), LinearLR (pour diminuer le lr de manière linéaire), ReduceLROnPlateau (pour diminuer le lr dès que le loss ne change plus), OneCycleLR (pour commencer avec un lr faible puis l'augmenter et le redescendre) et bien d'autres.   \n",
    "Pour accélerer la convergence d'un modèle, je vous conseillerais l'utilisation du OneCycleLR (pour en savoir plus, consultez ce [blogpost](https://www.datacamp.com/tutorial/cyclical-learning-neural-nets)) et pour obtenir un modèle très performant je vous conseillerais plutôt le ReduceLROnPlateau. Dans tous les cas, il est intéressant d'expérimenter vous-même les différents scheduler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 2.475, Validation loss: 2.455\n",
      "Epoch 10 - Training loss: 2.065, Validation loss: 2.176\n",
      "Epoch 20 - Training loss: 1.954, Validation loss: 2.120\n",
      "Epoch 30 - Training loss: 1.900, Validation loss: 2.090\n",
      "Epoch 40 - Training loss: 1.865, Validation loss: 2.073\n"
     ]
    }
   ],
   "source": [
    "lossi=[]\n",
    "lossvali=[]\n",
    "stepi = []\n",
    "for epoch in range(epochs):\n",
    "  loss_epoch=0\n",
    "  for x,y in train_loader:\n",
    "    # forward pass\n",
    "    logits=model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # retropropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Mise à jour des poids du modèle\n",
    "    optimizer.step()\n",
    "    loss_epoch+=loss\n",
    "  loss_epoch=loss_epoch/len(train_loader)\n",
    "  stepi.append(epoch)\n",
    "  lossi.append(loss_epoch.item())\n",
    "  \n",
    "  # Validation\n",
    "  loss_val=0\n",
    "  for x,y in val_loader:\n",
    "    logits=model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss_val+=loss\n",
    "  loss_val=loss_val/len(val_loader)\n",
    "  lossvali.append(loss_val.item())\n",
    "  if epoch%10==0:\n",
    "    print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi)\n",
    "plt.plot(stepi,lossvali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1087, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# On annule le calcul des gradients car on n'est plus en phase d'entraînement.\n",
    "model.eval()\n",
    "loss_test=0\n",
    "for x,y in test_loader:\n",
    "    \n",
    "  # forward pass\n",
    "  logits=model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "      \n",
    "  loss_test+=loss\n",
    "loss_test=loss_test/len(test_loader)\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le loss de test est un peu supérieur au loss de train donc le modèle a un peu overfit mais ça reste léger et on a encore de la marge pour augmenter les capacités du réseau.\n",
    "On peut maintenant vérifier la qualité de la génération de prénoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMDYN.\n",
      "MOUNTANE.\n",
      "KEMY.\n",
      "RAHINA.\n",
      "SIRIK.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "  out = []\n",
    "  context = [0] * block_size \n",
    "  while True:\n",
    "    logits=model(torch.tensor([context]))\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "  \n",
    "  print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les prénoms générés sont correctes mais améliorables.  \n",
    "Voyons voir si on peut obtenir un meilleur loss sur les données de test avec l'approche WaveNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation du WaveNet avec pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset est le même que pour la partie précédente, pas besoin de changer quoi que ce soit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment gérer l'architecture hierarchique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qu'on veut dans notre modèle c'est traîter en parallele des groupes de d'embedding en regroupant les caractères consécutifs.   \n",
    "Sur pytorch, si on fait passer un tenseur de taille $B \\times L \\times C$ dans une couche linéaire de taille $C \\times H$, on obtient un tenseur de taille $B \\times L \\times H$ et c'est exactement ce que l'on veut pour implémenter le réseau wavenet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, il faut trouver comment modifier la taille du tenseur pour faire les opérations du wavenet. Nos 8 embeddings sont regroupés par deux puis traités en parralèles, à la prochaine couches, on regroupe à nouveau par deux. Donc à chaque étape, on double la taille $H$ (ou $C$) et on divise par deux $L$.   \n",
    "Pour être plus clair, au début nous avons un tenseur de taille $B \\times 8 \\times 10$ que l'on veut transformer en tenseur de taille $B \\times 4 \\times 20$   \n",
    "On peut implémenter ça avec view() de pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "dummy=torch.randn([256,8,10])\n",
    "# On divise par deux L et on double H/C\n",
    "dummy=dummy.view(-1,dummy.shape[1]//2,dummy.shape[2]*2)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de formaliser ça avec une couche que l'on pourra utiliser dans notre réseau : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenConsecutive(nn.Module):\n",
    "  # n est le facteur de regroupement (toujours 2 pour nous)\n",
    "  def __init__(self, n):\n",
    "    super(FlattenConsecutive, self).__init__()\n",
    "    self.n = n   \n",
    "  def __call__(self, x):\n",
    "    # On récupère les dimensions de l'entrée\n",
    "    B, T, C = x.shape \n",
    "    # On fait la transformation x2 et /2\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1: \n",
    "      x = x.squeeze(1) # Si le tensor a une dimension qui vaut 1, on la supprime\n",
    "    self.out = x\n",
    "    return self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps de créer notre modèle, pour plus de simplicité, on utilise nn.Sequential() pour regrouper nos couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wavenet(nn.Module):\n",
    "  def __init__(self,embed_dim=10,hidden_dim=128, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    \n",
    "    self.net=nn.Sequential(nn.Embedding(46,embed_dim),\n",
    "      # B*8*10\n",
    "      FlattenConsecutive(2), nn.Linear(embed_dim*2,hidden_dim),nn.Tanh(),\n",
    "      # B*4*hidden_dim\n",
    "      FlattenConsecutive(2), nn.Linear(hidden_dim*2,hidden_dim),nn.Tanh(),\n",
    "      # B*2*hidden_dim\n",
    "      FlattenConsecutive(2), nn.Linear(hidden_dim*2,hidden_dim),nn.Tanh(),\n",
    "      # B*hidden_dim\n",
    "      nn.Linear(hidden_dim,46)\n",
    "    )   \n",
    "        \n",
    "  def forward(self,x):\n",
    "    logits=self.net(x)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On initialise notre modèle et les hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=wavenet()\n",
    "epochs=40\n",
    "lr=0.2\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et on lance l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 2.529, Validation loss: 2.449\n",
      "Epoch 10 - Training loss: 2.010, Validation loss: 2.078\n",
      "Epoch 20 - Training loss: 1.897, Validation loss: 2.032\n",
      "Epoch 30 - Training loss: 1.832, Validation loss: 2.051\n"
     ]
    }
   ],
   "source": [
    "lossi=[]\n",
    "lossvali=[]\n",
    "stepi = []\n",
    "for epoch in range(epochs):\n",
    "  loss_epoch=0\n",
    "  for x,y in train_loader:\n",
    "    logits=model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # retropropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Mise à jour des poids du modèle\n",
    "    loss_epoch+=loss\n",
    "  loss_epoch=loss_epoch/len(train_loader)\n",
    "  stepi.append(epoch)\n",
    "  lossi.append(loss_epoch.item())\n",
    "  \n",
    "  # Validation\n",
    "  loss_val=0\n",
    "  for x,y in val_loader:\n",
    "    logits=model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss_val+=loss\n",
    "  loss_val=loss_val/len(val_loader)\n",
    "  lossvali.append(loss_val.item())\n",
    "  if epoch%10==0:\n",
    "    print(f\"Epoch {epoch} - Training loss: {loss_epoch.item():.3f}, Validation loss: {loss_val.item():.3f}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi)\n",
    "plt.plot(stepi,lossvali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0407, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "loss_test=0\n",
    "for x,y in test_loader:\n",
    "      \n",
    "  # forward pass\n",
    "  logits=model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "      \n",
    "  loss_test+=loss\n",
    "loss_test=loss_test/len(test_loader)\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient un loss très correct et inférieur à celui du modèle Fully Connected avec un contexte de 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VARAL.\n",
      "SYMÈNA.\n",
      "ÉSAFAYM.\n",
      "KATHEO.\n",
      "KAIDER.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "  out = []\n",
    "  context = [0] * block_size \n",
    "  while True:\n",
    "    logits=model(torch.tensor([context]))\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "  \n",
    "  print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La génération de prénoms est de mieux en mieux !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Pour vous entraîner, essayez de modifier les paramètres d'entraînement, l'architecture du réseau et autres pour obtenir un loss inférieur à 2.0 sur les données de test. Point bonus si vous réduisez le nombre de paramètres du modèle. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
