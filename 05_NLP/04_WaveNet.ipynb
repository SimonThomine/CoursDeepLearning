{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch et WaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, nous allons nous nous inspire de l'architecture du modèle [wavenet](https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/)  proposé par google deepmind pour le traitement de l'audio.  \n",
    "Notre but dans ce cours est prendre plus de caractères pour le contexte de notre prédicteur du prochain mot.  \n",
    "Le cours commence par la transformation du cours précédent avec les fonctions de pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture d'un WaveNet est une architecture hierarchique qui accorde plus de poids aux éléments de contexte les plus proches.   \n",
    "Voici à quoi ressemble l'architecture  :   \n",
    "<img src=\"images/wavenet.png\" alt=\"bengio\" width=\"600\"/>    \n",
    "Figure extraite de l'article original.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation du modèle fully connected avec pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprenons le code du notebook précédent pour la génération du dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MARIE', 'JEAN', 'PIERRE', 'MICHEL', 'ANDRÉ', 'JEANNE', 'PHILIPPE', 'LOUIS']\n"
     ]
    }
   ],
   "source": [
    "# Lecture du dataset\n",
    "words = open('prenoms.txt', 'r').read().splitlines()\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('prenoms.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour changer, augmentons le contexte en le passant de 3 à 8. Cela nous donnera un indicateur de performance car nous utiliserons aussi 8 pour notre WaveNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # La longueur du contexte, combien de caractères pour prédire le suivant ?\n",
    "X, Y = [], []\n",
    "for k,w in enumerate(words):\n",
    "  \n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    context = context[1:] + [ix] \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=TensorDataset(X, Y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(TensorDataset(X, Y),[train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle et entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer, nous allons réimplementer le modèle du notebook précédent mais avec pytorch. Cela nous permettra également d'ajouter d'autres couches utiles comme la batch norm et d'autres fonctions d'activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcn(nn.Module):\n",
    "  def __init__(self,embed_dim=10,context_len=8,hidden_dim=300, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.embed_dim=embed_dim\n",
    "    self.context_len=context_len\n",
    "    \n",
    "    #La fonction nn.Embedding de pytorch est l'équivalent de la matrice C \n",
    "    self.embedding=nn.Embedding(46,embed_dim)\n",
    "    self.layer1=nn.Linear(embed_dim*context_len,hidden_dim)\n",
    "    self.layer2=nn.Linear(hidden_dim,46)\n",
    "\n",
    "  def forward(self,x):       \n",
    "    embed=self.embedding(x) # Remplace la matrice C\n",
    "    embed=embed.view(-1,self.embed_dim*self.context_len)\n",
    "    hidden=F.tanh(self.layer1(embed))\n",
    "    logits=self.layer2(hidden)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model=fcn(context_len=8)\n",
    "epochs=40\n",
    "lr=0.2\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss epoch0:  tensor(2.4962, grad_fn=<DivBackward0>)\n",
      "loss epoch5:  tensor(2.1687, grad_fn=<DivBackward0>)\n",
      "loss epoch10:  tensor(2.0629, grad_fn=<DivBackward0>)\n",
      "loss epoch15:  tensor(1.9992, grad_fn=<DivBackward0>)\n",
      "loss epoch20:  tensor(1.9553, grad_fn=<DivBackward0>)\n",
      "loss epoch25:  tensor(1.9237, grad_fn=<DivBackward0>)\n",
      "loss epoch30:  tensor(1.9002, grad_fn=<DivBackward0>)\n",
      "loss epoch35:  tensor(1.8826, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lossi=[]\n",
    "stepi = []\n",
    "for epoch in range(epochs):\n",
    "  loss_epoch=0\n",
    "  for x,y in train_loader:\n",
    "    \n",
    "    # forward pass\n",
    "    logits=model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    \n",
    "    # retropropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Mise à jour des poids du modèle\n",
    "\n",
    "    loss_epoch+=loss\n",
    "  \n",
    "  loss_epoch=loss_epoch/len(train_loader)\n",
    "  if epoch%5==0:\n",
    "    print(\"loss epoch\"+str(epoch) + \": \",loss_epoch)\n",
    "  stepi.append(epoch)\n",
    "  lossi.append(loss_epoch.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0976, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# On annule le calcul des gradients car on n'est plus en phase d'entraînement.\n",
    "model.eval()\n",
    "loss_test=0\n",
    "for x,y in test_loader:\n",
    "    \n",
    "  # forward pass\n",
    "  logits=model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "      \n",
    "  loss_test+=loss\n",
    "loss_test=loss_test/len(test_loader)\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le loss de test est un peu supérieur au loss de train donc le modèle a un peu overfit mais ça reste léger et on a encore de la marge pour augmenter les capacités du réseau.\n",
    "On peut maintenant vérifier la qualité de la génération de prénoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LYSELLEY.\n",
      "SAMIANA.\n",
      "CHARLES-ARMAZDA.\n",
      "PHORICIE.\n",
      "JAOUKHE.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "  out = []\n",
    "  context = [0] * block_size \n",
    "  while True:\n",
    "    logits=model(torch.tensor([context]))\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "  \n",
    "  print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les prénoms générés sont correctes mais améliorables.  \n",
    "Voyons voir si on peut obtenir un meilleur loss sur les données de test avec l'approche WaveNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation du WaveNet avec pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset est le même que pour la partie précédente, pas besoin de changer quoi que ce soit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment gérer l'architecture hierarchique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qu'on veut dans notre modèle c'est traîter en parallele des groupes de d'embedding en regroupant les caractères consécutifs.   \n",
    "Sur pytorch, si on fait passer un tenseur de taille $B \\times L \\times C$ dans une couche linéaire de taille $C \\times H$, on obtient un tenseur de taille $B \\times L \\times H$ et c'est exactement ce que l'on veut pour implémenter le réseau wavenet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, il faut trouver comment modifier la taille du tenseur pour faire les opérations du wavenet. Nos 8 embeddings sont regroupés par deux puis traités en parralèles, à la prochaine couches, on regroupe à nouveau par deux. Donc à chaque étape, on double la taille $H$ (ou $C$) et on divise par deux $L$.   \n",
    "Pour être plus clair, au début nous avons un tenseur de taille $B \\times 8 \\times 10$ que l'on veut transformer en tenseur de taille $B \\times 4 \\times 20$   \n",
    "On peut implémenter ça avec view() de pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "dummy=torch.randn([256,8,10])\n",
    "# On divise par deux L et on double H/C\n",
    "dummy=dummy.view(-1,dummy.shape[1]//2,dummy.shape[2]*2)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de formaliser ça avec une couche que l'on pourra utiliser dans notre réseau : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenConsecutive:\n",
    "  # n est le facteur de regroupement (toujours 2 pour nous)\n",
    "  def __init__(self, n):\n",
    "    self.n = n   \n",
    "  def __call__(self, x):\n",
    "    # On récupère les dimensions de l'entrée\n",
    "    B, T, C = x.shape \n",
    "    # On fait la transformation x2 et /2\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "\n",
    "    if x.shape[1] == 1: \n",
    "      x = x.squeeze(1) # Si le tensor a une dimension qui vaut 1, on la supprime\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wavenet(nn.Module):\n",
    "  def __init__(self,embed_dim=10,len_context=8,len_hidden=128, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.len_context=len_context\n",
    "    self.embed_dim=embed_dim\n",
    "    self.len_hidden=len_hidden\n",
    "    # TODO sequential sur tout \n",
    "        \n",
    "    self.embedding=nn.Embedding(46,embed_dim)\n",
    "    self.layer1=nn.Sequential(\n",
    "        nn.Linear(embed_dim*2,len_hidden,bias=False),\n",
    "        nn.Tanh()\n",
    "        )\n",
    "    self.layer2=nn.Sequential(\n",
    "        nn.Linear(len_hidden*2,len_hidden),\n",
    "        nn.Tanh()\n",
    "        )\n",
    "    self.layer3=nn.Sequential(\n",
    "        nn.Linear(len_hidden*2,len_hidden),\n",
    "        nn.Tanh()\n",
    "        )\n",
    "\n",
    "    self.linear=nn.Linear(len_hidden,46)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x=self.embedding(x)\n",
    "    x=x.view(-1,self.len_context//2,x.shape[-1]*2)\n",
    "    x=self.layer1(x)\n",
    "    x=x.view(-1,self.len_context//4,self.len_hidden*2)\n",
    "    x=self.layer2(x)\n",
    "    x=x.view(-1,self.len_hidden*2)\n",
    "    x=self.layer3(x)\n",
    "    logits=self.linear(x)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=wavenet()\n",
    "epochs=40\n",
    "lr=0.2\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss epoch0:  tensor(2.5294, grad_fn=<DivBackward0>)\n",
      "loss epoch5:  tensor(2.1316, grad_fn=<DivBackward0>)\n",
      "loss epoch10:  tensor(2.0214, grad_fn=<DivBackward0>)\n",
      "loss epoch15:  tensor(1.9542, grad_fn=<DivBackward0>)\n",
      "loss epoch20:  tensor(1.9070, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lossi_train=[]\n",
    "lossi_val=[]\n",
    "stepi = []\n",
    "for epoch in range(epochs):\n",
    "  loss_epoch=0\n",
    "  for x,y in train_loader:\n",
    "    logits=model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # retropropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Mise à jour des poids du modèle\n",
    "    loss_epoch+=loss\n",
    "  loss_epoch=loss_epoch/len(train_loader)\n",
    "  if epoch%5==0:\n",
    "    print(\"loss epoch\"+str(epoch) + \": \",loss_epoch)\n",
    "  stepi.append(epoch)\n",
    "  lossi_train.append(loss_epoch.item())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0293, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "loss_test=0\n",
    "for x,y in test_loader:\n",
    "      \n",
    "  # forward pass\n",
    "  logits=model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "      \n",
    "  loss_test+=loss\n",
    "loss_test=loss_test/len(test_loader)\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALEA.\n",
      "MAXI.\n",
      "EMMANUEL.\n",
      "BAISTES.\n",
      "VINCEY.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "  out = []\n",
    "  context = [0] * block_size \n",
    "  while True:\n",
    "    logits=model(torch.tensor([context]))\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "  \n",
    "  print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
