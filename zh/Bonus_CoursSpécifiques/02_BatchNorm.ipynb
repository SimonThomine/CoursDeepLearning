{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量归一化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**批量归一化（Batch Normalization）**于 2015 年在论文 [*Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift*](https://arxiv.org/pdf/1502.03167) 中被提出，对深度学习领域产生了重大影响。如今，归一化技术（如 *BatchNorm*、*LayerNorm* 或 *GroupNorm* 等）几乎已成为标准操作。\n",
    "\n",
    "**核心思想**：\n",
    "批量归一化的目标是确保网络每一层的预激活值（pre-activations）服从高斯分布。虽然良好的初始化能达到这一效果，但在多层网络中难以保证。批量归一化通过对 *batch* 维度上的预激活值进行归一化，在传入激活函数前确保其分布接近高斯分布。\n",
    "\n",
    "**优点**：\n",
    "1. 该归一化是可导的，不影响模型的优化过程。\n",
    "2. 通过减少内部协变量偏移（Internal Covariate Shift），加速训练并提升稳定性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码回顾\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将基于上一节的代码实现 **批量归一化**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../05_NLP/prenoms.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180834, 3]) torch.Size([180834])\n",
      "torch.Size([22852, 3]) torch.Size([22852])\n",
      "torch.Size([22639, 3]) torch.Size([22639])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # Contexte\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] \n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=10 # Dimension de l'embedding de C\n",
    "hidden_dim=200 # Dimension de la couche cachée\n",
    "\n",
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是前向传播的代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量归一化的实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据论文，批量归一化的步骤如下：\n",
    "\n",
    "![归一化流程](./images/norm.png)\n",
    "\n",
    "**第一步：归一化**\n",
    "1. 计算预激活值 *hpreact* 的均值（$\\mu$）和标准差（$\\sigma$）。\n",
    "2. 使用以下公式对 *hpreact* 进行归一化：\n",
    "   $\n",
    "   \\hat{h} = \\frac{h_{preact} - \\mu}{\\sigma + \\epsilon}\n",
    "   $\n",
    "   （$\\epsilon$ 为小常数，避免分母为零）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-6\n",
    "hpreact_mean = hpreact.mean(dim=0, keepdim=True)\n",
    "hpreact_std= hpreact.std(dim=0, keepdim=True)\n",
    "hpreact_norm = (hpreact - hpreact_mean) / (hpreact_std+epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二步：集成到前向传播**\n",
    "将上述归一化步骤嵌入前向传播中。\n",
    "\n",
    "**缩放与平移（Scale and Shift）**\n",
    "归一化后的数据服从标准高斯分布（均值 0，方差 1），但这会限制模型的表达能力。为此，引入两个可学习参数：\n",
    "- **$\\gamma$（缩放）**：调整数据的方差。\n",
    "- **$\\beta$（平移）**：调整数据的均值。\n",
    "\n",
    "![缩放与平移](./images/scaleshift.png)\n",
    "\n",
    "**公式**：\n",
    "$\n",
    "y = \\gamma \\hat{h} + \\beta\n",
    "$\n",
    "\n",
    "**注意**：$\\gamma$ 和 $\\beta$ 需作为模型参数进行优化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播的最终表达式为：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmean = hpreact.mean(0, keepdim=True)\n",
    "bnstd = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量归一化的潜在问题\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量归一化在实践中可能面临以下挑战：\n",
    "\n",
    "**1. 批次内依赖性**\n",
    "- 归一化基于 *batch* 维度进行，意味着每个样本的归一化结果会受同批次其他样本影响。\n",
    "- **影响**：\n",
    "  - 理论上可能引入噪声，但实践中这种随机性反而起到 **正则化** 作用，减少过拟合风险。\n",
    "  - 若希望避免批次依赖，可使用 *LayerNorm* 或 *GroupNorm* 等替代方案。但 *BatchNorm* 仍因经验效果佳而广泛应用。\n",
    "\n",
    "**2. 训练与推理不一致**\n",
    "- **训练阶段**：样本的归一化依赖于批次内其他样本的统计量（均值/方差）。\n",
    "- **推理阶段**：单个样本无法计算批次统计量，导致行为不一致。\n",
    "\n",
    "**解决方案**：\n",
    "- **指数移动平均（EMA）**：\n",
    "  在训练过程中动态更新全局均值/方差的估计值，避免额外遍历数据集。\n",
    "  实现示例（Python）：\n",
    "  ```python\n",
    "  # 伪代码：EMA 更新\n",
    "  running_mean = momentum * running_mean + (1 - momentum) * batch_mean\n",
    "  running_var  = momentum * running_var  + (1 - momentum) * batch_var\n",
    "  ```\n",
    "  - `momentum` 典型值为 0.1（PyTorch 默认值），可根据批次大小与数据集规模调整。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((46, embed_dim))\n",
    "W1 = torch.randn((block_size*embed_dim, hidden_dim))*0.01 # On initialise les poids à une petite valeur\n",
    "b1 = torch.randn(hidden_dim) *0 # On initialise les biais à 0\n",
    "W2 = torch.randn((hidden_dim, 46))*0.01\n",
    "b2 = torch.randn(46)*0 \n",
    "# Paramètres de batch normalization\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "bnmean_running = torch.zeros((1, hidden_dim))\n",
    "bnstd_running = torch.ones((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  \n",
    "# Forward\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "emb = C[Xb] \n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1 \n",
    "\n",
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)\n",
    "bnstdi = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "\n",
    "h = torch.tanh(hpreact) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：\n",
    "- 在我们的实现中，EMA 的 `momentum` 设为 0.001。\n",
    "- PyTorch 的 [*BatchNorm* 层](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) 默认 `momentum=0.1`。\n",
    "- **选择建议**：\n",
    "  - **大批次 + 小数据集**：可使用较大值（如 0.1）。\n",
    "  - **小批次 + 大数据集**：建议使用较小值（如 0.001），以更精确地估计全局统计量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们训练模型，验证批量归一化层的功能。\n",
    "**注意**：在小型模型中，性能差异可能不明显。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.8241\n",
      "  10000/ 200000: 1.9756\n",
      "  20000/ 200000: 2.7151\n",
      "  30000/ 200000: 2.3287\n",
      "  40000/ 200000: 2.1411\n",
      "  50000/ 200000: 2.3207\n",
      "  60000/ 200000: 2.3250\n",
      "  70000/ 200000: 2.0320\n",
      "  80000/ 200000: 2.0615\n",
      "  90000/ 200000: 2.2468\n",
      " 100000/ 200000: 2.2081\n",
      " 110000/ 200000: 2.1418\n",
      " 120000/ 200000: 1.9665\n",
      " 130000/ 200000: 1.8572\n",
      " 140000/ 200000: 2.0577\n",
      " 150000/ 200000: 2.1804\n",
      " 160000/ 200000: 1.8604\n",
      " 170000/ 200000: 1.9810\n",
      " 180000/ 200000: 1.8228\n",
      " 190000/ 200000: 1.9977\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "max_steps = 200000\n",
    "\n",
    "for i in range(max_steps):\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "  emb = C[Xb] \n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1 \n",
    "  \n",
    "  # Batch normalization\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad(): # On ne veut pas calculer de gradient pour ces opérations\n",
    "      bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "      bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    \n",
    "  h = torch.tanh(hpreact) \n",
    "  logits = h @ W2 + b2 \n",
    "  loss = F.cross_entropy(logits, Yb)\n",
    "  \n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  lr = 0.1 if i < 100000 else 0.01 \n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "  if i % 10000 == 0:\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补充说明\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 偏置（Bias）的作用**\n",
    "- 批量归一化通过 **中心化** 预激活值，使得偏置项（bias）失去作用（因其仅平移分布）。\n",
    "- **实践建议**：\n",
    "  - 可移除偏置项，减少参数数量。\n",
    "  - 保留偏置项也不影响模型，但会增加冗余参数。\n",
    "\n",
    "**2. 批量归一化的位置**\n",
    "- **理论位置**：应置于激活函数 **之前**，以归一化预激活值。\n",
    "- **实践变体**：部分研究/代码将其置于激活函数 **之后**，需根据具体框架灵活调整。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他归一化方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除批量归一化外，深度学习中常用的归一化方法还包括：\n",
    "\n",
    "![归一化方法对比](./images/types.png)\n",
    "*图片来源：[文章](https://arxiv.org/pdf/1803.08494)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 层归一化（Layer Normalization）**\n",
    "- **应用场景**：广泛用于语言模型（如 GPT、Llama）。\n",
    "- **归一化维度**：对 **单个样本的所有特征** 进行归一化（而非批次维度）。\n",
    "- **实现变化**：\n",
    "  在代码中，仅需将归一化轴从 `axis=0`（批次维度）改为 `axis=1`（特征维度）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization\n",
    "bnmeani = hpreact.mean(0, keepdim=True)  \n",
    "bnstdi = hpreact.std(0, keepdim=True)   \n",
    "# Layer normalization\n",
    "bnmeani = hpreact.mean(1, keepdim=True)  \n",
    "bnstdi = hpreact.std(1, keepdim=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 实例归一化（Instance Normalization）**\n",
    "- 对 **每个样本的每个通道** 独立归一化，常用于风格迁移任务。\n",
    "\n",
    "**3. 组归一化（Group Normalization）**\n",
    "- **折中方案**：将通道分组，对每组内的数据进行归一化。\n",
    "- **极端情况**：\n",
    "  - 组大小 = 1 → 等同于 *InstanceNorm*。\n",
    "  - 组大小 = 通道数 → 等同于 *LayerNorm*。\n",
    "- **优点**：对批次大小不敏感，适用于小批次训练。,\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
