{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本笔记本分析论文《[Swin Transformer: 基于移位窗口的分层视觉变换器](https://arxiv.org/pdf/2103.14030)》。该论文提出了一种改进的*变换器*架构，采用分层设计，适用于图像处理，并类似于卷积神经网络。\n",
    "笔记本的第一部分逐一解释了论文的主要创新点。第二部分展示了该架构的简化实现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章分析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该论文的核心思想是以分层方式将*注意力*机制应用于图像中越来越大的区域。这种方法基于以下两个基本原理：\n",
    "- 图像分析应从局部细节开始，逐步扩展到全局像素关系。这也是卷积神经网络（CNNs）表现优异的原因。\n",
    "- 通过限制*图像块*（patch）之间的通信，可以显著提升计算效率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分层架构\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Swin Transformer*的分层架构如下图所示：\n",
    "\n",
    "![hierarchique](./images/hierarchique.png)\n",
    "\n",
    "在我们的实现中，ViT 模型将*图像块*（patch）转换为*标记*（token），并对所有元素应用*变换器编码器*（transformer encoder）。这是一种简单且无数据偏见的架构，可适用于多种数据类型。\n",
    "\n",
    "*Swin*架构通过添加偏见，使其在图像处理上更高效且计算速度更快。如图所示，图像首先被划分为小*图像块*（大小为 $4 \\times 4$，与论文一致），并分组为多个窗口。注意力层仅在每个窗口内独立应用。随着网络深度增加，图像块的相对大小（C）和窗口大小逐渐扩大，最终覆盖整个图像，同时保持与 ViT 架构相同的*图像块*数量。类似于 CNN，网络首先处理局部信息，然后随着*感受野*（receptive field）的扩大，逐步处理更全局的信息。这一过程通过增加滤波器数量并降低图像分辨率来实现。\n",
    "\n",
    "这种新型*变换器*模块被称为*窗口多头自注意力*（W-MSA，注意这里的 M 代表*多头*，而非*掩码*）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推拉窗\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在与 CNN 的类比中，作者们发现将图像以任意位置划分为窗口可能会带来问题，因为这会破坏位于窗口边缘的相邻像素之间的连接。\n",
    "\n",
    "为了解决这个问题，作者们提出在每个*swin 块*（block swin）中使用*移位窗口*（shifting window）系统。如笔记本开头的图所示，这些*swin 块*成对排列。\n",
    "\n",
    "移位窗口的示意图如下：\n",
    "\n",
    "![shifting](./images/shifting.png)\n",
    "\n",
    "如图所示，使用这种技术后，窗口从 $2 \\times 2$ *图像块*扩展到 $3 \\times 3$ *图像块*（通常从 $n \\times n$ 扩展到 $(n+1) \\times (n+1)$）。这会给网络处理带来问题，特别是在*批处理*（batch）模式下。\n",
    "\n",
    "作者们提出使用*循环移位*（cyclic shift）来更高效地处理图像，具体操作如下：\n",
    "\n",
    "![cyclic](./images/cyclic.png)\n",
    "\n",
    "注意，使用这种方法时，需要对来自图像不同部分的*图像块*信息进行掩码。如图所示，白色、黄色、绿色和蓝色区域之间不会相互通信，这通过*掩码注意力*层来实现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相对位置偏差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT 架构使用*绝对位置嵌入*（position embedding）为不同*图像块*添加位置信息。然而，这种方法的问题在于它无法捕捉*图像块*之间的关系，因此在处理不同分辨率的图像时表现较差。\n",
    "\n",
    "相比之下，*Swin Transformer*采用*相对位置偏置*（relative position bias）来弥补这一不足。该偏置基于*图像块*之间的相对距离，并在计算两个*图像块*之间的注意力时加入。其主要优势在于能够更好地捕捉空间关系，并适应不同分辨率的图像。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有关架构的更多细节\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如笔记本开头的图所示，*Swin Transformer*的第三阶段（stage 3）包含更多层。在增加网络层数时，仅扩展第三阶段的层数，其他阶段保持不变。这种设计既能充分利用*swin*架构（如*移位窗口*等）的优势，又能保持足够的深度和处理速度。\n",
    "\n",
    "- 假设每个窗口包含 $M \\times M$ 个*图像块*，对于一个包含 $h \\times w$ 个*图像块*的图像，*多头自注意力*（MSA）层和*窗口多头自注意力*（W-MSA）层的计算复杂度分别为：\n",
    "$\\Omega(\\text{MSA}) = 4hwC^2 + 2(h w)^2 C$\n",
    "$\\Omega(\\text{W-MSA}) = 4hwC^2 + 2M^2hwC$\n",
    "前者的复杂度为二次方，而后者在 $M$ 固定时为线性复杂度。*Swin*架构因此能显著提升处理速度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简化实施\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们将使用 PyTorch 实现*Swin Transformer*。由于*移位窗口*和*相对位置偏置*部分的实现较为复杂，我们将暂不涉及，仅实现其分层架构。\n",
    "\n",
    "如果您希望查看作者提供的完整实现，请访问他们的 [GitHub](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py)。我们的实现参考了作者的代码，并基于之前的 ViT 实现进行了改进。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detection automatique du GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将图像转换为补丁\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在将图像转换为*图像块*（patch）时，我们将沿用之前笔记本中的函数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    # On rajoute une dimension pour le batch\n",
    "    B,C,_,_ = image.shape\n",
    "    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.permute(0,2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(B,-1, C, patch_size, patch_size)\n",
    "    patches_flat = patches.flatten(2, 4)\n",
    "    return patches_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head self-attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在*Swin*的实现中*，多头自注意力*层与 ViT 中的实现保持一致。虽然层本身没有变化，但在*swin 块*（swin block）中使用方式有所不同。\n",
    "\n",
    "因此，我们将沿用之前笔记本中的代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head_enc(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "    def __init__(self, head_size,n_embd,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape   \n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # On a supprimer le masquage du futur\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head_enc de taille head_size\n",
    "        self.heads = nn.ModuleList([Head_enc(head_size,n_embd,dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：如果需要实现*相对位置偏置*，则需修改该函数，因为该偏置是在计算*注意力*时直接加入的（详见[源代码](https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py)）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*前馈层*（feed forward layer）的实现同样保持不变：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### swin 块的实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们将实现一个函数，用于将图像划分为多个窗口。为此，我们需要将张量 $x$ 的维度从 $B \\times T \\times C$ 转换为 $B \\times H \\times W \\times C$。然后，我们将该张量重塑为多个窗口，并将其放入*批次*（batch）维度中，以便独立处理每个窗口。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size,input_resolution):\n",
    "    B,_,C = x.shape\n",
    "    H,W = input_resolution\n",
    "    x = x.view(B, H, W, C)\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，假设我们将一张 $224 \\times 224$ 的图像划分为 $4 \\times 4$ 的*图像块*，这将得到 $224/4 \\times 224/4 = 3136$ 个*图像块*，并将其投影到大小为 96 的嵌入维度 $C$（适用于 swin-T 和 swin-S 变体）。我们将其划分为 $M=7$ 个窗口，得到的张量如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 7, 7, 96])\n"
     ]
    }
   ],
   "source": [
    "# Pour un batch de taille 2\n",
    "window_size = 7\n",
    "n_embed = 96\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "windows=window_partition(dummy,window_size,(56,56))\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在将其传入*注意力*层之前，我们需要将其恢复为 $B \\times T \\times C$ 的维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 49, 96])\n"
     ]
    }
   ],
   "source": [
    "windows=windows.view(-1, window_size * window_size, n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们可以应用*注意力*层，在每个窗口内独立执行*自注意力*（self-attention）操作。完成后，需要应用逆变换，将结果恢复为非窗口格式：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 56, 56, 96])\n",
      "torch.Size([2, 3136, 96])\n"
     ]
    }
   ],
   "source": [
    "def window_reverse(windows, window_size,input_resolution):\n",
    "    H,W=input_resolution\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "windows=window_reverse(windows,window_size,(56,56))\n",
    "print(windows.shape)\n",
    "# et revenir en format BxTxC\n",
    "windows=windows.view(2,3136,n_embed)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们刚刚实现了*Swin Transformer*分层窗口处理的核心元素。现在，我们可以构建集成所有这些变换的*swin 块*：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swinblock(nn.Module):\n",
    "  def __init__(self, n_embd,n_head,input_resolution,window_size,dropout=0.) -> None:\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size,n_embd,dropout)\n",
    "    self.ffwd = FeedFoward(n_embd,dropout)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "    self.input_resolution = input_resolution\n",
    "    self.window_size = window_size\n",
    "    self.n_embd = n_embd\n",
    "    \n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    x=window_partition(x, self.window_size,self.input_resolution)\n",
    "    x=self.ln1(x)\n",
    "    x=x.view(-1, self.window_size * self.window_size, self.n_embd)\n",
    "    x=self.sa(x)\n",
    "    x=window_reverse(x,self.window_size,self.input_resolution)\n",
    "    x=x.view(B,T,self.n_embd)\n",
    "    x=x+self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补丁合并\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在*Swin Transformer*的分层架构中，每次扩大*感受野*（通过减少窗口数量）时，我们会将 4 个相邻的大小为 $C$ 的*图像块*在维度上拼接为 $4C$，然后应用一个线性层将其压缩至更小的 $2C$ 维度。这样，每次减少窗口数量时*，标记*（token）的数量也会减少 4 倍。\n",
    "\n",
    "我们可以通过以下方式获取相邻的*图像块*：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 96])\n"
     ]
    }
   ],
   "source": [
    "# Reprenons un exemple de nos 56x56 patchs\n",
    "dummy=torch.randn(2,3136,n_embed)\n",
    "B,T,C = dummy.shape\n",
    "H,W=T**0.5,T**0.5\n",
    "dummy=dummy.view(2,56,56,n_embed)\n",
    "# En python, 0::2 prend un élément sur 2 à partir de 0, 1::2 prend un élément sur 2 à partir de 1\n",
    "# De cette manière, on peut récupérer les à intervalles réguliers \n",
    "dummy0 = dummy[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy1 = dummy[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "dummy2 = dummy[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "dummy3 = dummy[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "print(dummy0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将拼接这些相邻的*图像块*：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 28, 384])\n",
      "torch.Size([2, 784, 384])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.cat([dummy0, dummy1, dummy2, dummy3], -1)  # B H/2 W/2 4*C\n",
    "print(dummy.shape)\n",
    "# On repasse en BxTxC\n",
    "dummy = dummy.view(B, -1, 4 * C)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已将*图像块*的数量减少至原来的四分之一，同时通道数增加了 4 倍。现在，我们将应用线性层来减少通道数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(4 * C, 2 * C, bias=False)\n",
    "dummy = layer(dummy)\n",
    "print(dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们已经准备好构建*合并*（merging）层：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, input_resolution, in_channels, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)\n",
    "        self.norm = norm_layer(4 * in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### swin 模型的构建\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在*Swin Transformer*的实现中，添加*分类标记*（cls_token）较为复杂。因此，我们将采用之前笔记本中提到的另一种方法，即*自适应平均池化*（adaptive average pooling）。这种方法能够确保输出尺寸固定，无论输入图像的大小如何。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 blocs de 2 couches au lieu de 4 car CIFAR-10 a de plus petites images\n",
    "class SwinTransformer(nn.Module):\n",
    "  def __init__(self,n_embed,patch_size,C,window_size,num_heads,img_dim=[16,8,4],depths=[2,2,2]) -> None:\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.proj_layer = nn.Linear(C*patch_size*patch_size, n_embed)\n",
    "    input_resolution = [(img_dim[0],img_dim[0]),(img_dim[1],img_dim[1]),(img_dim[2],img_dim[2])]\n",
    "    self.blocks1 = nn.Sequential(*[swinblock(n_embed,num_heads,input_resolution[0],window_size) for _ in range(depths[0])])\n",
    "    self.down1 = PatchMerging(input_resolution[0],in_channels=n_embed)\n",
    "    self.blocks2 = nn.Sequential(*[swinblock(n_embed*2,num_heads,input_resolution[1],window_size) for _ in range(depths[1])])\n",
    "    self.down2 = PatchMerging(input_resolution[1],in_channels=n_embed*2)\n",
    "    self.blocks3 = nn.Sequential(*[swinblock(n_embed*4,num_heads,input_resolution[2],window_size) for _ in range(depths[2])])\n",
    "    self.classi_head = nn.Linear(n_embed*4, 10)\n",
    "    self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = image_to_patches(x,self.patch_size)\n",
    "    x = self.proj_layer(x)\n",
    "    x = self.blocks1(x)\n",
    "    x = self.down1(x)\n",
    "    x = self.blocks2(x)\n",
    "    x = self.down2(x)\n",
    "    x = self.blocks3(x)\n",
    "    x = self.avgpool(x.transpose(1, 2)).flatten(1)\n",
    "    x = self.classi_head(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagenette 上的训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了测试我们的模型，我们将再次使用 CIFAR-10 数据集，尽管其较小的图像尺寸并不完全适合分层架构。\n",
    "\n",
    "**注意**：您可以选择数据集的一个子集来加速训练过程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille d'une image :  torch.Size([3, 32, 32])\n",
      "taille du train dataset :  40000\n",
      "taille du val dataset :  10000\n",
      "taille du test dataset :  10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# Transformation des données, normalisation et transformation en tensor pytorch\n",
    "transform = T.Compose([T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = datasets.CIFAR10(root='./../data', train=True,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(dataset))[:5000]\n",
    "# dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "testdataset = datasets.CIFAR10(root='./../data', train=False,download=False, transform=transform)\n",
    "# indices = torch.randperm(len(testdataset))[:1000]\n",
    "# testdataset = torch.utils.data.Subset(testdataset, indices)\n",
    "print(\"taille d'une image : \",dataset[0][0].shape)\n",
    "\n",
    "\n",
    "#Création des dataloaders pour le train, validation et test\n",
    "train_dataset, val_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\n",
    "print(\"taille du train dataset : \",len(train_dataset))\n",
    "print(\"taille du val dataset : \",len(val_dataset))\n",
    "print(\"taille du test dataset : \",len(testdataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "val_loader= DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(testdataset, batch_size=16,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 2\n",
    "n_embed = 24\n",
    "n_head = 4\n",
    "C=3 \n",
    "window_size = 4\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.0001 #1e-3\n",
    "\n",
    "model = SwinTransformer(n_embed,patch_size,C,window_size,n_head,img_dim=[16,8,4],depths=[2,2,2]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss train 1.9195597559928894, loss val 1.803518475151062,précision 33.94\n",
      "Epoch 1, loss train 1.7417401003360748, loss val 1.6992134885787964,précision 37.84\n",
      "Epoch 2, loss train 1.651085284280777, loss val 1.6203388486862182,précision 40.53\n",
      "Epoch 3, loss train 1.5808091670751572, loss val 1.5558069843292237,précision 43.03\n",
      "Epoch 4, loss train 1.522760990524292, loss val 1.5169190183639527,précision 44.3\n",
      "Epoch 5, loss train 1.4789127678394318, loss val 1.4665142657279968,précision 47.02\n",
      "Epoch 6, loss train 1.4392719486951828, loss val 1.4568698994636535,précision 47.65\n",
      "Epoch 7, loss train 1.4014943064451217, loss val 1.4456377569198609,précision 48.14\n",
      "Epoch 8, loss train 1.3745941290140151, loss val 1.4345624563694,précision 48.38\n",
      "Epoch 9, loss train 1.3492228104948998, loss val 1.398228020954132,précision 50.04\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss_val += F.cross_entropy(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, loss train {loss_train/len(train_loader)}, loss val {loss_val/len(val_loader)},précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成，我们在验证数据集上获得了 50% 的准确率。\n",
    "\n",
    "现在，我们来看测试数据集上的表现：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision 49.6\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Précision {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据集的准确率与验证数据集基本一致！\n",
    "\n",
    "**注意**：结果不够理想，主要有以下几个原因。首先，我们处理的是小尺寸图像，而*Swin Transformer*的分层架构更适合处理大尺寸图像。其次，我们的实现非常简化，缺少两个关键组件：*移位窗口*和*相对位置偏置*。本笔记本的目的是帮助您理解*Swin*架构的工作原理，而非提供一个完美的实现；）."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
