{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变分自编码器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本课程将介绍**变分自编码器**（Variational Autoencoders, VAE）。我们将从[自编码器课程](../04_Autoencodeurs/README.md)的简要回顾开始，然后介绍如何将 VAE 作为生成模型使用。本课程内容参考了[这篇博客文章](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)，并未深入探讨 VAE 的数学细节。\n",
    "本笔记本中的图示同样来自[该博客文章](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码器回顾\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**自编码器**是一种沙漏形的神经网络。它由两部分组成：\n",
    "- **编码器**：将输入数据压缩到一个低维的潜在空间\n",
    "- **解码器**：从潜在空间的表示中重构原始数据\n",
    "\n",
    "![自编码器结构](./images/AEbase.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自编码器有多种应用，但其**核心功能**是**数据压缩**。它通过**梯度下降优化**实现数据的高效压缩。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直观理解\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设解码器的潜在空间是**规则的**（即服从已知的概率分布），那么我们可以从该分布中**随机采样**一个元素来生成新数据。然而，在传统自编码器中，潜在空间通常**不规则**，因此无法用于数据生成。\n",
    "\n",
    "这是因为自编码器的**损失函数**仅关注重构质量，而**不约束**潜在空间的结构。\n",
    "\n",
    "因此，我们需要**强制**自编码器的潜在空间具有特定结构，以便能够从中生成新数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变分自编码器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**变分自编码器（VAE）**是一种**约束**潜在空间结构的自编码器，使其能够用于**数据生成**。其训练过程经过特殊调整以实现这一目标。\n",
    "\n",
    "与传统自编码器不同，VAE 的编码器将输入数据编码为一个**概率分布**（而非单一值），具体来说，是预测一个**正态分布**的两个参数：\n",
    "- 均值 $\\mu$\n",
    "- 方差 $\\sigma^2$\n",
    "\n",
    "VAE 的训练流程如下：\n",
    "1. 编码器将输入数据编码为概率分布（预测 $\\mu$ 和 $\\sigma^2$）\n",
    "2. 从该高斯分布中**随机采样**一个值\n",
    "3. 解码器根据采样值重构数据\n",
    "4. 通过**反向传播**更新网络权重\n",
    "\n",
    "![VAE 结构](./images/VAE.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了确保训练符合预期，我们需要在损失函数中添加一个**Kullback-Leibler 散度**（[KL 散度](https://zh.wikipedia.org/wiki/%E7%BB%B4%E7%89%B9%E5%B0%94%E5%88%86%E6%95%A3)）项。该项能够**推动**潜在空间的分布接近**标准正态分布**（均值为 0，方差为 1）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了生成**一致性**数据，需要满足两个关键性质：\n",
    "- **连续性**：潜在空间中相近的点，在输出空间中应生成相似的数据\n",
    "- **完备性**：解码后的数据在输出空间中应具有实际意义\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KL 散度**能够保证上述两个性质：\n",
    "- 若仅使用重构损失，VAE 可能退化为传统自编码器（预测接近 0 的方差，相当于单点预测）\n",
    "- KL 散度能够**鼓励**潜在空间中的分布相互接近，从而确保采样生成的数据始终保持一致性\n",
    "\n",
    "![正则化效果](./images/regu.png)\n",
    "\n",
    "> **注**：变分自编码器背后有重要的理论基础，但本课程不做深入探讨。如需了解更多细节，可参考斯坦福大学 CS236 课程，特别是[此链接](https://deepgenerativemodels.github.io/notes/vae/)的内容。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
