{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyamos un GPT desde cero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cuaderno explica cómo crear un modelo de lenguaje para predecir el próximo carácter, basado en la arquitectura del *transformer* (más precisamente, el decodificador).\n",
    "Para ello, utilizamos un archivo de texto `moliere.txt` que contiene todos los diálogos de las obras de Molière.\n",
    "Este conjunto de datos se creó a partir de las obras completas de Molière disponibles en [Gutenberg.org](https://www.gutenberg.org/). He limpiado los datos para conservar solo los diálogos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Pour utiliser le GPU automatiquement si vous en avez un \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura del conjunto de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos abriendo y visualizando el contenido de nuestro conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moliere.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères dans le dataset :  1687290\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de caractères dans le dataset : \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos los primeros 250 caracteres:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALÈRE.\n",
      "\n",
      "Eh bien, Sabine, quel conseil me donnes-tu?\n",
      "\n",
      "SABINE.\n",
      "\n",
      "Vraiment, il y a bien des nouvelles. Mon oncle veut résolûment que ma\n",
      "cousine épouse Villebrequin, et les affaires sont tellement avancées,\n",
      "que je crois qu'ils eussent été mariés dès aujo\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos `set()` para obtener los caracteres únicos presentes en el conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijlmnopqrstuvxyz«»ÇÈÉÊÏàâæçèéêëìîïòôùûŒœ\n",
      "Nombre de caractères différents :  85\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Nombre de caractères différents : \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del conjunto de datos de entrenamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en el curso 5, vamos a crear un *mapping* para convertir caracteres en enteros. Este *mapping* es una forma muy simple de *tokenización*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto rápido sobre la tokenización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué es la tokenización?**\n",
    "La *tokenización* es el proceso que convierte un texto en una secuencia de enteros. Cada entero puede representar un carácter, un grupo de caracteres o una palabra, según el método utilizado.\n",
    "\n",
    "**Equilibrio entre vocabulario y tamaño de secuencia**\n",
    "Un buen *tokenizador* encuentra un equilibrio entre el tamaño del vocabulario (26 para el alfabeto y aproximadamente 100,000 para las palabras en francés). Un vocabulario demasiado pequeño aumenta la longitud de las secuencias (ej.: \"Bonjour\" se convierte en 7 *tokens* si se usan caracteres, o 1 *token* si se usan palabras). En la práctica, los extremos son problemáticos, y se busca un punto medio.\n",
    "\n",
    "**Tokenizadores populares**\n",
    "Los *tokenizadores* son esenciales para el buen funcionamiento de un modelo de lenguaje. Su diseño depende del método y de los datos de entrenamiento. Entre los más utilizados se encuentran [SentencePiece](https://github.com/google/sentencepiece) de Google y [tiktoken](https://github.com/openai/tiktoken) de OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 50, 49, 46, 50, 56, 53, 1, 68, 1, 55, 50, 56, 54]\n",
      "Bonjour à Tous\n"
     ]
    }
   ],
   "source": [
    "# Creation d'un mapping de caractère à entiers et inversement\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encore : prend un string et output une liste d'entiers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: prend une liste d'entiers et output un string\n",
    "\n",
    "print(encode(\"Bonjour à tous\"))\n",
    "print(decode(encode(\"Bonjour à Tous\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a transformar nuestro conjunto de datos en secuencias de enteros y almacenarlo en forma de tensores PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33, 12, 23, 64, 29, 16,  8,  0,  0, 16, 44,  1, 38, 45, 41, 49,  6,  1,\n",
      "        30, 37, 38, 45, 49, 41,  6,  1, 52, 56, 41, 47,  1, 39, 50, 49, 54, 41,\n",
      "        45, 47,  1, 48, 41,  1, 40, 50, 49, 49, 41, 54,  7, 55, 56, 11,  0,  0,\n",
      "        30, 12, 13, 20, 25, 16,  8,  0,  0, 33, 53, 37, 45, 48, 41, 49, 55,  6,\n",
      "         1, 45, 47,  1, 59,  1, 37,  1, 38, 45, 41, 49,  1, 40, 41, 54,  1, 49,\n",
      "        50, 56, 57, 41, 47, 47, 41, 54,  8,  1, 24, 50, 49,  1, 50, 49, 39, 47,\n",
      "        41,  1, 57, 41, 56, 55,  1, 53, 73, 54, 50, 47, 82, 48, 41, 49, 55,  1,\n",
      "        52, 56, 41,  1, 48, 37,  0, 39, 50, 56, 54, 45, 49, 41,  1, 73, 51, 50,\n",
      "        56, 54, 41,  1, 33, 45, 47, 47, 41, 38, 53, 41, 52, 56, 45, 49,  6,  1,\n",
      "        41, 55,  1, 47, 41, 54,  1, 37, 42, 42, 37, 45, 53, 41, 54,  1, 54, 50,\n",
      "        49, 55,  1, 55, 41, 47, 47, 41, 48, 41, 49, 55,  1, 37, 57, 37, 49, 39,\n",
      "        73, 41, 54,  6,  0, 52, 56, 41,  1, 46, 41,  1, 39, 53, 50, 45, 54,  1,\n",
      "        52, 56,  3, 45, 47, 54,  1, 41, 56, 54, 54, 41, 49, 55,  1, 73, 55, 73,\n",
      "         1, 48, 37, 53, 45, 73, 54,  1, 40, 72, 54,  1, 37, 56, 46, 50])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:250]) # Les 250 premiers caractères encodé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a dividir nuestro texto en partes de entrenamiento y validación. Usamos una proporción de 0.9-0.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% pour le train et 10% pour la validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro modelo de lenguaje, también vamos a definir un tamaño de contexto *block_size*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33, 12, 23, 64, 29, 16,  8,  0,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, los 8 primeros caracteres representan el contexto y el 9º es la etiqueta. Este simple ejemplo en realidad agrupa varios casos, ya que nuestro modelo debe predecir el próximo carácter independientemente del contexto. En esta lista, tenemos 8 ejemplos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quand l'entrée est [33] le label est : 12\n",
      "Quand l'entrée est [33 12] le label est : 23\n",
      "Quand l'entrée est [33 12 23] le label est : 64\n",
      "Quand l'entrée est [33 12 23 64] le label est : 29\n",
      "Quand l'entrée est [33 12 23 64 29] le label est : 16\n",
      "Quand l'entrée est [33 12 23 64 29 16] le label est : 8\n",
      "Quand l'entrée est [33 12 23 64 29 16  8] le label est : 0\n",
      "Quand l'entrée est [33 12 23 64 29 16  8  0] le label est : 0\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Quand l'entrée est {context.numpy()} le label est : {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sabemos cómo crear un conjunto de entradas/etiquetas a partir de un solo ejemplo.\n",
    "Adaptemos este método para un procesamiento por *batch*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrée : \n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 69, 39, 41,  2,  0,  0, 27],\n",
      "        [53,  1, 56, 49,  1, 39, 84, 56],\n",
      "        [54, 11,  0,  0, 24, 12, 30, 14],\n",
      "        [ 1, 51, 72, 53, 41,  8,  0,  0]], device='cuda:0')\n",
      "Labels :\n",
      "torch.Size([4, 8])\n",
      "tensor([[69, 39, 41,  2,  0,  0, 27, 19],\n",
      "        [ 1, 56, 49,  1, 39, 84, 56, 53],\n",
      "        [11,  0,  0, 24, 12, 30, 14, 12],\n",
      "        [51, 72, 53, 41,  8,  0,  0, 33]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # La taille de batch (les séquences calculés en parallèles)\n",
    "block_size = 8 # La taille de contexte maximale pour une prédiction du modèle\n",
    "\n",
    "def get_batch(split):\n",
    "    # On genere un batch de données (sur train ou val)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    #On génére batch_size indice de début de séquence pris au hasard dans le dataset\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # On stocke dans notre tenseur torch\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # On met les sur le GPU si on en a un \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('Entrée : ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('Labels :')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada uno de estos 4 ejemplos agrupa 8 ejemplos distintos (como se explicó anteriormente), lo que suma un total de 32 ejemplos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo bigrama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el curso 5 sobre NLP, vimos el bigrama, el modelo de lenguaje más simple. Predice el próximo carácter a partir de un solo carácter de contexto. Denotamos $B$ para el tamaño del *batch*, $T$ para el tamaño del *block* y $C$ para el tamaño del vocabulario.\n",
    "\n",
    "Para probar su rendimiento en el conjunto de datos `moliere.txt`, implémentalo rápidamente en PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 85])\n",
      "tensor(4.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # Chaque token va directement lire la valeur du prochain à partir d'une look-up table entrainé\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # Taille (B,T)\n",
    "    logits = self.token_embedding_table(idx) \n",
    "    # Taille (B,T,C)\n",
    "    \n",
    "    # Pour gérer le cas de la génération (pas de target)\n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else: # Cas de l'entraînement\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx est de la taille (B,T) avec T le contexte actuel\n",
    "    for _ in range(max_new_tokens):\n",
    "      # Forward du modèle pour récuperer les prédictions\n",
    "      logits, _ = self(idx)\n",
    "      # On prend uniquement le dernier caractère\n",
    "      logits = logits[:, -1, :] # devient (B, C)\n",
    "      # On applique la softmax pour récuperer les probabilités\n",
    "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "      # On sample avec torch.multinomial\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # devient (B, 1)\n",
    "      # On ajouter l'élément sample à la séquence actuelle\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo está implementado pero no entrenado. Si lo probamos así, obtenemos resultados catastróficos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CZjb!DzPGŒR?'hô.ù\n",
      "cddhhf,séÇqmp.ÉMjôCùÊF:TAFYèL  àP;zbVmëtuPipL.ôHtSEé,t:æéÉYÈìïë?VGYxoùyçnï'lpôHà!ô\n"
     ]
    }
   ],
   "source": [
    "base=torch.zeros((1, 1), dtype=torch.long).to(device) # Le premier élément est un 0 (token de retour à la ligne)\n",
    "# On génère 100 éléments\n",
    "print(decode(m.generate(idx = base , max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es completamente aleatorio, lo cual es lógico porque el modelo está inicializado aleatoriamente.\n",
    "\n",
    "Ahora vamos a entrenar el modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquilae/anaconda3/envs/dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2493152618408203\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "steps=10000\n",
    "for step in range(steps): # Nombre d'étape d'entraînement (élements traités = steps*batch_size)\n",
    "\n",
    "    # On récupère un batch de données aléatoires\n",
    "    xb, yb = get_batch('train')\n",
    "    # On calcule le loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Retropropagation\n",
    "    loss.backward()\n",
    "    # Mise à jour des poids du modèle\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generemos a partir de nuestro modelo entrenado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "ELASGOXûÏï!\n",
      "ANDann donde se ns ntrar pous fa àTEn!.\n",
      "\n",
      "TELITEL'enomouvûûKbeue\n",
      "SGAvore oue mesontre\n",
      "t de pou n qur quvabou qude dente je père e em'eni\n",
      "\n",
      "La d'euhèmpon, j'es en paiqus de rau plenoilà jonont DARLysontausqus es ei voisangur s ve.\n",
      "\n",
      "\n",
      "\n",
      "DO lar dire tré quseuqu'arme à ai? t pe ne ndome l pa, \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos una mejora en la estructuración de los datos, y algunas palabras parecen casi correctas. Pero el resultado sigue siendo catastrófico, lo cual es lógico porque el bigrama es un modelo demasiado simple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora presentaremos paso a paso el concepto de *self-attention*, un elemento clave de la arquitectura de los *transformers*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué queremos hacer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos con una idea simple. Tenemos un tensor de tamaño $(B,T,C)$. Queremos que cada elemento $T$ sea el promedio del elemento actual y los elementos anteriores, sin tener en cuenta los elementos siguientes. Es la forma más simple de dar importancia a los elementos anteriores para predecir el valor actual (esta es la idea detrás del mecanismo de atención).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Python, podemos implementar esta idea de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création de notre tenseur random\n",
    "B,T,C = 4,4,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5023, -0.5911],\n",
      "        [ 1.0199, -0.2976],\n",
      "        [-1.7581,  0.0969],\n",
      "        [ 0.7444, -0.3360]])\n",
      "tensor([[ 1.5023, -0.5911],\n",
      "        [ 1.2611, -0.4443],\n",
      "        [ 0.2547, -0.2639],\n",
      "        [ 0.3771, -0.2819]])\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne des éléments précédents (incluant l'élément actuel) pour chaque valeur.\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos lo que queríamos: cada elemento corresponde al promedio del elemento actual con los elementos anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, sabemos que los bucles `for` son ineficientes para los cálculos. Preferiríamos una operación matricial para hacer lo mismo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuerdo sobre la multiplicación de matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiplicación matricial**: Matriz $(3 \\times 3)$ por Matriz $(3 \\times 2)$\n",
    "Matrices iniciales\n",
    "\n",
    "Sea la matriz $A$ de dimensiones $(3 \\times 3)$:\n",
    "\n",
    "$A =\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "y la matriz $B$ de dimensiones $(3 \\times 2)$:\n",
    "\n",
    "$B =\n",
    "\\begin{pmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "b_{31} & b_{32}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "La multiplicación matricial $C = A \\times B$ da una matriz $C$ de dimensiones $(3 \\times 2)$:\n",
    "\n",
    "$C =\n",
    "\\begin{pmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22} \\\\\n",
    "c_{31} & c_{32}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "donde cada elemento $c_{ij}$ se calcula de la siguiente manera:\n",
    "\n",
    "$c_{ij} = \\sum_{k=1}^{3} a_{ik} \\cdot b_{kj}$\n",
    "\n",
    "Es decir:\n",
    "\n",
    "- $c_{11} = a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}$\n",
    "- $c_{12} = a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32}$\n",
    "- $c_{21} = a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31}$\n",
    "- $c_{22} = a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}$\n",
    "- $c_{31} = a_{31}b_{11} + a_{32}b_{21} + a_{33}b_{31}$\n",
    "- $c_{32} = a_{31}b_{12} + a_{32}b_{22} + a_{33}b_{32}$\n",
    "\n",
    "Aquí hay un ejemplo en Python que ilustra esto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[7., 6.],\n",
      "        [5., 0.],\n",
      "        [1., 8.]])\n",
      "--\n",
      "c=\n",
      "tensor([[13., 14.],\n",
      "        [13., 14.],\n",
      "        [13., 14.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El truco matemático para el self-attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora es cuando la magia ocurre. Cuando, en lugar de una matriz de 1, tomamos una matriz triangular inferior y volvemos a hacer el cálculo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[1., 2.],\n",
      "        [1., 4.],\n",
      "        [6., 6.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 2.,  6.],\n",
      "        [ 8., 12.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada valor de la matriz es la suma del valor actual y los valores anteriores. ¡Es casi lo que queremos! Solo hay que normalizar según las filas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[1., 2.],\n",
      "        [8., 6.],\n",
      "        [9., 8.]])\n",
      "--\n",
      "c=\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [4.5000, 4.0000],\n",
      "        [6.0000, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Y listo! Hemos reemplazado nuestro doble bucle `for` por una simple multiplicación matricial y una normalización de los valores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a usarlo para calcular *xbow* y comparar su valor con el calculado con nuestro doble bucle:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C) fonctionne grâce au broadcasting de pytorch\n",
    "torch.allclose(xbow, xbow2) # Vérifie que tous les éléments sont identiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de la normalización, podemos usar la función *softmax*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "# On met toutes les valeurs égales à 0 à la valeur -inf\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos aplicar la *softmax* a la matriz y ¡TADAAA!:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica, la versión con *softmax* se utiliza para la capa *self-attention*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention: el corazón del transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualmente, la matriz $wei$ contiene valores uniformes en cada fila, lo que no proporciona ninguna información real sobre la importancia de las informaciones anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es aquí donde interviene el concepto de *self-attention*. Lo que queremos es una matriz $wei$ que podamos entrenar.\n",
    "\n",
    "Vamos a crear 3 valores a partir de nuestro valor $x$:\n",
    "\n",
    "**query**: *¿Qué estoy buscando?* Este valor representa lo que cada posición de la secuencia intenta encontrar en las otras posiciones.\n",
    "\n",
    "**key**: *¿Qué contengo?* Este valor representa lo que cada posición de la secuencia contiene como información, que podría ser relevante para otras posiciones.\n",
    "\n",
    "**value**: *¿Cuál es mi valor?* Este valor representa la información real a extraer de cada posición de la secuencia, si se considera pertinente.\n",
    "\n",
    "Para extraer los valores *query*, *key* y *value*, utilizamos una capa lineal que proyecta la entrada en una dimensión *head_size*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular la importancia de un elemento anterior de la secuencia en relación con el elemento actual, realizamos el producto escalar entre los *query* $Q$ y los *key* $K$ (transpuesta):\n",
    "\n",
    "$wei = QK^T$\n",
    "\n",
    "Para obtener pesos de atención (suma igual a 1), aplicamos la *softmax* y multiplicamos por los *value* $V$:\n",
    "\n",
    "$Output = \\text{softmax}\\left(wei\\right) \\cdot V$\n",
    "\n",
    "![Atención](./images/attention2.png)\n",
    "\n",
    "En Python, se implementa de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "\n",
    "head_size = 16 # Valeur de head_size (projection de x)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # Pour appliquer le softmax, il faut des valeurs -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra matriz $wei$ ahora es completamente entrenable, y es posible usar esta capa para entrenar una red neuronal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notas sobre la capa *self-attention***:\n",
    "\n",
    "- La atención es un mecanismo de comunicación que puede verse como un grafo con conexiones entre los nodos (en nuestro caso, los nodos finales están conectados a todos los nodos anteriores).\n",
    "- En la capa de atención, no hay ninguna noción de la posición de los elementos en relación con los demás. Para solucionar este problema, habrá que añadir un *positionnal_embedding* (ver continuación del curso).\n",
    "- Para ser precisos, no hay ninguna interacción a lo largo de la dimensión *batch*: cada elemento del *batch* se procesa de manera independiente de los demás. Es un poco como si tuviéramos *batch_size* grafos independientes.\n",
    "- Este *block* de atención se llama *decoder block*. Tiene la particularidad de que cada elemento solo se comunica con el pasado (gracias a la matriz triangular inferior). Sin embargo, existen otras capas de atención (*encoder*) que permiten la comunicación de todos los elementos entre sí (para la traducción, el análisis de sentimientos o incluso el procesamiento de imágenes).\n",
    "- Se habla de *self-attention* porque los *query*, *key* y *value* provienen de la misma fuente. Es posible tener *query*, *key* y *value* que provienen de fuentes diferentes: en ese caso se habla de *cross-attention*.\n",
    "- Si lees el papel [Attention is all you need](https://arxiv.org/pdf/1706.03762), notarás que hay una normalización por la raíz de la *head_size*:\n",
    "\n",
    "![Atención](./images/attention.png)\n",
    "\n",
    "Esto permite una estabilidad de la función *softmax* durante la inicialización de los pesos en particular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementemos ahora una clase *head* que realice las operaciones de la *self-attention*. Es simplemente lo que hemos visto anteriormente en forma de clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" Couche de self-attention unique \"\"\"\n",
    "\n",
    "    def __init__(self, head_size,n_embd,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Ajout de dropout pour la regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Le * C**-0.5 correspond à la normalisation par la racine de head_size\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el papel [Attention is all you need](https://arxiv.org/pdf/1706.03762), se propone una variante de la *self-attention*. Esta variante se llama *multi-head attention* y consiste simplemente en tener varias capas de *self-attention* en paralelo. El objetivo de esta capa es paralelizar el procesamiento para que sea más rápido en GPU.\n",
    "\n",
    "![Multi-Head Attention](./images/multihead.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación es bastante simple ya que se trata simplemente de varias capas *head*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Plusieurs couches de self attention en parallèle\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        # Création de num_head couches head de taille head_size\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Couche pour Linear (voir schema) après concatenation\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # Dropout si besoin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capa Feed Forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un último elemento del *transformer* que podemos ver en el papel [Attention is all you need](https://arxiv.org/pdf/1706.03762) es la capa *Feed Forward*, que es simplemente una pequeña red fully connected.\n",
    "\n",
    "Se implementa en Python de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 4*n_embd comme dans le papier\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capa transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos todos los elementos para implementar nuestra capa *transformer*, que utilizará *multi-head attention* y *feed forward*. En la figura principal del papel, también notamos que hay conexiones residuales entre el *input* y el *output* de las capas de *attention* y de *feed forward*. Estas conexiones facilitan el entrenamiento de un modelo profundo (más detalles en el papel [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)). Por lo tanto, también implementaremos estas conexiones residuales. Para la *layer norm*, no entraremos en detalles aquí, pero podemos comparar su utilidad con una capa de *batch norm* (más detalles en este [blogpost](https://medium.com/@hunter-j-phillips/layer-normalization-e9ae93eb3c9c)). Por lo tanto, utilizamos simplemente la implementación PyTorch de la [layer norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
    "\n",
    "Aquí está la implementación Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Block transformer\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # x+ car c'est une connexion résiduelle\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Aplicamos la *layer norm* antes de las capas (a diferencia del papel). Es la única parte del *transformer* que ha sido modificada desde la publicación del papel, y que mejora el rendimiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mayor claridad, vamos a crear nuestro modelo y optimizarlo en el siguiente cuaderno.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
