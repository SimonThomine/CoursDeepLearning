{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difusión (*Broadcasting*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **difusión (*broadcasting***) es un mecanismo que utiliza PyTorch para manejar tensores durante operaciones aritméticas no evidentes.\n",
    "\n",
    "Por ejemplo, está claro que no se puede sumar una matriz de $3 \\times 3$ con otra de $4 \\times 2$, ya que esto generaría un error. Sin embargo, es posible sumar un escalar a una matriz $3 \\times 3$ o un vector de tamaño $3$ a una matriz $3 \\times 3$, aunque la lógica no siempre sea obvia.\n",
    "\n",
    "El [mecanismo de *broadcasting* de PyTorch](https://pytorch.org/docs/stable/notes/broadcasting.html) se basa en reglas simples que deben conocerse al manipular tensores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reglas de *broadcasting*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que dos tensores sean **compatibles con *broadcasting*** (*broadcastables*), deben cumplir las siguientes reglas:\n",
    "- Cada tensor debe tener al menos una dimensión.\n",
    "- Al comparar las dimensiones (empezando por la última), sus tamaños deben ser iguales, o uno de ellos debe ser $1$, o una de las dimensiones debe estar ausente.\n",
    "\n",
    "Utilicemos ejemplos para aclarar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Deux tenseurs de la même taille sont toujours broadcastables\n",
    "x=torch.zeros(5,7,3)\n",
    "y=torch.zeros(5,7,3)\n",
    "\n",
    "# Les deux tenseurs suivants ne sont pas broadcastables car x n'a pas au moins une dimension\n",
    "x=torch.zeros((0,))\n",
    "y=torch.zeros(2,2)\n",
    "\n",
    "# On aligne les dimensions visuellement pour voir si les tenseurs sont broadcastables\n",
    "# En partant de la droite,\n",
    "# 1. x et y ont la même taille et sont de taille 1\n",
    "# 2. y est de taille 1\n",
    "# 3. x et y ont la même taille\n",
    "# 4. la dimension de y n'existe pas\n",
    "# Les deux tenseurs sont donc broadcastables\n",
    "x=torch.zeros(5,3,4,1)\n",
    "y=torch.zeros(  3,1,1)\n",
    "\n",
    "# A l'inverse, ces deux tenseurs ne sont pas broadcastables car 3. x et y n'ont pas la même taille\n",
    "x=torch.zeros(5,2,4,1)\n",
    "y=torch.zeros(  3,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que sabemos identificar dos tensores compatibles con *broadcasting*, definamos las reglas que se aplican durante las operaciones entre ellos:\n",
    "\n",
    "Las reglas son:\n",
    "- **Regla 1**: Si el número de dimensiones de $x$ e $y$ difiere, se añade un $1$ al inicio de las dimensiones del tensor con menos dimensiones para alinearlos.\n",
    "- **Regla 2**: Para cada dimensión, el tamaño resultante es el máximo entre los tamaños de $x$ e $y$.\n",
    "\n",
    "El tensor cuya dimensión se modifica se duplicará las veces necesarias para ajustarse.\n",
    "\n",
    "**Nota**: Si dos tensores no son compatibles con *broadcasting*, su suma generará un error. Sin embargo, en muchos casos, el *broadcasting* funcionará pero no producirá el resultado esperado. Por eso es crucial dominar estas reglas.\n",
    "\n",
    "Retomemos los dos ejemplos:\n",
    "\n",
    "Sumar un escalar a una matriz $3 \\times 3$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  tensor([[ 0.6092, -0.6887,  0.3060],\n",
      "        [ 1.3496,  1.7739, -0.4011],\n",
      "        [-0.8876,  0.7196, -0.3810]])\n",
      "y :  tensor(1)\n",
      "x+y :  tensor([[1.6092, 0.3113, 1.3060],\n",
      "        [2.3496, 2.7739, 0.5989],\n",
      "        [0.1124, 1.7196, 0.6190]])\n",
      "x+y shape :  torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(3,3)\n",
    "y=torch.tensor(1)\n",
    "print(\"x : \" ,x)\n",
    "print(\"y : \" ,y)\n",
    "print(\"x+y : \" ,x+y)\n",
    "print(\"x+y shape : \",(x+y).shape)\n",
    "# Le tenseur y est broadcasté pour avoir la même taille que x, il se transforme en tenseur de 1 de taille 3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumar un vector de tamaño $3$ a una matriz $3 \\times 3$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  tensor([[ 0.9929, -0.1435,  1.5740],\n",
      "        [ 1.2143,  1.3366,  0.6415],\n",
      "        [-0.2718,  0.3497, -0.2650]])\n",
      "y :  tensor([1, 2, 3])\n",
      "x+y :  tensor([[1.9929, 1.8565, 4.5740],\n",
      "        [2.2143, 3.3366, 3.6415],\n",
      "        [0.7282, 2.3497, 2.7350]])\n",
      "x+y shape :  torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(3,3)\n",
    "y=torch.tensor([1,2,3]) # tenseur de taille 3\n",
    "print(\"x : \" ,x)\n",
    "print(\"y : \" ,y)\n",
    "print(\"x+y : \" ,x+y)\n",
    "print(\"x+y shape : \",(x+y).shape)\n",
    "# Le tenseur y est broadcasté pour avoir la même taille que x, il se transforme en tenseur de 1 de taille 3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analicemos ahora otros ejemplos más complejos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x+y shape :  torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(5,3,4,1)\n",
    "y=torch.zeros(  3,1,1)\n",
    "print(\"x+y shape : \",(x+y).shape)\n",
    "# Le tenseur y a été étendu en taille 1x3x1x1 (règle 1) puis dupliqué en taille 5x3x4x1 (règle 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x+y shape :  torch.Size([3, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "x=torch.empty(1)\n",
    "y=torch.empty(3,1,7)\n",
    "print(\"x+y shape : \",(x+y).shape)\n",
    "# Le tenseur y a été étendu en taille 1x1x1 (règle 1) puis dupliqué en taille 3x1x7 (règle 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx+y shape : \u001b[39m\u001b[38;5;124m\"\u001b[39m,(\u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43my\u001b[49m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# L'opération n'est pas possible car les tenseurs ne sont pas broadcastables (dimension 3 en partant de la fin ne correspond pas)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "x=torch.empty(5,2,4,1)\n",
    "y=torch.empty(3,1,1)\n",
    "print(\"x+y shape : \",(x+y).shape)\n",
    "# L'opération n'est pas possible car les tenseurs ne sont pas broadcastables (dimension 3 en partant de la fin ne correspond pas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros aspectos a considerar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación con escalares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No siempre lo tenemos en cuenta, pero esto permite realizar comparaciones de manera sencilla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False, False])\n",
      "tensor([False,  True, False])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([10., 0, -4])\n",
    "print(a > 0)\n",
    "print(a==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible comparar dos tensores entre sí:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True, False])\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor([False,  True, False])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3])\n",
    "b=torch.tensor([4,2,6])\n",
    "# Comparaison élément par élément\n",
    "print(a==b)\n",
    "# Comparaison élément par élément et égalité pour tous les éléments\n",
    "print((a==b).all())\n",
    "# Comparaison élément par élément et égalité pour au moins un élément\n",
    "print((a==b).any())\n",
    "# Comparaison avec supérieur ou égal\n",
    "print(a>=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto puede ser muy útil para crear **máscaras** a partir de un umbral, por ejemplo, o para verificar si dos operaciones son equivalentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de `unsqueeze()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente vimos que es posible aplicar *broadcasting* a un tensor de tamaño $3$ hacia una matriz de $3 \\times 3$. PyTorch lo transforma automáticamente a un tamaño $1 \\times 3$ para realizar la operación. Sin embargo, puede que queramos hacer la operación en el otro sentido, es decir, sumar un tensor $3 \\times 1$ a una matriz de $3 \\times 3$.\n",
    "\n",
    "En este caso, debemos reemplazar manualmente la **Regla 1** usando la función [`unsqueeze()`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html), que permite añadir una dimensión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y :  tensor([[ 1.3517,  1.1880,  0.4483],\n",
      "        [ 0.5137, -0.5406, -0.1412],\n",
      "        [-0.0108,  1.3757,  0.6112]])\n",
      "x+y :  tensor([[2.3517, 3.1880, 3.4483],\n",
      "        [1.5137, 1.4594, 2.8588],\n",
      "        [0.9892, 3.3757, 3.6112]])\n",
      "x shape :  torch.Size([3, 1])\n",
      "x+y :  tensor([[2.3517, 2.1880, 1.4483],\n",
      "        [2.5137, 1.4594, 1.8588],\n",
      "        [2.9892, 4.3757, 3.6112]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2,3])\n",
    "y=torch.randn(3,3)\n",
    "print(\"y : \",y )\n",
    "print(\"x+y : \",x+y) \n",
    "\n",
    "x=x.unsqueeze(1)\n",
    "print(\"x shape : \",x.shape)\n",
    "print(\"x+y : \",x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pueden ver, pudimos eludir las reglas de PyTorch para obtener el resultado deseado.\n",
    "\n",
    "**Nota**:\n",
    "- La **Regla 1** de PyTorch equivale a aplicar `x.unsqueeze(0)` hasta que el número de dimensiones sea el mismo.\n",
    "- Es posible reemplazar `unsqueeze()` por `None` de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([1,2,3])\n",
    "# La première opération est l'équivalent de unsqueeze(0) et la seconde de unsqueeze(1)\n",
    "x[None].shape,x[...,None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de `keepdim`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones de PyTorch que reducen el tamaño de un tensor a lo largo de una dimensión (como [`torch.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html) para sumar o [`torch.mean`](https://pytorch.org/docs/stable/generated/torch.mean.html) para calcular la media) tienen un parámetro útil en ciertos casos.\n",
    "\n",
    "Estas operaciones modifican las dimensiones del tensor y eliminan automáticamente la dimensión sobre la cual se realizó la operación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(3,4,5)\n",
    "print(x.shape)\n",
    "x=x.sum(dim=1) # somme sur la dimension 1\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si desea conservar la dimensión sobre la cual se realiza la suma, puede usar el argumento `keepdim=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([3, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(3,4,5)\n",
    "print(x.shape)\n",
    "x=x.sum(dim=1,keepdim=True) # somme sur la dimension 1\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto puede ser muy útil para evitar errores con las dimensiones. Analicemos un caso en el que esto afecta al *broadcasting*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les deux opérations sont elles équivalentes ? : False\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(3,4,5)\n",
    "y=torch.randn(1,1,1)\n",
    "x_sum=x.sum(dim=1)\n",
    "x_sum_keepdim=x.sum(dim=1,keepdim=True)\n",
    "print(\"Les deux opérations sont elles équivalentes ? :\",(x_sum+y==x_sum_keepdim+y).all().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es lo que ocurrió:\n",
    "- En el primer caso, `x_sum` tiene un tamaño de $3 \\times 5$. La **Regla 1** lo transforma en $1 \\times 3 \\times 5$, y la **Regla 2** transforma $y$ en $1 \\times 3 \\times 5$.\n",
    "- En el segundo caso, `x_sum_keepdim` tiene un tamaño de $3 \\times 1 \\times 5$, y la **Regla 2** transforma $y$ en $1 \\times 3 \\times 5$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notación de Einstein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección no está directamente relacionada con el *broadcasting*, pero es importante conocerla.\n",
    "\n",
    "Para multiplicar matrices en PyTorch, hemos utilizado el operador `@` (o `torch.matmul`) hasta ahora. Sin embargo, existe otro método para realizar multiplicaciones matriciales mediante la **suma de Einstein** ([`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html)).\n",
    "\n",
    "Se trata de una notación compacta para expresar productos y sumas. Por ejemplo:\n",
    "**ik,kj -> ij**\n",
    "El lado izquierdo representa las dimensiones de las entradas, separadas por comas. Aquí tenemos dos tensores, cada uno con dos dimensiones (**i,k** y **k,j**). El lado derecho representa las dimensiones del resultado, es decir, un tensor de dimensiones **i,j**.\n",
    "\n",
    "Las reglas de la notación de Einstein son:\n",
    "- Los índices repetidos a la izquierda se suman implícitamente si no aparecen a la derecha.\n",
    "- Cada índice puede aparecer, como máximo, dos veces a la izquierda.\n",
    "- Los índices no repetidos a la izquierda deben aparecer a la derecha.\n",
    "\n",
    "Se puede utilizar para diversas operaciones:\n",
    "```python\n",
    "torch.einsum('ij->ji', a)\n",
    "```\n",
    "devuelve la matriz transpuesta de $a$.\n",
    "\n",
    "Mientras que\n",
    "```python\n",
    "torch.einsum('bi,ij,bj->b', a, b, c)\n",
    "```\n",
    "devuelve un vector de tamaño $b$ donde la coordenada $k$-ésima es la suma de $a[k,i] \\cdot b[i,j] \\cdot c[k,j]$. Esta notación es especialmente práctica cuando se manipulan *batches* con múltiples dimensiones. Por ejemplo, si tienes dos lotes de matrices y quieres calcular el producto matricial por *batch*, puedes usar:\n",
    "```python\n",
    "torch.einsum('bik,bkj->bij', a, b)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es un método práctico para realizar multiplicaciones matriciales en PyTorch. Además, es muy rápido y, a menudo, la forma más eficiente de llevar a cabo operaciones personalizadas en PyTorch.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
