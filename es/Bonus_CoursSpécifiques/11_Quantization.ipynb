{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuantización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos de Deep Learning se están volviendo cada vez más potentes y voluminosos. Tomemos como ejemplo los LLM (Large Language Models): los mejores modelos de código abierto, como Llama 3.1, ahora tienen cientos de miles de millones de parámetros.\n",
    "\n",
    "Cargar un modelo así en una sola GPU es imposible. Incluso con la GPU más potente del mercado (H100, con 80 GB de VRAM), se necesitan varias GPU para la inferencia y aún más para el entrenamiento.\n",
    "\n",
    "En la práctica, se observa que cuanto más parámetros tiene un modelo, mejores son sus resultados. Por lo tanto, no queremos reducir el tamaño de los modelos. Sin embargo, buscamos disminuir el espacio de memoria que ocupan.\n",
    "\n",
    "Este curso se inspira fuertemente en dos artículos: [una guía visual sobre la cuantización](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-5098) y [una explicación detallada de QLoRA](https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766). Las imágenes utilizadas también provienen de estos dos artículos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo representar los números en una computadora?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar los números de punto flotante en una computadora, se utiliza un cierto número de bits. La norma [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754) describe cómo los bits pueden representar un número. Esto se hace mediante tres partes: el signo, el exponente y la mantisa.\n",
    "\n",
    "Aquí tienes un ejemplo de representación FP16 (16 bits):\n",
    "\n",
    "![FP16](./images/Fp16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El signo determina el signo del número, el exponente da los dígitos antes del punto decimal y la mantisa los dígitos después del punto decimal. Aquí tienes un ejemplo en imagen de cómo convertir la representación FP16 a un número.\n",
    "\n",
    "![Convertir](./images/convert.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, cuanto más bits se utilizan para representar un valor, más preciso puede ser o cubrir un rango más amplio de valores. Por ejemplo, podemos comparar la precisión FP16 y FP32:\n",
    "\n",
    "![Comparar FP](./images/compareFP.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una última cosa importante a saber: hay dos formas de evaluar una representación. Por un lado, el *rango dinámico* que indica el rango de valores que se pueden representar, y la *precisión* que describe la diferencia entre dos valores cercanos.\n",
    "\n",
    "Cuanto mayor es el exponente, mayor es el *rango dinámico*, y cuanto mayor es la mantisa, mayor es la *precisión* (por lo tanto, dos valores cercanos son cercanos).\n",
    "\n",
    "En el aprendizaje profundo, a menudo preferimos usar la representación BF16 en lugar de FP16. La representación BF16 tiene un exponente más grande pero una precisión más baja.\n",
    "\n",
    "La figura siguiente ilustra las diferencias:\n",
    "\n",
    "![BF16](./images/BF16.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que entendemos los conceptos de precisión de los números de punto flotante, podemos calcular el espacio que ocupa un modelo en memoria según la precisión. En FP32, un número se representa con 32 bits, lo que equivale a 4 bytes (un byte vale 8 bits). Para obtener el uso de memoria de un modelo, podemos hacer el siguiente cálculo:\n",
    "$memory= \\frac{n_{bits}}{8}*n_{params}$\n",
    "\n",
    "Tomemos el ejemplo de un modelo de 70 mil millones de parámetros a diferentes niveles de precisión: doble (FP64), full-precision (FP32) y half-precision (FP16).\n",
    "Para FP64: $\\frac{64}{8} \\times 70B = 560GB$\n",
    "Para FP32: $\\frac{32}{8} \\times 70B = 280GB$\n",
    "Para FP16: $\\frac{16}{8} \\times 70B = 140GB$\n",
    "\n",
    "Nos damos cuenta de que es necesario encontrar una manera de reducir el tamaño de los modelos. Aquí, incluso el modelo en half-precision ocupa 140 GB, lo que equivale a 2 GPU H100.\n",
    "\n",
    "**Nota**: Aquí hablamos de la precisión para la inferencia. Para el entrenamiento, como hay que mantener las activaciones en memoria para el descenso de gradiente, terminamos con muchos más parámetros (ver parte sobre QLoRA más adelante en el curso).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a la cuantización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de la cuantización es reducir la precisión de un modelo pasando de una precisión alta como FP32 a una precisión más baja como INT8.\n",
    "\n",
    "**Nota**: INT8 es la forma de representar enteros de -127 a 127 en 8 bits.\n",
    "\n",
    "![Cuantización](./images/quantization.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, al reducir el número de bits para representar los valores, perdemos precisión.\n",
    "Para ilustrar esto, veamos una imagen:\n",
    "\n",
    "![Galletas](./images/cookies.webp)\n",
    "\n",
    "Notamos un \"grano\" en la imagen, debido a la falta de colores disponibles para representarla.\n",
    "Lo que queremos es reducir el número de bits para representar la imagen mientras conservamos al máximo la precisión de la imagen original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen varias formas de hacer cuantización: la cuantización simétrica y la cuantización asimétrica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto rápido sobre las precisiones comunes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FP16**: La *precisión* y el *rango dinámico* disminuyen en comparación con FP32.\n",
    "\n",
    "![FP16](./images/fp16.webp)\n",
    "\n",
    "**BF16**: La *precisión* disminuye fuertemente, pero el *rango dinámico* se mantiene igual en comparación con FP32.\n",
    "\n",
    "![BF16](./images/bf16.webp)\n",
    "\n",
    "**INT8**: Pasamos a una representación en entero.\n",
    "\n",
    "![INT8](./images/int8.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuantización simétrica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la cuantización simétrica, el rango de valores de nuestros flotantes originales se mapea de manera simétrica en el rango de valores de cuantización. Esto significa que el 0 en los flotantes se mapea en el 0 en la precisión de cuantización.\n",
    "\n",
    "![Cuantización simétrica](./images/symmetricq.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las formas más comunes y simples de realizar esta operación es usar el método *absmax (cuantización de máximo absoluto)*. Tomamos el valor máximo (en valor absoluto) y realizamos el mapeo en relación con este valor:\n",
    "\n",
    "![Absmax](./images/absmax.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fórmula es bastante básica: consideremos $b$ el número de bytes que queremos cuantizar, $\\alpha$ el valor absoluto más grande.\n",
    "Entonces podemos calcular el *factor de escala* de la siguiente manera:\n",
    "$s=\\frac{2^{b-1}-1}{\\alpha}$\n",
    "Luego podemos realizar la cuantización de $x$ de esta manera:\n",
    "$x_{quantized}=round(s \\times x)$\n",
    "Para desquantizar y recuperar un valor FP32, podemos hacerlo así:\n",
    "$x_{dequantized}=\\frac{x_{quantized}}{s}$\n",
    "\n",
    "Por supuesto, el valor desquantizado no será equivalente al valor antes de la cuantización:\n",
    "\n",
    "![Ejemplo Absmax](./images/absmaxExample.png)\n",
    "\n",
    "y podemos cuantizar los errores de cuantización:\n",
    "\n",
    "![Error Absmax](./images/absmaxError.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuantización asimétrica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de la cuantización simétrica, la cuantización asimétrica no es simétrica alrededor de 0. En su lugar, mapeamos el mínimo $\\beta$ y el máximo $\\alpha$ del *rango* de los flotantes originales al mínimo y máximo del *rango* cuantizado.\n",
    "El método más común para esto se llama *cuantización de punto cero*.\n",
    "\n",
    "![Cuantización asimétrica](./images/asymetric.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este método, el 0 ha cambiado de posición, por eso este método se llama asimétrico.\n",
    "\n",
    "Como el 0 ha sido desplazado, necesitamos calcular la posición del 0 (*punto cero*) para realizar el mapeo lineal.\n",
    "\n",
    "Podemos cuantizar de la siguiente manera:\n",
    "$s=\\frac{128 - - 127}{\\alpha- \\beta}$\n",
    "Calculamos el *punto cero*:\n",
    "$z=round(-s \\times \\beta)-2^{b-1}$\n",
    "y:\n",
    "$x_{quantized}=round(s \\times x + z)$\n",
    "Para desquantizar, podemos aplicar la siguiente fórmula:\n",
    "$x_{dequantized}=\\frac{x_{quantized}-z}{s}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos métodos tienen sus ventajas y desventajas, podemos compararlos mirando el comportamiento en un $x$ cualquiera:\n",
    "\n",
    "![Comparación](./images/compare.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorte y modificación de rango\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos que hemos presentado tienen un defecto mayor. Estos métodos no son en absoluto robustos a los *valores atípicos*. Imaginemos que nuestro vector $x$ contiene los siguientes valores: [-0.59, -0.21, -0.07, 0.13, 0.28, 0.57, 256]. Si hacemos nuestro *mapeo* habitual, obtendremos valores idénticos para todos los elementos excepto para el *valor atípico* (256):\n",
    "\n",
    "![Valor atípico](./images/outlier.png)\n",
    "\n",
    "Esto es muy problemático porque la pérdida de información es colosal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica, podemos decidir *recortar* ciertos valores para disminuir el *rango* en el espacio de los flotantes (antes de aplicar la cuantización). Por ejemplo, podríamos decidir limitar los valores en el rango [-5,5] y todos los valores fuera de este rango se mapearán a los valores máximos o mínimos de cuantización (127 o -127 para INT8):\n",
    "\n",
    "![Recorte](./images/clipping.png)\n",
    "\n",
    "Al hacer esto, reducimos enormemente el error en los no-*valores atípicos* pero lo aumentamos para los *valores atípicos* (lo que también puede ser problemático).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibración\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la parte anterior, usamos arbitrariamente un rango de valores de [-5,5]. La selección de este rango de valores no es aleatoria y está determinada por un método llamado *calibración*. La idea es encontrar un rango de valores que minimice el error de cuantización para el conjunto de valores. Los métodos de *calibración* utilizados son diferentes según el tipo de parámetros que queremos cuantificar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calibración para los pesos y los sesgos**:\n",
    "Los pesos y los sesgos son valores estáticos (fijos después del entrenamiento del modelo). Son valores que conocemos antes de realizar la inferencia.\n",
    "A menudo, como hay muchos más pesos que sesgos, conservamos la precisión base en los sesgos y realizamos la cuantización solo en los pesos.\n",
    "\n",
    "Para los pesos, hay varios métodos de calibración posibles:\n",
    "- Podemos elegir manualmente un porcentaje del rango de entrada\n",
    "- Podemos optimizar la distancia MSE entre los pesos base y los pesos cuantizados\n",
    "- Podemos minimizar la entropía (con la divergencia KL) entre los pesos base y los pesos cuantizados\n",
    "\n",
    "El método con porcentaje es similar al método que hemos utilizado anteriormente. Los otros dos métodos son más rigurosos y eficaces.\n",
    "\n",
    "**Calibración para las activaciones**:\n",
    "A diferencia de los pesos y los sesgos, las activaciones dependen del valor de entrada del modelo. Por lo tanto, es muy complicado cuantificarlas de manera eficiente. Estos valores se actualizan después de cada capa y solo podemos conocer sus valores durante la inferencia cuando la capa del modelo procesa los valores.\n",
    "Esto nos lleva a la siguiente parte que trata sobre dos métodos diferentes para la cuantización de las activaciones (y también de los pesos).\n",
    "Estos métodos son:\n",
    "- La *cuantización post-entrenamiento* (PTQ): la cuantización ocurre después del entrenamiento del modelo\n",
    "- El *entrenamiento con conocimiento de cuantización* (QAT): la cuantización se realiza durante el entrenamiento o el *fine-tuning* del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuantización Post-Entrenamiento (PTQ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las formas más frecuentes de hacer cuantización es realizarla después del entrenamiento del modelo. Desde un punto de vista práctico, esto es bastante lógico porque no requiere entrenar o *fine-tunear* el modelo.\n",
    "\n",
    "La cuantización de los pesos se realiza utilizando ya sea la cuantización simétrica o la cuantización asimétrica.\n",
    "\n",
    "Para las activaciones, no es lo mismo ya que no conocemos el rango de valores tomados por la distribución de las activaciones.\n",
    "Tenemos dos formas de cuantización para las activaciones:\n",
    "- La cuantización dinámica\n",
    "- La cuantización estática\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuantización dinámica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la cuantización dinámica, recopilamos las activaciones después de que los datos pasen por una capa. La distribución de la capa se cuantiza luego calculando el *punto cero* y el *factor de escala*.\n",
    "\n",
    "![Cuantización dinámica](./images/dynamicQ.webp)\n",
    "\n",
    "En este proceso, cada capa tiene sus propios valores de *punto cero* y *factor de escala* y, por lo tanto, la cuantización no es la misma.\n",
    "\n",
    "![Cuantización dinámica 2](./images/dynamicQ2.webp)\n",
    "\n",
    "**Nota**: Este proceso de cuantización ocurre **durante** la inferencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Este proceso de cuantización ocurre **durante** la inferencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuantización estática\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de la *cuantización dinámica*, la *cuantización estática* no calcula el *punto cero* y el *factor de escala* durante la inferencia. De hecho, en el método de cuantización estática, los valores de *punto cero* y *factor de escala* se calculan antes de la inferencia utilizando un *conjunto de datos* de *calibración*. Este *conjunto de datos* se supone que es representativo de los datos y permite calcular las distribuciones potenciales tomadas por las activaciones.\n",
    "\n",
    "![Cuantización estática](./images/staticQ.png)\n",
    "\n",
    "Después de recopilar los valores de las activaciones en todo el *conjunto de datos* de *calibración*, podemos usarlos para calcular el *factor de escala* y el *punto cero* que luego se utilizarán para todas las activaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencia entre cuantización dinámica y estática\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, la *cuantización dinámica* es un poco más precisa porque calcula los valores de *factor de escala* y *punto cero* para cada capa, pero este proceso también tiende a ralentizar el tiempo de inferencia.\n",
    "\n",
    "A la inversa, la *cuantización estática* es menos precisa pero más rápida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTQ: la cuantización en 4 bits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ideal, nos gustaría llevar la cuantización al máximo, es decir, 4 bits en lugar de 8 bits. En la práctica, esto no es fácil porque aumenta drásticamente el error si usamos simplemente los métodos que hemos visto hasta ahora.\n",
    "\n",
    "Sin embargo, hay algunos métodos que permiten reducir el número de bits hasta 2 bits (se recomienda quedarse en 4 bits).\n",
    "\n",
    "Entre estos métodos, encontramos dos principales:\n",
    "- GPTQ (usa solo la GPU)\n",
    "- GGUF (también puede usar la CPU en parte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPTQ es probablemente el método más utilizado para la cuantización de 4 bits. La idea es usar la cuantización asimétrica en cada capa de manera independiente:\n",
    "\n",
    "![GPTQ](./images/GPTQ.png)\n",
    "\n",
    "Durante el proceso de cuantización, los pesos se convierten en el inverso de la matriz Hessian (segunda derivada de la función de *pérdida*) lo que nos permite saber si la salida del modelo es sensible a los cambios de cada peso. De manera simplificada, esto permite calcular la importancia de cada peso en una capa. Los pesos asociados a valores pequeños en la Hessian son los más importantes porque un cambio de estos pesos afectará significativamente al modelo.\n",
    "\n",
    "![Hessian](./images/hessian.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego cuantificamos y desquantificamos los pesos para obtener nuestro *error de cuantización*. Este error nos permite ponderar el error de cuantización en relación con el error real y la matriz Hessian.\n",
    "\n",
    "![Error GPTQ](./images/GPTQError.png)\n",
    "\n",
    "El error ponderado se calcula de la siguiente manera:\n",
    "$q=\\frac{x_1-y_1}{h_1}$ donde $x_1$ es el valor antes de la cuantización, $y_1$ es el valor después de la cuantización/descuantización y $h_1$ es el valor correspondiente en la matriz Hessian.\n",
    "\n",
    "Luego redistribuimos este error de cuantización ponderado en los otros pesos de la línea. Esto permite mantener la función global y la salida de la red. Por ejemplo, para $x_2$:\n",
    "$x_2=x_2 + q \\times h_2$\n",
    "\n",
    "![Proceso GPTQ](./images/GPTQprocess.png)\n",
    "\n",
    "Hacemos este proceso hasta que todos los valores estén cuantizados.\n",
    "En la práctica, este método funciona bien porque todos los pesos están correlacionados entre sí, por lo que si un peso tiene un gran error de cuantización, los otros pesos se cambian para compensar el error (basado en la Hessian).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGUF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPTQ es un muy buen método para ejecutar un LLM en una GPU. Sin embargo, incluso con esta cuantización, a veces no tenemos suficiente memoria GPU para ejecutar un modelo LLM profundo. El método GGUF permite mover cualquier capa del LLM a la CPU.\n",
    "\n",
    "De esta manera, podemos usar la memoria RAM y la memoria de video (VRAM) al mismo tiempo.\n",
    "\n",
    "Este método de cuantización se cambia con frecuencia y depende del nivel de bits de cuantización que queramos.\n",
    "\n",
    "En general, el método funciona de la siguiente manera:\n",
    "\n",
    "Primero, los pesos de una capa se dividen en *bloques super* donde cada *bloque super* se divide nuevamente en *bloques sub*. Luego extraemos los valores $s$ y $\\alpha$ (*absmax*) para cada *bloque* (el *super* y los *sub*).\n",
    "\n",
    "![GGUF](./images/GGUF.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *factores de escala* $s$ de los *bloques sub* luego se cuantizan nuevamente usando la información del *bloque super* (que tiene su propio *factor de escala*). Este método se llama *cuantización por bloques*.\n",
    "\n",
    "**Nota**: En general, el nivel de cuantización es diferente entre los *bloques sub* y el *bloque super*: el *bloque super* tiene una precisión superior a los *bloques sub* la mayoría de las veces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con Conocimiento de Cuantización (QAT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de realizar la cuantización después del entrenamiento, podemos hacerlo durante el entrenamiento. De hecho, hacer la cuantización después del entrenamiento no tiene en cuenta el proceso de entrenamiento, lo que puede causar problemas.\n",
    "\n",
    "El *entrenamiento con conocimiento de cuantización* es un método que permite realizar la cuantización durante el entrenamiento y aprender los diferentes parámetros de cuantización durante la retropropagación:\n",
    "\n",
    "![QAT](./images/QAT.png)\n",
    "\n",
    "En la práctica, este método a menudo es más preciso que la PTQ porque la cuantización ya está prevista durante el entrenamiento y, por lo tanto, podemos adaptar el modelo específicamente con el objetivo de cuantizarlo en el futuro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enfoque funciona de la siguiente manera:\n",
    "Durante el entrenamiento, se introduce un proceso de cuantización/descuantización (*cuantización falsa*) (cuantización de 32 bits a 4 bits y luego descuantización de 4 bits a 32 bits, por ejemplo).\n",
    "\n",
    "![Cuantización falsa](./images/fakequantize.png)\n",
    "\n",
    "Este enfoque permite que el modelo considere la cuantización durante el entrenamiento y, por lo tanto, adapte la actualización de los pesos para favorecer buenos resultados del modelo cuantizado.\n",
    "\n",
    "Una forma de ver las cosas es imaginar que el modelo convergerá hacia mínimos amplios que minimizan el error de cuantización en lugar de mínimos estrechos que podrían causar errores durante la cuantización. Para un modelo entrenado sin *cuantización falsa*, no habría preferencias sobre el mínimo elegido para la convergencia:\n",
    "\n",
    "![Mínimos](./images/minimums.png)\n",
    "\n",
    "En la práctica, los modelos entrenados de manera clásica tienen un *pérdida* más baja que los modelos entrenados con QAT cuando la precisión es alta (FP32), pero tan pronto como cuantizamos el modelo, el modelo QAT será mucho más potente que un modelo cuantizado mediante un método PTQ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BitNet: cuantización de 1 bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ideal para reducir el tamaño de un modelo sería cuantizarlo en un solo bit. Esto parece una locura, ¿cómo podemos imaginar representar una red neuronal con solo 0 y 1 para cada peso?\n",
    "\n",
    "[BitNet](https://arxiv.org/pdf/2310.11453) propone representar los pesos de un modelo con un solo bit usando el valor -1 o 1 para un peso. Hay que imaginar que reemplazamos las capas lineales de la arquitectura transformers por capas BitLinear:\n",
    "\n",
    "![BitTransformer](./images/bitTransformer.png)\n",
    "\n",
    "La capa BitLinear funciona exactamente como una capa lineal básica, excepto que los pesos se representan con un único bit y las activaciones en INT8.\n",
    "\n",
    "Como se explicó anteriormente, hay una forma de *cuantización falsa* que permite al modelo aprender el efecto de la cuantización para forzarlo a adaptarse a esta nueva restricción:\n",
    "\n",
    "![BitNet](./images/bitnet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analicemos esta capa paso a paso:\n",
    "\n",
    "**Primer paso: Cuantización de los pesos**\n",
    "Durante el entrenamiento, los pesos se almacenan en INT8 y se cuantizan en 1-bit usando la función *signo*.\n",
    "Esta función simplemente centra la distribución de los pesos en 0 y convierte todo lo que es menor a 0 en -1 y todo lo que es mayor a 0 en 1.\n",
    "\n",
    "![Cuantización de los pesos](./images/weigthquanti.png)\n",
    "\n",
    "También se extrae un valor $\\beta$ (valor absoluto promedio) para el proceso de descuantización.\n",
    "\n",
    "**Segundo paso: Cuantización de las activaciones**\n",
    "Para las activaciones, la capa BitLinear usa la cuantización *absmax* para convertir de FP16 a INT8 y un valor $\\alpha$ (valor absoluto máximo) se almacena para la descuantización.\n",
    "\n",
    "**Tercer paso: Descuantización**\n",
    "A partir de los $\\alpha$ y $\\beta$ que hemos guardado, podemos usar estos valores para descuantizar y volver a la precisión FP16.\n",
    "\n",
    "Y eso es todo, el procedimiento es bastante simple y permite que el modelo se represente con solo -1 y 1.\n",
    "\n",
    "Los autores del artículo notaron que, usando esta técnica, obtenemos buenos resultados en modelos bastante profundos (más de 30B), pero los resultados son bastante mediocres para modelos más pequeños.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BitNet 1.58: ¡Necesitamos el cero!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método [BitNet1.58](https://arxiv.org/pdf/2402.17764) fue introducido para mejorar el modelo anterior, especialmente para el caso de modelos más pequeños.\n",
    "En este método, los autores proponen agregar el valor 0 además de -1 y 1. Esto no parece ser un gran cambio, pero este método permite mejorar enormemente el modelo BitNet original.\n",
    "\n",
    "**Nota**: El modelo se apoda 1.58 bits porque $log_2(3)=1.58$, por lo tanto, teóricamente, una representación de 3 valores usa 1.58 bits.\n",
    "\n",
    "Pero entonces, ¿por qué es útil el 0?\n",
    "En realidad, solo necesitamos volver a lo básico y mirar la multiplicación matricial.\n",
    "Una multiplicación matricial se puede descomponer en dos operaciones: la multiplicación de los pesos dos por dos y la suma de todos estos pesos.\n",
    "Con -1 y 1, al sumar, solo podíamos decidir agregar el valor o restarlo. Con la adición del 0, ahora podemos ignorar el valor:\n",
    "- 1: Quiero agregar este valor\n",
    "- 0: Quiero ignorar este valor\n",
    "- -1: Quiero restar este valor\n",
    "\n",
    "De esta manera, podemos filtrar eficazmente los valores, lo que permite una mejor representación.\n",
    "\n",
    "Para realizar la cuantización en 1.58 bits, usamos la cuantización *absmean* que es una variante de *absmax*. En lugar de basarnos en el máximo, nos basamos en el promedio en valor absoluto $\\alpha$ y luego redondeamos los valores a -1, 0 o 1:\n",
    "\n",
    "![BitNet 1.58](./images/bitnet158.png)\n",
    "\n",
    "Y eso es todo, son simplemente estas dos técnicas (representación ternaria y cuantización *absmean*) las que permiten mejorar drásticamente el método BitNet clásico y proponer modelos extremadamente cuantizados y aún potentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning de los modelos de lenguaje\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando calculamos la VRAM necesaria para un modelo, solo miramos para la inferencia. Si queremos entrenar el modelo, la VRAM necesaria es mucho mayor y dependerá del optimizador que usemos (ver [curso sobre optimizadores](../Bonus_CursosEspecíficos/05_Optimizador.ipynb)). Entonces podemos imaginar que los LLM necesitan una enorme cantidad de memoria para ser entrenados o *fine-tunados*.\n",
    "\n",
    "Para reducir esta necesidad de memoria, se han propuesto métodos de *fine-tuning* eficiente en parámetros (PEFT) y permiten reentrenar solo una parte del modelo. Además de permitir *fine-tunear* los modelos, esto también tiene el efecto de evitar el *olvido catastrófico* porque solo entrenamos una pequeña parte de los parámetros totales del modelo.\n",
    "\n",
    "Existen muchas métodos para el PEFT: LoRA, *Adapter*, *Prefix Tuning*, *Prompt Tuning*, QLoRA, etc.\n",
    "\n",
    "La idea con los métodos de tipo *Adapter*, LoRA y QLoRA es agregar una capa entrenable que permite adaptar el valor de los pesos (sin necesidad de reentrenar las capas base del modelo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método [LoRA (adaptación de rango bajo de grandes modelos de lenguaje)](https://arxiv.org/pdf/2106.09685) es una técnica de *fine-tuning* que permite adaptar un LLM a una tarea o dominio específico. Este método introduce matrices entrenables de descomposición en rango en cada capa del transformer, lo que reduce los parámetros entrenables del modelo porque las capas base están *congeladas*. El método puede potencialmente disminuir el número de parámetros entrenables en un factor de 10,000 mientras reduce la VRAM necesaria para el entrenamiento en un factor de hasta 3. El rendimiento de los modelos *fine-tunados* con este método es equivalente o mejor que los modelos *fine-tunados* de manera clásica en muchas tareas.\n",
    "\n",
    "![LoRA](./images/LoRA.webp)\n",
    "\n",
    "En lugar de modificar la matriz $W$ de una capa, el método LoRA agrega dos nuevas matrices $A$ y $B$ cuyo producto representa las modificaciones a aplicar a la matriz $W$.\n",
    "$Y=W+AB$\n",
    "Si $W$ es de tamaño $m \\times n$, entonces $A$ es de tamaño $m \\times r$ y $B$ de tamaño $r \\times n$, donde $r$ es el rango que es mucho más pequeño que $m$ o $n$ (lo que explica la disminución del número de parámetros). Durante el entrenamiento, solo $A$ y $B$ se modifican, lo que permite que el modelo aprenda la tarea específica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLoRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QLoRA es una versión mejorada de LoRA que permite agregar la cuantización de 4 bits para los parámetros del modelo preentrenado. Como hemos visto anteriormente, la cuantización permite reducir drásticamente la memoria necesaria para ejecutar el modelo. Al combinar LoRA y la cuantización, ahora podemos imaginar entrenar un LLM en una simple GPU de consumo, lo que parecía imposible hace solo unos años.\n",
    "\n",
    "**Nota**: QLoRA cuantiza los pesos en *Normal Float* 4 (NF4), que es un método de cuantización específico para los modelos de deep learning. Para obtener más información, puede consultar este [video](https://www.youtube.com/watch?v=TPcXVJ1VSRI&t=563s) en el tiempo indicado. El NF4 está diseñado específicamente para representar distribuciones gaussianas (y las redes neuronales se supone que tienen pesos que siguen una distribución gaussiana).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QLoRA es una versión mejorada de LoRA que permite agregar la cuantización de 4 bits para los parámetros del modelo preentrenado. Como hemos visto anteriormente, la cuantización permite reducir drásticamente la memoria necesaria para ejecutar el modelo. Al combinar LoRA y la cuantización, ahora podemos imaginar entrenar un LLM en una simple GPU de consumo, lo que parecía imposible hace solo unos años.\n",
    "\n",
    "**Nota**: QLoRA cuantiza los pesos en *Normal Float* 4 (NF4), que es un método de cuantización específico para los modelos de deep learning. Para obtener más información, puede consultar este [video](https://www.youtube.com/watch?v=TPcXVJ1VSRI&t=563s) en el tiempo indicado. El NF4 está diseñado específicamente para representar distribuciones gaussianas (y las redes neuronales se supone que tienen pesos que siguen una distribución gaussiana)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
