{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por transferencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **aprendizaje por transferencia** es una técnica común en *deep learning*. Consiste en reutilizar los pesos de una red preentrenada como base para entrenar un nuevo modelo.\n",
    "\n",
    "**Ventajas principales:**\n",
    "- El entrenamiento es más rápido si las tareas son similares.\n",
    "- El rendimiento es mejor que el de un modelo entrenado desde cero.\n",
    "- Se necesitan menos datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aprendizaje_por_transferencia](./images/transferlearning.png)\n",
    "\n",
    "*Imagen extraída de este [artículo](https://www.techtarget.com/searchcio/definition/transfer-learning).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Aprendizaje por transferencia o *fine-tuning*?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos dos términos suelen confundirse porque están estrechamente relacionados. En la práctica, el *fine-tuning* es una forma de aprendizaje por transferencia que consiste en volver a entrenar solo una parte de las capas del modelo reutilizado.\n",
    "\n",
    "**Definiciones claras:**\n",
    "- **Aprendizaje por transferencia**: Entrenar un modelo utilizando los pesos de un modelo preentrenado en otra tarea (se puede reentrenar todo el modelo o solo ciertas capas).\n",
    "- **Fine-tuning**: Reentrenar ciertas capas (generalmente las últimas) de un modelo preentrenado para adaptarlo a una nueva tarea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo aplicar el *fine-tuning*?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El *fine-tuning* consiste en reentrenar ciertas capas de un modelo preentrenado para adaptarlo a una nueva tarea. Por lo tanto, es necesario elegir cuántas capas reentrenar.\n",
    "\n",
    "**¿Cómo seleccionar el número de capas?**\n",
    "No existe una fórmula fija. Generalmente, se basa en la intuición y en estas reglas:\n",
    "- **Menos datos disponibles** → reentrenar menos capas (pocos datos: solo la última capa; muchos datos: casi todas las capas).\n",
    "- **Tareas más similares** → reentrenar menos capas.\n",
    "  *Ejemplo 1*: Detectar hámsters además de gatos, perros y conejos (tareas similares).\n",
    "  *Ejemplo 2*: Detectar enfermedades a partir de un modelo entrenado en gatos/perros/conejos (tareas muy diferentes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuándo usar aprendizaje por transferencia o *fine-tuning*?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, **usar un modelo preentrenado como base siempre es beneficioso** (a menos que los dominios sean muy distintos). Se recomienda aplicarlo siempre que sea posible.\n",
    "\n",
    "**Limitaciones a considerar:**\n",
    "- La arquitectura del modelo **no puede modificarse libremente** (especialmente en las capas no reentrenadas).\n",
    "- Se necesitan los pesos de un modelo preentrenado (hay muchos disponibles en línea; ver [curso 6 sobre HuggingFace](../06_HuggingFace/README.md)).\n",
    "\n",
    "**Nota**: Para clasificación de imágenes, es común usar modelos preentrenados en [ImageNet](https://www.image-net.org/), ya que sus $1000$ clases lo hacen bastante generalista.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo preparar el *dataset* de entrenamiento?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al aplicar *fine-tuning*, pueden plantearse **dos objetivos distintos**:\n",
    "\n",
    "- **Caso 1**: Entrenar el modelo para una tarea **totalmente diferente** a la original.\n",
    "  *Ejemplo*: Clasificar dinosaurios cuando el modelo estaba entrenado en mamíferos.\n",
    "  → El modelo puede *\"olvidar\"* su tarea original **sin consecuencias**.\n",
    "\n",
    "- **Caso 2**: Entrenar el modelo para una tarea **complementaria**, manteniendo el rendimiento en la original.\n",
    "  *Ejemplo*: Detectar aves **sin perder precisión** en mamíferos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La composición del *dataset* depende del caso:**\n",
    "\n",
    "- **Caso 1**: Solo incluye las **nuevas imágenes** a clasificar (ej.: únicamente dinosaurios).\n",
    "- **Caso 2**: Combina datos **antiguos y nuevos** (ej.: $50\\%$ mamíferos + $50\\%$ aves) para mantener el rendimiento en ambas tareas.\n",
    "  *Nota*: La proporción puede ajustarse según necesidades.\n",
    "\n",
    "> **Importante sobre *open-source* real**:\n",
    "> Para un *fine-tuning* efectivo, un modelo debe proporcionar:\n",
    "> 1. Código,\n",
    "> 2. Pesos preentrenados,\n",
    "> 3. Datos de entrenamiento.\n",
    ">\n",
    "> Esto es **especialmente crítico** en modelos de lenguaje grandes (*LLM*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos base (*foundation models*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los **modelos base** se entrenan con grandes volúmenes de datos (a menudo sin etiquetar) y sirven como punto de partida para aplicar *fine-tuning* o aprendizaje por transferencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelos base por dominio:**\n",
    "\n",
    "- **Procesamiento de Lenguaje Natural (NLP)**:\n",
    "  Ejemplos: GPT, BLOOM, Llama, Gemini.\n",
    "  *Aplicación*: Se ajustan para tareas específicas. Por ejemplo, **ChatGPT** es una versión con *fine-tuning* de GPT optimizada para conversaciones tipo *chatbot*.\n",
    "\n",
    "- **Imágenes**:\n",
    "  El concepto es menos claro que en NLP, pero destacan modelos como ViT, DINO o CLIP.\n",
    "\n",
    "- **Audio**:\n",
    "  Un ejemplo es el modelo **CLAP** (*Contrastive Language–Audio Pretraining*).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
