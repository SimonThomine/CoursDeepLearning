{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation de la couche de convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, nous allons implémenter une couche de convolution 1D. Dans le cours expliquant les couches de convolutions, nous avons parlé exclusivement de la convolution 2D car c'est cette couche qui est la plus utilisée (de loin). Cependant, pour bien comprendre le code et l'ingéniosité des couches de convolution, il est essentiel de commencer par la convolution 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution 1D, comment ça marche ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manière assez évidente, la convolution 1D est similaire à la convolution 2D mais sur une seule dimension. C'est lors de l'application de la couche de convolution 1D que l'on voit très rapidement pourquoi une couche de convolution est en fait une boucle *for*.   \n",
    "\n",
    "<img src=\"images/conv1d.png\" alt=\"conv1d\" width=\"800\"/>\n",
    "\n",
    "Figure extraite du [forum](https://ai.stackexchange.com/questions/28767/what-does-channel-mean-in-the-case-of-an-1d-convolution).\n",
    "\n",
    "Les paramètres classiques de la convolution 2D s'applique également : \n",
    "- Le *padding* va ajouter des valeurs au bord du vecteur 1D (début et fin)\n",
    "- Le *stride* définit le pas \n",
    "- La *kernel_size* définit la taille du filtre\n",
    "- Les *in_channels* et *out_channels* qui correspondent aux nombres de channels d'entrée et de sortie.\n",
    "- etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant à l'implémentation de la couche de convolution 1D.\n",
    "\n",
    "**Note** : Sur pytorch, les couches de convolutions ne sont pas implémentées en python mais en C++ pour une plus grande rapidité de calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans une couche de convolution, on a une dimension de filtre *kernel_size* et un nombre de filtre *out_channels*. L'idée va être de boucler sur l'ensemble de notre vecteur et de calculer les valeurs de sortie en appliquant chaque filtre à chaque position possible du vecteur d'entrée. A chaque position, on va appliquer une couche fully connected prenant en entrée les éléments contenus dans le filtre de taille $(KernelSize \\times InChannels)$ et renvoyant $OutChannels$ éléments. C'est comme si on appliquait une boucle *for* sur chaque position de notre séquence d'entrée.\n",
    "\n",
    "**Note** : Sur la figure explicative, il n'y a qu'une seule dimension de channel mais en réalité il y en a plusieurs la plupart du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3\n",
    "out_channels = 16\n",
    "kernel_size = 3\n",
    "kernel=nn.Linear(in_channels*kernel_size, out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, il faut appliquer cette couche de convolution sur l'ensemble des éléments de notre séquence avec un pas *stride*. On peut également ajouter du *padding* si on veut que la séquence d'entrée soit de la même taille que la séquence de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de l'entrée:  torch.Size([8, 3, 100])\n",
      "Dimension de la sortie:  torch.Size([8, 16, 100])\n"
     ]
    }
   ],
   "source": [
    "# Imaginons une séquence de 100 éléments, avec 3 canaux et un batch de 8\n",
    "dummy_input = torch.randn(8, in_channels, 100)\n",
    "print(\"Dimension de l'entrée: \",dummy_input.shape)\n",
    "stride=1\n",
    "padding=1\n",
    "outs=[]\n",
    "\n",
    "# On pad les deux côtés de l'entrée pour éviter les problèmes de dimensions\n",
    "dummy_input=F.pad(dummy_input, (padding, padding))\n",
    "\n",
    "for i in range(kernel_size,dummy_input.shape[2]+1,stride):\n",
    "  chunk=dummy_input[:,:,i-kernel_size:i]\n",
    "  # On redimensionne pour la couche fully connected\n",
    "  chunk=chunk.reshape(dummy_input.shape[0],-1)\n",
    "  # On applique la couche fully connected\n",
    "  out=kernel(chunk)\n",
    "  # On ajoute à la liste des sorties\n",
    "  outs.append(out)\n",
    "# On convertit la liste en un tenseur\n",
    "outs=torch.stack(outs, dim=2)\n",
    "print(\"Dimension de la sortie: \",outs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les convolutions 2D, on peut choisir de diminuer la taille de la séquence (ou des *feature map* si on parle d'une conv 2D). Pour cela, on peut utiliser un stride supérieur à 1 ou bien une couche de pooling.   \n",
    "\n",
    "En pratique, l'utilisation du stride est souvent préférée mais nous allons implémenter le *max pooling* pour bien comprendre son fonctionnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de la sortie après pooling:  torch.Size([8, 16, 50])\n"
     ]
    }
   ],
   "source": [
    "pooling=2\n",
    "outs2=[]\n",
    "for i in range(pooling,outs.shape[2]+1,pooling):\n",
    "  # On prend les éléments entre i-pooling et i\n",
    "  chunk=outs[:,:,i-pooling:i]\n",
    "  # On prend le max sur la dimension 2, pour le average pooling on aurait utilisé torch.mean\n",
    "  out2=torch.max(chunk, dim=2)[0]\n",
    "  outs2.append(out2)\n",
    "# On convertit la liste en un tenseur \n",
    "outs2=torch.stack(outs2, dim=2)\n",
    "print(\"Dimension de la sortie après pooling: \",outs2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons compris comment la convolution 1D et le *maxpooling* fonctionnent, on va créer des classes pour faciliter leurs utilisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, kernel_size, padding):\n",
    "    super(Conv1D, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.kernel_width = kernel_size\n",
    "    self.kernel = nn.Linear(kernel_size * in_channels, out_channels)\n",
    "    self.padding=padding\n",
    "\n",
    "  def forward(self, x):\n",
    "    x=F.pad(x, (self.padding, self.padding))\n",
    "    # Boucle en une seule ligne pour un code plus concis\n",
    "    l = [self.kernel(x[:, :, i - self.kernel_width: i].reshape(x.shape[0], self.in_channels * self.kernel_width)) for i in range(self.kernel_width, x.shape[2]+1, self.stride)]\n",
    "    return torch.stack(l, dim=2)\n",
    "\n",
    "\n",
    "class MaxPool1D(nn.Module):\n",
    "  def __init__(self, pooling):\n",
    "    super(MaxPool1D, self).__init__()\n",
    "    self.pooling = pooling\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # Boucle en une seule ligne pour un code plus concis\n",
    "    l = [torch.max(x[:, :, i - self.pooling: i], dim=2)[0] for i in range(self.pooling, x.shape[2]+1, self.pooling)]\n",
    "    return torch.stack(l, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas pratique : MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que l'on a implémenté nos couches de convolutions et de pooling, nous allons les tester sur MNIST. Sur MNIST, on traite des images, il est donc plus logique d'utiliser des convolutions 2D en pratique (cf cours suivant).   \n",
    "Ici, nous allons juste vérifier que notre implémentation des convolutions fonctionne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=T.ToTensor() # Pour convertir les éléments en tensor torch directement\n",
    "dataset = datasets.MNIST(root='./../data', train=True, download=True,transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./../data', train=False,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chiffre sur l'image est un 0\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(dataset[0][0].permute(1,2,0).numpy(), cmap='gray')\n",
    "plt.show()\n",
    "print(\"Le chiffre sur l'image est un \"+str(dataset[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset=torch.utils.data.random_split(dataset, [0.8,0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader= DataLoader(validation_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant créer notre modèle. Notons que l'on utilise un *stride* de 2 et pas de *maxpooling* pour gagner du temps sur le traitement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n",
      "Nombre de paramètres 33370\n"
     ]
    }
   ],
   "source": [
    "class cnn1d(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.conv1=Conv1D(1,8,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 8 filtres\n",
    "    self.conv2=Conv1D(8,16,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 16 filtres\n",
    "    self.conv3=Conv1D(16,32,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 32 filtres\n",
    "    self.fc=nn.Linear(3136,10)\n",
    "  \n",
    "  # La fonction forward est la fonction appelée lorsqu'on fait model(x)\n",
    "  def forward(self,x):\n",
    "    x=F.relu(self.conv1(x))\n",
    "    x=F.relu(self.conv2(x))\n",
    "    x=F.relu(self.conv3(x))\n",
    "    x=x.view(-1,x.shape[1]*x.shape[2]) # Pour convertir la feature map de taille CxL en vecteur 1D (avec une dimension batch)\n",
    "    output=self.fc(x)\n",
    "    return output\n",
    "dummy_input=torch.randn(8,1,784)\n",
    "model=cnn1d()\n",
    "output=model(dummy_input)\n",
    "print(output.shape)\n",
    "\n",
    "print(\"Nombre de paramètres\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle a presque 10 fois moins de paramètres que notre modèle fully connected du cours précédent ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs=5\n",
    "learning_rate=0.001\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 train loss 0.4011246860027313\n",
      "step 0 val loss 0.2103319615125656\n",
      "step 1 train loss 0.17427290976047516\n",
      "step 1 val loss 0.1769915670156479\n",
      "step 2 train loss 0.14464063942432404\n",
      "step 2 val loss 0.14992524683475494\n",
      "step 3 train loss 0.12802869081497192\n",
      "step 3 val loss 0.13225941359996796\n",
      "step 4 train loss 0.11609579622745514\n",
      "step 4 val loss 0.12663421034812927\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "  loss_train=0\n",
    "  for images, labels in train_loader:\n",
    "    images=images.view(images.shape[0],1,784)\n",
    "    preds=model(images)\n",
    "    loss=criterion(preds,labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_train+=loss   \n",
    "  if i % 1 == 0:\n",
    "    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n",
    "  loss_val=0    \n",
    "  for images, labels in val_loader:\n",
    "    with torch.no_grad(): # permet de ne pas calculer les gradients\n",
    "      images=images.view(images.shape[0],1,784)\n",
    "      preds=model(images)\n",
    "      loss=criterion(preds,labels)\n",
    "      loss_val+=loss \n",
    "  if i % 1 == 0:\n",
    "    print(f\"step {i} val loss {loss_val/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle en phase de test :  96.35\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images,labels in test_loader:\n",
    "  images=images.view(images.shape[0],1,784) \n",
    "  with torch.no_grad():\n",
    "    preds=model(images)\n",
    "    \n",
    "    _, predicted = torch.max(preds.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()     \n",
    "test_acc = 100 * correct / total\n",
    "print(\"Précision du modèle en phase de test : \",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une très bonne précision bien que légérement inférieure à celle obtenue avec le réseau fully connected du cours précédent. \n",
    "\n",
    "**Note** : L'entraînement était assez lent, c'est à cause de notre implémentation qui n'est pas très efficace, l'implémentation pytorch en C++ est beaucoup plus performante.\n",
    "\n",
    "**Note 2** : Nous avons utilisé des convolutions 1D pour traîter des images ce qui n'est pas optimal. L'idéal est d'utiliser des convolutions 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut implémenter la convolution 2D en suivant le même principe mais en deux dimensions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3\n",
    "out_channels = 16\n",
    "kernel_size = 3\n",
    "# On a un kernel de taille 3x3 car on est en 2D\n",
    "kernel=nn.Linear(in_channels*kernel_size**2, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de l'entrée:  torch.Size([8, 3, 10, 10])\n",
      "Dimension de l'entrée après padding:  torch.Size([8, 3, 12, 12])\n",
      "Dimension de la sortie:  torch.Size([8, 16, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Pour une image de taille 10x10 avec 3 canaux et un batch de 8\n",
    "dummy_input = torch.randn(8, in_channels, 10,10)\n",
    "b, c, h, w = dummy_input.shape\n",
    "print(\"Dimension de l'entrée: \",dummy_input.shape)\n",
    "stride=1\n",
    "padding=1\n",
    "outs=[]\n",
    "\n",
    "# Le padding change pour une image 2D, on doit pad en hauteur et en largeur\n",
    "dummy_input=F.pad(dummy_input, (padding, padding,padding,padding))\n",
    "print(\"Dimension de l'entrée après padding: \",dummy_input.shape)\n",
    "\n",
    "# On boucle sur les dimensions de l'image : W x H \n",
    "for i in range(kernel_size,dummy_input.shape[2]+1,stride):\n",
    "  for j in range(kernel_size,dummy_input.shape[3]+1,stride):\n",
    "    chunk=dummy_input[:,:,i-kernel_size:i,j-kernel_size:j]\n",
    "    # On redimensionne pour la couche fully connected\n",
    "    chunk=chunk.reshape(dummy_input.shape[0],-1)\n",
    "    # On applique la couche fully connected\n",
    "    out=kernel(chunk)\n",
    "    # On ajoute à la liste des sorties\n",
    "    outs.append(out)\n",
    "# On convertit la liste en un tenseur\n",
    "outs=torch.stack(outs, dim=2)\n",
    "outs=outs.reshape(b,out_channels,h, w)\n",
    "print(\"Dimension de la sortie: \",outs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut le faire en classe maintenant comme pour la conv1D : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "class Conv2D(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, kernel_size, padding):\n",
    "    super(Conv2D, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.kernel_width = kernel_size\n",
    "    self.kernel = nn.Linear(in_channels*kernel_size**2 , out_channels)\n",
    "    self.padding=padding\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, c, h, w = x.shape\n",
    "    x=F.pad(x, (self.padding, self.padding,self.padding,self.padding))\n",
    "    # Sur une seule ligne, c'est absolument illisible, on garde la boucle\n",
    "    l=[]\n",
    "    for i in range(self.kernel_width, x.shape[2]+1, self.stride):\n",
    "      for j in range(self.kernel_width, x.shape[3]+1, self.stride):\n",
    "        chunk=self.kernel(x[:,:,i-self.kernel_width:i,j-self.kernel_width:j].reshape(x.shape[0],-1))\n",
    "        l.append(chunk)\n",
    "    # La version en une ligne, pour les curieux\n",
    "    #l = [self.kernel(x[:, :, i - self.kernel_width: i, j - self.kernel_width: j].reshape(x.shape[0], ,-1)) for i in range(self.kernel_width, x.shape[2]+1, self.stride) for j in range(self.kernel_width, x.shape[3]+1, self.stride)]\n",
    "    outs=torch.stack(l, dim=2)\n",
    "    return outs.reshape(b,self.out_channels,h//self.stride, w//self.stride)\n",
    "\n",
    "dummy_input=torch.randn(8,3,32,32)\n",
    "model=Conv2D(3,16,stride=2,kernel_size=3,padding=1)\n",
    "output=model(dummy_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant créer notre modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n",
      "Nombre de paramètres 21578\n"
     ]
    }
   ],
   "source": [
    "class cnn2d(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.conv1=Conv2D(1,8,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 8 filtres\n",
    "    self.conv2=Conv2D(8,16,kernel_size=3,stride=2,padding=1) # Couche de convolution 1D de 16 filtres\n",
    "    self.conv3=Conv2D(16,32,kernel_size=3,stride=1,padding=1) # Couche de convolution 1D de 32 filtres\n",
    "    self.fc=nn.Linear(1568,10)\n",
    "  \n",
    "  # La fonction forward est la fonction appelée lorsqu'on fait model(x)\n",
    "  def forward(self,x):\n",
    "    x=F.relu(self.conv1(x))\n",
    "    x=F.relu(self.conv2(x))\n",
    "    x=F.relu(self.conv3(x))\n",
    "    x=x.view(-1,x.shape[1]*x.shape[2]*x.shape[3]) # Pour convertir la feature map de taille CxL en vecteur 1D (avec une dimension batch)\n",
    "    output=self.fc(x)\n",
    "    return output\n",
    "dummy_input=torch.randn(8,1,28,28)\n",
    "model=cnn2d()\n",
    "output=model(dummy_input)\n",
    "print(output.shape)\n",
    "\n",
    "print(\"Nombre de paramètres\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant entrainer notre modèle sur MNIST et regarder si on a des meilleurs résultats qu'avec les convolutions 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs=5\n",
    "learning_rate=0.001\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 train loss 0.36240848898887634\n",
      "step 0 val loss 0.14743468165397644\n",
      "step 1 train loss 0.1063414067029953\n",
      "step 1 val loss 0.1019362062215805\n",
      "step 2 train loss 0.07034476101398468\n",
      "step 2 val loss 0.08669546991586685\n",
      "step 3 train loss 0.05517915263772011\n",
      "step 3 val loss 0.07208992540836334\n",
      "step 4 train loss 0.04452721029520035\n",
      "step 4 val loss 0.0664198026061058\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "  loss_train=0\n",
    "  for images, labels in train_loader:\n",
    "    preds=model(images)\n",
    "    loss=criterion(preds,labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_train+=loss   \n",
    "  if i % 1 == 0:\n",
    "    print(f\"step {i} train loss {loss_train/len(train_loader)}\")\n",
    "  loss_val=0    \n",
    "  for images, labels in val_loader:\n",
    "    with torch.no_grad(): # permet de ne pas calculer les gradients\n",
    "      preds=model(images)\n",
    "      loss=criterion(preds,labels)\n",
    "      loss_val+=loss \n",
    "  if i % 1 == 0:\n",
    "    print(f\"step {i} val loss {loss_val/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En terme de loss, on est descendu plus bas que pour notre modèle avec les convolutions 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle en phase de test :  98.23\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images,labels in test_loader:\n",
    "  with torch.no_grad():\n",
    "    preds=model(images)\n",
    "    _, predicted = torch.max(preds.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()     \n",
    "test_acc = 100 * correct / total\n",
    "print(\"Précision du modèle en phase de test : \",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La précision est très bonne ! C'est mieux que ce que nous avions avec les [réseaux fully connected](../02_RéseauFullyConnected/03_TechniquesAvancées.ipynb) alors que l'on a 10 fois moins de paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : De la même manière, on peut implémenter des convolutions 3D qui peuvent être utilisées pour le traitement des vidéos (l'axe temporel est rajouté)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
